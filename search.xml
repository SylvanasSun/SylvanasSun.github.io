<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Map大家族的那点事儿]]></title>
    <url>%2F2018%2F03%2F16%2F2018-03-16-map_family%2F</url>
    <content type="text"><![CDATA[Map Map是一种用于快速查找的数据结构，它以键值对的形式存储数据，每一个键都是唯一的，且对应着一个值，如果想要查找Map中的数据，只需要传入一个键，Map会对键进行匹配并返回键所对应的值，可以说Map其实就是一个存放键值对的集合。Map被各种编程语言广泛使用，只不过在名称上可能会有些混淆，像Python中叫做字典（Dictionary），也有些语言称其为关联数组（Associative Array），但其实它们都是一样的，都是一个存放键值对的集合。至于Java中经常用到的HashMap也是Map的一种，它被称为散列表，关于散列表的细节我会在本文中解释HashMap的源码时提及。 Java还提供了一种与Map密切相关的数据结构：Set，它是数学意义上的集合，特性如下： 无序性：一个集合中，每个元素的地位都是相同的，元素之间也都是无序的。不过Java中也提供了有序的Set，这点倒是没有完全遵循。 互异性：一个集合中，任何两个元素都是不相同的。 确定性：给定一个集合以及其任一元素，该元素属于或者不属于该集合是必须可以确定的。 很明显，Map中的key就很符合这些特性，Set的实现其实就是在内部使用Map。例如，HashSet就定义了一个类型为HashMap的成员变量，向HashSet添加元素a，等同于向它内部的HashMap添加了一个key为a，value为一个Object对象的键值对，这个Object对象是HashSet的一个常量，它是一个虚拟值，没有什么实际含义，源码如下： 12345678private transient HashMap&lt;E,Object&gt; map;// Dummy value to associate with an Object in the backing Mapprivate static final Object PRESENT = new Object();public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125; 小插曲过后，让我们接着说Map，它是JDK的一个顶级接口，提供了三种集合视图（Collection Views）：包含所有key的集合、包含所有value的集合以及包含所有键值对的集合，Map中的元素顺序与它所返回的集合视图中的元素的迭代顺序相关，也就是说，Map本身是不保证有序性的，当然也有例外，比如TreeMap就对有序性做出了保证，这主要因为它是基于红黑树实现的。 所谓的集合视图就是由集合本身提供的一种访问数据的方式，同时对视图的任何修改也会影响到集合。好比Map.keySet()返回了它包含的key的集合，如果你调用了Map.remove(key)那么keySet.contains(key)也将返回false，再比如说Arrays.asList(T)可以把一个数组封装成一个List，这样你就可以通过List的API来访问和操作这些数据，如下列示例代码： 1234567String[] strings = &#123;"a", "b", "c"&#125;;List&lt;String&gt; list = Arrays.asList(strings);System.out.println(list.get(0)); // "a"strings[0] = "d";System.out.println(list.get(0)); // "d"list.set(0, "e");System.out.println(strings[0]); // "e" 是不是感觉很神奇，其实Arrays.asList()只是将传入的数组与Arrays中的一个内部类ArrayList（注意，它与java.util包下的ArrayList不是同一个）做了一个”绑定“，在调用get()时会直接根据下标返回数组中的元素，而调用set()时也会直接修改数组中对应下标的元素。相对于直接复制来说，集合视图的优点是内存利用率更高，假设你有一个数组，又很想使用List的API来操作它，那么你不用new一个ArrayList以拷贝数组中的元素，只需要一点额外的内存（通过Arrays.ArrayList对数组进行封装），原始数据依然是在数组中的，并不会复制成多份。 Map接口规范了Map数据结构的通用API（也含有几个用于简化操作的default方法，default是JDK8的新特性，它是接口中声明的方法的默认实现，即非抽象方法）并且还在内部定义了Entry接口（键值对的实体类），在JDK中提供的所有Map数据结构都实现了Map接口，下面为Map接口的源码（代码中的注释太长了，基本都是些实现的规范，为了篇幅我就尽量省略了）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330package java.util;import java.util.function.BiConsumer;import java.util.function.BiFunction;import java.util.function.Function;import java.io.Serializable;public interface Map&lt;K,V&gt; &#123; // 查询操作 /** * 返回这个Map中所包含的键值对的数量，如果大于Integer.MAX_VALUE， * 则应该返回Integer.MAX_VALUE。 */ int size(); /** * Map是否为空。 */ boolean isEmpty(); /** * Map中是否包含key，如果是返回true，否则false。 */ boolean containsKey(Object key); /** * Map中是否包含value，如果是返回true，否则false。 */ boolean containsValue(Object value); /** * 根据key查找value，如果Map不包含该key，则返回null。 */ V get(Object key); // 修改操作 /** * 添加一对键值对，如果Map中已含有这个key，那么新value将覆盖掉旧value， * 并返回旧value，如果Map中之前没有这个key，那么返回null。 */ V put(K key, V value); /** * 删除指定key并返回之前的value，如果Map中没有该key，则返回null。 */ V remove(Object key); // 批量操作 /** * 将指定Map中的所有键值对批量添加到当前Map。 */ void putAll(Map&lt;? extends K, ? extends V&gt; m); /** * 删除Map中所有的键值对。 */ void clear(); // 集合视图 /** * 返回包含Map中所有key的Set，对该视图的所有修改操作会对Map产生同样的影响，反之亦然。 */ Set&lt;K&gt; keySet(); /** * 返回包含Map中所有value的集合，对该视图的所有修改操作会对Map产生同样的影响，反之亦然。 */ Collection&lt;V&gt; values(); /** * 返回包含Map中所有键值对的Set，对该视图的所有修改操作会对Map产生同样的影响，反之亦然。 */ Set&lt;Map.Entry&lt;K, V&gt;&gt; entrySet(); /** * Entry代表一对键值对，规范了一些基本函数以及几个已实现的类函数（各种比较器）。 */ interface Entry&lt;K,V&gt; &#123; K getKey(); V getValue(); V setValue(V value); boolean equals(Object o); int hashCode(); public static &lt;K extends Comparable&lt;? super K&gt;, V&gt; Comparator&lt;Map.Entry&lt;K,V&gt;&gt; comparingByKey() &#123; return (Comparator&lt;Map.Entry&lt;K, V&gt;&gt; &amp; Serializable) (c1, c2) -&gt; c1.getKey().compareTo(c2.getKey()); &#125; public static &lt;K, V extends Comparable&lt;? super V&gt;&gt; Comparator&lt;Map.Entry&lt;K,V&gt;&gt; comparingByValue() &#123; return (Comparator&lt;Map.Entry&lt;K, V&gt;&gt; &amp; Serializable) (c1, c2) -&gt; c1.getValue().compareTo(c2.getValue()); &#125; public static &lt;K, V&gt; Comparator&lt;Map.Entry&lt;K, V&gt;&gt; comparingByKey(Comparator&lt;? super K&gt; cmp) &#123; Objects.requireNonNull(cmp); return (Comparator&lt;Map.Entry&lt;K, V&gt;&gt; &amp; Serializable) (c1, c2) -&gt; cmp.compare(c1.getKey(), c2.getKey()); &#125; public static &lt;K, V&gt; Comparator&lt;Map.Entry&lt;K, V&gt;&gt; comparingByValue(Comparator&lt;? super V&gt; cmp) &#123; Objects.requireNonNull(cmp); return (Comparator&lt;Map.Entry&lt;K, V&gt;&gt; &amp; Serializable) (c1, c2) -&gt; cmp.compare(c1.getValue(), c2.getValue()); &#125; &#125; // 比较和hashing /** * 将指定的对象与此Map进行比较是否相等。 */ boolean equals(Object o); /** * 返回此Map的hash code。 */ int hashCode(); // 默认方法（非抽象方法） /** * 根据key查找value，如果该key不存在或等于null则返回defaultValue。 */ default V getOrDefault(Object key, V defaultValue) &#123; V v; return (((v = get(key)) != null) || containsKey(key)) ? v : defaultValue; &#125; /** * 遍历Map并对每个键值对执行指定的操作（action）。 * BiConsumer是一个函数接口（具有一个抽象方法的接口，用于支持Lambda）， * 它代表了一个接受两个输入参数的操作，且不返回任何结果。 * 至于它奇怪的名字，根据Java中的其他函数接口的命名规范，Bi应该是Binary的缩写，意思是二元的。 */ default void forEach(BiConsumer&lt;? super K, ? super V&gt; action) &#123; Objects.requireNonNull(action); for (Map.Entry&lt;K, V&gt; entry : entrySet()) &#123; K k; V v; try &#123; k = entry.getKey(); v = entry.getValue(); &#125; catch(IllegalStateException ise) &#123; // this usually means the entry is no longer in the map. throw new ConcurrentModificationException(ise); &#125; action.accept(k, v); &#125; &#125; /** * 遍历Map，然后调用传入的函数function生成新value对旧value进行替换。 * BiFunction同样是一个函数接口，它接受两个输入参数并且返回一个结果。 */ default void replaceAll(BiFunction&lt;? super K, ? super V, ? extends V&gt; function) &#123; Objects.requireNonNull(function); for (Map.Entry&lt;K, V&gt; entry : entrySet()) &#123; K k; V v; try &#123; k = entry.getKey(); v = entry.getValue(); &#125; catch(IllegalStateException ise) &#123; // this usually means the entry is no longer in the map. throw new ConcurrentModificationException(ise); &#125; // ise thrown from function is not a cme. v = function.apply(k, v); try &#123; entry.setValue(v); &#125; catch(IllegalStateException ise) &#123; // this usually means the entry is no longer in the map. throw new ConcurrentModificationException(ise); &#125; &#125; &#125; /** * 如果指定的key不存在或者关联的value为null，则添加键值对。 */ default V putIfAbsent(K key, V value) &#123; V v = get(key); if (v == null) &#123; v = put(key, value); &#125; return v; &#125; /** * 当指定key关联的value与传入的参数value相等时删除该key。 */ default boolean remove(Object key, Object value) &#123; Object curValue = get(key); if (!Objects.equals(curValue, value) || (curValue == null &amp;&amp; !containsKey(key))) &#123; return false; &#125; remove(key); return true; &#125; /** * 当指定key关联的value与oldValue相等时，使用newValue进行替换。 */ default boolean replace(K key, V oldValue, V newValue) &#123; Object curValue = get(key); if (!Objects.equals(curValue, oldValue) || (curValue == null &amp;&amp; !containsKey(key))) &#123; return false; &#125; put(key, newValue); return true; &#125; /** * 当指定key关联到某个value时进行替换。 */ default V replace(K key, V value) &#123; V curValue; if (((curValue = get(key)) != null) || containsKey(key)) &#123; curValue = put(key, value); &#125; return curValue; &#125; /** * 当指定key没有关联到一个value或者value为null时，调用mappingFunction生成值并添加键值对到Map。 * Function是一个函数接口，它接受一个输入参数并返回一个结果，如果mappingFunction返回的结果 * 也为null，那么将不会调用put。 */ default V computeIfAbsent(K key, Function&lt;? super K, ? extends V&gt; mappingFunction) &#123; Objects.requireNonNull(mappingFunction); V v; if ((v = get(key)) == null) &#123; V newValue; if ((newValue = mappingFunction.apply(key)) != null) &#123; put(key, newValue); return newValue; &#125; &#125; return v; &#125; /** * 当指定key关联到一个value并且不为null时，调用remappingFunction生成newValue， * 如果newValue不为null，那么进行替换，否则删除该key。 */ default V computeIfPresent(K key, BiFunction&lt;? super K, ? super V, ? extends V&gt; remappingFunction) &#123; Objects.requireNonNull(remappingFunction); V oldValue; if ((oldValue = get(key)) != null) &#123; V newValue = remappingFunction.apply(key, oldValue); if (newValue != null) &#123; put(key, newValue); return newValue; &#125; else &#123; remove(key); return null; &#125; &#125; else &#123; return null; &#125; &#125; /** * remappingFunction根据key与其相关联的value生成newValue， * 当newValue等于null时删除该key，否则添加或者替换旧的映射。 */ default V compute(K key, BiFunction&lt;? super K, ? super V, ? extends V&gt; remappingFunction) &#123; Objects.requireNonNull(remappingFunction); V oldValue = get(key); V newValue = remappingFunction.apply(key, oldValue); if (newValue == null) &#123; // delete mapping if (oldValue != null || containsKey(key)) &#123; // something to remove remove(key); return null; &#125; else &#123; // nothing to do. Leave things as they were. return null; &#125; &#125; else &#123; // add or replace old mapping put(key, newValue); return newValue; &#125; &#125; /** * 当指定key没有关联到一个value或者value为null，将它与传入的参数value * 进行关联。否则，调用remappingFunction生成newValue并进行替换。 * 如果，newValue等于null，那么删除该key。 */ default V merge(K key, V value, BiFunction&lt;? super V, ? super V, ? extends V&gt; remappingFunction) &#123; Objects.requireNonNull(remappingFunction); Objects.requireNonNull(value); V oldValue = get(key); V newValue = (oldValue == null) ? value : remappingFunction.apply(oldValue, value); if(newValue == null) &#123; remove(key); &#125; else &#123; put(key, newValue); &#125; return newValue; &#125;&#125; 需要注意一点，这些default方法都是非线程安全的，任何保证线程安全的扩展类都必须重写这些方法，例如ConcurrentHashMap。 下图为Map的继承关系结构图，它也是本文接下来将要分析的Map实现类的大纲，这些实现类都是比较常用的，在JDK中Map的实现类有几十个，大部分都是我们用不到的，限于篇幅原因就不一一讲解了（本文包含许多源码与对实现细节的分析，建议读者抽出一段连续的空闲时间静下心来慢慢阅读）。 本文作者为SylvanasSun(sylvanas.sun@gmail.com)，首发于SylvanasSun’s Blog。原文链接：https://sylvanassun.github.io/2018/03/16/2018-03-16-map_family/（转载请务必保留本段声明，并且保留超链接。） AbstractMap AbstractMap是一个抽象类，它是Map接口的一个骨架实现，最小化实现了此接口提供的抽象函数。在Java的Collection框架中基本都遵循了这一规定，骨架实现在接口与实现类之间构建了一层抽象，其目的是为了复用一些比较通用的函数以及方便扩展，例如List接口拥有骨架实现AbstractList、Set接口拥有骨架实现AbstractSet等。 下面我们按照不同的操作类型来看看AbstractMap都实现了什么，首先是查询操作： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package java.util;import java.util.Map.Entry;public abstract class AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt; &#123; protected AbstractMap() &#123; &#125; // Query Operations public int size() &#123; return entrySet().size(); &#125; // 键值对的集合视图留给具体的实现类实现 public abstract Set&lt;Entry&lt;K,V&gt;&gt; entrySet(); public boolean isEmpty() &#123; return size() == 0; &#125; /** * 遍历entrySet，然后逐个进行比较。 */ public boolean containsValue(Object value) &#123; Iterator&lt;Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); if (value==null) &#123; while (i.hasNext()) &#123; Entry&lt;K,V&gt; e = i.next(); if (e.getValue()==null) return true; &#125; &#125; else &#123; while (i.hasNext()) &#123; Entry&lt;K,V&gt; e = i.next(); if (value.equals(e.getValue())) return true; &#125; &#125; return false; &#125; /** * 跟containsValue()同理，只不过比较的是key。 */ public boolean containsKey(Object key) &#123; Iterator&lt;Map.Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); if (key==null) &#123; while (i.hasNext()) &#123; Entry&lt;K,V&gt; e = i.next(); if (e.getKey()==null) return true; &#125; &#125; else &#123; while (i.hasNext()) &#123; Entry&lt;K,V&gt; e = i.next(); if (key.equals(e.getKey())) return true; &#125; &#125; return false; &#125; /** * 遍历entrySet，然后根据key取出关联的value。 */ public V get(Object key) &#123; Iterator&lt;Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); if (key==null) &#123; while (i.hasNext()) &#123; Entry&lt;K,V&gt; e = i.next(); if (e.getKey()==null) return e.getValue(); &#125; &#125; else &#123; while (i.hasNext()) &#123; Entry&lt;K,V&gt; e = i.next(); if (key.equals(e.getKey())) return e.getValue(); &#125; &#125; return null; &#125;&#125; 可以发现这些操作都是依赖于函数entrySet()的，它返回了一个键值对的集合视图，由于不同的实现子类的Entry实现可能也是不同的，所以一般是在内部实现一个继承于AbstractSet且泛型为Map.Entry的内部类作为EntrySet，接下来是修改操作与批量操作： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// Modification Operations/** * 没有提供实现，子类必须重写该方法，否则调用put()会抛出异常。 */public V put(K key, V value) &#123; throw new UnsupportedOperationException();&#125;/** * 遍历entrySet，先找到目标的entry，然后删除。 *（还记得之前说过的吗，集合视图中的操作也会影响到实际数据） */public V remove(Object key) &#123; Iterator&lt;Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); Entry&lt;K,V&gt; correctEntry = null; if (key==null) &#123; while (correctEntry==null &amp;&amp; i.hasNext()) &#123; Entry&lt;K,V&gt; e = i.next(); if (e.getKey()==null) correctEntry = e; &#125; &#125; else &#123; while (correctEntry==null &amp;&amp; i.hasNext()) &#123; Entry&lt;K,V&gt; e = i.next(); if (key.equals(e.getKey())) correctEntry = e; &#125; &#125; V oldValue = null; if (correctEntry !=null) &#123; oldValue = correctEntry.getValue(); i.remove(); &#125; return oldValue;&#125;// Bulk Operations/** * 遍历参数m，然后将每一个键值对put到该Map中。 */public void putAll(Map&lt;? extends K, ? extends V&gt; m) &#123; for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) put(e.getKey(), e.getValue());&#125;/** * 清空entrySet等价于清空该Map。 */public void clear() &#123; entrySet().clear();&#125; AbstractMap并没有实现put()函数，这样做是为了考虑到也许会有不可修改的Map实现子类继承它，而对于一个可修改的Map实现子类则必须重写put()函数。 AbstractMap没有提供entrySet()的实现，但是却提供了keySet()与values()集合视图的默认实现，它们都是依赖于entrySet()返回的集合视图实现的，源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100/** * keySet和values是lazy的，它们只会在第一次请求视图时进行初始化， * 而且它们是无状态的，所以只需要一个实例（初始化一次）。 */transient Set&lt;K&gt; keySet;transient Collection&lt;V&gt; values;/** * 返回一个AbstractSet的子类，可以发现它的行为都委托给了entrySet返回的集合视图 * 与当前的AbstractMap实例，所以说它自身是无状态的。 */public Set&lt;K&gt; keySet() &#123; Set&lt;K&gt; ks = keySet; if (ks == null) &#123; ks = new AbstractSet&lt;K&gt;() &#123; public Iterator&lt;K&gt; iterator() &#123; return new Iterator&lt;K&gt;() &#123; private Iterator&lt;Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); public boolean hasNext() &#123; return i.hasNext(); &#125; public K next() &#123; return i.next().getKey(); &#125; public void remove() &#123; i.remove(); &#125; &#125;; &#125; public int size() &#123; return AbstractMap.this.size(); &#125; public boolean isEmpty() &#123; return AbstractMap.this.isEmpty(); &#125; public void clear() &#123; AbstractMap.this.clear(); &#125; public boolean contains(Object k) &#123; return AbstractMap.this.containsKey(k); &#125; &#125;; keySet = ks; &#125; return ks;&#125;/** * 与keySet()基本一致，唯一的区别就是返回的是AbstractCollection的子类， * 主要是因为value不需要保持互异性。 */public Collection&lt;V&gt; values() &#123; Collection&lt;V&gt; vals = values; if (vals == null) &#123; vals = new AbstractCollection&lt;V&gt;() &#123; public Iterator&lt;V&gt; iterator() &#123; return new Iterator&lt;V&gt;() &#123; private Iterator&lt;Entry&lt;K,V&gt;&gt; i = entrySet().iterator(); public boolean hasNext() &#123; return i.hasNext(); &#125; public V next() &#123; return i.next().getValue(); &#125; public void remove() &#123; i.remove(); &#125; &#125;; &#125; public int size() &#123; return AbstractMap.this.size(); &#125; public boolean isEmpty() &#123; return AbstractMap.this.isEmpty(); &#125; public void clear() &#123; AbstractMap.this.clear(); &#125; public boolean contains(Object v) &#123; return AbstractMap.this.containsValue(v); &#125; &#125;; values = vals; &#125; return vals;&#125; 它还提供了两个Entry的实现类：SimpleEntry与SimpleImmutableEntry，这两个类的实现非常简单，区别也只是前者是可变的，而后者是不可变的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104private static boolean eq(Object o1, Object o2) &#123; return o1 == null ? o2 == null : o1.equals(o2);&#125;public static class SimpleEntry&lt;K,V&gt; implements Entry&lt;K,V&gt;, java.io.Serializable&#123; private static final long serialVersionUID = -8499721149061103585L; private final K key; private V value; public SimpleEntry(K key, V value) &#123; this.key = key; this.value = value; &#125; public SimpleEntry(Entry&lt;? extends K, ? extends V&gt; entry) &#123; this.key = entry.getKey(); this.value = entry.getValue(); &#125; public K getKey() &#123; return key; &#125; public V getValue() &#123; return value; &#125; public V setValue(V value) &#123; V oldValue = this.value; this.value = value; return oldValue; &#125; public boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; return eq(key, e.getKey()) &amp;&amp; eq(value, e.getValue()); &#125; public int hashCode() &#123; return (key == null ? 0 : key.hashCode()) ^ (value == null ? 0 : value.hashCode()); &#125; public String toString() &#123; return key + "=" + value; &#125;&#125;/** * 它与SimpleEntry的区别在于它是不可变的，value被final修饰，并且不支持setValue()。 */public static class SimpleImmutableEntry&lt;K,V&gt; implements Entry&lt;K,V&gt;, java.io.Serializable&#123; private static final long serialVersionUID = 7138329143949025153L; private final K key; private final V value; public SimpleImmutableEntry(K key, V value) &#123; this.key = key; this.value = value; &#125; public SimpleImmutableEntry(Entry&lt;? extends K, ? extends V&gt; entry) &#123; this.key = entry.getKey(); this.value = entry.getValue(); &#125; public K getKey() &#123; return key; &#125; public V getValue() &#123; return value; &#125; public V setValue(V value) &#123; throw new UnsupportedOperationException(); &#125; public boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; return eq(key, e.getKey()) &amp;&amp; eq(value, e.getValue()); &#125; public int hashCode() &#123; return (key == null ? 0 : key.hashCode()) ^ (value == null ? 0 : value.hashCode()); &#125; public String toString() &#123; return key + "=" + value; &#125;&#125; 我们通过阅读上述的源码不难发现，AbstractMap实现的操作都依赖于entrySet()所返回的集合视图。剩下的函数就没什么好说的了，有兴趣的话可以自己去看看。 TreeMap TreeMap是基于红黑树（一种自平衡的二叉查找树）实现的一个保证有序性的Map，在继承关系结构图中可以得知TreeMap实现了NavigableMap接口，而该接口又继承了SortedMap接口，我们先来看看这两个接口定义了一些什么功能。 SortedMap 首先是SortedMap接口，实现该接口的实现类应当按照自然排序保证key的有序性，所谓自然排序即是根据key的compareTo()函数（需要实现Comparable接口）或者在构造函数中传入的Comparator实现类来进行排序，集合视图遍历元素的顺序也应当与key的顺序一致。SortedMap接口还定义了以下几个有效利用有序性的函数： 1234567891011121314151617181920212223242526272829303132333435363738394041package java.util;public interface SortedMap&lt;K,V&gt; extends Map&lt;K,V&gt; &#123; /** * 用于在此Map中对key进行排序的比较器，如果为null，则使用key的compareTo()函数进行比较。 */ Comparator&lt;? super K&gt; comparator(); /** * 返回一个key的范围为从fromKey到toKey的局部视图（包括fromKey，不包括toKey，包左不包右）， * 如果fromKey和toKey是相等的，则返回一个空视图。 * 返回的局部视图同样是此Map的集合视图，所以对它的操作是会与Map互相影响的。 */ SortedMap&lt;K,V&gt; subMap(K fromKey, K toKey); /** * 返回一个严格地小于toKey的局部视图。 */ SortedMap&lt;K,V&gt; headMap(K toKey); /** * 返回一个大于或等于fromKey的局部视图。 */ SortedMap&lt;K,V&gt; tailMap(K fromKey); /** * 返回当前Map中的第一个key（最小）。 */ K firstKey(); /** * 返回当前Map中的最后一个key（最大）。 */ K lastKey(); Set&lt;K&gt; keySet(); Collection&lt;V&gt; values(); Set&lt;Map.Entry&lt;K, V&gt;&gt; entrySet();&#125; NavigableMap 然后是SortedMap的子接口NavigableMap，该接口扩展了一些用于导航（Navigation）的方法，像函数lowerEntry(key)会根据传入的参数key返回一个小于key的最大的一对键值对，例如，我们如下调用lowerEntry(6)，那么将返回key为5的键值对，如果没有key为5，则会返回key为4的键值对，以此类推，直到返回null（实在找不到的情况下）。 123456789public static void main(String[] args) &#123; NavigableMap&lt;Integer, Integer&gt; map = new TreeMap&lt;&gt;(); for (int i = 0; i &lt; 10; i++) map.put(i, i); assert map.lowerEntry(6).getKey() == 5; assert map.lowerEntry(5).getKey() == 4; assert map.lowerEntry(0).getKey() == null;&#125; NavigableMap定义的都是一些类似于lowerEntry(key)的方法和以逆序、升序排序的集合视图，这些方法利用有序性实现了相比SortedMap接口更加灵活的操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111package java.util;public interface NavigableMap&lt;K,V&gt; extends SortedMap&lt;K,V&gt; &#123; /** * 返回一个小于指定key的最大的一对键值对，如果找不到则返回null。 */ Map.Entry&lt;K,V&gt; lowerEntry(K key); /** * 返回一个小于指定key的最大的一个key，如果找不到则返回null。 */ K lowerKey(K key); /** * 返回一个小于或等于指定key的最大的一对键值对，如果找不到则返回null。 */ Map.Entry&lt;K,V&gt; floorEntry(K key); /** * 返回一个小于或等于指定key的最大的一个key，如果找不到则返回null。 */ K floorKey(K key); /** * 返回一个大于或等于指定key的最小的一对键值对，如果找不到则返回null。 */ Map.Entry&lt;K,V&gt; ceilingEntry(K key); /** * 返回一个大于或等于指定key的最小的一个key，如果找不到则返回null。 */ K ceilingKey(K key); /** * 返回一个大于指定key的最小的一对键值对，如果找不到则返回null。 */ Map.Entry&lt;K,V&gt; higherEntry(K key); /** * 返回一个大于指定key的最小的一个key，如果找不到则返回null。 */ K higherKey(K key); /** * 返回该Map中最小的键值对，如果Map为空则返回null。 */ Map.Entry&lt;K,V&gt; firstEntry(); /** * 返回该Map中最大的键值对，如果Map为空则返回null。 */ Map.Entry&lt;K,V&gt; lastEntry(); /** * 返回并删除该Map中最小的键值对，如果Map为空则返回null。 */ Map.Entry&lt;K,V&gt; pollFirstEntry(); /** * 返回并删除该Map中最大的键值对，如果Map为空则返回null。 */ Map.Entry&lt;K,V&gt; pollLastEntry(); /** * 返回一个以当前Map降序（逆序）排序的集合视图 */ NavigableMap&lt;K,V&gt; descendingMap(); /** * 返回一个包含当前Map中所有key的集合视图，该视图中的key以升序（正序）排序。 */ NavigableSet&lt;K&gt; navigableKeySet(); /** * 返回一个包含当前Map中所有key的集合视图，该视图中的key以降序（逆序）排序。 */ NavigableSet&lt;K&gt; descendingKeySet(); /** * 与SortedMap.subMap基本一致，区别在于多的两个参数fromInclusive和toInclusive， * 它们代表是否包含from和to，如果fromKey与toKey相等，并且fromInclusive与toInclusive * 都为true，那么不会返回空集合。 */ NavigableMap&lt;K,V&gt; subMap(K fromKey, boolean fromInclusive, K toKey, boolean toInclusive); /** * 返回一个小于或等于（inclusive为true的情况下）toKey的局部视图。 */ NavigableMap&lt;K,V&gt; headMap(K toKey, boolean inclusive); /** * 返回一个大于或等于（inclusive为true的情况下）fromKey的局部视图。 */ NavigableMap&lt;K,V&gt; tailMap(K fromKey, boolean inclusive); /** * 等价于subMap(fromKey, true, toKey, false)。 */ SortedMap&lt;K,V&gt; subMap(K fromKey, K toKey); /** * 等价于headMap(toKey, false)。 */ SortedMap&lt;K,V&gt; headMap(K toKey); /** * 等价于tailMap(fromKey, true)。 */ SortedMap&lt;K,V&gt; tailMap(K fromKey);&#125; NavigableMap接口相对于SortedMap接口来说灵活了许多，正因为TreeMap也实现了该接口，所以在需要数据有序而且想灵活地访问它们的时候，使用TreeMap就非常合适了。 红黑树 上文我们提到TreeMap的内部实现基于红黑树，而红黑树又是二叉查找树的一种。二叉查找树是一种有序的树形结构，优势在于查找、插入的时间复杂度只有O(log n)，特性如下： 任意节点最多含有两个子节点。 任意节点的左、右节点都可以看做为一棵二叉查找树。 如果任意节点的左子树不为空，那么左子树上的所有节点的值均小于它的根节点的值。 如果任意节点的右子树不为空，那么右子树上的所有节点的值均大于它的根节点的值。 任意节点的key都是不同的。 尽管二叉查找树看起来很美好，但事与愿违，二叉查找树在极端情况下会变得并不是那么有效率，假设我们有一个有序的整数序列：1,2,3,4,5,6,7,8,9,10,...，如果把这个序列按顺序全部插入到二叉查找树时会发生什么呢？二叉查找树会产生倾斜，序列中的每一个元素都大于它的根节点（前一个元素），左子树永远是空的，那么这棵二叉查找树就跟一个普通的链表没什么区别了，查找操作的时间复杂度只有O(n)。 为了解决这个问题需要引入自平衡的二叉查找树，所谓自平衡，即是在树结构将要倾斜的情况下进行修正，这个修正操作被称为旋转，通过旋转操作可以让树趋于平衡。 红黑树是平衡二叉查找树的一种实现，它的名字来自于它的子节点是着色的，每个子节点非黑即红，由于只有两种颜色（两种状态），一般使用boolean来表示，下面为TreeMap中实现的Entry，它代表红黑树中的一个节点： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677// Red-black mechanicsprivate static final boolean RED = false;private static final boolean BLACK = true;/** * Node in the Tree. Doubles as a means to pass key-value pairs back to * user (see Map.Entry). */static final class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; K key; V value; Entry&lt;K,V&gt; left; Entry&lt;K,V&gt; right; Entry&lt;K,V&gt; parent; boolean color = BLACK; /** * Make a new cell with given key, value, and parent, and with * &#123;@code null&#125; child links, and BLACK color. */ Entry(K key, V value, Entry&lt;K,V&gt; parent) &#123; this.key = key; this.value = value; this.parent = parent; &#125; /** * Returns the key. * * @return the key */ public K getKey() &#123; return key; &#125; /** * Returns the value associated with the key. * * @return the value associated with the key */ public V getValue() &#123; return value; &#125; /** * Replaces the value currently associated with the key with the given * value. * * @return the value associated with the key before this method was * called */ public V setValue(V value) &#123; V oldValue = this.value; this.value = value; return oldValue; &#125; public boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; return valEquals(key,e.getKey()) &amp;&amp; valEquals(value,e.getValue()); &#125; public int hashCode() &#123; int keyHash = (key==null ? 0 : key.hashCode()); int valueHash = (value==null ? 0 : value.hashCode()); return keyHash ^ valueHash; &#125; public String toString() &#123; return key + "=" + value; &#125;&#125; 任何平衡二叉查找树的查找操作都是与二叉查找树是一样的，因为查找操作并不会影响树的结构，也就不需要进行修正，代码如下： 1234567891011121314151617181920212223242526public V get(Object key) &#123; Entry&lt;K,V&gt; p = getEntry(key); return (p==null ? null : p.value);&#125;final Entry&lt;K,V&gt; getEntry(Object key) &#123; // 使用Comparator进行比较 if (comparator != null) return getEntryUsingComparator(key); if (key == null) throw new NullPointerException(); @SuppressWarnings("unchecked") Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; Entry&lt;K,V&gt; p = root; // 从根节点开始，不断比较key的大小进行查找 while (p != null) &#123; int cmp = k.compareTo(p.key); if (cmp &lt; 0) // 小于，转向左子树 p = p.left; else if (cmp &gt; 0) // 大于，转向右子树 p = p.right; else return p; &#125; return null; // 没有相等的key，返回null&#125; 而插入和删除操作与平衡二叉查找树的细节是息息相关的，关于红黑树的实现细节，我之前写过的一篇博客红黑树的那点事儿已经讲的很清楚了，对这方面不了解的读者建议去阅读一下，就不在这里重复叙述了。 集合视图 最后看一下TreeMap的集合视图的实现，集合视图一般都是实现了一个封装了当前实例的类，所以对集合视图的修改本质上就是在修改当前实例，TreeMap也不例外。 TreeMap的headMap()、tailMap()以及subMap()函数都返回了一个静态内部类AscendingSubMap，从名字上也能猜出来，为了支持倒序，肯定也还有一个DescendingSubMap，它们都继承于NavigableSubMap，一个继承AbstractMap并实现了NavigableMap的抽象类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154 abstract static class NavigableSubMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements NavigableMap&lt;K,V&gt;, java.io.Serializable &#123; private static final long serialVersionUID = -2102997345730753016L; final TreeMap&lt;K,V&gt; m; /** * (fromStart, lo, loInclusive) 与 (toEnd, hi, hiInclusive)代表了两个三元组， * 如果fromStart为true，那么范围的下限（绝对）为map（被封装的TreeMap）的起始key， * 其他值将被忽略。 * 如果loInclusive为true，lo将会被包含在范围内，否则lo是在范围外的。 * toEnd与hiInclusive与上述逻辑相似，只不过考虑的是上限。 */ final K lo, hi; final boolean fromStart, toEnd; final boolean loInclusive, hiInclusive; NavigableSubMap(TreeMap&lt;K,V&gt; m, boolean fromStart, K lo, boolean loInclusive, boolean toEnd, K hi, boolean hiInclusive) &#123; if (!fromStart &amp;&amp; !toEnd) &#123; if (m.compare(lo, hi) &gt; 0) throw new IllegalArgumentException("fromKey &gt; toKey"); &#125; else &#123; if (!fromStart) // type check m.compare(lo, lo); if (!toEnd) m.compare(hi, hi); &#125; this.m = m; this.fromStart = fromStart; this.lo = lo; this.loInclusive = loInclusive; this.toEnd = toEnd; this.hi = hi; this.hiInclusive = hiInclusive; &#125; // internal utilities final boolean tooLow(Object key) &#123; if (!fromStart) &#123; int c = m.compare(key, lo); // 如果key小于lo，或等于lo（需要lo不包含在范围内） if (c &lt; 0 || (c == 0 &amp;&amp; !loInclusive)) return true; &#125; return false; &#125; final boolean tooHigh(Object key) &#123; if (!toEnd) &#123; int c = m.compare(key, hi); // 如果key大于hi，或等于hi（需要hi不包含在范围内） if (c &gt; 0 || (c == 0 &amp;&amp; !hiInclusive)) return true; &#125; return false; &#125; final boolean inRange(Object key) &#123; return !tooLow(key) &amp;&amp; !tooHigh(key); &#125; final boolean inClosedRange(Object key) &#123; return (fromStart || m.compare(key, lo) &gt;= 0) &amp;&amp; (toEnd || m.compare(hi, key) &gt;= 0); &#125; // 判断key是否在该视图的范围之内 final boolean inRange(Object key, boolean inclusive) &#123; return inclusive ? inRange(key) : inClosedRange(key); &#125; /* * 以abs开头的函数为关系操作的绝对版本。 */ /* * 获得最小的键值对： * 如果fromStart为true，那么直接返回当前map实例的第一个键值对即可， * 否则，先判断lo是否包含在范围内， * 如果是，则获得当前map实例中大于或等于lo的最小的键值对， * 如果不是，则获得当前map实例中大于lo的最小的键值对。 * 如果得到的结果e超过了范围的上限，那么返回null。 */ final TreeMap.Entry&lt;K,V&gt; absLowest() &#123; TreeMap.Entry&lt;K,V&gt; e = (fromStart ? m.getFirstEntry() : (loInclusive ? m.getCeilingEntry(lo) : m.getHigherEntry(lo))); return (e == null || tooHigh(e.key)) ? null : e; &#125; // 与absLowest()相反 final TreeMap.Entry&lt;K,V&gt; absHighest() &#123; TreeMap.Entry&lt;K,V&gt; e = (toEnd ? m.getLastEntry() : (hiInclusive ? m.getFloorEntry(hi) : m.getLowerEntry(hi))); return (e == null || tooLow(e.key)) ? null : e; &#125; // 下面的逻辑就都很简单了，注意会先判断key是否越界， // 如果越界就返回绝对值。 final TreeMap.Entry&lt;K,V&gt; absCeiling(K key) &#123; if (tooLow(key)) return absLowest(); TreeMap.Entry&lt;K,V&gt; e = m.getCeilingEntry(key); return (e == null || tooHigh(e.key)) ? null : e; &#125; final TreeMap.Entry&lt;K,V&gt; absHigher(K key) &#123; if (tooLow(key)) return absLowest(); TreeMap.Entry&lt;K,V&gt; e = m.getHigherEntry(key); return (e == null || tooHigh(e.key)) ? null : e; &#125; final TreeMap.Entry&lt;K,V&gt; absFloor(K key) &#123; if (tooHigh(key)) return absHighest(); TreeMap.Entry&lt;K,V&gt; e = m.getFloorEntry(key); return (e == null || tooLow(e.key)) ? null : e; &#125; final TreeMap.Entry&lt;K,V&gt; absLower(K key) &#123; if (tooHigh(key)) return absHighest(); TreeMap.Entry&lt;K,V&gt; e = m.getLowerEntry(key); return (e == null || tooLow(e.key)) ? null : e; &#125; /** 返回升序遍历的绝对上限 */ final TreeMap.Entry&lt;K,V&gt; absHighFence() &#123; return (toEnd ? null : (hiInclusive ? m.getHigherEntry(hi) : m.getCeilingEntry(hi))); &#125; /** 返回降序遍历的绝对下限 */ final TreeMap.Entry&lt;K,V&gt; absLowFence() &#123; return (fromStart ? null : (loInclusive ? m.getLowerEntry(lo) : m.getFloorEntry(lo))); &#125; // 剩下的就是实现NavigableMap的方法以及一些抽象方法// 和NavigableSubMap中的集合视图函数。 // 大部分操作都是靠当前实例map的方法和上述用于判断边界的方法提供支持 ..... &#125; 一个局部视图最重要的是要能够判断出传入的key是否属于该视图的范围内，在上面的代码中可以发现NavigableSubMap提供了非常多的辅助函数用于判断范围，接下来我们看看NavigableSubMap的迭代器是如何实现的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109/** * Iterators for SubMaps */abstract class SubMapIterator&lt;T&gt; implements Iterator&lt;T&gt; &#123; TreeMap.Entry&lt;K,V&gt; lastReturned; TreeMap.Entry&lt;K,V&gt; next; final Object fenceKey; int expectedModCount; SubMapIterator(TreeMap.Entry&lt;K,V&gt; first, TreeMap.Entry&lt;K,V&gt; fence) &#123; expectedModCount = m.modCount; lastReturned = null; next = first; // UNBOUNDED是一个虚拟值（一个Object对象），表示无边界。 fenceKey = fence == null ? UNBOUNDED : fence.key; &#125; // 只要next不为null并且没有超过边界 public final boolean hasNext() &#123; return next != null &amp;&amp; next.key != fenceKey; &#125; final TreeMap.Entry&lt;K,V&gt; nextEntry() &#123; TreeMap.Entry&lt;K,V&gt; e = next; // 已经遍历到头或者越界了 if (e == null || e.key == fenceKey) throw new NoSuchElementException(); // modCount是一个记录操作数的计数器 // 如果与expectedModCount不一致 // 则代表当前map实例在遍历过程中已被修改过了（从其他线程） if (m.modCount != expectedModCount) throw new ConcurrentModificationException(); // 向后移动next指针 // successor()返回指定节点的继任者 // 它是节点e的右子树的最左节点 // 也就是比e大的最小的节点 // 如果e没有右子树，则会试图向上寻找 next = successor(e); lastReturned = e; // 记录最后返回的节点 return e; &#125; final TreeMap.Entry&lt;K,V&gt; prevEntry() &#123; TreeMap.Entry&lt;K,V&gt; e = next; if (e == null || e.key == fenceKey) throw new NoSuchElementException(); if (m.modCount != expectedModCount) throw new ConcurrentModificationException(); // 向前移动next指针 // predecessor()返回指定节点的前任 // 它与successor()逻辑相反。 next = predecessor(e); lastReturned = e; return e; &#125; final void removeAscending() &#123; if (lastReturned == null) throw new IllegalStateException(); if (m.modCount != expectedModCount) throw new ConcurrentModificationException(); // 被删除的节点被它的继任者取代 // 执行完删除后，lastReturned实际指向了它的继任者 if (lastReturned.left != null &amp;&amp; lastReturned.right != null) next = lastReturned; m.deleteEntry(lastReturned); lastReturned = null; expectedModCount = m.modCount; &#125; final void removeDescending() &#123; if (lastReturned == null) throw new IllegalStateException(); if (m.modCount != expectedModCount) throw new ConcurrentModificationException(); m.deleteEntry(lastReturned); lastReturned = null; expectedModCount = m.modCount; &#125;&#125;final class SubMapEntryIterator extends SubMapIterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123; SubMapEntryIterator(TreeMap.Entry&lt;K,V&gt; first, TreeMap.Entry&lt;K,V&gt; fence) &#123; super(first, fence); &#125; public Map.Entry&lt;K,V&gt; next() &#123; return nextEntry(); &#125; public void remove() &#123; removeAscending(); &#125;&#125;final class DescendingSubMapEntryIterator extends SubMapIterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123; DescendingSubMapEntryIterator(TreeMap.Entry&lt;K,V&gt; last, TreeMap.Entry&lt;K,V&gt; fence) &#123; super(last, fence); &#125; public Map.Entry&lt;K,V&gt; next() &#123; return prevEntry(); &#125; public void remove() &#123; removeDescending(); &#125;&#125; 到目前为止，我们已经针对集合视图讨论了许多，想必大家也能够理解集合视图的概念了，由于SortedMap与NavigableMap的缘故，TreeMap中的集合视图是非常多的，包括各种局部视图和不同排序的视图，有兴趣的读者可以自己去看看源码，后面的内容不会再对集合视图进行过多的解释了。 HashMap 光从名字上应该也能猜到，HashMap肯定是基于hash算法实现的，这种基于hash实现的map叫做散列表（hash table）。 散列表中维护了一个数组，数组的每一个元素被称为一个桶（bucket），当你传入一个key = &quot;a&quot;进行查询时，散列表会先把key传入散列（hash）函数中进行寻址，得到的结果就是数组的下标，然后再通过这个下标访问数组即可得到相关联的值。 我们都知道数组中数据的组织方式是线性的，它会直接分配一串连续的内存地址序列，要找到一个元素只需要根据下标来计算地址的偏移量即可（查找一个元素的起始地址为：数组的起始地址加上下标乘以该元素类型占用的地址大小）。因此散列表在理想的情况下，各种操作的时间复杂度只有O(1)，这甚至超过了二叉查找树，虽然理想的情况并不总是满足的，关于这点之后我们还会提及。 为什么是hash？ hash算法是一种可以从任何数据中提取出其“指纹”的数据摘要算法，它将任意大小的数据（输入）映射到一个固定大小的序列（输出）上，这个序列被称为hash code、数据摘要或者指纹。比较出名的hash算法有MD5、SHA。 hash是具有唯一性且不可逆的，唯一性指的是相同的输入产生的hash code永远是一样的，而不可逆也比较容易理解，数据摘要算法并不是压缩算法，它只是生成了一个该数据的摘要，没有将数据进行压缩。压缩算法一般都是使用一种更节省空间的编码规则将数据重新编码，解压缩只需要按着编码规则解码就是了，试想一下，一个几百MB甚至几GB的数据生成的hash code都只是一个拥有固定长度的序列，如果再能逆向解压缩，那么其他压缩算法该情何以堪？ 我们上述讨论的仅仅是在密码学中的hash算法，而在散列表中所需要的散列函数是要能够将key寻址到buckets中的一个位置，散列函数的实现影响到整个散列表的性能。 一个完美的散列函数要能够做到均匀地将key分布到buckets中，每一个key分配到一个bucket，但这是不可能的。虽然hash算法具有唯一性，但同时它还具有重复性，唯一性保证了相同输入的输出是一致的，却没有保证不同输入的输出是不一致的，也就是说，完全有可能两个不同的key被分配到了同一个bucket（因为它们的hash code可能是相同的），这叫做碰撞冲突。总之，理想很丰满，现实很骨感，散列函数只能尽可能地减少冲突，没有办法完全消除冲突。 散列函数的实现方法非常多，一个优秀的散列函数要看它能不能将key分布均匀。首先介绍一种最简单的方法：除留余数法，先对key进行hash得到它的hash code，然后再用该hash code对buckets数组的元素数量取余，得到的结果就是bucket的下标，这种方法简单高效，也可以当做对集群进行负载均衡的路由算法。 1234private int hash(Key key) &#123; // &amp; 0x7fffffff 是为了屏蔽符号位，M为bucket数组的长度 return (key.hashCode() &amp; 0x7fffffff) % M;&#125; 要注意一点，只有整数才能进行取余运算，如果hash code是一个字符串或别的类型，那么你需要将它转换为整数才能使用除留余数法，不过Java在Object对象中提供了hashCode()函数，该函数返回了一个int值，所以任何你想要放入HashMap的自定义的抽象数据类型，都必须实现该函数和equals()函数，这两个函数之间也遵守着一种约定：如果a.equals(b) == true，那么a与b的hashCode()也必须是相同的。 下面为String类的hashCode()函数，它先遍历了内部的字符数组，然后在每一次循环中计算hash code（将hash code乘以一个素数并加上当前循环项的字符）： 123456789101112131415161718/** The value is used for character storage. */private final char value[];/** Cache the hash code for the string */private int hash; // Default to 0public int hashCode() &#123; int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) &#123; char val[] = value; for (int i = 0; i &lt; value.length; i++) &#123; h = 31 * h + val[i]; &#125; hash = h; &#125; return h;&#125; HashMap没有采用这么简单的方法，有一个原因是HashMap中的buckets数组的长度永远为一个2的幂，而不是一个素数，如果长度为素数，那么可能会更适合简单暴力的除留余数法（当然除留余数法虽然简单却并不是那么高效的），顺便一提，时代的眼泪Hashtable就使用了除留余数法，它没有强制约束buckets数组的长度。 HashMap在内部实现了一个hash()函数，首先要对hashCode()的返回值进行处理： 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 该函数将key.hashCode()的低16位和高16位做了个异或运算，其目的是为了扰乱低位的信息以实现减少碰撞冲突。之后还需要把hash()的返回值与table.length - 1做与运算（table为buckets数组），得到的结果即是数组的下标。 table.length - 1就像是一个低位掩码（这个设计也优化了扩容操作的性能），它和hash()做与操作时必然会将高位屏蔽（因为一个HashMap不可能有特别大的buckets数组，至少在不断自动扩容之前是不可能的，所以table.length - 1的大部分高位都为0），只保留低位，看似没什么毛病，但这其实暗藏玄机，它会导致总是只有最低的几位是有效的，这样就算你的hashCode()实现得再好也难以避免发生碰撞。这时，hash()函数的价值就体现出来了，它对hash code的低位添加了随机性并且混合了高位的部分特征，显著减少了碰撞冲突的发生（关于hash()函数的效果如何，可以参考这篇文章An introduction to optimising a hashing strategy）。 HashMap的散列函数具体流程如下图： 解决冲突 在上文中我们已经多次提到碰撞冲突，但是散列函数不可能是完美的，key分布完全均匀的情况是不存在的，所以碰撞冲突总是难以避免。 那么发生碰撞冲突时怎么办？总不能丢弃数据吧？必须要有一种合理的方法来解决这个问题，HashMap使用了叫做分离链接（Separate chaining，也有人翻译成拉链法）的策略来解决冲突。它的主要思想是每个bucket都应当是一个互相独立的数据结构，当发生冲突时，只需要把数据放入bucket中（因为bucket本身也是一个可以存放数据的数据结构），这样查询一个key所消耗的时间为访问bucket所消耗的时间加上在bucket中查找的时间。 HashMap的buckets数组其实就是一个链表数组，在发生冲突时只需要把Entry（还记得Entry吗？HashMap的Entry实现就是一个简单的链表节点，它包含了key和value以及hash code）放到链表的尾部，如果未发生冲突（位于该下标的bucket为null），那么就把该Entry做为链表的头部。而且HashMap还使用了Lazy策略，buckets数组只会在第一次调用put()函数时进行初始化，这是一种防止内存浪费的做法，像ArrayList也是Lazy的，它在第一次调用add()时才会初始化内部的数组。 不过链表虽然实现简单，但是在查找的效率上只有O(n)，而且我们大部分的操作都是在进行查找，在hashCode()设计的不是非常良好的情况下，碰撞冲突可能会频繁发生，链表也会变得越来越长，这个效率是非常差的。Java 8对其实现了优化，链表的节点数量在到达阈值时会转化为红黑树，这样查找所需的时间就只有O(log n)了，阈值的定义如下： 123456789/** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */static final int TREEIFY_THRESHOLD = 8; 如果在插入Entry时发现一条链表超过阈值，就会执行以下的操作，对该链表进行树化；相对的，如果在删除Entry（或进行扩容）时发现红黑树的节点太少（根据阈值UNTREEIFY_THRESHOLD），也会把红黑树退化成链表。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/** * 替换指定hash所处位置的链表中的所有节点为TreeNode， * 如果buckets数组太小，就进行扩容。 */final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; // MIN_TREEIFY_CAPACITY = 64，小于该值代表数组中的节点并不是很多 // 所以选择进行扩容，只有数组长度大于该值时才会进行树化。 if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; // 转换链表节点为树节点，注意要处理好连接关系 do &#123; TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); // 从头部开始构造树 &#125;&#125; // 该函数定义在TreeNode中 final void treeify(Node&lt;K,V&gt;[] tab) &#123; TreeNode&lt;K,V&gt; root = null; for (TreeNode&lt;K,V&gt; x = this, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; x.left = x.right = null; if (root == null) &#123; // 初始化root节点 x.parent = null; x.red = false; root = x; &#125; else &#123; K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; for (TreeNode&lt;K,V&gt; p = root;;) &#123; int dir, ph; K pk = p.key; // 确定节点的方向 if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; // 如果kc == null // 并且k没有实现Comparable接口 // 或者k与pk是没有可比较性的（类型不同） // 或者k与pk是相等的（返回0也有可能是相等） else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); // 确定方向后插入节点，修正红黑树的平衡 TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; root = balanceInsertion(root, x); break; &#125; &#125; &#125; &#125; // 确保给定的root是该bucket中的第一个节点 moveRootToFront(tab, root); &#125; static int tieBreakOrder(Object a, Object b) &#123; int d; if (a == null || b == null || (d = a.getClass().getName(). compareTo(b.getClass().getName())) == 0) // System.identityHashCode()将调用并返回传入对象的默认hashCode() // 也就是说，无论是否重写了hashCode()，都将调用Object.hashCode()。 // 如果传入的对象是null，那么就返回0 d = (System.identityHashCode(a) &lt;= System.identityHashCode(b) ? -1 : 1); return d; &#125; 解决碰撞冲突的另一种策略叫做开放寻址法（Open addressing），它与分离链接法的思想截然不同。在开放寻址法中，所有Entry都会存储在buckets数组，一个明显的区别是，分离链接法中的每个bucket都是一个链表或其他的数据结构，而开放寻址法中的每个bucket就仅仅只是Entry本身。 开放寻址法是基于数组中的空位来解决冲突的，它的想法很简单，与其使用链表等数据结构，不如直接在数组中留出空位来当做一个标记，反正都要占用额外的内存。 当你查找一个key的时候，首先会从起始位置（通过散列函数计算出的数组索引）开始，不断检查当前bucket是否为目标Entry（通过比较key来判断），如果当前bucket不是目标Entry，那么就向后查找（查找的间隔取决于实现），直到碰见一个空位（null），这代表你想要找的key不存在。 如果你想要put一个全新的Entry（Map中没有这个key存在），依然会从起始位置开始进行查找，如果起始位置不是空的，则代表发生了碰撞冲突，只好不断向后查找，直到发现一个空位。 开放寻址法的名字也是来源于此，一个Entry的位置并不是完全由hash值决定的，所以也叫做Closed hashing，相对的，分离链接法也被称为Open hashing或Closed addressing。 根据向后探测（查找）的算法不同，开放寻址法有多种不同的实现，我们介绍一种最简单的算法：线性探测法（Linear probing），在发生碰撞时，简单地将索引加一，如果到达了数组的尾部就折回到数组的头部，直到找到目标或一个空位。 基于线性探测法的查找操作如下： 123456789101112private K[] keys; // 存储key的数组private V[] vals; // 存储值的数组 public V get(K key) &#123; // m是buckets数组的长度，即keys和vals的长度。 // 当i等于m时，取模运算会得0（折回数组头部） for (int i = hash(key); keys[i] != null; i = (i + 1) % m) &#123; if (keys[i].equals(key)) return vals[i]; &#125; return null;&#125; 插入操作稍微麻烦一些，需要在插入之前判断当前数组的剩余容量，然后决定是否扩容。数组的剩余容量越多，代表Entry之间的间隔越大以及越早碰见空位（向后探测的次数就越少），效率自然就会变高。代价就是额外消耗的内存较多，这也是在用空间换取时间。 1234567891011121314151617public void put(K key, V value) &#123; // n是Entry的数量，如果n超过了数组长度的一半，就扩容一倍 if (n &gt;= m / 2) resize(2 * m); int i; for (i = hash(key); keys[i] != null; i = (i + 1) % m) &#123; if (keys[i].equals(key)) &#123; vals[i] = value; return; &#125; &#125; // 没有找到目标，那么就插入一对新的Entry keys[i] = key; vals[i] = value; n++;&#125; 接下来是删除操作，需要注意一点，我们不能简单地把目标key所在的位置（keys和vals数组）设置为null，这样会导致此位置之后的Entry无法被探测到，所以需要将目标右侧的所有Entry重新插入到散列表中： 123456789101112131415161718192021222324252627282930public V delete(K key) &#123; int i = hash(key); // 先找到目标的索引 while (!key.equals(keys[i])) &#123; i = (i + 1) % m; &#125; V oldValue = vals[i]; // 删除目标key和value keys[i] = null; vals[i] = null; // 指针移动到下一个索引 i = (i + 1) % m; while (keys[i] != null) &#123; // 先删除然后重新插入 K keyToRehash = keys[i]; V valToRehash = vals[i]; keys[i] = null; vals[i] = null; n--; put(keyToRehash, valToRehash); i = (i + 1) % m; &#125; n--; // 当前Entry小于等于数组长度的八分之一时，进行缩容 if (n &gt; 0 &amp;&amp; n &lt;= m / 8) resize(m / 2); return oldValue;&#125; 动态扩容 散列表以数组的形式组织bucket，问题在于数组是静态分配的，为了保证查找的性能，需要在Entry数量大于一个临界值时进行扩容，否则就算散列函数的效果再好，也难免产生碰撞。 所谓扩容，其实就是用一个容量更大（在原容量上乘以二）的数组来替换掉当前的数组，这个过程需要把旧数组中的数据重新hash到新数组，所以扩容也能在一定程度上减缓碰撞。 HashMap通过负载因子（Load Factor）乘以buckets数组的长度来计算出临界值，算法：threshold = load_factor * capacity。比如，HashMap的默认初始容量为16（capacity = 16），默认负载因子为0.75（load_factor = 0.75），那么临界值就为threshold = 0.75 * 16 = 12，只要Entry的数量大于12，就会触发扩容操作。 还可以通过下列的构造函数来自定义负载因子，负载因子越小查找的性能就会越高，但同时额外占用的内存就会越多，如果没有特殊需要不建议修改默认值。 1234567891011121314151617/** * 可以发现构造函数中根本就没初始化buckets数组。 * （之前说过buckets数组会推迟到第一次调用put()时进行初始化） */public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; // tableSizeFor()确保initialCapacity必须为一个2的N次方 this.threshold = tableSizeFor(initialCapacity);&#125; buckets数组的大小约束对于整个HashMap都至关重要，为了防止传入一个不是2次幂的整数，必须要有所防范。tableSizeFor()函数会尝试修正一个整数，并转换为离该整数最近的2次幂。 123456789101112/** * Returns a power of two size for the given target capacity. */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 还记得数组索引的计算方法吗？index = (table.length - 1) &amp; hash，这其实是一种优化手段，由于数组的大小永远是一个2次幂，在扩容之后，一个元素的新索引要么是在原位置，要么就是在原位置加上扩容前的容量。这个方法的巧妙之处全在于&amp;运算，之前提到过&amp;运算只会关注n - 1（n = 数组长度）的有效位，当扩容之后，n的有效位相比之前会多增加一位（n会变成之前的二倍，所以确保数组长度永远是2次幂很重要），然后只需要判断hash在新增的有效位的位置是0还是1就可以算出新的索引位置，如果是0，那么索引没有发生变化，如果是1，索引就为原索引加上扩容前的容量。 这样在每次扩容时都不用重新计算hash，省去了不少时间，而且新增有效位是0还是1是带有随机性的，之前两个碰撞的Entry又有可能在扩容时再次均匀地散布开。下面是resize()的源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; // table就是buckets数组 int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; // oldCap大于0，进行扩容，设置阈值与新的容量 if (oldCap &gt; 0) &#123; // 超过最大值不会进行扩容，并且把阈值设置成Interger.MAX_VALUE if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，扩容为原来的2倍 // 向左移1位等价于乘2 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; // oldCap = 0，oldThr大于0，那么就把阈值做为新容量以进行初始化 // 这种情况发生在用户调用了带有参数的构造函数（会对threshold进行初始化） else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; // oldCap与oldThr都为0，这种情况发生在用户调用了无参构造函数 // 采用默认值进行初始化 else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 如果newThr还没有被赋值，那么就根据newCap计算出阈值 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // 如果oldTab != null，代表这是扩容操作 // 需要将扩容前的数组数据迁移到新数组 if (oldTab != null) &#123; // 遍历oldTab的每一个bucket，然后移动到newTab for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; // 索引j的bucket只有一个Entry（未发生过碰撞） // 直接移动到newTab if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; // 如果是一个树节点（代表已经转换成红黑树了） // 那么就将这个节点拆分为lower和upper两棵树 // 首先会对这个节点进行遍历 // 只要当前节点的hash &amp; oldCap == 0就链接到lower树 // 注意这里是与oldCap进行与运算，而不是oldCap - 1(n - 1) // oldCap就是扩容后新增有效位的掩码 // 比如oldCap=16，二进制10000，n-1 = 1111，扩容后的n-1 = 11111 // 只要hash &amp; oldCap == 0，就代表hash的新增有效位为0 // 否则就链接到upper树（新增有效位为1） // lower会被放入newTab[原索引j]，upper树会被放到newTab[原索引j + oldCap] // 如果lower或者upper树的节点少于阈值，会被退化成链表 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order // 下面操作的逻辑与分裂树节点基本一致 // 只不过split()操作的是TreeNode // 而且会将两条TreeNode链表组织成红黑树 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 使用HashMap时还需要注意一点，它不会动态地进行缩容，也就是说，你不应该保留一个已经删除过大量Entry的HashMap（如果不打算继续添加元素的话），此时它的buckets数组经过多次扩容已经变得非常大了，这会占用非常多的无用内存，这样做的好处是不用多次对数组进行扩容或缩容操作。不过一般也不会出现这种情况，如果遇见了，请毫不犹豫地丢掉它，或者把数据转移到一个新的HashMap。 添加元素 我们已经了解了HashMap的内部实现与工作原理，它在内部维护了一个数组，每一个key都会经过散列函数得出在数组的索引，如果两个key的索引相同，那么就使用分离链接法解决碰撞冲突，当Entry的数量大于临界值时，对数组进行扩容。 接下来以一个添加元素（put()）的过程为例来梳理一下知识，下图是put()函数的流程图： 然后是源码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // table == null or table.length == 0 // 第一次调用put()，初始化table if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 没有发生碰撞，直接放入到数组 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 发生碰撞（头节点就是目标节点） if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 节点为红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 节点为链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; // 未找到目标节点，在链表尾部链接新节点 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st // 链表过长，转换为红黑树 treeifyBin(tab, hash); break; &#125; // 找到目标节点，退出循环 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 节点已存在，替换value if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; // afterNodeXXXXX是提供给LinkedHashMap重写的函数 // 在HashMap中没有意义 afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 超过临界值，进行扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; WeakHashMap WeakHashMap是一个基于Map接口实现的散列表，实现细节与HashMap类似（都有负载因子、散列函数等等，但没有HashMap那么多优化手段），它的特殊之处在于每个key都是一个弱引用。 首先我们要明白什么是弱引用，Java将引用分为四类（从JDK1.2开始），强度依次逐渐减弱： 强引用： 就是平常使用的普通引用对象，例如Object obj = new Object()，这就是一个强引用，强引用只要还存在，就不会被垃圾收集器回收。 软引用： 软引用表示一个还有用但并非必需的对象，不像强引用，它还需要通过SoftReference类来间接引用目标对象（除了强引用都是如此）。被软引用关联的对象，在将要发生内存溢出异常之前，会被放入回收范围之中以进行第二次回收（如果第二次回收之后依旧没有足够的内存，那么就会抛出OOM异常）。 弱引用： 同样是表示一个非必需的对象，但要比软引用的强度还要弱，需要通过WeakReference类来间接引用目标对象。被弱引用关联的对象只能存活到下一次垃圾回收发生之前，当触发垃圾回收时，无论当前内存是否足够，都会回收掉只被弱引用关联的对象（如果这个对象还被强引用所引用，那么就不会被回收）。 虚引用： 这是一种最弱的引用关系，需要通过PhantomReference类来间接引用目标对象。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来获得对象实例。虚引用的唯一作用就是能在这个对象被回收时收到一个系统通知（结合ReferenceQueue使用）。基于这点可以通过虚引用来实现对象的析构函数，这比使用finalize()函数是要靠谱多了。 WeakHashMap适合用来当做一个缓存来使用。假设你的缓存系统是基于强引用实现的，那么你就必须以手动（或者用一条线程来不断轮询）的方式来删除一个无效的缓存项，而基于弱引用实现的缓存项只要没被其他强引用对象关联，就会被直接放入回收队列。 需要注意的是，只有key是被弱引用关联的，而value一般都是一个强引用对象。因此，需要确保value没有关联到它的key，否则会对key的回收产生阻碍。在极端的情况下，一个value对象A引用了另一个key对象D，而与D相对应的value对象C又反过来引用了与A相对应的key对象B，这就会产生一个引用循环，导致D与B都无法被正常回收。想要解决这个问题，就只能把value也变成一个弱引用，例如m.put(key, new WeakReference(value))，弱引用之间的互相引用不会产生影响。 查找操作的实现跟HashMap相比简单了许多，只要读懂了HashMap，基本都能看懂，源码如下： 1234567891011121314151617181920212223242526272829303132333435/** * Value representing null keys inside tables. */private static final Object NULL_KEY = new Object();/** * Use NULL_KEY for key if it is null. */private static Object maskNull(Object key) &#123; return (key == null) ? NULL_KEY : key;&#125;/** * Returns index for hash code h. */private static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125;public V get(Object key) &#123; // WeakHashMap允许null key与null value // null key会被替换为一个虚拟值 Object k = maskNull(key); int h = hash(k); Entry&lt;K,V&gt;[] tab = getTable(); int index = indexFor(h, tab.length); Entry&lt;K,V&gt; e = tab[index]; // 遍历链表 while (e != null) &#123; if (e.hash == h &amp;&amp; eq(k, e.get())) return e.value; e = e.next; &#125; return null;&#125; 尽管key是一个弱引用，但仍需手动地回收那些已经无效的Entry。这个操作会在getTable()函数中执行，不管是查找、添加还是删除，都需要调用getTable()来获得buckets数组，所以这是种防止内存泄漏的被动保护措施。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * The table, resized as necessary. Length MUST Always be a power of two. */Entry&lt;K,V&gt;[] table;/** * Reference queue for cleared WeakEntries */private final ReferenceQueue&lt;Object&gt; queue = new ReferenceQueue&lt;&gt;();/** * Expunges stale entries from the table. */private void expungeStaleEntries() &#123; // 遍历ReferenceQueue，然后清理table中无效的Entry for (Object x; (x = queue.poll()) != null; ) &#123; synchronized (queue) &#123; @SuppressWarnings("unchecked") Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;) x; int i = indexFor(e.hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; p = prev; while (p != null) &#123; Entry&lt;K,V&gt; next = p.next; if (p == e) &#123; if (prev == e) table[i] = next; else prev.next = next; // Must not null out e.next; // stale entries may be in use by a HashIterator e.value = null; // Help GC size--; break; &#125; prev = p; p = next; &#125; &#125; &#125;&#125;/** * Returns the table after first expunging stale entries. */private Entry&lt;K,V&gt;[] getTable() &#123; expungeStaleEntries(); return table;&#125; 然后是插入操作与删除操作，实现都比较简单： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public V put(K key, V value) &#123; Object k = maskNull(key); int h = hash(k); Entry&lt;K,V&gt;[] tab = getTable(); int i = indexFor(h, tab.length); for (Entry&lt;K,V&gt; e = tab[i]; e != null; e = e.next) &#123; if (h == e.hash &amp;&amp; eq(k, e.get())) &#123; V oldValue = e.value; if (value != oldValue) e.value = value; return oldValue; &#125; &#125; modCount++; Entry&lt;K,V&gt; e = tab[i]; // e被连接在new Entry的后面 tab[i] = new Entry&lt;&gt;(k, value, queue, h, e); if (++size &gt;= threshold) resize(tab.length * 2); return null;&#125;public V remove(Object key) &#123; Object k = maskNull(key); int h = hash(k); Entry&lt;K,V&gt;[] tab = getTable(); int i = indexFor(h, tab.length); Entry&lt;K,V&gt; prev = tab[i]; Entry&lt;K,V&gt; e = prev; while (e != null) &#123; Entry&lt;K,V&gt; next = e.next; if (h == e.hash &amp;&amp; eq(k, e.get())) &#123; modCount++; size--; if (prev == e) tab[i] = next; else prev.next = next; return e.value; &#125; prev = e; e = next; &#125; return null;&#125; 我们并没有在put()函数中发现key被转换成弱引用，这是怎么回事？key只有在第一次被放入buckets数组时才需要转换成弱引用，也就是new Entry&lt;&gt;(k, value, queue, h, e)，WeakHashMap的Entry实现其实就是WeakReference的子类。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * The entries in this hash table extend WeakReference, using its main ref * field as the key. */private static class Entry&lt;K,V&gt; extends WeakReference&lt;Object&gt; implements Map.Entry&lt;K,V&gt; &#123; V value; final int hash; Entry&lt;K,V&gt; next; /** * Creates new entry. */ Entry(Object key, V value, ReferenceQueue&lt;Object&gt; queue, int hash, Entry&lt;K,V&gt; next) &#123; super(key, queue); this.value = value; this.hash = hash; this.next = next; &#125; @SuppressWarnings("unchecked") public K getKey() &#123; return (K) WeakHashMap.unmaskNull(get()); &#125; public V getValue() &#123; return value; &#125; public V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; K k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) &#123; V v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; &#125; return false; &#125; public int hashCode() &#123; K k = getKey(); V v = getValue(); return Objects.hashCode(k) ^ Objects.hashCode(v); &#125; public String toString() &#123; return getKey() + "=" + getValue(); &#125;&#125; 有关使用WeakReference的一个典型案例是ThreadLocal，感兴趣的读者可以参考我之前写的博客聊一聊Spring中的线程安全性。 LinkedHashMap LinkedHashMap继承HashMap并实现了Map接口，同时具有可预测的迭代顺序（按照插入顺序排序）。它与HashMap的不同之处在于，维护了一条贯穿其全部Entry的双向链表（因为额外维护了链表的关系，性能上要略差于HashMap，不过集合视图的遍历时间与元素数量成正比，而HashMap是与buckets数组的长度成正比的），可以认为它是散列表与链表的结合。 123456789101112131415161718192021222324/** * The head (eldest) of the doubly linked list. */transient LinkedHashMap.Entry&lt;K,V&gt; head;/** * The tail (youngest) of the doubly linked list. */transient LinkedHashMap.Entry&lt;K,V&gt; tail;/** * 迭代顺序模式的标记位，如果为true，采用访问排序，否则，采用插入顺序 * 默认插入顺序（构造函数中默认设置为false） */final boolean accessOrder;/** * Constructs an empty insertion-ordered &lt;tt&gt;LinkedHashMap&lt;/tt&gt; instance * with the default initial capacity (16) and load factor (0.75). */public LinkedHashMap() &#123; super(); accessOrder = false;&#125; LinkedHashMap的Entry实现也继承自HashMap，只不过多了指向前后的两个指针。 123456789/** * HashMap.Node subclass for normal LinkedHashMap entries. */static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; &#123; Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125;&#125; 你也可以通过构造函数来构造一个迭代顺序为访问顺序（accessOrder设为true）的LinkedHashMap，这个访问顺序指的是按照最近被访问的Entry的顺序进行排序（从最近最少访问到最近最多访问）。基于这点可以简单实现一个采用LRU（Least Recently Used）策略的缓存。 123456public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) &#123; super(initialCapacity, loadFactor); this.accessOrder = accessOrder;&#125; LinkedHashMap复用了HashMap的大部分代码，所以它的查找实现是非常简单的，唯一稍微复杂点的操作是保证访问顺序。 12345678public V get(Object key) &#123; Node&lt;K,V&gt; e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) afterNodeAccess(e); return e.value;&#125; 还记得这些afterNodeXXXX命名格式的函数吗？我们之前已经在HashMap中见识过了，这些函数在HashMap中只是一个空实现，是专门用来让LinkedHashMap重写实现的hook函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 // 在HashMap.removeNode()的末尾处调用 // 将e从LinkedHashMap的双向链表中删除 void afterNodeRemoval(Node&lt;K,V&gt; e) &#123; // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.before = p.after = null; if (b == null) head = a; else b.after = a; if (a == null) tail = b; else a.before = b; &#125; // 在HashMap.putVal()的末尾处调用 // evict是一个模式标记，如果为false代表buckets数组处于创建模式 // HashMap.put()函数对此标记设置为true void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; // LinkedHashMap.removeEldestEntry()永远返回false // 避免了最年长元素被删除的可能（就像一个普通的Map一样） if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125; &#125; // HashMap.get()没有调用此函数，所以LinkedHashMap重写了get()// get()与put()都会调用afterNodeAccess()来保证访问顺序 // 将e移动到tail，代表最近访问到的节点 void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else &#123; p.before = last; last.after = p; &#125; tail = p; ++modCount; &#125; &#125; 注意removeEldestEntry()默认永远返回false，这时它的行为与普通的Map无异。如果你把removeEldestEntry()重写为永远返回true，那么就有可能使LinkedHashMap处于一个永远为空的状态（每次put()或者putAll()都会删除头节点）。 一个比较合理的实现示例： 123protected boolean removeEldestEntry(Map.Entry eldest)&#123; return size() &gt; MAX_SIZE;&#125; LinkedHashMap重写了newNode()等函数，以初始化或连接节点到它内部的双向链表： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 // 链接节点p到链表尾部（或初始化链表）private void linkNodeLast(LinkedHashMap.Entry&lt;K,V&gt; p) &#123; LinkedHashMap.Entry&lt;K,V&gt; last = tail; tail = p; if (last == null) head = p; else &#123; p.before = last; last.after = p; &#125;&#125;// 用dst替换掉srcprivate void transferLinks(LinkedHashMap.Entry&lt;K,V&gt; src, LinkedHashMap.Entry&lt;K,V&gt; dst) &#123; LinkedHashMap.Entry&lt;K,V&gt; b = dst.before = src.before; LinkedHashMap.Entry&lt;K,V&gt; a = dst.after = src.after; // src是头节点 if (b == null) head = dst; else b.after = dst; // src是尾节点 if (a == null) tail = dst; else a.before = dst;&#125; Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = new LinkedHashMap.Entry&lt;K,V&gt;(hash, key, value, e); linkNodeLast(p); return p;&#125;Node&lt;K,V&gt; replacementNode(Node&lt;K,V&gt; p, Node&lt;K,V&gt; next) &#123; LinkedHashMap.Entry&lt;K,V&gt; q = (LinkedHashMap.Entry&lt;K,V&gt;)p; LinkedHashMap.Entry&lt;K,V&gt; t = new LinkedHashMap.Entry&lt;K,V&gt;(q.hash, q.key, q.value, next); transferLinks(q, t); return t;&#125;TreeNode&lt;K,V&gt; newTreeNode(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(hash, key, value, next); linkNodeLast(p); return p;&#125;TreeNode&lt;K,V&gt; replacementTreeNode(Node&lt;K,V&gt; p, Node&lt;K,V&gt; next) &#123; LinkedHashMap.Entry&lt;K,V&gt; q = (LinkedHashMap.Entry&lt;K,V&gt;)p; TreeNode&lt;K,V&gt; t = new TreeNode&lt;K,V&gt;(q.hash, q.key, q.value, next); transferLinks(q, t); return t;&#125; 遍历LinkedHashMap所需要的时间与Entry数量成正比，这是因为迭代器直接对双向链表进行迭代，而链表中只会含有Entry节点。迭代的顺序是从头节点开始一直到尾节点，插入操作会将新节点链接到尾部，所以保证了插入顺序，而访问顺序会通过afterNodeAccess()来保证，访问次数越多的节点越接近尾部。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253abstract class LinkedHashIterator &#123; LinkedHashMap.Entry&lt;K,V&gt; next; LinkedHashMap.Entry&lt;K,V&gt; current; int expectedModCount; LinkedHashIterator() &#123; next = head; expectedModCount = modCount; current = null; &#125; public final boolean hasNext() &#123; return next != null; &#125; final LinkedHashMap.Entry&lt;K,V&gt; nextNode() &#123; LinkedHashMap.Entry&lt;K,V&gt; e = next; if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (e == null) throw new NoSuchElementException(); current = e; next = e.after; return e; &#125; public final void remove() &#123; Node&lt;K,V&gt; p = current; if (p == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); current = null; K key = p.key; removeNode(hash(key), key, null, false, false); expectedModCount = modCount; &#125;&#125;final class LinkedKeyIterator extends LinkedHashIterator implements Iterator&lt;K&gt; &#123; public final K next() &#123; return nextNode().getKey(); &#125;&#125;final class LinkedValueIterator extends LinkedHashIterator implements Iterator&lt;V&gt; &#123; public final V next() &#123; return nextNode().value; &#125;&#125;final class LinkedEntryIterator extends LinkedHashIterator implements Iterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public final Map.Entry&lt;K,V&gt; next() &#123; return nextNode(); &#125;&#125; ConcurrentHashMap 我们上述所讲的Map都是非线程安全的，这意味着不应该在多个线程中对这些Map进行修改操作，轻则会产生数据不一致的问题，甚至还会因为并发插入元素而导致链表成环（插入会触发扩容，而扩容操作需要将原数组中的元素rehash到新数组，这时并发操作就有可能产生链表的循环引用从而成环），这样在查找时就会发生死循环，影响到整个应用程序。 Collections.synchronizedMap(Map&lt;K,V&gt; m)可以将一个Map转换成线程安全的实现，其实也就是通过一个包装类，然后把所有功能都委托给传入的Map实现，而且包装类是基于synchronized关键字来保证线程安全的（时代的眼泪Hashtable也是基于synchronized关键字），底层使用的是互斥锁（同一时间内只能由持有锁的线程访问，其他竞争线程进入睡眠状态），性能与吞吐量差强人意。 123456789101112131415161718192021222324252627282930public static &lt;K,V&gt; Map&lt;K,V&gt; synchronizedMap(Map&lt;K,V&gt; m) &#123; return new SynchronizedMap&lt;&gt;(m);&#125;private static class SynchronizedMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Serializable &#123; private static final long serialVersionUID = 1978198479659022715L; private final Map&lt;K,V&gt; m; // Backing Map final Object mutex; // Object on which to synchronize SynchronizedMap(Map&lt;K,V&gt; m) &#123; this.m = Objects.requireNonNull(m); mutex = this; &#125; SynchronizedMap(Map&lt;K,V&gt; m, Object mutex) &#123; this.m = m; this.mutex = mutex; &#125; public int size() &#123; synchronized (mutex) &#123;return m.size();&#125; &#125; public boolean isEmpty() &#123; synchronized (mutex) &#123;return m.isEmpty();&#125; &#125; ............&#125; 然而ConcurrentHashMap的实现细节远没有这么简单，因此性能也要高上许多。它没有使用一个全局锁来锁住自己，而是采用了减少锁粒度的方法，尽量减少因为竞争锁而导致的阻塞与冲突，而且ConcurrentHashMap的检索操作是不需要锁的。 在Java 7中，ConcurrentHashMap把内部细分成了若干个小的HashMap，称之为段（Segment），默认被分为16个段。对于一个写操作而言，会先根据hash code进行寻址，得出该Entry应被存放在哪一个Segment，然后只要对该Segment加锁即可。 理想情况下，一个默认的ConcurrentHashMap可以同时接受16个线程进行写操作（如果都是对不同Segment进行操作的话）。 分段锁对于size()这样的全局操作来说就没有任何作用了，想要得出Entry的数量就需要遍历所有Segment，获得所有的锁，然后再统计总数。事实上，ConcurrentHashMap会先试图使用无锁的方式统计总数，这个尝试会进行3次，如果在相邻的2次计算中获得的Segment的modCount次数一致，代表这两次计算过程中都没有发生过修改操作，那么就可以当做最终结果返回，否则，就要获得所有Segment的锁，重新计算size。 本文主要讨论的是Java 8的ConcurrentHashMap，它与Java 7的实现差别较大。完全放弃了段的设计，而是变回与HashMap相似的设计，使用buckets数组与分离链接法（同样会在超过阈值时树化，对于构造红黑树的逻辑与HashMap差别不大，只不过需要额外使用CAS来保证线程安全），锁的粒度也被细分到每个数组元素（个人认为这样做的原因是因为HashMap在Java 8中也实现了不少优化，即使碰撞严重，也能保证一定的性能，而且Segment不仅臃肿还有弱一致性的问题存在），所以它的并发级别与数组长度相关（Java 7则是与段数相关）。 12345/** * The array of bins. Lazily initialized upon first insertion. * Size is always a power of two. Accessed directly by iterators. */transient volatile Node&lt;K,V&gt;[] table; 寻址 ConcurrentHashMap的散列函数与HashMap并没有什么区别，同样是把key的hash code的高16位与低16位进行异或运算（因为ConcurrentHashMap的buckets数组长度也永远是一个2的N次方），然后将扰乱后的hash code与数组的长度减一（实际可访问到的最大索引）进行与运算，得出的结果即是目标所在的位置。 1234567// 2^31 - 1，int类型的最大值// 该掩码表示节点hash的可用位，用来保证hash永远为一个正整数static final int HASH_BITS = 0x7fffffff;static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; 下面是查找操作的源码，实现比较简单。 1234567891011121314151617181920212223public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; if ((eh = e.hash) == h) &#123; // 先尝试判断链表头是否为目标，如果是就直接返回 if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; else if (eh &lt; 0) // eh &lt; 0代表这是一个特殊节点（TreeBin或ForwardingNode） // 所以直接调用find()进行遍历查找 return (p = e.find(h, key)) != null ? p.val : null; // 遍历链表 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 一个普通的节点（链表节点）的hash不可能小于0（已经在spread()函数中修正过了），所以小于0的只可能是一个特殊节点，它不能用while循环中遍历链表的方式来进行遍历。 TreeBin是红黑树的头部节点（红黑树的节点为TreeNode），它本身不含有key与value，而是指向一个TreeNode节点的链表与它们的根节点，同时使用CAS（ConcurrentHashMap并不是完全基于互斥锁实现的，而是与CAS这种乐观策略搭配使用，以提高性能）实现了一个读写锁，迫使Writer（持有这个锁）在树重构操作之前等待Reader完成。 ForwardingNode是一个在数据转移过程（由扩容引起）中使用的临时节点，它会被插入到头部。它与TreeBin（和TreeNode）都是Node类的子类。 为了判断出哪些是特殊节点，TreeBin和ForwardingNode的hash域都只是一个虚拟值： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final V setValue(V value) &#123; throw new UnsupportedOperationException(); &#125; ...... /** * Virtualized support for map.get(); overridden in subclasses. */ Node&lt;K,V&gt; find(int h, Object k) &#123; Node&lt;K,V&gt; e = this; if (k != null) &#123; do &#123; K ek; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; &#125; while ((e = e.next) != null); &#125; return null; &#125;&#125;/* * Encodings for Node hash fields. See above for explanation. */static final int MOVED = -1; // hash for forwarding nodesstatic final int TREEBIN = -2; // hash for roots of treesstatic final int RESERVED = -3; // hash for transient reservations static final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; .... TreeBin(TreeNode&lt;K,V&gt; b) &#123; super(TREEBIN, null, null, null); .... &#125; .... &#125;static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125; .....&#125; 可见性 我们在get()函数中并没有发现任何与锁相关的代码，那么它是怎么保证线程安全的呢？一个操作ConcurrentHashMap.get(&quot;a&quot;)，它的步骤基本分为以下几步： 根据散列函数计算出的索引访问table。 从table中取出头节点。 遍历头节点直到找到目标节点。 从目标节点中取出value并返回。 所以只要保证访问table与节点的操作总是能够返回最新的数据就可以了。ConcurrentHashMap并没有采用锁的方式，而是通过volatile关键字来保证它们的可见性。在上文贴出的代码中可以发现，table、Node.val和Node.next都是被volatile关键字所修饰的。 volatile关键字保证了多线程环境下变量的可见性与有序性，底层实现基于内存屏障（Memory Barrier）。 为了优化性能，现代CPU工作时的指令执行顺序与应用程序的代码顺序其实是不一致的（有些编译器也会进行这种优化），也就是所谓的乱序执行技术。乱序执行可以提高CPU流水线的工作效率，只要保证数据符合程序逻辑上的正确性即可（遵循happens-before原则）。不过如今是多核时代，如果随便乱序而不提供防护措施那是会出问题的。每一个cpu上都会进行乱序优化，单cpu所保证的逻辑次序可能会被其他cpu所破坏。 内存屏障就是针对此情况的防护措施。可以认为它是一个同步点（但它本身也是一条cpu指令）。例如在IA32指令集架构中引入的SFENCE指令，在该指令之前的所有写操作必须全部完成，读操作仍可以乱序执行。LFENCE指令则保证之前的所有读操作必须全部完成，另外还有粒度更粗的MFENCE指令保证之前的所有读写操作都必须全部完成。 内存屏障就像是一个保护指令顺序的栅栏，保护后面的指令不被前面的指令跨越。将内存屏障插入到写操作与读操作之间，就可以保证之后的读操作可以访问到最新的数据，因为屏障前的写操作已经把数据写回到内存（根据缓存一致性协议，不会直接写回到内存，而是改变该cpu私有缓存中的状态，然后通知给其他cpu这个缓存行已经被修改过了，之后另一个cpu在读操作时就可以发现该缓存行已经是无效的了，这时它会从其他cpu中读取最新的缓存行，然后之前的cpu才会更改状态并写回到内存）。 例如，读一个被volatile修饰的变量V总是能够从JMM（Java Memory Model）主内存中获得最新的数据。因为内存屏障的原因，每次在使用变量V（通过JVM指令use，后面说的也都是JVM中的指令而不是cpu）之前都必须先执行load指令（把从主内存中得到的数据放入到工作内存），根据JVM的规定，load指令必须发生在read指令（从主内存中读取数据）之后，所以每次访问变量V都会先从主内存中读取。相对的，写操作也因为内存屏障保证的指令顺序，每次都会直接写回到主内存。 不过volatile关键字并不能保证操作的原子性，对该变量进行并发的连续操作是非线程安全的，所幸ConcurrentHashMap只是用来确保访问到的变量是最新的，所以也不会发生什么问题。 出于性能考虑，Doug Lea（java.util.concurrent包的作者）直接通过Unsafe类来对table进行操作。 Java号称是安全的编程语言，而保证安全的代价就是牺牲程序员自由操控内存的能力。像在C/C++中可以通过操作指针变量达到操作内存的目的（其实操作的是虚拟地址），但这种灵活性在新手手中也经常会带来一些愚蠢的错误，比如内存访问越界。 Unsafe从字面意思可以看出是不安全的，它包含了许多本地方法（在JVM平台上运行的其他语言编写的程序，主要为C/C++，由JNI实现），这些方法支持了对指针的操作，所以它才被称为是不安全的。虽然不安全，但毕竟是由C/C++实现的，像一些与操作系统交互的操作肯定是快过Java的，毕竟Java与操作系统之间还隔了一层抽象（JVM），不过代价就是失去了JVM所带来的多平台可移植性（本质上也只是一个c/cpp文件，如果换了平台那就要重新编译）。 对table进行操作的函数有以下三个，都使用到了Unsafe（在java.util.concurrent包随处可见）： 12345678910111213141516171819@SuppressWarnings("unchecked")static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; // 从tab数组中获取一个引用，遵循Volatile语义 // 参数2是一个在tab中的偏移量，用来寻找目标对象 return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125;static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; // 通过CAS操作将tab数组中位于参数2偏移量位置的值替换为v // c是期望值，如果期望值与实际值不符，返回false // 否则，v会成功地被设置到目标位置，返回true return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125;static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; // 设置tab数组中位于参数2偏移量位置的值，遵循Volatile语义 U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; 如果对Unsafe感兴趣，可以参考这篇文章：Java Magic. Part 4: sun.misc.Unsafe 初始化 ConcurrentHashMap与HashMap一样是Lazy的，buckets数组会在第一次访问put()函数时进行初始化，它的默认构造函数甚至是个空函数。 12345/** * Creates a new, empty map with the default initial table size (16). */public ConcurrentHashMap() &#123;&#125; 但是有一点需要注意，ConcurrentHashMap是工作在多线程并发环境下的，如果有多个线程同时调用了put()函数该怎么办？这会导致重复初始化，所以必须要有对应的防护措施。 ConcurrentHashMap声明了一个用于控制table的初始化与扩容的实例变量sizeCtl，默认值为0。当它是一个负数的时候，代表table正处于初始化或者扩容的状态。-1表示table正在进行初始化，-N则表示当前有N-1个线程正在进行扩容。 在其他情况下，如果table还未初始化（table == null），sizeCtl表示table进行初始化的数组大小（所以从构造函数传入的initialCapacity在经过计算后会被赋给它）。如果table已经初始化过了，则表示下次触发扩容操作的阈值，算法stzeCtl = n - (n &gt;&gt;&gt; 2)，也就是n的75%，与默认负载因子（0.75）的HashMap一致。 1private transient volatile int sizeCtl; 初始化table的操作位于函数initTable()，源码如下： 1234567891011121314151617181920212223242526272829303132/** * Initializes table, using the size recorded in sizeCtl. */private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; // sizeCtl小于0，这意味着已经有其他线程进行初始化了 // 所以当前线程让出CPU时间片 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin // 否则，通过CAS操作尝试修改sizeCtl else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if ((tab = table) == null || tab.length == 0) &#123; // 默认构造函数，sizeCtl = 0，使用默认容量（16）进行初始化 // 否则，会根据sizeCtl进行初始化 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; // 计算阈值，n的75% sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; // 阈值赋给sizeCtl sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; sizeCtl是一个volatile变量，只要有一个线程CAS操作成功，sizeCtl就会被暂时地修改为-1，这样其他线程就能够根据sizeCtl得知table是否已经处于初始化状态中，最后sizeCtl会被设置成阈值，用于触发扩容操作。 扩容 ConcurrentHashMap触发扩容的时机与HashMap类似，要么是在将链表转换成红黑树时判断table数组的长度是否小于阈值（64），如果小于就进行扩容而不是树化，要么就是在添加元素的时候，判断当前Entry数量是否超过阈值，如果超过就进行扩容。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; // 小于MIN_TREEIFY_CAPACITY，进行扩容 if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) tryPresize(n &lt;&lt; 1); else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; synchronized (b) &#123; // 将链表转换成红黑树... &#125; &#125; &#125;&#125;...final V putVal(K key, V value, boolean onlyIfAbsent) &#123; ... addCount(1L, binCount); // 计数 return null;&#125;private final void addCount(long x, int check) &#123; // 计数... if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; // s(元素个数)大于等于sizeCtl，触发扩容 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; // 扩容标志位 int rs = resizeStamp(n); // sizeCtl为负数，代表正有其他线程进行扩容 if (sc &lt; 0) &#123; // 扩容已经结束，中断循环 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // 进行扩容，并设置sizeCtl，表示扩容线程 + 1 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; // 触发扩容（第一个进行扩容的线程） // 并设置sizeCtl告知其他线程 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); // 统计个数，用于循环检测是否还需要扩容 s = sumCount(); &#125; &#125;&#125; 可以看到有关sizeCtl的操作牵涉到了大量的位运算，我们先来理解这些位运算的意义。首先是resizeStamp()，该函数返回一个用于数据校验的标志位，意思是对长度为n的table进行扩容。它将n的前导零（最高有效位之前的零的数量）和1 &lt;&lt; 15做或运算，这时低16位的最高位为1，其他都为n的前导零。 1234static final int resizeStamp(int n) &#123; // RESIZE_STAMP_BITS = 16 return Integer.numberOfLeadingZeros(n) | (1 &lt;&lt; (RESIZE_STAMP_BITS - 1));&#125; 初始化sizeCtl（扩容操作被第一个线程首次进行）的算法为(rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2，首先RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS = 16，那么rs &lt;&lt; 16等于将这个标志位移动到了高16位，这时最高位为1，所以sizeCtl此时是个负数，然后加二（至于为什么是2，还记得有关sizeCtl的说明吗？1代表初始化状态，所以实际的线程个数是要减去1的）代表当前有一个线程正在进行扩容， 这样sizeCtl就被分割成了两部分，高16位是一个对n的数据校验的标志位，低16位表示参与扩容操作的线程个数 + 1。 可能会有读者有所疑惑，更新进行扩容的线程数量的操作为什么是sc + 1而不是sc - 1，这是因为对sizeCtl的操作都是基于位运算的，所以不会关心它本身的数值是多少，只关心它在二进制上的数值，而sc + 1会在低16位上加1。 tryPresize()函数跟addCount()的后半段逻辑类似，不断地根据sizeCtl判断当前的状态，然后选择对应的策略。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849private final void tryPresize(int size) &#123; // 对size进行修正 int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; // sizeCtl是默认值或正整数 // 代表table还未初始化 // 或还没有其他线程正在进行扩容 while ((sc = sizeCtl) &gt;= 0) &#123; Node&lt;K,V&gt;[] tab = table; int n; if (tab == null || (n = tab.length) == 0) &#123; n = (sc &gt; c) ? sc : c; // 设置sizeCtl，告诉其他线程，table现在正处于初始化状态 if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if (table == tab) &#123; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; // 计算下次触发扩容的阈值 sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; // 将阈值赋给sizeCtl sizeCtl = sc; &#125; &#125; &#125; // 没有超过阈值或者大于容量的上限，中断循环 else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; // 进行扩容，与addCount()后半段的逻辑一致 else if (tab == table) &#123; int rs = resizeStamp(n); if (sc &lt; 0) &#123; Node&lt;K,V&gt;[] nt; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); &#125; &#125;&#125; 扩容操作的核心在于数据的转移，在单线程环境下数据的转移很简单，无非就是把旧数组中的数据迁移到新的数组。但是这在多线程环境下是行不通的，需要保证线程安全性，在扩容的时候其他线程也可能正在添加元素，这时又触发了扩容怎么办？有人可能会说，这不难啊，用一个互斥锁把数据转移操作的过程锁住不就好了？这确实是一种可行的解决方法，但同样也会带来极差的吞吐量。 互斥锁会导致所有访问临界区的线程陷入阻塞状态，这会消耗额外的系统资源，内核需要保存这些线程的上下文并放到阻塞队列，持有锁的线程耗时越长，其他竞争线程就会一直被阻塞，因此吞吐量低下，导致响应时间缓慢。而且锁总是会伴随着死锁问题，一旦发生死锁，整个应用程序都会因此受到影响，所以加锁永远是最后的备选方案。 Doug Lea没有选择直接加锁，而是基于CAS实现无锁的并发同步策略，令人佩服的是他不仅没有把其他线程拒之门外，甚至还邀请它们一起来协助工作。 那么如何才能让多个线程协同工作呢？Doug Lea把整个table数组当做多个线程之间共享的任务队列，然后只需维护一个指针，当有一个线程开始进行数据转移，就会先移动指针，表示指针划过的这片bucket区域由该线程负责。 这个指针被声明为一个volatile整型变量，它的初始位置位于table的尾部，即它等于table.length，很明显这个任务队列是逆向遍历的。 1234567891011121314/** * The next table index (plus one) to split while resizing. */private transient volatile int transferIndex;/** * 一个线程需要负责的最小bucket数 */private static final int MIN_TRANSFER_STRIDE = 16; /** * The next table to use; non-null only while resizing. */private transient volatile Node&lt;K,V&gt;[] nextTable; 一个已经迁移完毕的bucket会被替换成ForwardingNode节点，用来标记此bucket已经被其他线程迁移完毕了。我们之前提到过ForwardingNode，它是一个特殊节点，可以通过hash域的虚拟值来识别它，它同样重写了find()函数，用来在新数组中查找目标。 数据迁移的操作位于transfer()函数，多个线程之间依靠sizeCtl与transferIndex指针来协同工作，每个线程都有自己负责的区域，一个完成迁移的bucket会被设置为ForwardingNode，其他线程遇见这个特殊节点就跳过该bucket，处理下一个bucket。 transfer()函数可以大致分为三部分，第一部分对后续需要使用的变量进行初始化： 123456789101112131415161718192021222324252627/** * Moves and/or copies the nodes in each bin to new table. See * above for explanation. */private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; // 根据当前机器的CPU数量来决定每个线程负责的bucket数 // 避免因为扩容线程过多，反而影响到性能 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // 初始化nextTab，容量为旧数组的一倍 if (nextTab == null) &#123; // initiating try &#123; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; // 初始化指针 &#125; int nextn = nextTab.length; ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab 第二部分为当前线程分配任务和控制当前线程的任务进度，这部分是transfer()的核心逻辑，描述了如何与其他线程协同工作： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// i指向当前bucket，bound表示当前线程所负责的bucket区域的边界for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; // 这个循环使用CAS不断尝试为当前线程分配任务 // 直到分配成功或任务队列已经被全部分配完毕 // 如果当前线程已经被分配过bucket区域 // 那么会通过--i指向下一个待处理bucket然后退出该循环 while (advance) &#123; int nextIndex, nextBound; // --i表示将i指向下一个待处理的bucket // 如果--i &gt;= bound，代表当前线程已经分配过bucket区域 // 并且还留有未处理的bucket if (--i &gt;= bound || finishing) advance = false; // transferIndex指针 &lt;= 0 表示所有bucket已经被分配完毕 else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; // 移动transferIndex指针 // 为当前线程设置所负责的bucket区域的范围 // i指向该范围的第一个bucket，注意i是逆向遍历的 // 这个范围为(bound, i)，i是该区域最后一个bucket，遍历顺序是逆向的 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; // 当前线程已经处理完了所负责的所有bucket if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; // 如果任务队列已经全部完成 if (finishing) &#123; nextTable = null; table = nextTab; // 设置新的阈值 sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; // 工作中的扩容线程数量减1 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; // (resizeStamp &lt;&lt; RESIZE_STAMP_SHIFT) + 2代表当前有一个扩容线程 // 相对的，(sc - 2) != resizeStamp &lt;&lt; RESIZE_STAMP_SHIFT // 表示当前还有其他线程正在进行扩容，所以直接返回 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; // 否则，当前线程就是最后一个进行扩容的线程 // 设置finishing标识 finishing = advance = true; i = n; // recheck before commit &#125; &#125; // 如果待处理bucket是空的 // 那么插入ForwardingNode，以通知其他线程 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // 如果待处理bucket的头节点是ForwardingNode // 说明此bucket已经被处理过了，跳过该bucket else if ((fh = f.hash) == MOVED) advance = true; // already processed 最后一部分是具体的迁移过程（对当前指向的bucket），这部分的逻辑与HashMap类似，拿旧数组的容量当做一个掩码，然后与节点的hash进行与操作，可以得出该节点的新增有效位，如果新增有效位为0就放入一个链表A，如果为1就放入另一个链表B，链表A在新数组中的位置不变（跟在旧数组的索引一致），链表B在新数组中的位置为原索引加上旧数组容量。 这个方法减少了rehash的计算量，而且还能达到均匀分布的目的，如果不能理解请去看本文中HashMap扩容操作的解释。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687else &#123; // 对于节点的操作还是要加上锁的 // 不过这个锁的粒度很小，只锁住了bucket的头节点 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; // hash code不为负，代表这是条链表 if (fh &gt;= 0) &#123; // fh &amp; n 获得hash code的新增有效位，用于将链表分离成两类 // 要么是0要么是1，关于这个位运算的更多细节 // 请看本文中有关HashMap扩容操作的解释 int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; // 这个循环用于记录最后一段连续的同一类节点 // 这个类别是通过fh &amp; n来区分的 // 这段连续的同类节点直接被复用，不会产生额外的复制 for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; // 0被放入ln链表，1被放入hn链表 // lastRun是连续同类节点的起始节点 if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; // 将最后一段的连续同类节点之前的节点按类别复制到ln或hn // 链表的插入方向是往头部插入的，Node构造函数的第四个参数是next // 所以就算遇到类别与lastRun一致的节点也只会被插入到头部 for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; // ln链表被放入到原索引位置，hn放入到原索引 + 旧数组容量 // 这一点与HashMap一致，如果看不懂请去参考本文对HashMap扩容的讲解 setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); // 标记该bucket已被处理 advance = true; &#125; // 对红黑树的操作，逻辑与链表一样，按新增有效位进行分类 else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; // 元素数量没有超过UNTREEIFY_THRESHOLD，退化成链表 ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; 计数 在Java 7中ConcurrentHashMap对每个Segment单独计数，想要得到总数就需要获得所有Segment的锁，然后进行统计。由于Java 8抛弃了Segment，显然是不能再这样做了，而且这种方法虽然简单准确但也舍弃了性能。 Java 8声明了一个volatile变量baseCount用于记录元素的个数，对这个变量的修改操作是基于CAS的，每当插入元素或删除元素时都会调用addCount()函数进行计数。 1234567891011121314151617181920212223242526272829private transient volatile long baseCount;private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; // 尝试使用CAS更新baseCount失败 // 转用CounterCells进行更新 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; boolean uncontended = true; // 在CounterCells未初始化 // 或尝试通过CAS更新当前线程的CounterCell失败时 // 调用fullAddCount()，该函数负责初始化CounterCells和更新计数 if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; fullAddCount(x, uncontended); return; &#125; if (check &lt;= 1) return; // 统计总数 s = sumCount(); &#125; if (check &gt;= 0) &#123; // 判断是否需要扩容，在上文中已经讲过了 &#125;&#125; counterCells是一个元素为CounterCell的数组，该数组的大小与当前机器的CPU数量有关，并且它不会被主动初始化，只有在调用fullAddCount()函数时才会进行初始化。 CounterCell是一个简单的内部静态类，每个CounterCell都是一个用于记录数量的单元： 12345678910111213/** * Table of counter cells. When non-null, size is a power of 2. */private transient volatile CounterCell[] counterCells;/** * A padded cell for distributing counts. Adapted from LongAdder * and Striped64. See their internal docs for explanation. */@sun.misc.Contended static final class CounterCell &#123; volatile long value; CounterCell(long x) &#123; value = x; &#125;&#125; 注解@sun.misc.Contended用于解决伪共享问题。所谓伪共享，即是在同一缓存行（CPU缓存的基本单位）中存储了多个变量，当其中一个变量被修改时，就会影响到同一缓存行内的其他变量，导致它们也要跟着被标记为失效，其他变量的缓存命中率将会受到影响。解决伪共享问题的方法一般是对该变量填充一些无意义的占位数据，从而使它独享一个缓存行。 ConcurrentHashMap的计数设计与LongAdder类似。在一个低并发的情况下，就只是简单地使用CAS操作来对baseCount进行更新，但只要这个CAS操作失败一次，就代表有多个线程正在竞争，那么就转而使用CounterCell数组进行计数，数组内的每个ConuterCell都是一个独立的计数单元。 每个线程都会通过ThreadLocalRandom.getProbe() &amp; m寻址找到属于它的CounterCell，然后进行计数。ThreadLocalRandom是一个线程私有的伪随机数生成器，每个线程的probe都是不同的（这点基于ThreadLocalRandom的内部实现，它在内部维护了一个probeGenerator，这是一个类型为AtomicInteger的静态常量，每当初始化一个ThreadLocalRandom时probeGenerator都会先自增一个常量然后返回的整数即为当前线程的probe，probe变量被维护在Thread对象中），可以认为每个线程的probe就是它在CounterCell数组中的hash code。 这种方法将竞争数据按照线程的粒度进行分离，相比所有竞争线程对一个共享变量使用CAS不断尝试在性能上要效率多了，这也是为什么在高并发环境下LongAdder要优于AtomicInteger的原因。 fullAddCount()函数根据当前线程的probe寻找对应的CounterCell进行计数，如果CounterCell数组未被初始化，则初始化CounterCell数组和CounterCell。该函数的实现与Striped64类（LongAdder的父类）的longAccumulate()函数是一样的，把CounterCell数组当成一个散列表，每个线程的probe就是hash code，散列函数也仅仅是简单的(n - 1) &amp; probe。 CounterCell数组的大小永远是一个2的n次方，初始容量为2，每次扩容的新容量都是之前容量乘以二，处于性能考虑，它的最大容量上限是机器的CPU数量。 所以说CounterCell数组的碰撞冲突是很严重的，因为它的bucket基数太小了。而发生碰撞就代表着一个CounterCell会被多个线程竞争，为了解决这个问题，Doug Lea使用无限循环加上CAS来模拟出一个自旋锁来保证线程安全，自旋锁的实现基于一个被volatile修饰的整数变量，该变量只会有两种状态：0和1，当它被设置为0时表示没有加锁，当它被设置为1时表示已被其他线程加锁。这个自旋锁用于保护初始化CounterCell、初始化CounterCell数组以及对CounterCell数组进行扩容时的安全。 CounterCell更新计数是依赖于CAS的，每次循环都会尝试通过CAS进行更新，如果成功就退出无限循环，否则就调用ThreadLocalRandom.advanceProbe()函数为当前线程更新probe，然后重新开始循环，以期望下一次寻址到的CounterCell没有被其他线程竞争。 如果连着两次CAS更新都没有成功，那么会对CounterCell数组进行一次扩容，这个扩容操作只会在当前循环中触发一次，而且只能在容量小于上限时触发。 fullAddCount()函数的主要流程如下： 首先检查当前线程有没有初始化过ThreadLocalRandom，如果没有则进行初始化。ThreadLocalRandom负责更新线程的probe，而probe又是在数组中进行寻址的关键。 检查CounterCell数组是否已经初始化，如果已初始化，那么就根据probe找到对应的CounterCell。 如果这个CounterCell等于null，需要先初始化CounterCell，通过把计数增量传入构造函数，所以初始化只要成功就说明更新计数已经完成了。初始化的过程需要获取自旋锁。 如果不为null，就按上文所说的逻辑对CounterCell实施更新计数。 CounterCell数组未被初始化，尝试获取自旋锁，进行初始化。数组初始化的过程会附带初始化一个CounterCell来记录计数增量，所以只要初始化成功就表示更新计数完成。 如果自旋锁被其他线程占用，无法进行数组的初始化，只好通过CAS更新baseCount。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126private final void fullAddCount(long x, boolean wasUncontended) &#123; int h; // 当前线程的probe等于0，证明该线程的ThreadLocalRandom还未被初始化 // 以及当前线程是第一次进入该函数 if ((h = ThreadLocalRandom.getProbe()) == 0) &#123; // 初始化ThreadLocalRandom，当前线程会被设置一个probe ThreadLocalRandom.localInit(); // force initialization // probe用于在CounterCell数组中寻址 h = ThreadLocalRandom.getProbe(); // 未竞争标志 wasUncontended = true; &#125; // 冲突标志 boolean collide = false; // True if last slot nonempty for (;;) &#123; CounterCell[] as; CounterCell a; int n; long v; // CounterCell数组已初始化 if ((as = counterCells) != null &amp;&amp; (n = as.length) &gt; 0) &#123; // 如果寻址到的Cell为空，那么创建一个新的Cell if ((a = as[(n - 1) &amp; h]) == null) &#123; // cellsBusy是一个只有0和1两个状态的volatile整数 // 它被当做一个自旋锁，0代表无锁，1代表加锁 if (cellsBusy == 0) &#123; // Try to attach new Cell // 将传入的x作为初始值创建一个新的CounterCell CounterCell r = new CounterCell(x); // Optimistic create // 通过CAS尝试对自旋锁加锁 if (cellsBusy == 0 &amp;&amp; U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) &#123; // 加锁成功，声明Cell是否创建成功的标志 boolean created = false; try &#123; // Recheck under lock CounterCell[] rs; int m, j; // 再次检查CounterCell数组是否不为空 // 并且寻址到的Cell为空 if ((rs = counterCells) != null &amp;&amp; (m = rs.length) &gt; 0 &amp;&amp; rs[j = (m - 1) &amp; h] == null) &#123; // 将之前创建的新Cell放入数组 rs[j] = r; created = true; &#125; &#125; finally &#123; // 释放锁 cellsBusy = 0; &#125; // 如果已经创建成功，中断循环 // 因为新Cell的初始值就是传入的增量，所以计数已经完毕了 if (created) break; // 如果未成功 // 代表as[(n - 1) &amp; h]这个位置的Cell已经被其他线程设置 // 那么就从循环头重新开始 continue; // Slot is now non-empty &#125; &#125; collide = false; &#125; // as[(n - 1) &amp; h]非空 // 在addCount()函数中通过CAS更新当前线程的Cell进行计数失败 // 会传入wasUncontended = false，代表已经有其他线程进行竞争 else if (!wasUncontended) // CAS already known to fail // 设置未竞争标志，之后会重新计算probe，然后重新执行循环 wasUncontended = true; // Continue after rehash // 尝试进行计数，如果成功，那么就退出循环 else if (U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x)) break; // 尝试更新失败，检查counterCell数组是否已经扩容 // 或者容量达到最大值（CPU的数量） else if (counterCells != as || n &gt;= NCPU) // 设置冲突标志，防止跳入下面的扩容分支 // 之后会重新计算probe collide = false; // At max size or stale // 设置冲突标志，重新执行循环 // 如果下次循环执行到该分支，并且冲突标志仍然为true // 那么会跳过该分支，到下一个分支进行扩容 else if (!collide) collide = true; // 尝试加锁，然后对counterCells数组进行扩容 else if (cellsBusy == 0 &amp;&amp; U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) &#123; try &#123; // 检查是否已被扩容 if (counterCells == as) &#123;// Expand table unless stale // 新数组容量为之前的1倍 CounterCell[] rs = new CounterCell[n &lt;&lt; 1]; // 迁移数据到新数组 for (int i = 0; i &lt; n; ++i) rs[i] = as[i]; counterCells = rs; &#125; &#125; finally &#123; // 释放锁 cellsBusy = 0; &#125; collide = false; // 重新执行循环 continue; // Retry with expanded table &#125; // 为当前线程重新计算probe h = ThreadLocalRandom.advanceProbe(h); &#125; // CounterCell数组未初始化，尝试获取自旋锁，然后进行初始化 else if (cellsBusy == 0 &amp;&amp; counterCells == as &amp;&amp; U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) &#123; boolean init = false; try &#123; // Initialize table if (counterCells == as) &#123; // 初始化CounterCell数组，初始容量为2 CounterCell[] rs = new CounterCell[2]; // 初始化CounterCell rs[h &amp; 1] = new CounterCell(x); counterCells = rs; init = true; &#125; &#125; finally &#123; cellsBusy = 0; &#125; // 初始化CounterCell数组成功，退出循环 if (init) break; &#125; // 如果自旋锁被占用，则只好尝试更新baseCount else if (U.compareAndSwapLong(this, BASECOUNT, v = baseCount, v + x)) break; // Fall back on using base &#125;&#125; 对于统计总数，只要能够理解CounterCell的思想，就很简单了。仔细想一想，每次计数的更新都会被分摊在baseCount和CounterCell数组中的某一CounterCell，想要获得总数，把它们统计相加就是了。 123456789101112131415161718public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n);&#125; final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum;&#125; 其实size()函数返回的总数可能并不是百分百精确的，试想如果前一个遍历过的CounterCell又进行了更新会怎么样？尽管只是一个估算值，但在大多数场景下都还能接受，而且性能上是要比Java 7好上太多了。 添加元素 添加元素的主要逻辑与HashMap没什么区别，有所区别的复杂操作如扩容和计数我们上文都已经深入解析过了，所以整体来说putVal()函数还是比较简单的，可能唯一需要注意的就是在对节点进行操作的时候需要通过互斥锁保证线程安全，这个互斥锁的粒度很小，只对需要操作的这个bucket加锁。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public V put(K key, V value) &#123; return putVal(key, value, false);&#125;/** Implementation for put and putIfAbsent */final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; // 节点计数器，用于判断是否需要树化 // 无限循环+CAS，无锁的标准套路 for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; // 初始化table if (tab == null || (n = tab.length) == 0) tab = initTable(); // bucket为null，通过CAS创建头节点，如果成功就结束循环 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; // bucket为ForwardingNode // 当前线程前去协助进行扩容 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; V oldVal = null; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; // 节点是链表 if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; // 找到目标，设置value if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; // 未找到节点，插入新节点到链表尾部 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; // 节点是红黑树 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; // 根据bucket中的节点数决定是否树化 if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); // oldVal不等于null，说明没有新节点 // 所以直接返回，不进行计数 if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // 计数 addCount(1L, binCount); return null;&#125; 至于删除元素的操作位于函数replaceNode(Object key, V value, Object cv)，当table[key].val等于期望值cv时（或cv等于null），更新节点的值为value，如果value等于null，那么删除该节点。 remove()函数通过调用replaceNode(key, null, null)来达成删除目标节点的目的，replaceNode()的具体实现与putVal()没什么差别，只不过对链表的操作有所不同而已，所以就不多叙述了。 并行计算 Java 8除了对ConcurrentHashMap重新设计以外，还引入了基于Lambda表达式的Stream API。它是对集合对象功能上的增强（所以不止ConcurrentHashMap，其他集合也都实现了该API），以一种优雅的方式来批量操作、聚合或遍历集合中的数据。 最重要的是，它还提供了并行模式，充分利用了多核CPU的优势实现并行计算。让我们看看如下的示例代码： 12345678910public static void main(String[] args) &#123; ConcurrentHashMap&lt;String, Integer&gt; map = new ConcurrentHashMap&lt;&gt;(); String keys = "ABCDEFG"; for (int i = 1; i &lt;= keys.length(); i++) &#123; map.put(String.valueOf(keys.charAt(i - 1)), i); &#125; map.forEach(2, (k, v) -&gt; System.out.println("key-" + k + ":value-" + v + ". by thread-&gt;" + Thread.currentThread().getName()));&#125; 这段代码通过两个线程（包括主线程）并行地遍历map中的元素，然后输出到控制台，输出如下： 1234567key-A:value-1. by thread-&gt;mainkey-D:value-4. by thread-&gt;ForkJoinPool.commonPool-worker-2key-B:value-2. by thread-&gt;mainkey-E:value-5. by thread-&gt;ForkJoinPool.commonPool-worker-2key-C:value-3. by thread-&gt;mainkey-F:value-6. by thread-&gt;ForkJoinPool.commonPool-worker-2key-G:value-7. by thread-&gt;ForkJoinPool.commonPool-worker-2 很明显，有两个线程在进行工作，那么这是怎么实现的呢？我们先来看看forEach()函数： 1234567public void forEach(long parallelismThreshold, BiConsumer&lt;? super K,? super V&gt; action) &#123; if (action == null) throw new NullPointerException(); new ForEachMappingTask&lt;K,V&gt; (null, batchFor(parallelismThreshold), 0, 0, table, action).invoke();&#125; parallelismThreshold是需要并行执行该操作的线程数量，action则是回调函数（我们想要执行的操作）。action的类型为BiConsumer，是一个用于支持Lambda表达式的FunctionalInterface，它接受两个输入参数并返回0个结果。 12345678910@FunctionalInterfacepublic interface BiConsumer&lt;T, U&gt; &#123; /** * Performs this operation on the given arguments. * * @param t the first input argument * @param u the second input argument */ void accept(T t, U u); 看来实现并行计算的关键在于ForEachMappingTask对象，通过它的继承关系结构图可以发现，ForEachMappingTask其实就是ForkJoinTask。 集合的并行计算是基于Fork/Join框架实现的，工作线程交由ForkJoinPool线程池维护。它推崇分而治之的思想，将一个大的任务分解成多个小的任务，通过fork()函数（有点像Linux的fork()系统调用来创建子进程）来开启一个工作线程执行其中一个小任务，通过join()函数等待工作线程执行完毕（需要等所有工作线程执行完毕才能合并最终结果），只要所有的小任务都已经处理完成，就代表这个大的任务也完成了。 像上文中的示例代码就是将遍历这个大任务分解成了N个小任务，然后交由两个工作线程进行处理。 1234567891011121314151617181920212223242526272829303132static final class ForEachMappingTask&lt;K,V&gt; extends BulkTask&lt;K,V,Void&gt; &#123; final BiConsumer&lt;? super K, ? super V&gt; action; ForEachMappingTask (BulkTask&lt;K,V,?&gt; p, int b, int i, int f, Node&lt;K,V&gt;[] t, BiConsumer&lt;? super K,? super V&gt; action) &#123; super(p, b, i, f, t); this.action = action; &#125; public final void compute() &#123; final BiConsumer&lt;? super K, ? super V&gt; action; if ((action = this.action) != null) &#123; for (int i = baseIndex, f, h; batch &gt; 0 &amp;&amp; (h = ((f = baseLimit) + i) &gt;&gt;&gt; 1) &gt; i;) &#123; // 记录待完成任务的数量 addToPendingCount(1); // 开启一个工作线程执行任务 // 其余参数是任务的区间以及table和回调函数 new ForEachMappingTask&lt;K,V&gt; (this, batch &gt;&gt;&gt;= 1, baseLimit = h, f, tab, action).fork(); &#125; for (Node&lt;K,V&gt; p; (p = advance()) != null; ) // 调用回调函数 action.accept(p.key, p.val); // 与addToPendingCount()相反 // 它会减少待完成任务的计数器 // 如果该计数器为0，代表所有任务已经完成了 propagateCompletion(); &#125; &#125;&#125; 其他并行计算函数的实现也都差不多，只不过具体的Task实现不同，例如search()： 1234567public &lt;U&gt; U search(long parallelismThreshold, BiFunction&lt;? super K, ? super V, ? extends U&gt; searchFunction) &#123; if (searchFunction == null) throw new NullPointerException(); return new SearchMappingsTask&lt;K,V,U&gt; (null, batchFor(parallelismThreshold), 0, 0, table, searchFunction, new AtomicReference&lt;U&gt;()).invoke();&#125; 为了节省篇幅（说实话现在似乎很少有人能耐心看完一篇长文(:з」∠)），有关Stream API是如何使用Fork/Join框架进行工作以及实现细节就不多讲了，以后有机会再说吧。 参考文献 Associative array - Wikiwand Hash table - Wikiwand Hash function - Wikiwand ConcurrentHashMap (Java Platform SE 8 ) LongAdder (Java Platform SE 8 ) Java Magic. Part 4: sun.misc.Unsafe ConcurrentHashMap in Java 8 - DZone Java]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>数据结构</tag>
        <tag>后端</tag>
        <tag>2018</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot自动配置的"魔法"是如何实现的？]]></title>
    <url>%2F2018%2F01%2F08%2F2018-01-08-spring_boot_auto_configure%2F</url>
    <content type="text"><![CDATA[Spring Boot是Spring旗下众多的子项目之一，其理念是约定优于配置，它通过实现了自动配置（大多数用户平时习惯设置的配置作为默认配置）的功能来为用户快速构建出标准化的应用。Spring Boot的特点可以概述为如下几点： 内置了嵌入式的Tomcat、Jetty等Servlet容器，应用可以不用打包成War格式，而是可以直接以Jar格式运行。 提供了多个可选择的”starter”以简化Maven的依赖管理（也支持Gradle），让您可以按需加载需要的功能模块。 尽可能地进行自动配置，减少了用户需要动手写的各种冗余配置项，Spring Boot提倡无XML配置文件的理念，使用Spring Boot生成的应用完全不会生成任何配置代码与XML配置文件。 提供了一整套的对应用状态的监控与管理的功能模块（通过引入spring-boot-starter-actuator），包括应用的线程信息、内存信息、应用是否处于健康状态等，为了满足更多的资源监控需求，Spring Cloud中的很多模块还对其进行了扩展。 有关Spring Boot的使用方法就不做多介绍了，如有兴趣请自行阅读官方文档Spring Boot或其他文章。 如今微服务的概念愈来愈热，转型或尝试微服务的团队也在如日渐增，而对于技术选型，Spring Cloud是一个比较好的选择，它提供了一站式的分布式系统解决方案，包含了许多构建分布式系统与微服务需要用到的组件，例如服务治理、API网关、配置中心、消息总线以及容错管理等模块。可以说，Spring Cloud”全家桶”极其适合刚刚接触微服务的团队。似乎有点跑题了，不过说了这么多，我想要强调的是，Spring Cloud中的每个组件都是基于Spring Boot构建的，而理解了Spring Boot的自动配置的原理，显然也是有好处的。 Spring Boot的自动配置看起来神奇，其实原理非常简单，背后全依赖于@Conditional注解来实现的。 本文作者为SylvanasSun(sylvanas.sun@gmail.com)，首发于SylvanasSun’s Blog。原文链接：https://sylvanassun.github.io/2018/01/08/2018-01-08-spring_boot_auto_configure/（转载请务必保留本段声明，并且保留超链接。） 什么是@Conditional？ @Conditional是由Spring 4提供的一个新特性，用于根据特定条件来控制Bean的创建行为。而在我们开发基于Spring的应用的时候，难免会需要根据条件来注册Bean。 例如，你想要根据不同的运行环境，来让Spring注册对应环境的数据源Bean，对于这种简单的情况，完全可以使用@Profile注解实现，就像下面代码所示： 1234567891011121314@Configurationpublic class AppConfig &#123; @Bean @Profile("DEV") public DataSource devDataSource() &#123; ... &#125; @Bean @Profile("PROD") public DataSource prodDataSource() &#123; ... &#125;&#125; 剩下只需要设置对应的Profile属性即可，设置方法有如下三种： 通过context.getEnvironment().setActiveProfiles(&quot;PROD&quot;)来设置Profile属性。 通过设定jvm的spring.profiles.active参数来设置环境（Spring Boot中可以直接在application.properties配置文件中设置该属性）。 通过在DispatcherServlet的初始参数中设置。 12345678&lt;servlet&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;spring.profiles.active&lt;/param-name&gt; &lt;param-value&gt;PROD&lt;/param-value&gt; &lt;/init-param&gt;&lt;/servlet&gt; 但这种方法只局限于简单的情况，而且通过源码我们可以发现@Profile自身也使用了@Conditional注解。 123456789101112131415161718192021222324252627282930313233343536373839package org.springframework.context.annotation;@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Conditional(&#123;ProfileCondition.class&#125;) // 组合了Conditional注解public @interface Profile &#123; String[] value();&#125;package org.springframework.context.annotation;class ProfileCondition implements Condition &#123; ProfileCondition() &#123; &#125; // 通过提取出@Profile注解中的value值来与profiles配置信息进行匹配 public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; if(context.getEnvironment() != null) &#123; MultiValueMap attrs = metadata.getAllAnnotationAttributes(Profile.class.getName()); if(attrs != null) &#123; Iterator var4 = ((List)attrs.get("value")).iterator(); Object value; do &#123; if(!var4.hasNext()) &#123; return false; &#125; value = var4.next(); &#125; while(!context.getEnvironment().acceptsProfiles((String[])((String[])value))); return true; &#125; &#125; return true; &#125;&#125; 在业务复杂的情况下，显然需要使用到@Conditional注解来提供更加灵活的条件判断，例如以下几个判断条件： 在类路径中是否存在这样的一个类。 在Spring容器中是否已经注册了某种类型的Bean（如未注册，我们可以让其自动注册到容器中，上一条同理）。 一个文件是否在特定的位置上。 一个特定的系统属性是否存在。 在Spring的配置文件中是否设置了某个特定的值。 举个栗子，假设我们有两个基于不同数据库实现的DAO，它们全都实现了UserDao，其中JdbcUserDAO与MySql进行连接，MongoUserDAO与MongoDB进行连接。现在，我们有了一个需求，需要根据命令行传入的系统参数来注册对应的UserDao，就像java -jar app.jar -DdbType=MySQL会注册JdbcUserDao，而java -jar app.jar -DdbType=MongoDB则会注册MongoUserDao。使用@Conditional可以很轻松地实现这个功能，仅仅需要在你自定义的条件类中去实现Condition接口，让我们来看下面的代码。（以下案例来自：https://dzone.com/articles/how-springboot-autoconfiguration-magic-works） 123456789101112131415161718192021222324252627282930313233343536373839404142434445public interface UserDAO &#123; ....&#125;public class JdbcUserDAO implements UserDAO &#123; ....&#125;public class MongoUserDAO implements UserDAO &#123; ....&#125;public class MySQLDatabaseTypeCondition implements Condition &#123; @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata metadata) &#123; String enabledDBType = System.getProperty("dbType"); // 获得系统参数 dbType // 如果该值等于MySql，则条件成立 return (enabledDBType != null &amp;&amp; enabledDBType.equalsIgnoreCase("MySql")); &#125;&#125;// 与上述逻辑一致public class MongoDBDatabaseTypeCondition implements Condition &#123; @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata metadata) &#123; String enabledDBType = System.getProperty("dbType"); return (enabledDBType != null &amp;&amp; enabledDBType.equalsIgnoreCase("MongoDB")); &#125;&#125;// 根据条件来注册不同的Bean@Configurationpublic class AppConfig &#123; @Bean @Conditional(MySQLDatabaseTypeCondition.class) public UserDAO jdbcUserDAO() &#123; return new JdbcUserDAO(); &#125; @Bean @Conditional(MongoDBDatabaseTypeCondition.class) public UserDAO mongoUserDAO() &#123; return new MongoUserDAO(); &#125;&#125; 现在，我们又有了一个新需求，我们想要根据当前工程的类路径中是否存在MongoDB的驱动类来确认是否注册MongoUserDAO。为了实现这个需求，可以创建检查MongoDB驱动是否存在的两个条件类。 1234567891011121314151617181920212223public class MongoDriverPresentsCondition implements Condition &#123; @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata metadata) &#123; try &#123; Class.forName("com.mongodb.Server"); return true; &#125; catch (ClassNotFoundException e) &#123; return false; &#125; &#125;&#125;public class MongoDriverNotPresentsCondition implements Condition &#123; @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata metadata) &#123; try &#123; Class.forName("com.mongodb.Server"); return false; &#125; catch (ClassNotFoundException e) &#123; return true; &#125; &#125;&#125; 假如，你想要在UserDAO没有被注册的情况下去注册一个UserDAOBean，那么我们可以定义一个条件类来检查某个类是否在容器中已被注册。 1234567public class UserDAOBeanNotPresentsCondition implements Condition &#123; @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata metadata) &#123; UserDAO userDAO = conditionContext.getBeanFactory().getBean(UserDAO.class); return (userDAO == null); &#125;&#125; 如果你想根据配置文件中的某项属性来决定是否注册MongoDAO，例如app.dbType是否等于MongoDB，我们可以实现以下的条件类。 1234567public class MongoDbTypePropertyCondition implements Condition &#123; @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata metadata) &#123; String dbType = conditionContext.getEnvironment().getProperty("app.dbType"); return "MONGO".equalsIgnoreCase(dbType); &#125;&#125; 我们已经尝试并实现了各种类型的条件判断，接下来，我们可以选择一种更为优雅的方式，就像@Profile一样，以注解的方式来完成条件判断。首先，我们需要定义一个注解类。 1234567@Target(&#123; ElementType.TYPE, ElementType.METHOD &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Conditional(DatabaseTypeCondition.class)public @interface DatabaseType &#123; String value();&#125; 具体的条件判断逻辑在DatabaseTypeCondition类中，它会根据系统参数dbType来判断注册哪一个Bean。 1234567891011public class DatabaseTypeCondition implements Condition &#123; @Override public boolean matches(ConditionContext conditionContext, AnnotatedTypeMetadata metadata) &#123; Map&lt;String, Object&gt; attributes = metadata .getAnnotationAttributes(DatabaseType.class.getName()); String type = (String) attributes.get("value"); // 默认值为MySql String enabledDBType = System.getProperty("dbType", "MySql"); return (enabledDBType != null &amp;&amp; type != null &amp;&amp; enabledDBType.equalsIgnoreCase(type)); &#125;&#125; 最后，在配置类应用该注解即可。 123456789101112131415@Configuration@ComponentScanpublic class AppConfig &#123; @Bean @DatabaseType("MySql") public UserDAO jdbcUserDAO() &#123; return new JdbcUserDAO(); &#125; @Bean @DatabaseType("mongoDB") public UserDAO mongoUserDAO() &#123; return new MongoUserDAO(); &#125;&#125; AutoConfigure源码分析 通过了解@Conditional注解的机制其实已经能够猜到自动配置是如何实现的了，接下来我们通过源码来看看它是怎么做的。本文中讲解的源码基于Spring Boot 1.5.9版本（最新的正式版本）。 使用过Spring Boot的童鞋应该都很清楚，它会替我们生成一个入口类，其命名规格为ArtifactNameApplication，通过这个入口类，我们可以发现一些信息。 12345678@SpringBootApplicationpublic class DemoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(DemoApplication.class, args); &#125;&#125; 首先该类被@SpringBootApplication注解修饰，我们可以先从它开始分析，查看源码后可以发现它是一个包含许多注解的组合注解。 12345678910111213141516171819202122232425262728293031323334353637383940@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@SpringBootConfiguration@EnableAutoConfiguration@ComponentScan( excludeFilters = &#123;@Filter( type = FilterType.CUSTOM, classes = &#123;TypeExcludeFilter.class&#125;), @Filter( type = FilterType.CUSTOM, classes = &#123;AutoConfigurationExcludeFilter.class&#125;)&#125;)public @interface SpringBootApplication &#123; @AliasFor( annotation = EnableAutoConfiguration.class, attribute = "exclude" ) Class&lt;?&gt;[] exclude() default &#123;&#125;; @AliasFor( annotation = EnableAutoConfiguration.class, attribute = "excludeName" ) String[] excludeName() default &#123;&#125;; @AliasFor( annotation = ComponentScan.class, attribute = "basePackages" ) String[] scanBasePackages() default &#123;&#125;; @AliasFor( annotation = ComponentScan.class, attribute = "basePackageClasses" ) Class&lt;?&gt;[] scanBasePackageClasses() default &#123;&#125;;&#125; 该注解相当于同时声明了@Configuration、@EnableAutoConfiguration与@ComponentScan三个注解（如果我们想定制自定义的自动配置实现，声明这三个注解就足够了），而@EnableAutoConfiguration是我们的关注点，从它的名字可以看出来，它是用来开启自动配置的，源码如下： 12345678910111213@Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@AutoConfigurationPackage@Import(&#123;EnableAutoConfigurationImportSelector.class&#125;)public @interface EnableAutoConfiguration &#123; String ENABLED_OVERRIDE_PROPERTY = "spring.boot.enableautoconfiguration"; Class&lt;?&gt;[] exclude() default &#123;&#125;; String[] excludeName() default &#123;&#125;;&#125; 我们发现@Import（Spring 提供的一个注解，可以导入配置类或者Bean到当前类中）导入了EnableAutoConfigurationImportSelector类，根据名字来看，它应该就是我们要找到的目标了。不过查看它的源码发现它已经被Deprecated了，而官方API中告知我们去查看它的父类AutoConfigurationImportSelector。 12345678910/** @deprecated */@Deprecatedpublic class EnableAutoConfigurationImportSelector extends AutoConfigurationImportSelector &#123; public EnableAutoConfigurationImportSelector() &#123; &#125; protected boolean isEnabled(AnnotationMetadata metadata) &#123; return this.getClass().equals(EnableAutoConfigurationImportSelector.class)?((Boolean)this.getEnvironment().getProperty("spring.boot.enableautoconfiguration", Boolean.class, Boolean.valueOf(true))).booleanValue():true; &#125;&#125; 由于AutoConfigurationImportSelector的源码太长了，这里我只截出关键的地方，显然方法selectImports是选择自动配置的主入口，它调用了其他的几个方法来加载元数据等信息，最后返回一个包含许多自动配置类信息的字符串数组。 123456789101112131415161718192021public String[] selectImports(AnnotationMetadata annotationMetadata) &#123; if(!this.isEnabled(annotationMetadata)) &#123; return NO_IMPORTS; &#125; else &#123; try &#123; AutoConfigurationMetadata ex = AutoConfigurationMetadataLoader.loadMetadata(this.beanClassLoader); AnnotationAttributes attributes = this.getAttributes(annotationMetadata); List configurations = this.getCandidateConfigurations(annotationMetadata, attributes); configurations = this.removeDuplicates(configurations); configurations = this.sort(configurations, ex); Set exclusions = this.getExclusions(annotationMetadata, attributes); this.checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = this.filter(configurations, ex); this.fireAutoConfigurationImportEvents(configurations, exclusions); return (String[])configurations.toArray(new String[configurations.size()]); &#125; catch (IOException var6) &#123; throw new IllegalStateException(var6); &#125; &#125;&#125; 重点在于方法getCandidateConfigurations()返回了自动配置类的信息列表，而它通过调用SpringFactoriesLoader.loadFactoryNames()来扫描加载含有META-INF/spring.factories文件的jar包，该文件记录了具有哪些自动配置类。（建议还是用IDE去看源码吧，这些源码单行实在太长了，估计文章中的观看效果很差） 12345678910111213141516171819202122232425262728protected List&lt;String&gt; getCandidateConfigurations(AnnotationMetadata metadata, AnnotationAttributes attributes) &#123; List configurations = SpringFactoriesLoader .loadFactoryNames(this.getSpringFactoriesLoaderFactoryClass(), this.getBeanClassLoader()); Assert.notEmpty(configurations, "No auto configuration classes found in META-INF spring.factories. If you are using a custom packaging, make sure that file is correct."); return configurations;&#125; public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, ClassLoader classLoader) &#123; String factoryClassName = factoryClass.getName(); try &#123; Enumeration ex = classLoader != null?classLoader.getResources("META-INF/spring.factories"):ClassLoader.getSystemResources("META-INF/spring.factories"); ArrayList result = new ArrayList(); while(ex.hasMoreElements()) &#123; URL url = (URL)ex.nextElement(); Properties properties = PropertiesLoaderUtils.loadProperties(new UrlResource(url)); String factoryClassNames = properties.getProperty(factoryClassName); result.addAll(Arrays.asList(StringUtils.commaDelimitedListToStringArray(factoryClassNames))); &#125; return result; &#125; catch (IOException var8) &#123; throw new IllegalArgumentException("Unable to load [" + factoryClass.getName() + "] factories from location [" + "META-INF/spring.factories" + "]", var8); &#125;&#125; 自动配置类中的条件注解 接下来，我们在spring.factories文件中随便找一个自动配置类，来看看是怎样实现的。我查看了MongoDataAutoConfiguration的源码，发现它声明了@ConditionalOnClass注解，通过看该注解的源码后可以发现，这是一个组合了@Conditional的组合注解，它的条件类是OnClassCondition。 1234567891011121314151617@Configuration@ConditionalOnClass(&#123;Mongo.class, MongoTemplate.class&#125;)@EnableConfigurationProperties(&#123;MongoProperties.class&#125;)@AutoConfigureAfter(&#123;MongoAutoConfiguration.class&#125;)public class MongoDataAutoConfiguration &#123; ....&#125;@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Conditional(&#123;OnClassCondition.class&#125;)public @interface ConditionalOnClass &#123; Class&lt;?&gt;[] value() default &#123;&#125;; String[] name() default &#123;&#125;;&#125; 然后，我们开始看OnClassCondition的源码，发现它并没有直接实现Condition接口，只好往上找，发现它的父类SpringBootCondition实现了Condition接口。 123456789101112131415161718192021222324252627class OnClassCondition extends SpringBootCondition implements AutoConfigurationImportFilter, BeanFactoryAware, BeanClassLoaderAware &#123; .....&#125;public abstract class SpringBootCondition implements Condition &#123; private final Log logger = LogFactory.getLog(this.getClass()); public SpringBootCondition() &#123; &#125; public final boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; String classOrMethodName = getClassOrMethodName(metadata); try &#123; ConditionOutcome ex = this.getMatchOutcome(context, metadata); this.logOutcome(classOrMethodName, ex); this.recordEvaluation(context, classOrMethodName, ex); return ex.isMatch(); &#125; catch (NoClassDefFoundError var5) &#123; throw new IllegalStateException("Could not evaluate condition on " + classOrMethodName + " due to " + var5.getMessage() + " not found. Make sure your own configuration does not rely on that class. This can also happen if you are @ComponentScanning a springframework package (e.g. if you put a @ComponentScan in the default package by mistake)", var5); &#125; catch (RuntimeException var6) &#123; throw new IllegalStateException("Error processing condition on " + this.getName(metadata), var6); &#125; &#125; public abstract ConditionOutcome getMatchOutcome(ConditionContext var1, AnnotatedTypeMetadata var2);&#125; SpringBootCondition实现的matches方法依赖于一个抽象方法this.getMatchOutcome(context, metadata)，我们在它的子类OnClassCondition中可以找到这个方法的具体实现。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; ClassLoader classLoader = context.getClassLoader(); ConditionMessage matchMessage = ConditionMessage.empty(); // 找出所有ConditionalOnClass注解的属性 List onClasses = this.getCandidates(metadata, ConditionalOnClass.class); List onMissingClasses; if(onClasses != null) &#123; // 找出不在类路径中的类 onMissingClasses = this.getMatches(onClasses, OnClassCondition.MatchType.MISSING, classLoader); // 如果存在不在类路径中的类，匹配失败 if(!onMissingClasses.isEmpty()) &#123; return ConditionOutcome.noMatch(ConditionMessage.forCondition(ConditionalOnClass.class, new Object[0]).didNotFind("required class", "required classes").items(Style.QUOTE, onMissingClasses)); &#125; matchMessage = matchMessage.andCondition(ConditionalOnClass.class, new Object[0]).found("required class", "required classes").items(Style.QUOTE, this.getMatches(onClasses, OnClassCondition.MatchType.PRESENT, classLoader)); &#125; // 接着找出所有ConditionalOnMissingClass注解的属性 // 它与ConditionalOnClass注解的含义正好相反，所以以下逻辑也与上面相反 onMissingClasses = this.getCandidates(metadata, ConditionalOnMissingClass.class); if(onMissingClasses != null) &#123; List present = this.getMatches(onMissingClasses, OnClassCondition.MatchType.PRESENT, classLoader); if(!present.isEmpty()) &#123; return ConditionOutcome.noMatch(ConditionMessage.forCondition(ConditionalOnMissingClass.class, new Object[0]).found("unwanted class", "unwanted classes").items(Style.QUOTE, present)); &#125; matchMessage = matchMessage.andCondition(ConditionalOnMissingClass.class, new Object[0]).didNotFind("unwanted class", "unwanted classes").items(Style.QUOTE, this.getMatches(onMissingClasses, OnClassCondition.MatchType.MISSING, classLoader)); &#125; return ConditionOutcome.match(matchMessage);&#125;// 获得所有annotationType注解的属性private List&lt;String&gt; getCandidates(AnnotatedTypeMetadata metadata, Class&lt;?&gt; annotationType) &#123; MultiValueMap attributes = metadata.getAllAnnotationAttributes(annotationType.getName(), true); ArrayList candidates = new ArrayList(); if(attributes == null) &#123; return Collections.emptyList(); &#125; else &#123; this.addAll(candidates, (List)attributes.get("value")); this.addAll(candidates, (List)attributes.get("name")); return candidates; &#125;&#125;private void addAll(List&lt;String&gt; list, List&lt;Object&gt; itemsToAdd) &#123; if(itemsToAdd != null) &#123; Iterator var3 = itemsToAdd.iterator(); while(var3.hasNext()) &#123; Object item = var3.next(); Collections.addAll(list, (String[])((String[])item)); &#125; &#125;&#125; // 根据matchType.matches方法来进行匹配private List&lt;String&gt; getMatches(Collection&lt;String&gt; candidates, OnClassCondition.MatchType matchType, ClassLoader classLoader) &#123; ArrayList matches = new ArrayList(candidates.size()); Iterator var5 = candidates.iterator(); while(var5.hasNext()) &#123; String candidate = (String)var5.next(); if(matchType.matches(candidate, classLoader)) &#123; matches.add(candidate); &#125; &#125; return matches;&#125; 关于match的具体实现在MatchType中，它是一个枚举类，提供了PRESENT和MISSING两种实现，前者返回类路径中是否存在该类，后者相反。 1234567891011121314151617181920212223242526272829303132333435private static enum MatchType &#123; PRESENT &#123; public boolean matches(String className, ClassLoader classLoader) &#123; return OnClassCondition.MatchType.isPresent(className, classLoader); &#125; &#125;, MISSING &#123; public boolean matches(String className, ClassLoader classLoader) &#123; return !OnClassCondition.MatchType.isPresent(className, classLoader); &#125; &#125;; private MatchType() &#123; &#125; // 跟我们之前看过的案例一样，都利用了类加载功能来进行判断 private static boolean isPresent(String className, ClassLoader classLoader) &#123; if(classLoader == null) &#123; classLoader = ClassUtils.getDefaultClassLoader(); &#125; try &#123; forName(className, classLoader); return true; &#125; catch (Throwable var3) &#123; return false; &#125; &#125; private static Class&lt;?&gt; forName(String className, ClassLoader classLoader) throws ClassNotFoundException &#123; return classLoader != null?classLoader.loadClass(className):Class.forName(className); &#125; public abstract boolean matches(String var1, ClassLoader var2);&#125; 现在终于真相大白，@ConditionalOnClass的含义是指定的类必须存在于类路径下，MongoDataAutoConfiguration类中声明了类路径下必须含有Mongo.class, MongoTemplate.class这两个类，否则该自动配置类不会被加载。 在Spring Boot中到处都有类似的注解，像@ConditionalOnBean（容器中是否有指定的Bean），@ConditionalOnWebApplication（当前工程是否为一个Web工程）等等，它们都只是@Conditional注解的扩展。当你揭开神秘的面纱，去探索本质时，发现其实Spring Boot自动配置的原理就是如此简单，在了解这些知识后，你完全可以自己去实现自定义的自动配置类，然后编写出自定义的starter。]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
        <tag>后端</tag>
        <tag>2018</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SkipList的那点事儿]]></title>
    <url>%2F2017%2F12%2F31%2F2017-12-31-skip_list%2F</url>
    <content type="text"><![CDATA[Skip List的工作原理 Skip List（跳跃表）是一种支持快速查找的数据结构，插入、查找和删除操作都仅仅只需要O(log n)对数级别的时间复杂度，它的效率甚至可以与红黑树等二叉平衡树相提并论，而且实现的难度要比红黑树简单多了。 Skip List主要思想是将链表与二分查找相结合，它维护了一个多层级的链表结构（用空间换取时间），可以把Skip List看作一个含有多个行的链表集合，每一行就是一条链表，这样的一行链表被称为一层，每一层都是下一层的”快速通道”，即如果x层和y层都含有元素a，那么x层的a会与y层的a相互连接（垂直）。最底层的链表是含有所有节点的普通序列，而越接近顶层的链表，含有的节点则越少。 对一个目标元素的搜索会从顶层链表的头部元素开始，然后遍历该链表，直到找到元素大于或等于目标元素的节点，如果当前元素正好等于目标，那么就直接返回它。如果当前元素小于目标元素，那么就垂直下降到下一层继续搜索，如果当前元素大于目标或到达链表尾部，则移动到前一个节点的位置，然后垂直下降到下一层。正因为Skip List的搜索过程会不断地从一层跳跃到下一层的，所以被称为跳跃表。 Skip List还有一个明显的特征，即它是一个不准确的概率性结构，这是因为Skip List在决定是否将节点冗余复制到上一层的时候（而在到达或超过顶层时，需要构建新的顶层）依赖于一个概率函数，举个栗子，我们使用一个最简单的概率函数：丢硬币，即概率P为0.5，那么依赖于该概率函数实现的Skip List会不断地”丢硬币”，如果硬币为正面就将节点复制到上一层，直到硬币为反。 理解Skip List的原理并不困难，下面我们将使用Java来动手实现一个支持基本需求（查找，插入和删除）的Skip List。 本文作者为SylvanasSun(sylvanas.sun@gmail.com)，首发于SylvanasSun’s Blog。原文链接：https://sylvanassun.github.io/2017/12/31/2017-12-31-skip_list/（转载请务必保留本段声明，并且保留超链接。） 节点与基本实现 对于一个普通的链表节点一般只含有一个指向后续节点的指针（双向链表的节点含有两个指针，一个指向前节点，一个指向后节点），由于Skip List是一个多层级的链表结构，我们的设计要让节点拥有四个指针，分别对应该节点的前后左右，为了方便地将头链表永远置于顶层，还需要设置一个int属性表示该链表所处的层级。 1234567891011121314151617181920212223242526272829303132333435363738 protected static class Node&lt;K extends Comparable&lt;K&gt;, V&gt; &#123; private K key; private V value; private int level; // 该节点所处的层级 private Node&lt;K, V&gt; up, down, next, previous; public Node(K key, V value, int level) &#123; this.key = key; this.value = value; this.level = level; &#125; @Override public String toString() &#123; StringBuilder sb = new StringBuilder(); sb.append("Node[") .append("key:"); if (this.key == null) sb.append("None"); else sb.append(this.key.toString()); sb.append(" value:"); if (this.value == null) sb.append("None"); else sb.append(this.value.toString()); sb.append("]"); return sb.toString(); &#125; // 余下都是get,set方法, 这里省略 .....&#125; 接下来是SkipList的基本实现，为了能够让Key进行比较，我们规定Key的类型必须实现了Comparable接口，同时为了支持ForEach循环，该类还实现了Iterable接口。 123456789101112131415161718192021222324252627public class SkipList&lt;K extends Comparable&lt;K&gt;, V&gt; implements Iterable&lt;K&gt; &#123; // 一个随机数生成器 protected static final Random randomGenerator = new Random(); // 默认的概率 protected static final double DEFAULT_PROBABILITY = 0.5; // 头节点 private Node&lt;K, V&gt; head; private double probability; // SkipList中的元素数量（不计算多个层级中的冗余元素） private int size; public SkipList() &#123; this(DEFAULT_PROBABILITY); &#125; public SkipList(double probability) &#123; this.head = new Node&lt;K, V&gt;(null, null, 0); this.probability = probability; this.size = 0; &#125; .....&#125; 我们还需要定义几个辅助方法，如下所示（都很简单）： 12345678910111213141516171819202122232425262728293031// 对key进行检查// 因为每条链表的头节点就是一个key为null的节点，所以不允许其他节点的key也为null protected void checkKeyValidity(K key) &#123; if (key == null) throw new IllegalArgumentException("Key must be not null!"); &#125;// a是否小于等于b protected boolean lessThanOrEqual(K a, K b) &#123; return a.compareTo(b) &lt;= 0; &#125;// 概率函数 protected boolean isBuildLevel() &#123; return randomGenerator.nextDouble() &lt; probability; &#125;// 将y水平插入到x的后面 protected void horizontalInsert(Node&lt;K, V&gt; x, Node&lt;K, V&gt; y) &#123; y.setPrevious(x); y.setNext(x.getNext()); if (x.getNext() != null) x.getNext().setPrevious(y); x.setNext(y); &#125;// x与y进行垂直连接 protected void verticalLink(Node&lt;K, V&gt; x, Node&lt;K, V&gt; y) &#123; x.setDown(y); y.setUp(x); &#125; 查找 查找一个节点的过程如下： 从顶层链表的头部开始进行遍历，比较每一个节点的元素与目标元素的大小。 如果当前元素小于目标元素，则继续遍历。 如果当前元素等于目标元素，返回该节点。 如果当前元素大于目标元素，移动到前一个节点（必须小于等于目标元素），然后跳跃到下一层继续遍历。 如果遍历至链表尾部，跳跃到下一层继续遍历。 1234567891011121314151617181920212223242526272829303132333435363738 protected Node&lt;K, V&gt; findNode(K key) &#123; Node&lt;K, V&gt; node = head; Node&lt;K, V&gt; next = null; Node&lt;K, V&gt; down = null; K nodeKey = null; while (true) &#123; // 不断遍历直到遇见大于目标元素的节点 next = node.getNext(); while (next != null &amp;&amp; lessThanOrEqual(next.getKey(), key)) &#123; node = next; next = node.getNext(); &#125; // 当前元素等于目标元素，中断循环 nodeKey = node.getKey(); if (nodeKey != null &amp;&amp; nodeKey.compareTo(key) == 0) break; // 否则，跳跃到下一层级 down = node.getDown(); if (down != null) &#123; node = down; &#125; else &#123; break; &#125; &#125; return node; &#125; public V get(K key) &#123; checkKeyValidity(key); Node&lt;K, V&gt; node = findNode(key);// 如果找到的节点并不等于目标元素，则目标元素不存在于SkipList中 if (node.getKey().compareTo(key) == 0) return node.getValue(); else return null; &#125; 插入 插入操作的过程要稍微复杂些，主要在于复制节点到上一层与构建新层的操作上。 123456789101112131415161718192021222324252627282930313233343536373839 public void add(K key, V value) &#123; checkKeyValidity(key);// 直接找到key，然后修改对应的value即可 Node&lt;K, V&gt; node = findNode(key); if (node.getKey() != null &amp;&amp; node.getKey().compareTo(key) == 0) &#123; node.setValue(value); return; &#125; // 将newNode水平插入到node之后 Node&lt;K, V&gt; newNode = new Node&lt;K, V&gt;(key, value, node.getLevel()); horizontalInsert(node, newNode); int currentLevel = node.getLevel(); int headLevel = head.getLevel(); while (isBuildLevel()) &#123; // 如果当前层级已经到达或超越顶层 // 那么需要构建一个新的顶层 if (currentLevel &gt;= headLevel) &#123; Node&lt;K, V&gt; newHead = new Node&lt;K, V&gt;(null, null, headLevel + 1); verticalLink(newHead, head); head = newHead; headLevel = head.getLevel(); &#125; // 找到node对应的上一层节点 while (node.getUp() == null) &#123; node = node.getPrevious(); &#125; node = node.getUp(); // 将newNode复制到上一层 Node&lt;K, V&gt; tmp = new Node&lt;K, V&gt;(key, value, node.getLevel()); horizontalInsert(node, tmp); verticalLink(tmp, newNode); newNode = tmp; currentLevel++; &#125; size++; &#125; 删除 对于删除一个节点，需要先找到节点所在的位置（位于最底层链表中的位置），之后再自底向上地删除该节点在每一行中的冗余复制。 12345678910111213141516171819202122232425262728public void remove(K key) &#123; checkKeyValidity(key); Node&lt;K, V&gt; node = findNode(key); if (node == null || node.getKey().compareTo(key) != 0) throw new NoSuchElementException("The key is not exist!"); // 移动到最底层 while (node.getDown() != null) node = node.getDown(); // 自底向上地进行删除 Node&lt;K, V&gt; prev = null; Node&lt;K, V&gt; next = null; for (; node != null; node = node.getUp()) &#123; prev = node.getPrevious(); next = node.getNext(); if (prev != null) prev.setNext(next); if (next != null) next.setPrevious(prev); &#125; // 对顶层链表进行调整，去除无效的顶层链表 while (head.getNext() == null &amp;&amp; head.getDown() != null) &#123; head = head.getDown(); head.setUp(null); &#125; size--;&#125; 迭代器 由于我们的SkipList实现了Iterable接口，所以还需要实现一个迭代器。对于迭代一个Skip List，只需要找到最底层的链表并且移动到它的首节点，然后进行遍历即可。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364 @Override public String toString() &#123; StringBuilder sb = new StringBuilder(); Node&lt;K, V&gt; node = head; // 移动到最底层 while (node.getDown() != null) node = node.getDown(); while (node.getPrevious() != null) node = node.getPrevious(); // 第一个节点是头部节点，没有任何意义，所以需要移动到后一个节点 if (node.getNext() != null) node = node.getNext();// 遍历 while (node != null) &#123; sb.append(node.toString()).append("\n"); node = node.getNext(); &#125; return sb.toString(); &#125; @Override public Iterator&lt;K&gt; iterator() &#123; return new SkipListIterator&lt;K, V&gt;(head); &#125; protected static class SkipListIterator&lt;K extends Comparable&lt;K&gt;, V&gt; implements Iterator&lt;K&gt; &#123; private Node&lt;K, V&gt; node; public SkipListIterator(Node&lt;K, V&gt; node) &#123; while (node.getDown() != null) node = node.getDown(); while (node.getPrevious() != null) node = node.getPrevious(); if (node.getNext() != null) node = node.getNext(); this.node = node; &#125; @Override public boolean hasNext() &#123; return this.node != null; &#125; @Override public K next() &#123; K result = node.getKey(); node = node.getNext(); return result; &#125; @Override public void remove() &#123; throw new UnsupportedOperationException(); &#125; &#125; 本文中实现的SkipList完整代码地址 参考文献 Skip list - Wikipedia]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
        <tag>2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯的那点事儿]]></title>
    <url>%2F2017%2F12%2F20%2F2017-12-20-naive_bayes%2F</url>
    <content type="text"><![CDATA[在机器学习领域中，朴素贝叶斯是一种基于贝叶斯定理的简单概率分类器（分类又被称为监督式学习，所谓监督式学习即从已知样本数据中的特征信息去推测可能出现的输出以完成分类，反之聚类问题被称为非监督式学习），朴素贝叶斯在处理文本数据时可以得到较好的分类结果，所以它被广泛应用于文本分类/垃圾邮件过滤/自然语言处理等场景。 朴素贝叶斯假设了样本的每个特征之间是互相独立、互不影响的，比方说，如果有一个水果是红色的，形状为圆形，并且直径大约为70毫米，那么它就有可能被认为是苹果（具有最高概率的类将会被认为是最有可能的类，这被称为最大后验概率 Maximum A Posteriori），即使上述的这些特征可能会有依赖关系或有其他特征存在，朴素贝叶斯都会认为这些特征都独立地贡献了这个水果是一个苹果的概率，这种假设关系太过于理想，所以这也是朴素贝叶斯的”Naive”之处。 朴素贝叶斯的原名为Naive Bayes Classifier，朴素本身并不是一个正确的翻译，之所以这样翻译是因为朴素贝叶斯虽然Naive，但不代表它的效率会差，相反它的优点正在于实现简单与只需要少量的训练数据，还有另一个原因是它与贝叶斯网络等算法相比，确实是“朴素”了些。 在继续探讨朴素贝叶斯之前，我们先需要理解贝叶斯定理与它的前置理论条件概率与全概率公式。 本文作者为SylvanasSun(sylvanas.sun@gmail.com)，首发于SylvanasSun’s Blog。原文链接：https://sylvanassun.github.io/2017/12/20/2017-12-20-naive_bayes/（转载请务必保留本段声明，并且保留超链接。） 条件概率 条件概率（Conditional Probability）是指在事件B发生的情况下，事件A发生的概率，用$P(A|B)$表示，读作在B条件下的A的概率。 在上方的文氏图中，描述了两个事件A和B，与它们的交集A ∩ B，代入条件概率公式，可推出事件A发生的概率为$P(A|B) = \frac{P({A}\bigcap{B})}{P(B)}$。 对该公式稍作变换可推得${P({A}\bigcap{B})} = {P(A|B)}{P(B)}$与${P({A}\bigcap{B})} = {P(B|A)}{P(A)}$（P(B|A)为在A条件下的B的概率）。 然后根据这个关系可推得${P(A|B)}{P(B)} = {P(B|A)}{P(A)}$。 让我们举个栗子，假设有两个人在扔两个个六面的骰子D1与D2，我们来预测D1与D2的向上面的结果的概率。 在Table1中描述了一个含有36个结果的样本空间，标红处为D1的向上面为2的6个结果，概率为$P(D1=2) = \frac{6}{36} = \frac{1}{6}$。 Table2描述了D1 + D2 &lt;= 5的概率，一共10个结果，用条件概率公式表示为${P(D1+D2\leq5)} = \frac{10}{36}$。 Table3描述了满足Table2的条件同时也满足D1 = 2的结果，它选中了Table2中的3个结果，用条件概率公式表示为${P(D1=2 | D1+D2\leq5)} = \frac{3}{10} = 0.3$。 全概率公式 全概率公式是将边缘概率与条件概率关联起来的基本规则，它表示了一个结果的总概率，可以通过几个不同的事件来实现。 全概率公式将对一复杂事件的概率求解问题转化为了在不同情况下发生的简单事件的概率的求和问题，公式为$P(B) = {\sum_{i=1}^n}P(A_i)P(B|A_i)$。 假定一个样本空间S，它是两个事件A与C之和，同时事件B与它们两个都有交集，如下图所示： 那么事件B的概率可以表示为$P(B) = P({B}\bigcap{A}) + P({B}\bigcap{C})$ 通过条件概率，可以推断出$P({B}\bigcap{A}) = P(B|A)P(A)$，所以$P(B) = P(B|A)P(A) + P(B|C)P(C)$ 这就是全概率公式，即事件B的概率等于事件A与事件C的概率分别乘以B对这两个事件的条件概率之和。 同样举个栗子来应用这个公式，假设有两家工厂生产并对外提供电灯泡，工厂X生产的电灯泡在99%的情况下能够工作超过5000小时，工厂Y生产的电灯泡在95%的情况下能够工作超过5000小时。工厂X在市场的占有率为60%，工厂Y为40%，如何推测出购买的灯泡的工作时间超过5000小时的概率是多少呢？ 运用全概率公式，可以得出：$$\begin{equation}\begin{split}Pr(A) &amp;=Pr(A | B_x) \cdot Pr(B_x) + Pr(A|B_y) \cdot Pr(B_y)\&amp;= \frac{99}{100} \cdot \frac{6}{10} + \frac{95}{100} \cdot \frac{4}{10}\&amp;= \frac{594 + 380}{1000}\&amp;= \frac{974}{1000}\end{split}\end{equation}$$ $Pr(B_x) = \frac{6}{10}$：购买到工厂X制造的电灯泡的概率。 $Pr(B_y) = \frac{4}{10}$：购买到工厂y制造的电灯泡的概率。 $Pr(A|B_x) = \frac{99}{100}$：工厂x制造的电灯泡工作时间超过5000小时的概率。 $Pr(A|B_y) = \frac{95}{100}$：工厂y制造的电灯泡工作时间超过5000小时的概率。 因此，可以得知购买一个工作时间超过5000小时的电灯泡的概率为97.4%。 贝叶斯定理 贝叶斯定理最早由英国数学家（同时也是神学家和哲学家）Thomas Bayes（1701-1761）提出，有趣的是他生前并没有发表过什么有关数学的学术文章，就连他最著名的成就贝叶斯定理也是由他的朋友Richard Price从他死后的遗物（笔记）中找到并发表的。 Thomas Bayes在晚年对概率学产生了兴趣，所谓的贝叶斯定理只是他生前为了解决一个逆概率问题（为了证明上帝是否存在，似乎哲学家们都很喜欢这个问题啊）所写的一篇文章。在那个时期，人们已经能够计算出正向概率问题，比方说，有一个袋子中有X个白球，Y个黑球，你伸手进去摸到黑球的概率是多少？这就是一个正向概率问题，而逆概率问题正好反过来，我们事先并不知道袋子中球的比例，而是不断伸手去摸好几个球，然后根据它们的颜色来推测黑球与白球的比例。 贝叶斯定理是关于随机事件A和B的条件概率的一则定理。通常，事件A在事件B（发生）的条件下的概率，与事件B在事件A（发生）的条件下的概率是不一样的，但它们两者之间是有确定的关系的，贝叶斯定理陈述了这个关系。 贝叶斯定理的一个主要应用为贝叶斯推理，它是一种建立在主观判断基础之上的推理方法，也就是说，你只需要先预估一个值，然后再去根据实际结果去不断修正，不需要任何客观因素。这种推理方式需要大量的计算，因此一直遭到其他人的诟病，无法得到广泛的应用，直到计算机的高速发展，并且人们发现很多事情都是无法事先进行客观判断的，因此贝叶斯推理才得以东山再起。 说了这么多理论知识（很多数学理论都像是在说绕口令），让我们来看一看公式吧，其实只需要把我们在上面推导出的条件概率公式继续进行推理，就可以得出贝叶斯公式。 $$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$ $P(A|B)$：在B条件下的事件A的概率，在贝叶斯定理中，条件概率也被称为后验概率，即在事件B发生之后，我们对事件A概率的重新评估。 $P(B|A)$：在A条件下的事件B的概率，与上一条同理。 $P(A)$与$P(B)$被称为先验概率（也被称为边缘概率），即在事件B发生之前，我们对事件A概率的一个推断（不考虑任何事件B方面的因素），后面同理。 $\frac{P(B|A)}{P(B)}$被称为标准相似度，它是一个调整因子，主要是为了保证预测概率更接近真实概率。 根据这些术语，贝叶斯定理表述为： 后验概率 = 标准相似度 * 先验概率。 让我们以著名的假阳性问题为例，假设某种疾病的发病率为0.001（1000个人中会有一个人得病），现有一种试剂在患者确实得病的情况下，有99%的几率呈现为阳性，而在患者没有得病的情况下，它有5%的几率呈现为阳性（也就是假阳性），如有一位病人的检验成果为阳性，那么他的得病概率是多少呢？ 代入贝叶斯定理，假定事件A表示为得病的概率（P(A) = 0.001），这是我们的先验概率，它是在病人在实际注射试剂（缺乏实验的结果）之前预计的发病率，再假定事件B为试剂结果为阳性的概率，我们需要计算的是条件概率P(A|B)，即在事件B条件下的A概率，这就是后验概率，也就是病人在注射试剂之后（得到实验结果）得出的发病率。 由于还有未得病的概率，所以还需要假设事件C为未得病的先验概率（P(C) = 1 - 0.001 = 0.999），那么P(B|C)后验概率表示的是未得病条件下的试剂结果为阳性的概率，之后再代入全概率公式就可得出最终结果。 $$\begin{equation}\begin{split}P(A|B)&amp;=\frac{P(B|A)P(A)}{P(B)}\&amp;= \frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|C)P(C)}\&amp;= \frac{0.99 \times 0.001}{0.99 \times 0.001 + 0.05 \times 0.999}\approx 0.019\end{split}\end{equation}$$ 最终结果约等于2%，即使一个病人的试剂结果为阳性，他的患病几率也只有2%而已。 朴素贝叶斯的概率模型 我们设一个待分类项$X = {f_1,f_2,\cdots,f_n}$，其中每个f为X的一个特征属性，然后设一个类别集合$C_1,C_2,\cdots,C_m$。 然后需要计算$P(C_1|X),P(C_2|X),\cdots,P(C_m|X)$，我们可以根据一个训练样本集合（已知分类的待分类项集合），然后统计得到在各类别下各个特征属性的条件概率： $P(f_1|C_1),P(f_2|C_1),\cdots,P(f_n|C_1),\cdots,P(f_1|C_2),P(f_2|C_2),\cdots,P(f_n|C_2),\cdots,P(f_1|C_m),P(f_2|C_m),\cdots,P(f_n|C_m)$ 如果$P(C_k|X) = MAX(P(C_1|X),P(C_2|X),\cdots,P(C_m|X))$，则${X}\in{C_k}$（贝叶斯分类其实就是取概率最大的那一个）。 朴素贝叶斯会假设每个特征都是独立的，根据贝叶斯定理可推得：$P(C_i|X) = \frac{P(X|C_i)P(C_i)}{P(X)}$，由于分母对于所有类别为常数，因此只需要将分子最大化即可，又因为各特征是互相独立的，所以最终推得： 根据上述的公式推导，朴素贝叶斯的流程可如下图所示： 接下来我们通过一个案例来过一遍上图的流程。 现有一网站想要通过程序自动识别出账号的真实性（将账号分类为真实账号与不真实账号，所谓不真实账号即带有虚假信息或恶意注册的小号）。 首先需要确定特征属性和类别，然后获取训练样本。假设一个账号具有三个特征：日志数量/注册天数（F1）、好友数量/注册天数（F2）、是否使用了真实的头像（True为1，False为0）。 该网站使用曾经人工检测过的10000个账号作为训练样本，那么计算每个类别的概率为$P(C_0) = 8900 \div 10000 = 0.89, P(C_1) = 1100 \div 10000 = 0.11$，C0为真实账号的类别概率也就是89%，C1为虚假账号的类别概率也就是11%。 之后需要计算每个类别下的各个特征的条件概率，代入朴素贝叶斯分类器，可得$P(F_1|C)P(F_2|C)P(F_3|C)P(C)$，不过有一个问题是，F1与F2是连续变量，不适宜按照某个特定值计算概率。解决方法为将连续值转化为离散值，然后计算区间的概率，比如将F1分解为[0,0.05]、[0.05,0.2]、[0.2,+∞]三个区间，然后计算每个区间的概率即可。 已知某一账号的数据如下：$F_1 = 0.1,F_2 = 0.2,F_3 = 0$，推测该账号是真实账号还是虚假账号。在此例中，F1为0.1，落在第二个区间内，所以在计算的时候，就使用第二个区间的发生概率。根据训练样本可得出结果为： $$\begin{equation}\begin{split}P(F_1|C_0) = 0.5, P(F_1|C_1) = 0.1\P(F_2|C_0) = 0.7, P(F_2|C_1) = 0.2\P(F_3|C_0) = 0.2, P(F_3|C_1) = 0.9\end{split}\end{equation}$$ 接下来使用训练后的分类器可得出该账号的真实账号概率与虚假账号概率，然后取最大概率作为它的类别： $$\begin{equation}\begin{split}P(F_1|C_0)P(F_2|C_0)P(F_3|C_0)P(C_0) &amp;= 0.5 \times 0.7 \times 0.2 \times 0.89\&amp;= 0.0623\end{split}\end{equation}$$$$\begin{equation}\begin{split}P(F_1|C_1)P(F_2|C_1)P(F_3|C_1)P(C_1) &amp;= 0.1 \times 0.2 \times 0.9 \times 0.11\&amp;= 0.00198\end{split}\end{equation}$$ 最终结果为该账号是一个真实账号。 朴素贝叶斯的算法模型 在朴素贝叶斯中含有以下三种算法模型： Gaussian Naive Bayes：适合在特征变量具有连续性的时候使用，同时它还假设特征遵从于高斯分布（正态分布）。举个栗子，假设我们有一组人体特征的统计资料，该数据中的特征：身高、体重和脚掌长度等都为连续变量，很明显我们不能采用离散变量的方法来计算概率，由于样本太少，也无法分成区间计算，那么要怎么办呢？解决方法是假设特征项都是正态分布，然后通过样本计算出均值与标准差，这样就得到了正态分布的密度函数，有了密度函数，就可以代入值，进而算出某一点的密度函数的值。 MultiNomial Naive Bayes：与Gaussian Naive Bayes相反，多项式模型更适合处理特征是离散变量的情况，该模型会在计算先验概率$P(C_m)$和条件概率$P(F_n|Cm)$时会做一些平滑处理。具体公式为，其中T为总的样本数，m为总类别数，$T{cm}$即类别为$C_m$的样本个数，a是一个平滑值。条件概率的公式为，n为特征的个数，T_cmfn为类别为C_m特征为F_n的样本个数。当平滑值a = 1与0 &lt; a &lt; 1时，被称作为Laplace平滑，当a = 0时不做平滑。它的思想其实就是对每类别下所有划分的计数加1，这样如果训练样本数量足够大时，就不会对结果产生影响，并且解决了$P(F|C)$的频率为0的现象（某个类别下的某个特征划分没有出现，这会严重影响分类器的质量）。 Bernoulli Naive Bayes：Bernoulli适用于在特征属性为二进制的场景下，它对每个特征的取值是基于布尔值的，一个典型例子就是判断单词有没有在文本中出现。 朴素贝叶斯的实现 了解了足够多的理论，接下来我们要动手使用python来实现一个Gaussian Naive Bayes，目的是解决皮马人（一个印第安人部落）的糖尿病问题，样本数据（请从该超链接中获取）是一个csv格式的文件，每个值都是一个数字，该文件描述了从患者的年龄、怀孕次数和验血结果等方面的即时测量数据。每个记录都有一个类别值（一个布尔值，以0或1表示），该值表述了患者是否在五年内得过糖尿病。这是一个在机器学习文献中被大量研究过的数据集，一个比较好的预测精度应该在70%~76%。样本数据的每列含义如下： 12345678910111213141516列1：怀孕次数列2：在口服葡萄糖耐量试验中，血浆葡萄糖的浓度（2小时）列3：心脏的舒张压（(mm Hg)）列4：肱三头肌皮肤褶皱厚度（mm）列5：二小时内的血清胰岛素（mu U/ml）列6：体质指数 （(weight in kg/(height in m)^2)）列7：糖尿病家族作用列8：年龄列9：类别布尔值，0为5年没得过糖尿病，1为5年内得过糖尿病------------------------------------6,148,72,35,0,33.6,0.627,50,11,85,66,29,0,26.6,0.351,31,08,183,64,0,0,23.3,0.672,32,11,89,66,23,94,28.1,0.167,21,00,137,40,35,168,43.1,2.288,33,1......... 首先要做的是读取这个csv文件，并解析成我们可以直接使用的数据结构。由于样本数据文件中没有任何的空行和标记符号，每行都是对应的一行数据，只需要简单地把每一行封装到一个list中即可（返回结果为一个list，它的每一项元素都是包含一行数据的list），注意该文件中的数据都为数字，需要先做类型转换。 123456789import csvdef load_csv_file(filename): with open(filename) as f: lines = csv.reader(f) data_set = list(lines) for i in range(len(data_set)): data_set[i] = [float(x) for x in data_set[i]] return data_set 获得了样本数据后，为了评估模型的准确性还需要将它切分为训练数据集（朴素贝叶斯需要使用它来进行预测）与测试数据集。数据在切分过程中是随机选取的，但我们会选择一个比率来控制训练数据集与测试数据集的大小，一般为67%：33%，这是一个比较常见的比率。 12345678910import randomdef split_data_set(data_set, split_ratio): train_size = int(len(data_set) * split_ratio) train_set = [] data_set_copy = list(data_set) while len(train_set) &lt; train_size: index = random.randrange(len(data_set_copy)) train_set.append(data_set_copy.pop(index)) return [train_set, data_set_copy] 切分了样本数据后，还要对训练数据集进行更细致的处理，由于Gaussian Naive Bayes假设了每个特征都遵循正态分布，所以需要从训练数据集中抽取出摘要，它包含了均值与标准差，摘要的数量由类别和特征属性的组合数决定，例如，如果有3个类别与7个特征属性，那么就需要对每个特征属性和类别计算出均值和标准差，这就是21个摘要。 在计算训练数据集的摘要之前，我们的第一个任务是要将训练数据集中的特征与类别进行分离，也就是说，构造出一个key为类别，值为所属该类别的数据行的散列表。 123456789def separate_by_class(data_set, class_index): result = &#123;&#125; for i in range(len(data_set)): vector = data_set[i] class_val = vector[class_index] if (class_val not in result): result[class_val] = [] result[class_val].append(vector) return result 由于已经知道了类别只有一个，而且在每行数据的最后一个，所以只需要将-1传入到class_index参数即可。然后就是计算训练数据集的摘要（每个类别中的每个特征属性的均值与标准差），均值会被作为正态分布的中间值，而标准差则描述了数据的离散程度，在计算概率时，它会被作为正态分布中每个特征属性的期望分布。 标准差就是方差的平方根，只要先求出方差（每个特征值与平均值的差的平方之和的平均值）就可以得出标准差。 123456789import mathdef mean(numbers): return sum(numbers) / float(len(numbers))def stdev(numbers): avg = mean(numbers) variance = sum([pow(x - avg, 2) for x in numbers]) / float(len(numbers)) return math.sqrt(variance) 有了这些辅助函数，计算摘要就很简单了，具体步骤就是先从训练数据集中构造出key为类别的散列表，然后根据类别与每个特征进行计算求出均值与标准差即可。 12345678910111213def summarize(data_set): # 使用zip函数将每个元素中的第n个属性封装为一个元组 # 简单地说，就是把每列（特征）都打包到一个元组中 summaries = [(mean(feature), stdev(feature)) for feature in zip(*data_set)] del summaries[-1] # 最后一行是类别与类别的摘要 所以删除 return summariesdef summarize_by_class(data_set): class_map = separate_by_class(data_set, -1) summaries = &#123;&#125; for class_val, data in class_map.items(): summaries[class_val] = summarize(data) return summaries 数据的处理阶段已经完成了，下面的任务是要去根据训练数据集来进行预测，该阶段需要计算类概率与每个特征与类别的条件概率，然后选出概率最大的类别作为分类结果。关键在于计算条件概率，需要用到正态分布的密度函数，而它所依赖的参数（特征，均值，标准差）我们已经准备好了。 123456789101112131415def calculate_probability(x, mean, stdev): exponent = math.exp(-(math.pow(x - mean, 2) / (2 * math.pow(stdev, 2)))) return (1 / (math.sqrt(2 * math.pi) * stdev)) * exponentdef calculate_conditional_probabilities(summaries, input_vector): probabilities = &#123;&#125; for class_val, class_summaries in summaries.items(): probabilities[class_val] = 1 for i in range(len(class_summaries)): mean, stdev = class_summaries[i] # input_vector是test_set的一行数据，x为该行中的某一特征属性 x = input_vector[i] # 将概率相乘 probabilities[class_val] *= calculate_probability(x, mean, stdev) return probabilities 函数calculate_conditional_probabilities()返回了一个key为类别，值为其概率的散列表，这个散列表记录了每个特征类别的条件概率，之后只需要选出其中最大概率的类别即可。 12345678def predict(summaries, input_vector): probabilities = calculate_conditional_probabilities(summaries, input_vector) best_label, best_prob = None, -1 for class_val, probability in probabilities.items(): if best_label is None or probability &gt; best_prob: best_label = class_val best_prob = probability return best_label 最后我们定义一个函数来对测试数据集中的每个数据实例进行预测以预估模型的准确性，该函数返回了一个预测值列表，包含了每个数据实例的预测值。根据这个返回值，就可以对预测结果进行准确性的评估了。 1234567891011121314def get_predictions(summaries, test_set): predictions = [] for i in range(len(test_set)): result = predict(summaries, test_set[i]) predictions.append(result) return predictionsdef get_accuracy(predictions, test_set): correct = 0 for x in range(len(test_set)): # 分类结果与测试数据集一致，调整值自增 if test_set[x][-1] == predictions[x]: correct += 1 return (correct / float(len(test_set))) * 100.0 完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import csv, random, math"""A simple classifier base on the gaussian naive bayes andproblem of the pima indians diabetes.(https://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)"""def load_csv_file(filename): with open(filename) as f: lines = csv.reader(f) data_set = list(lines) for i in range(len(data_set)): data_set[i] = [float(x) for x in data_set[i]] return data_setdef split_data_set(data_set, split_ratio): train_size = int(len(data_set) * split_ratio) train_set = [] data_set_copy = list(data_set) while len(train_set) &lt; train_size: index = random.randrange(len(data_set_copy)) train_set.append(data_set_copy.pop(index)) return [train_set, data_set_copy]def separate_by_class(data_set, class_index): result = &#123;&#125; for i in range(len(data_set)): vector = data_set[i] class_val = vector[class_index] if (class_val not in result): result[class_val] = [] result[class_val].append(vector) return resultdef mean(numbers): return sum(numbers) / float(len(numbers))def stdev(numbers): avg = mean(numbers) variance = sum([pow(x - avg, 2) for x in numbers]) / float(len(numbers)) return math.sqrt(variance)def summarize(data_set): summaries = [(mean(feature), stdev(feature)) for feature in zip(*data_set)] del summaries[-1] return summariesdef summarize_by_class(data_set): class_map = separate_by_class(data_set, -1) summaries = &#123;&#125; for class_val, data in class_map.items(): summaries[class_val] = summarize(data) return summariesdef calculate_probability(x, mean, stdev): exponent = math.exp(-(math.pow(x - mean, 2) / (2 * math.pow(stdev, 2)))) return (1 / (math.sqrt(2 * math.pi) * stdev)) * exponentdef calculate_conditional_probabilities(summaries, input_vector): probabilities = &#123;&#125; for class_val, class_summaries in summaries.items(): probabilities[class_val] = 1 for i in range(len(class_summaries)): mean, stdev = class_summaries[i] x = input_vector[i] probabilities[class_val] *= calculate_probability(x, mean, stdev) return probabilitiesdef predict(summaries, input_vector): probabilities = calculate_conditional_probabilities(summaries, input_vector) best_label, best_prob = None, -1 for class_val, probability in probabilities.items(): if best_label is None or probability &gt; best_prob: best_label = class_val best_prob = probability return best_labeldef get_predictions(summaries, test_set): predictions = [] for i in range(len(test_set)): result = predict(summaries, test_set[i]) predictions.append(result) return predictionsdef get_accuracy(predictions, test_set): correct = 0 for x in range(len(test_set)): if test_set[x][-1] == predictions[x]: correct += 1 return (correct / float(len(test_set))) * 100.0def main(): filename = 'pima-indians-diabetes.data.csv' split_ratio = 0.67 data_set = load_csv_file(filename) train_set, test_set = split_data_set(data_set, split_ratio) print('Split %s rows into train set = %s and test set = %s rows' %(len(data_set), len(train_set), len(test_set))) # prepare model summaries = summarize_by_class(train_set) # predict and test predictions = get_predictions(summaries, test_set) accuracy = get_accuracy(predictions, test_set) print('Accuracy: %s' % accuracy)main() 参考文献 Bayes’ theorem - Wikipedia Conditional probability - Wikipedia Law of total probability - Wikipedia Naive Bayes classifier - Wikipedia 6 Easy Steps to Learn Naive Bayes Algorithm (with code in Python) How the Naive Bayes Classifier works in Machine Learning Naive Bayes Classifier From Scratch in Python 数学之美番外篇：平凡而又神奇的贝叶斯方法 – 刘未鹏 | Mind Hacks 朴素贝叶斯分类器的应用 - 阮一峰的网络日志 算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification) - T2噬菌体 - 博客园]]></content>
      <categories>
        <category>机器学习</category>
        <category>监督式学习</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>机器学习</tag>
        <tag>监督式学习</tag>
        <tag>贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty的那点事儿]]></title>
    <url>%2F2017%2F11%2F30%2F2017-11-30-netty_introduction%2F</url>
    <content type="text"><![CDATA[Netty是一个基于异步与事件驱动的网络应用程序框架，它支持快速与简单地开发可维护的高性能的服务器与客户端。 所谓事件驱动就是由通过各种事件响应来决定程序的流程，在Netty中到处都充满了异步与事件驱动，这种特点使得应用程序可以以任意的顺序响应在任意的时间点产生的事件，它带来了非常高的可伸缩性，让你的应用可以在需要处理的工作不断增长时，通过某种可行的方式或者扩大它的处理能力来适应这种增长。 Netty提供了高性能与易用性，它具有以下特点： 拥有设计良好且统一的API，支持NIO与OIO（阻塞IO）等多种传输类型，支持真正的无连接UDP Socket。 简单而强大的线程模型，可高度定制线程（池）。 良好的模块化与解耦，支持可扩展和灵活的事件模型，可以很轻松地分离关注点以复用逻辑组件（可插拔的）。 性能高效，拥有比Java核心API更高的吞吐量，通过zero-copy功能以实现最少的内存复制消耗。 内置了许多常用的协议编解码器，如HTTP、SSL、WebScoket等常见协议可以通过Netty做到开箱即用。用户也可以利用Netty简单方便地实现自己的应用层协议。 大多数人使用Netty主要还是为了提高应用的性能，而高性能则离不开非阻塞IO。Netty的非阻塞IO是基于Java NIO的，并且对其进行了封装（直接使用Java NIO API在高复杂度下的应用中是一项非常繁琐且容易出错的操作，而Netty帮你封装了这些复杂操作）。 NIO可以称为New IO也可以称为Non-blocking IO，它比Java旧的阻塞IO在性能上要高效许多（如果让每一个连接中的IO操作都单独创建一个线程，那么阻塞IO并不会比NIO在性能上落后，但不可能创建无限多的线程，在连接数非常多的情况下会很糟糕）。 ByteBuffer：NIO的数据传输是基于缓冲区的，ByteBuffer正是NIO数据传输中所使用的缓冲区抽象。ByteBuffer支持在堆外分配内存，并且尝试避免在执行I/O操作中的多余复制。一般的I/O操作都需要进行系统调用，这样会先切换到内核态，内核态要先从文件读取数据到它的缓冲区，只有等数据准备完毕后，才会从内核态把数据写到用户态，所谓的阻塞IO其实就是说的在等待数据准备好的这段时间内进行阻塞。如果想要避免这个额外的内核操作，可以通过使用mmap（虚拟内存映射）的方式来让用户态直接操作文件。 Channel：它类似于文件描述符，简单地来说它代表了一个实体（如一个硬件设备、文件、Socket或者一个能够执行一个或多个不同的I/O操作的程序组件）。你可以从一个Channel中读取数据到缓冲区，也可以将一个缓冲区中的数据写入到Channel。 Selector：选择器是NIO实现的关键，NIO采用的是I/O多路复用的方式来实现非阻塞，Selector通过在一个线程中监听每个Channel的IO事件来确定有哪些已经准备好进行IO操作的Channel，因此可以在任何时间检查任意的读操作或写操作的完成状态。这种方式避免了等待IO操作准备数据时的阻塞，使用较少的线程便可以处理许多连接，减少了线程切换与维护的开销。 了解了NIO的实现思想之后，我觉得还很有必要了解一下Unix中的I/O模型，Unix中拥有以下5种I/O模型： 阻塞I/O（Blocking I/O） 非阻塞I/O（Non-blocking I/O） I/O多路复用（I/O multiplexing (select and poll)） 信号驱动I/O（signal driven I/O (SIGIO)） 异步I/O（asynchronous I/O (the POSIX aio_functions)） 阻塞I/O模型是最常见的I/O模型，通常我们使用的InputStream/OutputStream都是基于阻塞I/O模型。在上图中，我们使用UDP作为例子，recvfrom()函数是UDP协议用于接收数据的函数，它需要使用系统调用并一直阻塞到内核将数据准备好，之后再由内核缓冲区复制数据到用户态（即是recvfrom()接收到数据），所谓阻塞就是在等待内核准备数据的这段时间内什么也不干。 举个生活中的例子，阻塞I/O就像是你去餐厅吃饭，在等待饭做好的时间段中，你只能在餐厅中坐着干等（如果你在玩手机那么这就是非阻塞I/O了）。 在非阻塞I/O模型中，内核在数据尚未准备好的情况下回返回一个错误码EWOULDBLOCK，而recvfrom并没有在失败的情况下选择阻塞休眠，而是不断地向内核询问是否已经准备完毕，在上图中，前三次内核都返回了EWOULDBLOCK，直到第四次询问时，内核数据准备完毕，然后开始将内核中缓存的数据复制到用户态。这种不断询问内核以查看某种状态是否完成的方式被称为polling（轮询）。 非阻塞I/O就像是你在点外卖，只不过你非常心急，每隔一段时间就要打电话问外卖小哥有没有到。 I/O多路复用的思想跟非阻塞I/O是一样的，只不过在非阻塞I/O中，是在recvfrom的用户态（或一个线程）中去轮询内核，这种方式会消耗大量的CPU时间。而I/O多路复用则是通过select()或poll()系统调用来负责进行轮询，以实现监听I/O读写事件的状态。如上图中，select监听到一个datagram可读时，就交由recvfrom去发送系统调用将内核中的数据复制到用户态。 这种方式的优点很明显，通过I/O多路复用可以监听多个文件描述符，且在内核中完成监控的任务。但缺点是至少需要两个系统调用（select()与recvfrom()）。 I/O多路复用同样适用于点外卖这个例子，只不过你在等外卖的期间完全可以做自己的事情，当外卖到的时候会通过外卖APP或者由外卖小哥打电话来通知你。 Unix中提供了两种I/O多路复用函数，select()和poll()。select()的兼容性更好，但它在单个进程中所能监控的文件描述符是有限的，这个值与FD_SETSIZE相关，32位系统中默认为1024，64位系统中为2048。select()还有一个缺点就是他轮询的方式，它采取了线性扫描的轮询方式，每次都要遍历FD_SETSIZE个文件描述符，不管它们是否活不活跃的。poll()本质上与select()的实现没有区别，不过在数据结构上区别很大，用户必须分配一个pollfd结构数组，该数组维护在内核态中，正因如此，poll()并不像select()那样拥有大小上限的限制，但缺点同样也很明显，大量的fd数组会在用户态与内核态之间不断复制，不管这样的复制是否有意义。 还有一种比select()与poll()更加高效的实现叫做epoll()，它是由Linux内核2.6推出的可伸缩的I/O多路复用实现，目的是为了替代select()与poll()。epoll()同样没有文件描述符上限的限制，它使用一个文件描述符来管理多个文件描述符，并使用一个红黑树来作为存储结构。同时它还支持边缘触发（edge-triggered）与水平触发（level-triggered）两种模式（poll()只支持水平触发），在边缘触发模式下，epoll_wait仅会在新的事件对象首次被加入到epoll时返回，而在水平触发模式下，epoll_wait会在事件状态未变更前不断地触发。也就是说，边缘触发模式只会在文件描述符变为就绪状态时通知一次，水平触发模式会不断地通知该文件描述符直到被处理。 关于epoll_wait请参考如下epoll API。 12345678910// 创建一个epoll对象并返回它的文件描述符。// 参数flags允许修改epoll的行为，它只有一个有效值EPOLL_CLOEXEC。int epoll_create1(int flags);// 配置对象，该对象负责描述监控哪些文件描述符和哪些事件。int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);// 等待与epoll_ctl注册的任何事件，直至事件发生一次或超时。// 返回在events中发生的事件，最多同时返回maxevents个。int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); epoll另一亮点是采用了事件驱动的方式而不是轮询，在epoll_ctl中注册的文件描述符在事件触发的时候会通过一个回调机制来激活该文件描述符，epoll_wait便可以收到通知。这样效率就不会与文件描述符的数量成正比。epoll还采用了mmap来减少内核态与用户态之间的数据传输开销。 在Java NIO2（从JDK1.7开始引入）中，只要Linux内核版本在2.6以上，就会采用epoll，如下源码所示（DefaultSelectorProvider.java）。 123456789101112131415161718192021222324252627public static SelectorProvider create() &#123; String osname = AccessController.doPrivileged( new GetPropertyAction("os.name")); if ("SunOS".equals(osname)) &#123; return new sun.nio.ch.DevPollSelectorProvider(); &#125; // use EPollSelectorProvider for Linux kernels &gt;= 2.6 if ("Linux".equals(osname)) &#123; String osversion = AccessController.doPrivileged( new GetPropertyAction("os.version")); String[] vers = osversion.split("\\.", 0); if (vers.length &gt;= 2) &#123; try &#123; int major = Integer.parseInt(vers[0]); int minor = Integer.parseInt(vers[1]); if (major &gt; 2 || (major == 2 &amp;&amp; minor &gt;= 6)) &#123; return new sun.nio.ch.EPollSelectorProvider(); &#125; &#125; catch (NumberFormatException x) &#123; // format not recognized &#125; &#125; &#125; return new sun.nio.ch.PollSelectorProvider();&#125; 信号驱动I/O模型使用到了信号，内核在数据准备就绪时会通过信号来进行通知。我们首先开启了一个信号驱动I/O套接字，并使用sigaction系统调用来安装信号处理程序，内核直接返回，不会阻塞用户态。当datagram准备好时，内核会发送SIGIO信号，recvfrom接收到信号后会发送系统调用开始进行I/O操作。 这种模型的优点是主进程（线程）不会被阻塞，当数据准备就绪时，通过信号处理程序来通知主进程（线程）准备进行I/O操作与对数据的处理。 我们之前讨论的各种I/O模型无论是阻塞还是非阻塞，它们所说的阻塞都是指的数据准备阶段。异步I/O模型同样依赖于信号处理程序来进行通知，但与以上I/O模型都不相同的是，异步I/O模型通知的是I/O操作已经完成，而不是数据准备完成。 可以说异步I/O模型才是真正的非阻塞，主进程只管做自己的事情，然后在I/O操作完成时调用回调函数来完成一些对数据的处理操作即可。 闲扯了这么多，想必大家已经对I/O模型有了一个深刻的认识。之后，我们将会结合部分源码（Netty4.X）来探讨Netty中的各大核心组件，以及如何使用Netty，你会发现实现一个Netty程序是多么简单（而且还伴随了高性能与可维护性）。 本文作者为SylvanasSun(sylvanas.sun@gmail.com)，首发于SylvanasSun’s Blog。原文链接：https://sylvanassun.github.io/2017/11/30/2017-11-30-netty_introduction/（转载请务必保留本段声明，并且保留超链接。） ByteBuf 网络传输的基本单位是字节，在Java NIO中提供了ByteBuffer作为字节缓冲区容器，但该类的API使用起来不太方便，所以Netty实现了ByteBuf作为其替代品，下面是使用ByteBuf的优点： 相比ByteBuffer使用起来更加简单。 通过内置的复合缓冲区类型实现了透明的zero-copy。 容量可以按需增长。 读和写使用了不同的索引指针。 支持链式调用。 支持引用计数与池化。 可以被用户自定义的缓冲区类型扩展。 在讨论ByteBuf之前，我们先需要了解一下ByteBuffer的实现，这样才能比较深刻地明白它们之间的区别。 ByteBuffer继承于abstract class Buffer（所以还有LongBuffer、IntBuffer等其他类型的实现），本质上它只是一个有限的线性的元素序列，包含了三个重要的属性。 Capacity：缓冲区中元素的容量大小，你只能将capacity个数量的元素写入缓冲区，一旦缓冲区已满就需要清理缓冲区才能继续写数据。 Position：指向下一个写入数据位置的索引指针，初始位置为0，最大为capacity-1。当写模式转换为读模式时，position需要被重置为0。 Limit：在写模式中，limit是可以写入缓冲区的最大索引，也就是说它在写模式中等价于缓冲区的容量。在读模式中，limit表示可以读取数据的最大索引。 由于Buffer中只维护了position一个索引指针，所以它在读写模式之间的切换需要调用一个flip()方法来重置指针。使用Buffer的流程一般如下： 写入数据到缓冲区。 调用flip()方法。 从缓冲区中读取数据 调用buffer.clear()或者buffer.compact()清理缓冲区，以便下次写入数据。 12345678910111213141516171819RandomAccessFile aFile = new RandomAccessFile("data/nio-data.txt", "rw");FileChannel inChannel = aFile.getChannel();// 分配一个48字节大小的缓冲区ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buf); // 读取数据到缓冲区while (bytesRead != -1) &#123; buf.flip(); // 将position重置为0 while(buf.hasRemaining())&#123; System.out.print((char) buf.get()); // 读取数据并输出到控制台 &#125; buf.clear(); // 清理缓冲区 bytesRead = inChannel.read(buf);&#125;aFile.close(); Buffer中核心方法的实现也非常简单，主要就是在操作指针position。 12345678910111213141516171819202122232425262728293031323334353637383940 /** * Sets this buffer's mark at its position. * * @return This buffer */ public final Buffer mark() &#123; mark = position; // mark属性是用来标记当前索引位置的 return this; &#125;// 将当前索引位置重置为mark所标记的位置 public final Buffer reset() &#123; int m = mark; if (m &lt; 0) throw new InvalidMarkException(); position = m; return this; &#125; // 翻转这个Buffer，将limit设置为当前索引位置，然后再把position重置为0 public final Buffer flip() &#123; limit = position; position = 0; mark = -1; return this; &#125;// 清理缓冲区// 说是清理,也只是把postion与limit进行重置,之后再写入数据就会覆盖之前的数据了 public final Buffer clear() &#123; position = 0; limit = capacity; mark = -1; return this; &#125;// 返回剩余空间 public final int remaining() &#123; return limit - position; &#125; Java NIO中的Buffer API操作的麻烦之处就在于读写转换需要手动重置指针。而ByteBuf没有这种繁琐性，它维护了两个不同的索引，一个用于读取，一个用于写入。当你从ByteBuf读取数据时，它的readerIndex将会被递增已经被读取的字节数，同样的，当你写入数据时，writerIndex则会递增。readerIndex的最大范围在writerIndex的所在位置，如果试图移动readerIndex超过该值则会触发异常。 ByteBuf中名称以read或write开头的方法将会递增它们其对应的索引，而名称以get或set开头的方法则不会。ByteBuf同样可以指定一个最大容量，试图移动writerIndex超过该值则会触发异常。 1234567891011121314151617181920212223242526272829303132333435363738394041424344 public byte readByte() &#123; this.checkReadableBytes0(1); // 检查readerIndex是否已越界 int i = this.readerIndex; byte b = this._getByte(i); this.readerIndex = i + 1; // 递增readerIndex return b; &#125; private void checkReadableBytes0(int minimumReadableBytes) &#123; this.ensureAccessible(); if(this.readerIndex &gt; this.writerIndex - minimumReadableBytes) &#123; throw new IndexOutOfBoundsException(String.format("readerIndex(%d) + length(%d) exceeds writerIndex(%d): %s", new Object[]&#123;Integer.valueOf(this.readerIndex), Integer.valueOf(minimumReadableBytes), Integer.valueOf(this.writerIndex), this&#125;)); &#125; &#125; public ByteBuf writeByte(int value) &#123; this.ensureAccessible(); this.ensureWritable0(1); // 检查writerIndex是否会越过capacity this._setByte(this.writerIndex++, value); return this; &#125; private void ensureWritable0(int minWritableBytes) &#123; if(minWritableBytes &gt; this.writableBytes()) &#123; if(minWritableBytes &gt; this.maxCapacity - this.writerIndex) &#123; throw new IndexOutOfBoundsException(String.format("writerIndex(%d) + minWritableBytes(%d) exceeds maxCapacity(%d): %s", new Object[]&#123;Integer.valueOf(this.writerIndex), Integer.valueOf(minWritableBytes), Integer.valueOf(this.maxCapacity), this&#125;)); &#125; else &#123; int newCapacity = this.alloc().calculateNewCapacity(this.writerIndex + minWritableBytes, this.maxCapacity); this.capacity(newCapacity); &#125; &#125; &#125; // get与set只对传入的索引进行了检查，然后对其位置进行get或set public byte getByte(int index) &#123; this.checkIndex(index); return this._getByte(index); &#125; public ByteBuf setByte(int index, int value) &#123; this.checkIndex(index); this._setByte(index, value); return this; &#125; ByteBuf同样支持在堆内和堆外进行分配。在堆内分配也被称为支撑数组模式，它能在没有使用池化的情况下提供快速的分配和释放。 12345678ByteBuf heapBuf = Unpooled.copiedBuffer(bytes);if (heapBuf.hasArray()) &#123; // 判断是否有一个支撑数组 byte[] array = heapBuf.array(); // 计算第一个字节的偏移量 int offset = heapBuf.arrayOffset() + heapBuf.readerIndex(); int length = heapBuf.readableBytes(); // 获得可读字节 handleArray(array,offset,length); // 调用你的处理方法&#125; 另一种模式为堆外分配，Java NIO ByteBuffer类在JDK1.4时就已经允许JVM实现通过JNI调用来在堆外分配内存（调用malloc()函数在JVM堆外分配内存），这主要是为了避免额外的缓冲区复制操作。 12345678ByteBuf directBuf = Unpooled.directBuffer(capacity);if (!directBuf.hasArray()) &#123; int length = directBuf.readableBytes(); byte[] array = new byte[length]; // 将字节复制到数组中 directBuf.getBytes(directBuf.readerIndex(),array); handleArray(array,0,length);&#125; ByteBuf还支持第三种模式，它被称为复合缓冲区，为多个ByteBuf提供了一个聚合视图。在这个视图中，你可以根据需要添加或者删除ByteBuf实例，ByteBuf的子类CompositeByteBuf实现了该模式。 一个适合使用复合缓冲区的场景是HTTP协议，通过HTTP协议传输的消息都会被分成两部分——头部和主体，如果这两部分由应用程序的不同模块产生，将在消息发送时进行组装，并且该应用程序还会为多个消息复用相同的消息主体，这样对于每个消息都将会创建一个新的头部，产生了很多不必要的内存操作。使用CompositeByteBuf是一个很好的选择，它消除了这些额外的复制，以帮助你复用这些消息。 1234567CompositeByteBuf messageBuf = Unpooled.compositeBuffer();ByteBuf headerBuf = ....;ByteBuf bodyBuf = ....;messageBuf.addComponents(headerBuf,bodyBuf);for (ByteBuf buf : messageBuf) &#123; System.out.println(buf.toString());&#125; CompositeByteBuf透明的实现了zero-copy，zero-copy其实就是避免数据在两个内存区域中来回的复制。从操作系统层面上来讲，zero-copy指的是避免在内核态与用户态之间的数据缓冲区复制（通过mmap避免），而Netty中的zero-copy更偏向于在用户态中的数据操作的优化，就像使用CompositeByteBuf来复用多个ByteBuf以避免额外的复制，也可以使用wrap()方法来将一个字节数组包装成ByteBuf，又或者使用ByteBuf的slice()方法把它分割为多个共享同一内存区域的ByteBuf，这些都是为了优化内存的使用率。 那么如何创建ByteBuf呢？在上面的代码中使用到了Unpooled，它是Netty提供的一个用于创建与分配ByteBuf的工具类，建议都使用这个工具类来创建你的缓冲区，不要自己去调用构造函数。经常使用的是wrappedBuffer()与copiedBuffer()，它们一个是用于将一个字节数组或ByteBuffer包装为一个ByteBuf，一个是根据传入的字节数组与ByteBuffer/ByteBuf来复制出一个新的ByteBuf。 12345678910111213141516 // 通过array.clone()来复制一个数组进行包装 public static ByteBuf copiedBuffer(byte[] array) &#123; return array.length == 0?EMPTY_BUFFER:wrappedBuffer((byte[])array.clone()); &#125;// 默认是堆内分配 public static ByteBuf wrappedBuffer(byte[] array) &#123; return (ByteBuf)(array.length == 0?EMPTY_BUFFER:new UnpooledHeapByteBuf(ALLOC, array, array.length)); &#125; // 也提供了堆外分配的方法 private static final ByteBufAllocator ALLOC; public static ByteBuf directBuffer(int initialCapacity) &#123; return ALLOC.directBuffer(initialCapacity); &#125; 相对底层的分配方法是使用ByteBufAllocator，Netty实现了PooledByteBufAllocator和UnpooledByteBufAllocator，前者使用了jemalloc（一种malloc()的实现）来分配内存，并且实现了对ByteBuf的池化以提高性能。后者分配的是未池化的ByteBuf，其分配方式与之前讲的一致。 1234Channel channel = ...;ByteBufAllocator allocator = channel.alloc();ByteBuf buffer = allocator.directBuffer();do something....... 为了优化内存使用率，Netty提供了一套手动的方式来追踪不活跃对象，像UnpooledHeapByteBuf这种分配在堆内的对象得益于JVM的GC管理，无需额外操心，而UnpooledDirectByteBuf是在堆外分配的，它的内部基于DirectByteBuffer，DirectByteBuffer会先向Bits类申请一个额度（Bits还拥有一个全局变量totalCapacity，记录了所有DirectByteBuffer总大小），每次申请前都会查看是否已经超过-XX:MaxDirectMemorySize所设置的上限，如果超限就会尝试调用Sytem.gc()，以试图回收一部分内存，然后休眠100毫秒，如果内存还是不足，则只能抛出OOM异常。堆外内存的回收虽然有了这么一层保障，但为了提高性能与使用率，主动回收也是很有必要的。由于Netty还实现了ByteBuf的池化，像PooledHeapByteBuf和PooledDirectByteBuf就必须依赖于手动的方式来进行回收（放回池中）。 Netty使用了引用计数器的方式来追踪那些不活跃的对象。引用计数的接口为ReferenceCounted，它的思想很简单，只要ByteBuf对象的引用计数大于0，就保证该对象不会被释放回收，可以通过手动调用release()与retain()方法来操作该对象的引用计数值递减或递增。用户也可以通过自定义一个ReferenceCounted的实现类，以满足自定义的规则。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package io.netty.buffer;public abstract class AbstractReferenceCountedByteBuf extends AbstractByteBuf &#123; // 由于ByteBuf的实例对象会非常多,所以这里没有将refCnt包装为AtomicInteger // 而是使用一个全局的AtomicIntegerFieldUpdater来负责操作refCnt private static final AtomicIntegerFieldUpdater&lt;AbstractReferenceCountedByteBuf&gt; refCntUpdater = AtomicIntegerFieldUpdater.newUpdater(AbstractReferenceCountedByteBuf.class, "refCnt"); // 每个ByteBuf的初始引用值都为1 private volatile int refCnt = 1; public int refCnt() &#123; return this.refCnt; &#125; protected final void setRefCnt(int refCnt) &#123; this.refCnt = refCnt; &#125; public ByteBuf retain() &#123; return this.retain0(1); &#125; // 引用计数值递增increment，increment必须大于0 public ByteBuf retain(int increment) &#123; return this.retain0(ObjectUtil.checkPositive(increment, "increment")); &#125; public static int checkPositive(int i, String name) &#123; if(i &lt;= 0) &#123; throw new IllegalArgumentException(name + ": " + i + " (expected: &gt; 0)"); &#125; else &#123; return i; &#125; &#125; // 使用CAS操作不断尝试更新值 private ByteBuf retain0(int increment) &#123; int refCnt; int nextCnt; do &#123; refCnt = this.refCnt; nextCnt = refCnt + increment; if(nextCnt &lt;= increment) &#123; throw new IllegalReferenceCountException(refCnt, increment); &#125; &#125; while(!refCntUpdater.compareAndSet(this, refCnt, nextCnt)); return this; &#125; public boolean release() &#123; return this.release0(1); &#125; public boolean release(int decrement) &#123; return this.release0(ObjectUtil.checkPositive(decrement, "decrement")); &#125; private boolean release0(int decrement) &#123; int refCnt; do &#123; refCnt = this.refCnt; if(refCnt &lt; decrement) &#123; throw new IllegalReferenceCountException(refCnt, -decrement); &#125; &#125; while(!refCntUpdater.compareAndSet(this, refCnt, refCnt - decrement)); if(refCnt == decrement) &#123; this.deallocate(); return true; &#125; else &#123; return false; &#125; &#125; protected abstract void deallocate(); &#125; Channel Netty中的Channel与Java NIO的概念一样，都是对一个实体或连接的抽象，但Netty提供了一套更加通用的API。就以网络套接字为例，在Java中OIO与NIO是截然不同的两套API，假设你之前使用的是OIO而又想更改为NIO实现，那么几乎需要重写所有代码。而在Netty中，只需要更改短短几行代码（更改Channel与EventLoop的实现类，如把OioServerSocketChannel替换为NioServerSocketChannel），就可以完成OIO与NIO（或其他）之间的转换。 每个Channel最终都会被分配一个ChannelPipeline和ChannelConfig，前者持有所有负责处理入站与出站数据以及事件的ChannelHandler，后者包含了该Channel的所有配置设置，并且支持热更新，由于不同的传输类型可能具有其特别的配置，所以该类可能会实现为ChannelConfig的不同子类。 Channel是线程安全的（与之后要讲的线程模型有关），因此你完全可以在多个线程中复用同一个Channel，就像如下代码所示。 12345678910111213final Channel channel = ...final ByteBuf buffer = Unpooled.copiedBuffer("Hello,World!", CharsetUtil.UTF_8).retain();Runnable writer = new Runnable() &#123; @Override public void run() &#123; channel.writeAndFlush(buffer.duplicate()); &#125;&#125;;Executor executor = Executors.newCachedThreadPool();executor.execute(writer);executor.execute(writer);....... Netty除了支持常见的NIO与OIO，还内置了其他的传输类型。 Nmae Package Description NIO io.netty.channel.socket.nio 以Java NIO为基础实现 OIO io.netty.channel.socket.oio 以java.net为基础实现，使用阻塞I/O模型 Epoll io.netty.channel.epoll 由JNI驱动epoll()实现的更高性能的非阻塞I/O，它只能使用在Linux Local io.netty.channel.local 本地传输，在JVM内部通过管道进行通信 Embedded io.netty.channel.embedded 允许在不需要真实网络传输的环境下使用ChannelHandler，主要用于对ChannelHandler进行测试 NIO、OIO、Epoll我们应该已经很熟悉了，下面主要说说Local与Embedded。 Local传输用于在同一个JVM中运行的客户端和服务器程序之间的异步通信，与服务器Channel相关联的SocketAddress并没有绑定真正的物理网络地址，它会被存储在注册表中，并在Channel关闭时注销。因此Local传输不会接受真正的网络流量，也就是说它不能与其他传输实现进行互操作。 Embedded传输主要用于对ChannelHandler进行单元测试，ChannelHandler是用于处理消息的逻辑组件，Netty通过将入站消息与出站消息都写入到EmbeddedChannel中的方式（提供了write/readInbound()与write/readOutbound()来读写入站与出站消息）来实现对ChannelHandler的单元测试。 ChannelHandler ChannelHandler充当了处理入站和出站数据的应用程序逻辑的容器，该类是基于事件驱动的，它会响应相关的事件然后去调用其关联的回调函数，例如当一个新的连接被建立时，ChannelHandler的channelActive()方法将会被调用。 关于入站消息和出站消息的数据流向定义，如果以客户端为主视角来说的话，那么从客户端流向服务器的数据被称为出站，反之为入站。 入站事件是可能被入站数据或者相关的状态更改而触发的事件，包括：连接已被激活、连接失活、读取入站数据、用户事件、发生异常等。 出站事件是未来将会触发的某个动作的结果的事件，这些动作包括：打开或关闭远程节点的连接、将数据写（或冲刷）到套接字。 ChannelHandler的主要用途包括： 对入站与出站数据的业务逻辑处理 记录日志 将数据从一种格式转换为另一种格式，实现编解码器。以一次HTTP协议（或者其他应用层协议）的流程为例，数据在网络传输时的单位为字节，当客户端发送请求到服务器时，服务器需要通过解码器（处理入站消息）将字节解码为协议的消息内容，服务器在发送响应的时候（处理出站消息），还需要通过编码器将消息内容编码为字节。 捕获异常 提供Channel生命周期内的通知，如Channel活动时与非活动时 Netty中到处都充满了异步与事件驱动，而回调函数正是用于响应事件之后的操作。由于异步会直接返回一个结果，所以Netty提供了ChannelFuture（实现了java.util.concurrent.Future）来作为异步调用返回的占位符，真正的结果会在未来的某个时刻完成，到时候就可以通过ChannelFuture对其进行访问，每个Netty的出站I/O操作都将会返回一个ChannelFuture。 Netty还提供了ChannelFutureListener接口来监听ChannelFuture是否成功，并采取对应的操作。 12345678910111213141516Channel channel = ...ChannelFuture future = channel.connect(new InetSocketAddress("192.168.0.1",6666));// 注册一个监听器future.addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) &#123; if (future.isSuccess()) &#123; // do something.... &#125; else &#123; // 输出错误信息 Throwable cause = future.cause(); cause.printStackTrace(); // do something.... &#125; &#125;&#125;); ChannelFutureListener接口中还提供了几个简单的默认实现，方便我们使用。 12345678910111213141516171819202122232425262728293031package io.netty.channel;import io.netty.channel.ChannelFuture;import io.netty.util.concurrent.GenericFutureListener;public interface ChannelFutureListener extends GenericFutureListener&lt;ChannelFuture&gt; &#123; // 在Future完成时关闭 ChannelFutureListener CLOSE = new ChannelFutureListener() &#123; public void operationComplete(ChannelFuture future) &#123; future.channel().close(); &#125; &#125;; // 如果失败则关闭 ChannelFutureListener CLOSE_ON_FAILURE = new ChannelFutureListener() &#123; public void operationComplete(ChannelFuture future) &#123; if(!future.isSuccess()) &#123; future.channel().close(); &#125; &#125; &#125;; // 将异常信息传递给下一个ChannelHandler ChannelFutureListener FIRE_EXCEPTION_ON_FAILURE = new ChannelFutureListener() &#123; public void operationComplete(ChannelFuture future) &#123; if(!future.isSuccess()) &#123; future.channel().pipeline().fireExceptionCaught(future.cause()); &#125; &#125; &#125;;&#125; ChannelHandler接口定义了对它生命周期进行监听的回调函数，在ChannelHandler被添加到ChannelPipeline或者被移除时都会调用这些函数。 12345678910111213141516171819package io.netty.channel;public interface ChannelHandler &#123; void handlerAdded(ChannelHandlerContext var1) throws Exception; void handlerRemoved(ChannelHandlerContext var1) throws Exception; /** @deprecated */ @Deprecated void exceptionCaught(ChannelHandlerContext var1, Throwable var2) throws Exception; // 该注解表明这个ChannelHandler可被其他线程复用 @Inherited @Documented @Target(&#123;ElementType.TYPE&#125;) @Retention(RetentionPolicy.RUNTIME) public @interface Sharable &#123; &#125;&#125; 入站消息与出站消息由其对应的接口ChannelInboundHandler与ChannelOutboundHandler负责，这两个接口定义了监听Channel的生命周期的状态改变事件的回调函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package io.netty.channel;import io.netty.channel.ChannelHandler;import io.netty.channel.ChannelHandlerContext;public interface ChannelInboundHandler extends ChannelHandler &#123; // 当channel被注册到EventLoop时被调用 void channelRegistered(ChannelHandlerContext var1) throws Exception; // 当channel已经被创建，但还未注册到EventLoop（或者从EventLoop中注销）被调用 void channelUnregistered(ChannelHandlerContext var1) throws Exception; // 当channel处于活动状态（连接到远程节点）被调用 void channelActive(ChannelHandlerContext var1) throws Exception; // 当channel处于非活动状态（没有连接到远程节点）被调用 void channelInactive(ChannelHandlerContext var1) throws Exception; // 当从channel读取数据时被调用 void channelRead(ChannelHandlerContext var1, Object var2) throws Exception; // 当channel的上一个读操作完成时被调用 void channelReadComplete(ChannelHandlerContext var1) throws Exception; // 当ChannelInboundHandler.fireUserEventTriggered()方法被调用时被调用 void userEventTriggered(ChannelHandlerContext var1, Object var2) throws Exception; // 当channel的可写状态发生改变时被调用 void channelWritabilityChanged(ChannelHandlerContext var1) throws Exception; // 当处理过程中发生异常时被调用 void exceptionCaught(ChannelHandlerContext var1, Throwable var2) throws Exception;&#125;package io.netty.channel;import io.netty.channel.ChannelHandler;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelPromise;import java.net.SocketAddress;public interface ChannelOutboundHandler extends ChannelHandler &#123; // 当请求将Channel绑定到一个地址时被调用 // ChannelPromise是ChannelFuture的一个子接口，定义了如setSuccess(),setFailure()等方法 void bind(ChannelHandlerContext var1, SocketAddress var2, ChannelPromise var3) throws Exception; // 当请求将Channel连接到远程节点时被调用 void connect(ChannelHandlerContext var1, SocketAddress var2, SocketAddress var3, ChannelPromise var4) throws Exception; // 当请求将Channel从远程节点断开时被调用 void disconnect(ChannelHandlerContext var1, ChannelPromise var2) throws Exception; // 当请求关闭Channel时被调用 void close(ChannelHandlerContext var1, ChannelPromise var2) throws Exception; // 当请求将Channel从它的EventLoop中注销时被调用 void deregister(ChannelHandlerContext var1, ChannelPromise var2) throws Exception; // 当请求从Channel读取数据时被调用 void read(ChannelHandlerContext var1) throws Exception; // 当请求通过Channel将数据写到远程节点时被调用 void write(ChannelHandlerContext var1, Object var2, ChannelPromise var3) throws Exception; // 当请求通过Channel将缓冲中的数据冲刷到远程节点时被调用 void flush(ChannelHandlerContext var1) throws Exception;&#125; 通过实现ChannelInboundHandler或者ChannelOutboundHandler就可以完成用户自定义的应用逻辑处理程序，不过Netty已经帮你实现了一些基本操作，用户只需要继承并扩展ChannelInboundHandlerAdapter或ChannelOutboundHandlerAdapter来作为自定义实现的起始点。 ChannelInboundHandlerAdapter与ChannelOutboundHandlerAdapter都继承于ChannelHandlerAdapter，该抽象类简单实现了ChannelHandler接口。 123456789101112131415161718192021222324252627282930313233343536public abstract class ChannelHandlerAdapter implements ChannelHandler &#123; boolean added; public ChannelHandlerAdapter() &#123; &#125; // 该方法不允许将此ChannelHandler共享复用 protected void ensureNotSharable() &#123; if(this.isSharable()) &#123; throw new IllegalStateException("ChannelHandler " + this.getClass().getName() + " is not allowed to be shared"); &#125; &#125; // 使用反射判断实现类有没有@Sharable注解，以确认该类是否为可共享复用的 public boolean isSharable() &#123; Class clazz = this.getClass(); Map cache = InternalThreadLocalMap.get().handlerSharableCache(); Boolean sharable = (Boolean)cache.get(clazz); if(sharable == null) &#123; sharable = Boolean.valueOf(clazz.isAnnotationPresent(Sharable.class)); cache.put(clazz, sharable); &#125; return sharable.booleanValue(); &#125; public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; &#125; public void handlerRemoved(ChannelHandlerContext ctx) throws Exception &#123; &#125; public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.fireExceptionCaught(cause); &#125;&#125; ChannelInboundHandlerAdapter与ChannelOutboundHandlerAdapter默认只是简单地将请求传递给ChannelPipeline中的下一个ChannelHandler，源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class ChannelInboundHandlerAdapter extends ChannelHandlerAdapter implements ChannelInboundHandler &#123; public ChannelInboundHandlerAdapter() &#123; &#125; public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; ctx.fireChannelRegistered(); &#125; public void channelUnregistered(ChannelHandlerContext ctx) throws Exception &#123; ctx.fireChannelUnregistered(); &#125; public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; ctx.fireChannelActive(); &#125; public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; ctx.fireChannelInactive(); &#125; public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ctx.fireChannelRead(msg); &#125; public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.fireChannelReadComplete(); &#125; public void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception &#123; ctx.fireUserEventTriggered(evt); &#125; public void channelWritabilityChanged(ChannelHandlerContext ctx) throws Exception &#123; ctx.fireChannelWritabilityChanged(); &#125; public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.fireExceptionCaught(cause); &#125;&#125;public class ChannelOutboundHandlerAdapter extends ChannelHandlerAdapter implements ChannelOutboundHandler &#123; public ChannelOutboundHandlerAdapter() &#123; &#125; public void bind(ChannelHandlerContext ctx, SocketAddress localAddress, ChannelPromise promise) throws Exception &#123; ctx.bind(localAddress, promise); &#125; public void connect(ChannelHandlerContext ctx, SocketAddress remoteAddress, SocketAddress localAddress, ChannelPromise promise) throws Exception &#123; ctx.connect(remoteAddress, localAddress, promise); &#125; public void disconnect(ChannelHandlerContext ctx, ChannelPromise promise) throws Exception &#123; ctx.disconnect(promise); &#125; public void close(ChannelHandlerContext ctx, ChannelPromise promise) throws Exception &#123; ctx.close(promise); &#125; public void deregister(ChannelHandlerContext ctx, ChannelPromise promise) throws Exception &#123; ctx.deregister(promise); &#125; public void read(ChannelHandlerContext ctx) throws Exception &#123; ctx.read(); &#125; public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception &#123; ctx.write(msg, promise); &#125; public void flush(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush(); &#125;&#125; 对于处理入站消息，另外一种选择是继承SimpleChannelInboundHandler，它是Netty的一个继承于ChannelInboundHandlerAdapter的抽象类，并在其之上实现了自动释放资源的功能。 我们在了解ByteBuf时就已经知道了Netty使用了一套自己实现的引用计数算法来主动释放资源，假设你的ChannelHandler继承于ChannelInboundHandlerAdapter或ChannelOutboundHandlerAdapter，那么你就有责任去管理你所分配的ByteBuf，一般来说，一个消息对象（ByteBuf）已经被消费（或丢弃）了，并且不会传递给ChannelHandler链中的下一个处理器（如果该消息到达了实际的传输层，那么当它被写入或Channel关闭时，都会被自动释放），那么你就需要去手动释放它。通过一个简单的工具类ReferenceCountUtil的release方法，就可以做到这一点。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// 这个泛型为消息对象的类型public abstract class SimpleChannelInboundHandler&lt;I&gt; extends ChannelInboundHandlerAdapter &#123; private final TypeParameterMatcher matcher; private final boolean autoRelease; protected SimpleChannelInboundHandler() &#123; this(true); &#125; protected SimpleChannelInboundHandler(boolean autoRelease) &#123; this.matcher = TypeParameterMatcher.find(this, SimpleChannelInboundHandler.class, "I"); this.autoRelease = autoRelease; &#125; protected SimpleChannelInboundHandler(Class&lt;? extends I&gt; inboundMessageType) &#123; this(inboundMessageType, true); &#125; protected SimpleChannelInboundHandler(Class&lt;? extends I&gt; inboundMessageType, boolean autoRelease) &#123; this.matcher = TypeParameterMatcher.get(inboundMessageType); this.autoRelease = autoRelease; &#125; public boolean acceptInboundMessage(Object msg) throws Exception &#123; return this.matcher.match(msg); &#125; // SimpleChannelInboundHandler只是替你做了ReferenceCountUtil.release() public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; boolean release = true; try &#123; if(this.acceptInboundMessage(msg)) &#123; this.channelRead0(ctx, msg); &#125; else &#123; release = false; ctx.fireChannelRead(msg); &#125; &#125; finally &#123; if(this.autoRelease &amp;&amp; release) &#123; ReferenceCountUtil.release(msg); &#125; &#125; &#125; // 这个方法才是我们需要实现的方法 protected abstract void channelRead0(ChannelHandlerContext var1, I var2) throws Exception;&#125; // ReferenceCountUtil中的源码，release方法对消息对象的类型进行判断然后调用它的release()方法 public static boolean release(Object msg) &#123; return msg instanceof ReferenceCounted?((ReferenceCounted)msg).release():false; &#125; ChannelPipeline 为了模块化与解耦合，不可能由一个ChannelHandler来完成所有应用逻辑，所以Netty采用了拦截器链的设计。ChannelPipeline就是用来管理ChannelHandler实例链的容器，它的职责就是保证实例链的流动。 每一个新创建的Channel都将会被分配一个新的ChannelPipeline，这种关联关系是永久性的，一个Channel一生只能对应一个ChannelPipeline。 一个入站事件被触发时，它会先从ChannelPipeline的最左端（头部）开始一直传播到ChannelPipeline的最右端（尾部），而出站事件正好与入站事件顺序相反（从最右端一直传播到最左端）。这个顺序是定死的，Netty总是将ChannelPipeline的入站口作为头部，而将出站口作为尾部。在事件传播的过程中，ChannelPipeline会判断下一个ChannelHandler的类型是否和事件的运动方向相匹配，如果不匹配，就跳过该ChannelHandler并继续检查下一个（保证入站事件只会被ChannelInboundHandler处理），一个ChannelHandler也可以同时实现ChannelInboundHandler与ChannelOutboundHandler，它在入站事件与出站事件中都会被调用。 在阅读ChannelHandler的源码时，发现很多方法需要一个ChannelHandlerContext类型的参数，该接口是ChannelPipeline与ChannelHandler之间相关联的关键。ChannelHandlerContext可以通知ChannelPipeline中的当前ChannelHandler的下一个ChannelHandler，还可以动态地改变当前ChannelHandler在ChannelPipeline中的位置（通过调用ChannelPipeline中的各种方法来修改）。 ChannelHandlerContext负责了在同一个ChannelPipeline中的ChannelHandler与其他ChannelHandler之间的交互，每个ChannelHandlerContext都对应了一个ChannelHandler。在DefaultChannelPipeline的源码中，已经表现的很明显了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class DefaultChannelPipeline implements ChannelPipeline &#123; ......... // 头部节点和尾部节点的引用变量 // ChannelHandlerContext在ChannelPipeline中是以链表的形式组织的 final AbstractChannelHandlerContext head; final AbstractChannelHandlerContext tail; ......... // 添加一个ChannelHandler到链表尾部 public final ChannelPipeline addLast(String name, ChannelHandler handler) &#123; return this.addLast((EventExecutorGroup)null, name, handler); &#125; public final ChannelPipeline addLast(EventExecutorGroup group, String name, ChannelHandler handler) &#123; final AbstractChannelHandlerContext newCtx; synchronized(this) &#123; // 检查ChannelHandler是否为一个共享对象(@Sharable) // 如果该ChannelHandler没有@Sharable注解，并且是已被添加过的那么就抛出异常 checkMultiplicity(handler); // 返回一个DefaultChannelHandlerContext，注意该对象持有了传入的ChannelHandler newCtx = this.newContext(group, this.filterName(name, handler), handler); this.addLast0(newCtx); // 如果当前ChannelPipeline没有被注册，那么就先加到未决链表中 if(!this.registered) &#123; newCtx.setAddPending(); this.callHandlerCallbackLater(newCtx, true); return this; &#125; // 否则就调用ChannelHandler中的handlerAdded() EventExecutor executor = newCtx.executor(); if(!executor.inEventLoop()) &#123; newCtx.setAddPending(); executor.execute(new Runnable() &#123; public void run() &#123; DefaultChannelPipeline.this.callHandlerAdded0(newCtx); &#125; &#125;); return this; &#125; &#125; this.callHandlerAdded0(newCtx); return this; &#125; // 将新的ChannelHandlerContext插入到尾部与尾部之前的节点之间 private void addLast0(AbstractChannelHandlerContext newCtx) &#123; AbstractChannelHandlerContext prev = this.tail.prev; newCtx.prev = prev; newCtx.next = this.tail; prev.next = newCtx; this.tail.prev = newCtx; &#125; .....&#125; ChannelHandlerContext还定义了许多与Channel和ChannelPipeline重合的方法（像read()、write()、connect()这些用于出站的方法或者如fireChannelXXXX()这样用于入站的方法），不同之处在于调用Channel或者ChannelPipeline上的这些方法，它们将会从头沿着整个ChannelHandler实例链进行传播，而调用位于ChannelHandlerContext上的相同方法，则会从当前所关联的ChannelHandler开始，且只会传播给实例链中的下一个ChannelHandler。而且，事件之间的移动（从一个ChannelHandler到下一个ChannelHandler）也是通过ChannelHandlerContext中的方法调用完成的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class DefaultChannelPipeline implements ChannelPipeline &#123; public final ChannelPipeline fireChannelRead(Object msg) &#123; // 注意这里将头节点传入了进去 AbstractChannelHandlerContext.invokeChannelRead(this.head, msg); return this; &#125; &#125;abstract class AbstractChannelHandlerContext extends DefaultAttributeMap implements ChannelHandlerContext, ResourceLeakHint &#123; static void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) &#123; final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, "msg"), next); EventExecutor executor = next.executor(); if(executor.inEventLoop()) &#123; next.invokeChannelRead(m); &#125; else &#123; executor.execute(new Runnable() &#123; public void run() &#123; next.invokeChannelRead(m); &#125; &#125;); &#125; &#125; private void invokeChannelRead(Object msg) &#123; if(this.invokeHandler()) &#123; try &#123; ((ChannelInboundHandler)this.handler()).channelRead(this, msg); &#125; catch (Throwable var3) &#123; this.notifyHandlerException(var3); &#125; &#125; else &#123; // 寻找下一个ChannelHandler this.fireChannelRead(msg); &#125; &#125; public ChannelHandlerContext fireChannelRead(Object msg) &#123; invokeChannelRead(this.findContextInbound(), msg); return this; &#125; private AbstractChannelHandlerContext findContextInbound() &#123; AbstractChannelHandlerContext ctx = this; do &#123; ctx = ctx.next; &#125; while(!ctx.inbound); // 直到找到一个ChannelInboundHandler return ctx; &#125; &#125; EventLoop 为了最大限度地提供高性能和可维护性，Netty设计了一套强大又易用的线程模型。在一个网络框架中，最重要的能力是能够快速高效地处理在连接的生命周期内发生的各种事件，与之相匹配的程序构造被称为事件循环，Netty定义了接口EventLoop来负责这项工作。 如果是经常用Java进行多线程开发的童鞋想必经常会使用到线程池，也就是Executor这套API。Netty就是从Executor（java.util.concurrent）之上扩展了自己的EventExecutorGroup（io.netty.util.concurrent），同时为了与Channel的事件进行交互，还扩展了EventLoopGroup接口（io.netty.channel）。在io.netty.util.concurrent包下的EventExecutorXXX负责实现线程并发相关的工作，而在io.netty.channel包下的EventLoopXXX负责实现网络编程相关的工作（处理Channel中的事件）。 在Netty的线程模型中，一个EventLoop将由一个永远不会改变的Thread驱动，而一个Channel一生只会使用一个EventLoop（但是一个EventLoop可能会被指派用于服务多个Channel），在Channel中的所有I/O操作和事件都由EventLoop中的线程处理，也就是说一个Channel的一生之中都只会使用到一个线程。不过在Netty3，只有入站事件会被EventLoop处理，所有出站事件都会由调用线程处理，这种设计导致了ChannelHandler的线程安全问题。Netty4简化了线程模型，通过在同一个线程处理所有事件，既解决了这个问题，还提供了一个更加简单的架构。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384package io.netty.channel;public abstract class SingleThreadEventLoop extends SingleThreadEventExecutor implements EventLoop &#123; protected static final int DEFAULT_MAX_PENDING_TASKS = Math.max(16, SystemPropertyUtil.getInt("io.netty.eventLoop.maxPendingTasks", 2147483647)); private final Queue&lt;Runnable&gt; tailTasks; protected SingleThreadEventLoop(EventLoopGroup parent, ThreadFactory threadFactory, boolean addTaskWakesUp) &#123; this(parent, threadFactory, addTaskWakesUp, DEFAULT_MAX_PENDING_TASKS, RejectedExecutionHandlers.reject()); &#125; protected SingleThreadEventLoop(EventLoopGroup parent, Executor executor, boolean addTaskWakesUp) &#123; this(parent, executor, addTaskWakesUp, DEFAULT_MAX_PENDING_TASKS, RejectedExecutionHandlers.reject()); &#125; protected SingleThreadEventLoop(EventLoopGroup parent, ThreadFactory threadFactory, boolean addTaskWakesUp, int maxPendingTasks, RejectedExecutionHandler rejectedExecutionHandler) &#123; super(parent, threadFactory, addTaskWakesUp, maxPendingTasks, rejectedExecutionHandler); this.tailTasks = this.newTaskQueue(maxPendingTasks); &#125; protected SingleThreadEventLoop(EventLoopGroup parent, Executor executor, boolean addTaskWakesUp, int maxPendingTasks, RejectedExecutionHandler rejectedExecutionHandler) &#123; super(parent, executor, addTaskWakesUp, maxPendingTasks, rejectedExecutionHandler); this.tailTasks = this.newTaskQueue(maxPendingTasks); &#125; // 返回它所在的EventLoopGroup public EventLoopGroup parent() &#123; return (EventLoopGroup)super.parent(); &#125; public EventLoop next() &#123; return (EventLoop)super.next(); &#125; // 注册Channel,这里ChannelPromise和Channel关联到了一起 public ChannelFuture register(Channel channel) &#123; return this.register((ChannelPromise)(new DefaultChannelPromise(channel, this))); &#125; public ChannelFuture register(ChannelPromise promise) &#123; ObjectUtil.checkNotNull(promise, "promise"); promise.channel().unsafe().register(this, promise); return promise; &#125; // 剩下这些函数都是用于调度任务 public final void executeAfterEventLoopIteration(Runnable task) &#123; ObjectUtil.checkNotNull(task, "task"); if(this.isShutdown()) &#123; reject(); &#125; if(!this.tailTasks.offer(task)) &#123; this.reject(task); &#125; if(this.wakesUpForTask(task)) &#123; this.wakeup(this.inEventLoop()); &#125; &#125; final boolean removeAfterEventLoopIterationTask(Runnable task) &#123; return this.tailTasks.remove(ObjectUtil.checkNotNull(task, "task")); &#125; protected boolean wakesUpForTask(Runnable task) &#123; return !(task instanceof SingleThreadEventLoop.NonWakeupRunnable); &#125; protected void afterRunningAllTasks() &#123; this.runAllTasksFrom(this.tailTasks); &#125; protected boolean hasTasks() &#123; return super.hasTasks() || !this.tailTasks.isEmpty(); &#125; public int pendingTasks() &#123; return super.pendingTasks() + this.tailTasks.size(); &#125; interface NonWakeupRunnable extends Runnable &#123; &#125;&#125; 为了确保一个Channel的整个生命周期中的I/O事件会被一个EventLoop负责，Netty通过inEventLoop()方法来判断当前执行的线程的身份，确定它是否是分配给当前Channel以及它的EventLoop的那一个线程。如果当前（调用）线程正是EventLoop中的线程，那么所提交的任务将会被直接执行，否则，EventLoop将调度该任务以便稍后执行，并将它放入内部的任务队列（每个EventLoop都有它自己的任务队列，从SingleThreadEventLoop的源码就能发现很多用于调度内部任务队列的方法），在下次处理它的事件时，将会执行队列中的那些任务。这种设计可以让任何线程与Channel直接交互，而无需在ChannelHandler中进行额外的同步。 从性能上来考虑，千万不要将一个需要长时间来运行的任务放入到任务队列中，它会影响到该队列中的其他任务的执行。解决方案是使用一个专门的EventExecutor来执行它（ChannelPipeline提供了带有EventExecutorGroup参数的addXXX()方法，该方法可以将传入的ChannelHandler绑定到你传入的EventExecutor之中），这样它就会在另一条线程中执行，与其他任务隔离。 12345678910111213141516171819202122232425262728293031public abstract class SingleThreadEventExecutor extends AbstractScheduledEventExecutor implements OrderedEventExecutor &#123;..... public void execute(Runnable task) &#123; if(task == null) &#123; throw new NullPointerException("task"); &#125; else &#123; boolean inEventLoop = this.inEventLoop(); if(inEventLoop) &#123; this.addTask(task); &#125; else &#123; this.startThread(); this.addTask(task); if(this.isShutdown() &amp;&amp; this.removeTask(task)) &#123; reject(); &#125; &#125; if(!this.addTaskWakesUp &amp;&amp; this.wakesUpForTask(task)) &#123; this.wakeup(inEventLoop); &#125; &#125; &#125; public boolean inEventLoop(Thread thread) &#123; return thread == this.thread; &#125; .....&#125; EventLoopGroup负责管理和分配EventLoop（创建EventLoop和为每个新创建的Channel分配EventLoop），根据不同的传输类型，EventLoop的创建和分配方式也不同。例如，使用NIO传输类型，EventLoopGroup就会只使用较少的EventLoop（一个EventLoop服务于多个Channel），这是因为NIO基于I/O多路复用，一个线程可以处理多个连接，而如果使用的是OIO，那么新创建一个Channel（连接）就需要分配一个EventLoop（线程）。 Bootstrap 在深入了解地Netty的核心组件之后，发现它们的设计都很模块化，如果想要实现你自己的应用程序，就需要将这些组件组装到一起。Netty通过Bootstrap类，以对一个Netty应用程序进行配置（组装各个组件），并最终使它运行起来。对于客户端程序和服务器程序所使用到的Bootstrap类是不同的，后者需要使用ServerBootstrap，这样设计是因为，在如TCP这样有连接的协议中，服务器程序往往需要一个以上的Channel，通过父Channel来接受来自客户端的连接，然后创建子Channel用于它们之间的通信，而像UDP这样无连接的协议，它不需要每个连接都创建子Channel，只需要一个Channel即可。 一个比较明显的差异就是Bootstrap与ServerBootstrap的group()方法，后者提供了一个接收2个EventLoopGroup的版本。 12345678910111213141516171819202122232425262728// 该方法在Bootstrap的父类AbstractBootstrap中，泛型B为它当前子类的类型（为了链式调用） public B group(EventLoopGroup group) &#123; if(group == null) &#123; throw new NullPointerException("group"); &#125; else if(this.group != null) &#123; throw new IllegalStateException("group set already"); &#125; else &#123; this.group = group; return this; &#125; &#125;// ServerBootstrap中的实现，它也支持只用一个EventLoopGroup public ServerBootstrap group(EventLoopGroup group) &#123; return this.group(group, group); &#125; public ServerBootstrap group(EventLoopGroup parentGroup, EventLoopGroup childGroup) &#123; super.group(parentGroup); if(childGroup == null) &#123; throw new NullPointerException("childGroup"); &#125; else if(this.childGroup != null) &#123; throw new IllegalStateException("childGroup set already"); &#125; else &#123; this.childGroup = childGroup; return this; &#125; &#125; Bootstrap其实没有什么可以好说的，它就只是一个装配工，将各个组件拼装组合到一起，然后进行一些配置，有关它的详细API请参考Netty JavaDoc。下面我们将通过一个经典的Echo客户端与服务器的例子，来梳理一遍创建Netty应用的流程。 首先实现的是服务器，我们先实现一个EchoServerInboundHandler，处理入站消息。 1234567891011121314151617181920212223242526public class EchoServerInboundHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ByteBuf in = (ByteBuf) msg; System.out.printf("Server received: %s \n", in.toString(CharsetUtil.UTF_8)); // 由于读事件不是一次性就能把完整消息发送过来的，这里并没有调用writeAndFlush ctx.write(in); // 直接把消息写回给客户端(会被出站消息处理器处理,不过我们的应用没有实现任何出站消息处理器) &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; // 等读事件已经完成时,冲刷之前写数据的缓冲区 // 然后添加了一个监听器，它会在Future完成时进行关闭该Channel. ctx.writeAndFlush(Unpooled.EMPTY_BUFFER) .addListener(ChannelFutureListener.CLOSE); &#125; // 处理异常，输出异常信息，然后关闭Channel @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 服务器的应用逻辑只有这么多，剩下就是用ServerBootstrap进行配置了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class EchoServer &#123; private final int port; public EchoServer(int port) &#123; this.port = port; &#125; public void start() throws Exception &#123; final EchoServerInboundHandler serverHandler = new EchoServerInboundHandler(); EventLoopGroup group = new NioEventLoopGroup(); // 传输类型使用NIO try &#123; ServerBootstrap b = new ServerBootstrap(); b.group(group) // 配置EventLoopGroup .channel(NioServerSocketChannel.class) // 配置Channel的类型 .localAddress(new InetSocketAddress(port)) // 配置端口号 .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; // 实现一个ChannelInitializer，它可以方便地添加多个ChannelHandler @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; socketChannel.pipeline().addLast(serverHandler); &#125; &#125;); // i绑定地址，同步等待它完成 ChannelFuture f = b.bind().sync(); // 关闭这个Future f.channel().closeFuture().sync(); &#125; finally &#123; // 关闭应用程序，一般来说Netty应用只需要调用这个方法就够了 group.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; if (args.length != 1) &#123; System.err.printf( "Usage: %s &lt;port&gt; \n", EchoServer.class.getSimpleName() ); return; &#125; int port = Integer.parseInt(args[0]); new EchoServer(port).start(); &#125;&#125; 接下来实现客户端，同样需要先实现一个入站消息处理器。 1234567891011121314151617181920212223public class EchoClientInboundHandler extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123; /** * 我们在Channel连接到远程节点直接发送一条消息给服务器 */ @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; ctx.writeAndFlush(Unpooled.copiedBuffer("Hello, Netty!", CharsetUtil.UTF_8)); &#125; @Override protected void channelRead0(ChannelHandlerContext channelHandlerContext, ByteBuf byteBuf) throws Exception &#123; // 输出从服务器Echo的消息 System.out.printf("Client received: %s \n", byteBuf.toString(CharsetUtil.UTF_8)); &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 然后配置客户端。 123456789101112131415161718192021222324252627282930313233343536373839404142public class EchoClient &#123; private final String host; private final int port; public EchoClient(String host, int port) &#123; this.host = host; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup group = new NioEventLoopGroup(); try &#123; Bootstrap b = new Bootstrap(); b.group(group) .channel(NioSocketChannel.class) .remoteAddress(new InetSocketAddress(host, port)) // 服务器的地址 .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel socketChannel) throws Exception &#123; socketChannel.pipeline().addLast(new EchoClientInboundHandler()); &#125; &#125;); ChannelFuture f = b.connect().sync(); // 连接到服务器 f.channel().closeFuture().sync(); &#125; finally &#123; group.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; if (args.length != 2) &#123; System.err.printf("Usage: %s &lt;host&gt; &lt;port&gt; \n", EchoClient.class.getSimpleName()); return; &#125; String host = args[0]; int port = Integer.parseInt(args[1]); new EchoClient(host, port).start(); &#125;&#125; 实现一个Netty应用程序就是如此简单，用户大多数都是在编写各种应用逻辑的ChannelHandler（或者使用Netty内置的各种实用ChannelHandler），然后只需要将它们全部添加到ChannelPipeline即可。 参考文献 Netty: Home Chapter 6. I/O Multiplexing: The select and poll Functions - Shichao’s Notes epoll(7) - Linux manual page Java NIO]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>2017</tag>
        <tag>网络</tag>
        <tag>后端</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker的那点事儿]]></title>
    <url>%2F2017%2F11%2F19%2F2017-11-19-docker_introduction%2F</url>
    <content type="text"><![CDATA[Docker是什么？ Docker是一个基于轻量级虚拟化技术的容器，整个项目基于Go语言开发，并采用了Apache 2.0协议。Docker可以将我们的应用程序打包封装到一个容器中，该容器包含了应用程序的代码、运行环境、依赖库、配置文件等必需的资源，通过容器就可以实现方便快速并且与平台解耦的自动化部署方式，无论你部署时的环境如何，容器中的应用程序都会运行在同一种环境下。 举个栗子，小明写了一个CMS系统，该系统的技术栈非常广，需要依赖于各种开源库和中间件。如果按照纯手动的部署方式，小明需要安装各种开源软件，还需要写好每个开源软件的配置文件。如果只是部署一次，这点时间开销还是可以接受的，但如果小明每隔几天就需要换个服务器去部署他的程序，那么这些繁琐的重复工作无疑是会令人发狂的。这时候，Docker的用处就派上场了，小明只需要根据应用程序的部署步骤编写一份Dockerfile文件（将安装、配置等操作交由Docker自动化处理），然后构建并发布他的镜像，这样，不管在什么机器上，小明都只需要拉取他需要的镜像，然后就可以直接部署运行了，这正是Docker的魅力所在。 那么镜像又是什么呢？镜像是Docker中的一个重要概念： Image（镜像）：它类似于虚拟机中使用到的镜像，由于任何应用程序都需要有它自己的运行环境，Image就是用来提供所需运行环境的一个模板。 Container（容器）：Container是Docker提供的一个抽象层，它就像一个轻量级的沙盒，其中包含了一个极简的Linux系统环境与运行在其中的应用程序。Container是Image的运行实例（Image本身是只读的，Container启动时，Docker会在Image的上层创建一个可写层，任何在Container中的修改都不会影响到Image，如果想要在Image保存Container中的修改，Docker采用了基于Container生成新的Image层的策略），Docker引擎利用Container来操作并隔离每个应用（也就是说，每个容器中的应用都是互相独立的）。 其实从Docker与Container的英文单词原意中就可以体会出Docker的思想。Container可以释义为集装箱，集装箱是一个可以便于机械设备装卸的封装货物的通用标准规格，它的发明简化了物流运输的机械化过程，使其建立起了一套标准化的物流运输体系。而Docker的意思为码头工人，可以认为，Docker就像是在码头上辛勤工作的工人，把应用打包成一个个具有某种标准化规格的”集装箱”（其实这里指出的集装箱对应的是Image，在Docker中Container更像是一个运行中的沙盒），当货物运输到目的地后，码头工人们（Docker）就可以把集装箱拆开取出其中的货物（基于Image来创建Container并运行）。这种标准化与隔离性可以很方便地组合使用多个Image来构建你的应用环境（Docker也提倡每个Image都遵循单一职责原则，也就是只做好一件事），或者与其他人共享你的Image。 本文作者为SylvanasSun(sylvanas.sun@gmail.com)，首发于SylvanasSun’s Blog。原文链接：https://sylvanassun.github.io/2017/11/19/2017-11-19-docker_introduction/（转载请务必保留本段声明，并且保留超链接。） Docker VS 虚拟机 在上文中我们提到了Docker是基于轻量级虚拟化技术的，所以它与我们平常使用的虚拟机是不一样的。虚拟机技术可以分成以下两类： 系统虚拟机：通过软件对计算机系统的模拟来提供一个真实计算机的替代品。它是物理硬件的抽象并提供了运行完整操作系统所需的功能。虚拟机通过物理机器来管理和共享硬件，这样实现了多个虚拟机环境彼此之间的隔离，一台机器上可以运行多个虚拟机，每个虚拟机包括一个操作系统的完整副本。在系统虚拟机中，所运行的所有软件或操作都只会影响到该虚拟机的环境。我们经常使用的VMWare就是系统虚拟机的实现。 程序虚拟机：允许程序独立运行在平台之外。比较典型的例子就是JVM，Java通过JVM这一抽象层使得Java程序与操作系统和硬件平台解耦（因为每个Java程序都是运行在JVM中的），因此实现了所谓的compile once, run everywhere。 Docker所用到的技术与上述两种都不相同，它使用了更轻量级的虚拟化技术，多个Container共享了同一个操作系统内核，并且就像运行在本地上一样。Container技术相对于虚拟机来说，只是一个应用程序层的抽象，它将代码与依赖关系打包到一起，多个Container可以在同一台机器上运行（意味着一个虚拟机上也可以运行多个Container），并与其它Container共享操作系统内核，每一个Container都在用户空间中作为一个独立的进程运行，这些特性都证明了Container要比虚拟机更加灵活与轻量（一般都是结合虚拟机与Docker一起使用）。 Container技术其实并不是个新鲜事物，最早可以追溯到UNIX中的chroot（在1979年的V7 Unix中引入），它可以改变当前正在运行的进程及其子目录的根目录，在这种修改过的环境下运行的程序不能在指定的目录树之外访问文件，从而限制用户的活动范围，为进程提供了隔离空间。 之后各种Unix版本涌现出很多Container技术，在2006年，Google提出了”Process Containers”期望在Linux内核中实现进程资源隔离的相关特性，由于Container在Linux内核中的定义过于宽泛混乱，后来该项目改名为CGroups（Control Groups），实现了对进程的资源限制。 2008年，LXC（Linux Containers）发布，它是一种在操作系统层级上的虚拟化方法，用于在Linux系统上通过共享一个内核来运行多个互相隔离的程序（Container）。LXC正是结合了Linux内核中的CGroups和对分离的名称空间的支持来为应用程序提供了一个隔离的环境。而Docker也是基于LXC实现的（Docker的前身是dotClound公司中的内部项目，它是一家提供PaaS服务的公司。），并作出了许多改进。 使用Docker 在使用Docker之前你需要先安装Docker（这好像是一句废话。。。），根据不同的平台安装方法都不相同，可以去参考Install Docker | Docker Documentation或者自行Google。 安装完毕之后，输入docker --version来确认是否安装成功。 12$ docker --versionDocker version 17.05.0-ce-rc1, build 2878a85 从Docker Hub中可以pull到其他人发布的Image，我们也可以注册一个账号去发布自己的Image与他人共享。 1234567891011[root@Jack ~]# docker search redis # 查看redis镜像是否存在[root@Jack ~]# docker pull redis # 拉取redis镜像到本机Using default tag: latestTrying to pull repository docker.io/library/redis ... latest: Pulling from docker.io/library/redisDigest: sha256:cd277716dbff2c0211c8366687d275d2b53112fecbf9d6c86e9853edb0900956[root@Jack ~]# docker images # 查看镜像信息REPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/python 3.6-onbuild 7195f9298ffb 2 weeks ago 691.1 MBdocker.io/mongo latest d22888af0ce0 2 weeks ago 360.9 MBdocker.io/redis latest 8f2e175b3bd1 2 weeks ago 106.6 MB 有了Image，之后就可以在其之上运行一个Container了，命令如下。 12345678910111213[root@Jack ~]# docker run -d -p 6379:6379 redis # 运行redis，-p代表将本机上6379端口映射到Container的6379端口 -d代表在后台启动[root@Jack ~]# docker ps -a # 查看容器信息，如果不加-a只会显示当前运行中的容器# 如果想要进入容器中，那么需要执行以下命令[root@Jack ~]# docker ps # 先获得容器的idCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1f928073b7eb redis "docker-entrypoint.sh" 45 seconds ago Up 44 seconds 0.0.0.0:6379-&gt;6379/tcp desperate_khorana[root@Jack ~]# docker exec -it 1f928073b7eb /bin/bash # 然后再执行该命令进入到容器中root@1f928073b7eb:/data# touch hello_docker.txt # 在容器中创建一个文件root@1f928073b7eb:/data# exit # 退出exit[root@Jack ~]# # 也可以在启动时直接进入 命令如下[root@Jack ~]# docker run -d -it -p 6379:6379 redis /bin/bash 我们对Container做出了修改，如果想要保留这个修改，可以通过commit命令来生成一个新的Image。 12345678910# -m为描述信息 -a为作者 1f9是你要保存的容器id 取前3个字符 docker可以自行识别# sylvanassun/redis为镜像名 :test 为一个tag 一般用于标识版本[root@Jack ~]# docker commit -m "test" -a "SylvanasSun" 1f9 sylvanassun/redis:testsha256:e7073e8e5bd70b8d58092fd6bd8c2551e65dd29241c235eddf2a7f4b4b25cbbd[root@Jack ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEsylvanassun/redis test e7073e8e5bd7 2 seconds ago 106.6 MBdocker.io/python 3.6-onbuild 7195f9298ffb 2 weeks ago 691.1 MBdocker.io/mongo latest d22888af0ce0 2 weeks ago 360.9 MBdocker.io/redis latest 8f2e175b3bd1 2 weeks ago 106.6 MB 想删除一个容器或镜像也很简单，但在删除镜像前需要先删除依赖于它的容器。 12345678[root@Jack ~]# docker stop 1f9 # 关闭运行中的容器，相应的也有docker start id命令来启动一个容器1f9[root@Jack ~]# docker rm 1f9 # 删除容器1f9[root@Jack ~]# docker rmi e70 # 删除上面保存的镜像Untagged: sylvanassun/redis:testDeleted: sha256:e7073e8e5bd70b8d58092fd6bd8c2551e65dd29241c235eddf2a7f4b4b25cbbdDeleted: sha256:751db4a870e5f703082b31c1614a19c86e0c967334a61f5d22b2511072aef56d 如果想要自己构建一个镜像，那么需要编写Dockerfile文件，该文件描述了镜像的依赖环境以及如何配置你的应用环境。 1234567891011121314151617181920# 使用python:2.7-slim 作为父镜像FROM python:2.7-slim# 跳转到/app 其实就是cd命令WORKDIR /app# 将当前目录的内容(.)复制到镜像的/app目录下ADD . /app# RUN代表运行的shell命令，下面这条命令是根据requirements.txt安装python应用的依赖包RUN pip install --trusted-host pypi.python.org -r requirements.txt# 暴露80端口让外界访问EXPOSE 80# 定义环境变量ENV NAME World# 当容器启动时执行的命令，它与RUN不同，只在容器启动时执行一次CMD ["python", "app.py"] 然后就可以通过docker build -t xxx/xxxx .命令来构建镜像，-t后面是镜像名与tag等信息，注意.表示在当前目录下寻找Dockerfile文件。 学会如何构建自己的镜像之后，你是否也想将它发布到Docker Hub上与他人分享呢？要想做到这一点，需要先注册一个Docker Hub账号，之后通过docker login命令登录，然后再docker push image name，就像在使用Git一样简单。 关于Docker的更多命令与使用方法，请参考Docker Documentation | Docker Documentation，另外我还推荐使用Docker Compose来构建镜像，它可以很方便地组合管理多个镜像。 结语 Docker提供了非常强大的自动化部署方式与灵活性，对多个应用程序之间做到了解耦，提供了开发上的敏捷性、可控性以及可移植性。同时，Docker也在不断地帮助越来越多的企业实现了向云端迁移、向微服务转型以及向DevOps模式的实践。 如今，微服务与DevOps火爆程度日益渐高，你又有何理由选择拒绝Docker呢？让我们一起选择拥抱Docker，拥抱未来！]]></content>
      <categories>
        <category>后端</category>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>后端</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊一聊Spring中的线程安全性]]></title>
    <url>%2F2017%2F11%2F06%2F2017-11-06-spring_and_thread-safe%2F</url>
    <content type="text"><![CDATA[Spring与线程安全 Spring作为一个IOC/DI容器，帮助我们管理了许许多多的“bean”。但其实，Spring并没有保证这些对象的线程安全，需要由开发者自己编写解决线程安全问题的代码。 Spring对每个bean提供了一个scope属性来表示该bean的作用域。它是bean的生命周期。例如，一个scope为singleton的bean，在第一次被注入时，会创建为一个单例对象，该对象会一直被复用到应用结束。 singleton：默认的scope，每个scope为singleton的bean都会被定义为一个单例对象，该对象的生命周期是与Spring IOC容器一致的（但在第一次被注入时才会创建）。 prototype：bean被定义为在每次注入时都会创建一个新的对象。 request：bean被定义为在每个HTTP请求中创建一个单例对象，也就是说在单个请求中都会复用这一个单例对象。 session：bean被定义为在一个session的生命周期内创建一个单例对象。 application：bean被定义为在ServletContext的生命周期中复用一个单例对象。 websocket：bean被定义为在websocket的生命周期中复用一个单例对象。 我们交由Spring管理的大多数对象其实都是一些无状态的对象，这种不会因为多线程而导致状态被破坏的对象很适合Spring的默认scope，每个单例的无状态对象都是线程安全的（也可以说只要是无状态的对象，不管单例多例都是线程安全的，不过单例毕竟节省了不断创建对象与GC的开销）。 无状态的对象即是自身没有状态的对象，自然也就不会因为多个线程的交替调度而破坏自身状态导致线程安全问题。无状态对象包括我们经常使用的DO、DTO、VO这些只作为数据的实体模型的贫血对象，还有Service、DAO和Controller，这些对象并没有自己的状态，它们只是用来执行某些操作的。例如，每个DAO提供的函数都只是对数据库的CRUD，而且每个数据库Connection都作为函数的局部变量（局部变量是在用户栈中的，而且用户栈本身就是线程私有的内存区域，所以不存在线程安全问题），用完即关（或交还给连接池）。 有人可能会认为，我使用request作用域不就可以避免每个请求之间的安全问题了吗？这是完全错误的，因为Controller默认是单例的，一个HTTP请求是会被多个线程执行的，这就又回到了线程的安全问题。当然，你也可以把Controller的scope改成prototype，实际上Struts2就是这么做的，但有一点要注意，Spring MVC对请求的拦截粒度是基于每个方法的，而Struts2是基于每个类的，所以把Controller设为多例将会频繁的创建与回收对象，严重影响到了性能。 通过阅读上文其实已经说的很清楚了，Spring根本就没有对bean的多线程安全问题做出任何保证与措施。对于每个bean的线程安全问题，根本原因是每个bean自身的设计。不要在bean中声明任何有状态的实例变量或类变量，如果必须如此，那么就使用ThreadLocal把变量变为线程私有的，如果bean的实例变量或类变量需要在多个线程之间共享，那么就只能使用synchronized、lock、CAS等这些实现线程同步的方法了。 下面将通过解析ThreadLocal的源码来了解它的实现与作用，ThreadLocal是一个很好用的工具类，它在某些情况下解决了线程安全问题（在变量不需要被多个线程共享时）。 本文作者为SylvanasSun(sylvanas.sun@gmail.com)，首发于SylvanasSun’s Blog。原文链接：https://sylvanassun.github.io/2017/11/06/2017-11-06-spring_and_thread-safe/（转载请务必保留本段声明，并且保留超链接。） ThreadLocal ThreadLocal是一个为线程提供线程局部变量的工具类。它的思想也十分简单，就是为线程提供一个线程私有的变量副本，这样多个线程都可以随意更改自己线程局部的变量，不会影响到其他线程。不过需要注意的是，ThreadLocal提供的只是一个浅拷贝，如果变量是一个引用类型，那么就要考虑它内部的状态是否会被改变，想要解决这个问题可以通过重写ThreadLocal的initialValue()函数来自己实现深拷贝，建议在使用ThreadLocal时一开始就重写该函数。 ThreadLocal与像synchronized这样的锁机制是不同的。首先，它们的应用场景与实现思路就不一样，锁更强调的是如何同步多个线程去正确地共享一个变量，ThreadLocal则是为了解决同一个变量如何不被多个线程共享。从性能开销的角度上来讲，如果锁机制是用时间换空间的话，那么ThreadLocal就是用空间换时间。 ThreadLocal中含有一个叫做ThreadLocalMap的内部类，该类为一个采用线性探测法实现的HashMap。它的key为ThreadLocal对象而且还使用了WeakReference，ThreadLocalMap正是用来存储变量副本的。 123456789101112131415161718192021222324252627282930 /** * ThreadLocalMap is a customized hash map suitable only for * maintaining thread local values. No operations are exported * outside of the ThreadLocal class. The class is package private to * allow declaration of fields in class Thread. To help deal with * very large and long-lived usages, the hash table entries use * WeakReferences for keys. However, since reference queues are not * used, stale entries are guaranteed to be removed only when * the table starts running out of space. */ static class ThreadLocalMap &#123; /** * The entries in this hash map extend WeakReference, using * its main ref field as the key (which is always a * ThreadLocal object). Note that null keys (i.e. entry.get() * == null) mean that the key is no longer referenced, so the * entry can be expunged from table. Such entries are referred to * as "stale entries" in the code that follows. */ static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125; ....&#125; ThreadLocal中只含有三个成员变量，这三个变量都是与ThreadLocalMap的hash策略相关的。 1234567891011121314151617181920212223242526272829303132/** * ThreadLocals rely on per-thread linear-probe hash maps attached * to each thread (Thread.threadLocals and * inheritableThreadLocals). The ThreadLocal objects act as keys, * searched via threadLocalHashCode. This is a custom hash code * (useful only within ThreadLocalMaps) that eliminates collisions * in the common case where consecutively constructed ThreadLocals * are used by the same threads, while remaining well-behaved in * less common cases. */private final int threadLocalHashCode = nextHashCode();/** * The next hash code to be given out. Updated atomically. Starts at * zero. */private static AtomicInteger nextHashCode = new AtomicInteger();/** * The difference between successively generated hash codes - turns * implicit sequential thread-local IDs into near-optimally spread * multiplicative hash values for power-of-two-sized tables. */private static final int HASH_INCREMENT = 0x61c88647;/** * Returns the next hash code. */private static int nextHashCode() &#123; return nextHashCode.getAndAdd(HASH_INCREMENT);&#125; 唯一的实例变量threadLocalHashCode是用来进行寻址的hashcode，它由函数nextHashCode()生成，该函数简单地通过一个增量HASH_INCREMENT来生成hashcode。至于为什么这个增量为0x61c88647，主要是因为ThreadLocalMap的初始大小为16，每次扩容都会为原来的2倍，这样它的容量永远为2的n次方，该增量选为0x61c88647也是为了尽可能均匀地分布，减少碰撞冲突。 1234567891011121314151617/** * The initial capacity -- MUST be a power of two. */private static final int INITIAL_CAPACITY = 16; /** * Construct a new map initially containing (firstKey, firstValue). * ThreadLocalMaps are constructed lazily, so we only create * one when we have at least one entry to put in it. */ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY);&#125; 要获得当前线程私有的变量副本需要调用get()函数。首先，它会调用getMap()函数去获得当前线程的ThreadLocalMap，这个函数需要接收当前线程的实例作为参数。如果得到的ThreadLocalMap为null，那么就去调用setInitialValue()函数来进行初始化，如果不为null，就通过map来获得变量副本并返回。 setInitialValue()函数会去先调用initialValue()函数来生成初始值，该函数默认返回null，我们可以通过重写这个函数来返回我们想要在ThreadLocal中维护的变量。之后，去调用getMap()函数获得ThreadLocalMap，如果该map已经存在，那么就用新获得value去覆盖旧值，否则就调用createMap()函数来创建新的map。 123456789101112131415161718192021222324252627282930313233343536373839404142/** * Returns the value in the current thread's copy of this * thread-local variable. If the variable has no value for the * current thread, it is first initialized to the value returned * by an invocation of the &#123;@link #initialValue&#125; method. * * @return the current thread's value of this thread-local */public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125; /** * Variant of set() to establish initialValue. Used instead * of set() in case user has overridden the set() method. * * @return the initial value */private T setInitialValue() &#123; T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value;&#125; protected T initialValue() &#123; return null;&#125; ThreadLocal的set()与remove()函数要比get()的实现还要简单，都只是通过getMap()来获得ThreadLocalMap然后对其进行操作。 12345678910111213141516171819202122232425262728293031323334/** * Sets the current thread's copy of this thread-local variable * to the specified value. Most subclasses will have no need to * override this method, relying solely on the &#123;@link #initialValue&#125; * method to set the values of thread-locals. * * @param value the value to be stored in the current thread's copy of * this thread-local. */public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125;/** * Removes the current thread's value for this thread-local * variable. If this thread-local variable is subsequently * &#123;@linkplain #get read&#125; by the current thread, its value will be * reinitialized by invoking its &#123;@link #initialValue&#125; method, * unless its value is &#123;@linkplain #set set&#125; by the current thread * in the interim. This may result in multiple invocations of the * &#123;@code initialValue&#125; method in the current thread. * * @since 1.5 */ public void remove() &#123; ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this); &#125; getMap()函数与createMap()函数的实现也十分简单，但是通过观察这两个函数可以发现一个秘密：ThreadLocalMap是存放在Thread中的。 123456789101112131415161718192021222324252627282930313233 /** * Get the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @return the map */ ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals; &#125; /** * Create the map associated with a ThreadLocal. Overridden in * InheritableThreadLocal. * * @param t the current thread * @param firstValue value for the initial entry of the map */ void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue); &#125;// Thread中的源码 /* ThreadLocal values pertaining to this thread. This map is maintained * by the ThreadLocal class. */ ThreadLocal.ThreadLocalMap threadLocals = null; /* * InheritableThreadLocal values pertaining to this thread. This map is * maintained by the InheritableThreadLocal class. */ ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; 仔细想想其实就能够理解这种设计的思想。有一种普遍的方法是通过一个全局的线程安全的Map来存储各个线程的变量副本，但是这种做法已经完全违背了ThreadLocal的本意，设计ThreadLocal的初衷就是为了避免多个线程去并发访问同一个对象，尽管它是线程安全的。而在每个Thread中存放与它关联的ThreadLocalMap是完全符合ThreadLocal的思想的，当想要对线程局部变量进行操作时，只需要把Thread作为key来获得Thread中的ThreadLocalMap即可。这种设计相比采用一个全局Map的方法会多占用很多内存空间，但也因此不需要额外的采取锁等线程同步方法而节省了时间上的消耗。 ThreadLocal中的内存泄漏 我们要考虑一种会发生内存泄漏的情况，如果ThreadLocal被设置为null后，而且没有任何强引用指向它，根据垃圾回收的可达性分析算法，ThreadLocal将会被回收。这样一来，ThreadLocalMap中就会含有key为null的Entry，而且ThreadLocalMap是在Thread中的，只要线程迟迟不结束，这些无法访问到的value会形成内存泄漏。为了解决这个问题，ThreadLocalMap中的getEntry()、set()和remove()函数都会清理key为null的Entry，以下面的getEntry()函数的源码为例。 123456789101112131415161718192021222324252627282930313233343536373839404142434445 /** * Get the entry associated with key. This method * itself handles only the fast path: a direct hit of existing * key. It otherwise relays to getEntryAfterMiss. This is * designed to maximize performance for direct hits, in part * by making this method readily inlinable. * * @param key the thread local object * @return the entry associated with key, or null if no such */ private Entry getEntry(ThreadLocal&lt;?&gt; key) &#123; int i = key.threadLocalHashCode &amp; (table.length - 1); Entry e = table[i]; if (e != null &amp;&amp; e.get() == key) return e; else return getEntryAfterMiss(key, i, e); &#125; /** * Version of getEntry method for use when key is not found in * its direct hash slot. * * @param key the thread local object * @param i the table index for key's hash code * @param e the entry at table[i] * @return the entry associated with key, or null if no such */ private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) &#123; Entry[] tab = table; int len = tab.length;// 清理key为null的Entry while (e != null) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) return e; if (k == null) expungeStaleEntry(i); else i = nextIndex(i, len); e = tab[i]; &#125; return null; &#125; 在上文中我们发现了ThreadLocalMap的key是一个弱引用，那么为什么使用弱引用呢？使用强引用key与弱引用key的差别如下： 强引用key：ThreadLocal被设置为null，由于ThreadLocalMap持有ThreadLocal的强引用，如果不手动删除，那么ThreadLocal将不会回收，产生内存泄漏。 弱引用key：ThreadLocal被设置为null，由于ThreadLocalMap持有ThreadLocal的弱引用，即便不手动删除，ThreadLocal仍会被回收，ThreadLocalMap在之后调用set()、getEntry()和remove()函数时会清除所有key为null的Entry。 但要注意的是，ThreadLocalMap仅仅含有这些被动措施来补救内存泄漏问题。如果你在之后没有调用ThreadLocalMap的set()、getEntry()和remove()函数的话，那么仍然会存在内存泄漏问题。 在使用线程池的情况下，如果不及时进行清理，内存泄漏问题事小，甚至还会产生程序逻辑上的问题。所以，为了安全地使用ThreadLocal，必须要像每次使用完锁就解锁一样，在每次使用完ThreadLocal后都要调用remove()来清理无用的Entry。 参考文献 Are Spring objects thread safe? - Stack Overflow Spring Singleton, Request, Session Beans and Thread Safety | Java Enterprise Ecosystem. Spring Framework Documentation]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>2017</tag>
        <tag>后端</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚拟内存的那点事儿]]></title>
    <url>%2F2017%2F10%2F29%2F2017-10-29-virtual_memory%2F</url>
    <content type="text"><![CDATA[概述 我们都知道一个进程是与其他进程共享CPU和内存资源的。正因如此，操作系统需要有一套完善的内存管理机制才能防止进程之间内存泄漏的问题。 为了更加有效地管理内存并减少出错，现代操作系统提供了一种对主存的抽象概念，即是虚拟内存（Virtual Memory）。虚拟内存为每个进程提供了一个一致的、私有的地址空间，它让每个进程产生了一种自己在独享主存的错觉（每个进程拥有一片连续完整的内存空间）。 理解不深刻的人会认为虚拟内存只是“使用硬盘空间来扩展内存“的技术，这是不对的。虚拟内存的重要意义是它定义了一个连续的虚拟地址空间，使得程序的编写难度降低。并且，把内存扩展到硬盘空间只是使用虚拟内存的必然结果，虚拟内存空间会存在硬盘中，并且会被内存缓存（按需），有的操作系统还会在内存不够的情况下，将某一进程的内存全部放入硬盘空间中，并在切换到该进程时再从硬盘读取（这也是为什么Windows会经常假死的原因…）。 虚拟内存主要提供了如下三个重要的能力： 它把主存看作为一个存储在硬盘上的虚拟地址空间的高速缓存，并且只在主存中缓存活动区域（按需缓存）。 它为每个进程提供了一个一致的地址空间，从而降低了程序员对内存管理的复杂性。 它还保护了每个进程的地址空间不会被其他进程破坏。 介绍了虚拟内存的基本概念之后，接下来的内容将会从虚拟内存在硬件中如何运作逐渐过渡到虚拟内存在操作系统（Linux）中的实现。 本文作者为SylvanasSun(sylvanas.sun@gmail.com)，首发于SylvanasSun’s Blog。原文链接：https://sylvanassun.github.io/2017/10/29/2017-10-29-virtual_memory/（转载请务必保留本段声明，并且保留超链接。） CPU寻址 内存通常被组织为一个由M个连续的字节大小的单元组成的数组，每个字节都有一个唯一的物理地址（Physical Address PA），作为到数组的索引。CPU访问内存最简单直接的方法就是使用物理地址，这种寻址方式被称为物理寻址。 现代处理器使用的是一种称为虚拟寻址（Virtual Addressing）的寻址方式。使用虚拟寻址，CPU需要将虚拟地址翻译成物理地址，这样才能访问到真实的物理内存。 虚拟寻址需要硬件与操作系统之间互相合作。CPU中含有一个被称为内存管理单元（Memory Management Unit, MMU）的硬件，它的功能是将虚拟地址转换为物理地址。MMU需要借助存放在内存中的页表来动态翻译虚拟地址，该页表由操作系统管理。 页表 虚拟内存空间被组织为一个存放在硬盘上的M个连续的字节大小的单元组成的数组，每个字节都有一个唯一的虚拟地址，作为到数组的索引（这点其实与物理内存是一样的）。 操作系统通过将虚拟内存分割为大小固定的块来作为硬盘和内存之间的传输单位，这个块被称为虚拟页（Virtual Page, VP），每个虚拟页的大小为P=2^p字节。物理内存也会按照这种方法分割为物理页（Physical Page, PP），大小也为P字节。 CPU在获得虚拟地址之后，需要通过MMU将虚拟地址翻译为物理地址。而在翻译的过程中还需要借助页表，所谓页表就是一个存放在物理内存中的数据结构，它记录了虚拟页与物理页的映射关系。 页表是一个元素为页表条目（Page Table Entry, PTE）的集合，每个虚拟页在页表中一个固定偏移量的位置上都有一个PTE。下面是PTE仅含有一个有效位标记的页表结构，该有效位代表这个虚拟页是否被缓存在物理内存中。 虚拟页VP 0、VP 4、VP 6、VP 7被缓存在物理内存中，虚拟页VP 2和VP 5被分配在页表中，但并没有缓存在物理内存，虚拟页VP 1和VP 3还没有被分配。 在进行动态内存分配时，例如malloc()函数或者其他高级语言中的new关键字，操作系统会在硬盘中创建或申请一段虚拟内存空间，并更新到页表（分配一个PTE，使该PTE指向硬盘上这个新创建的虚拟页）。 由于CPU每次进行地址翻译的时候都需要经过PTE，所以如果想控制内存系统的访问，可以在PTE上添加一些额外的许可位（例如读写权限、内核权限等），这样只要有指令违反了这些许可条件，CPU就会触发一个一般保护故障，将控制权传递给内核中的异常处理程序。一般这种异常被称为“段错误（Segmentation Fault）”。 页命中 如上图所示，MMU根据虚拟地址在页表中寻址到了PTE 4，该PTE的有效位为1，代表该虚拟页已经被缓存在物理内存中了，最终MMU得到了PTE中的物理内存地址（指向PP 1）。 缺页 如上图所示，MMU根据虚拟地址在页表中寻址到了PTE 2，该PTE的有效位为0，代表该虚拟页并没有被缓存在物理内存中。虚拟页没有被缓存在物理内存中（缓存未命中）被称为缺页。 当CPU遇见缺页时会触发一个缺页异常，缺页异常将控制权转向操作系统内核，然后调用内核中的缺页异常处理程序，该程序会选择一个牺牲页，如果牺牲页已被修改过，内核会先将它复制回硬盘（采用写回机制而不是直写也是为了尽量减少对硬盘的访问次数），然后再把该虚拟页覆盖到牺牲页的位置，并且更新PTE。 当缺页异常处理程序返回时，它会重新启动导致缺页的指令，该指令会把导致缺页的虚拟地址重新发送给MMU。由于现在已经成功处理了缺页异常，所以最终结果是页命中，并得到物理地址。 这种在硬盘和内存之间传送页的行为称为页面调度（paging）：页从硬盘换入内存和从内存换出到硬盘。当缺页异常发生时，才将页面换入到内存的策略称为按需页面调度（demand paging），所有现代操作系统基本都使用的是按需页面调度的策略。 虚拟内存跟CPU高速缓存（或其他使用缓存的技术）一样依赖于局部性原则。虽然处理缺页消耗的性能很多（毕竟还是要从硬盘中读取），而且程序在运行过程中引用的不同虚拟页的总数可能会超出物理内存的大小，但是局部性原则保证了在任意时刻，程序将趋向于在一个较小的活动页面（active page）集合上工作，这个集合被称为工作集（working set）。根据空间局部性原则（一个被访问过的内存地址以及其周边的内存地址都会有很大几率被再次访问）与时间局部性原则（一个被访问过的内存地址在之后会有很大几率被再次访问），只要将工作集缓存在物理内存中，接下来的地址翻译请求很大几率都在其中，从而减少了额外的硬盘流量。 如果一个程序没有良好的局部性，将会使工作集的大小不断膨胀，直至超过物理内存的大小，这时程序会产生一种叫做抖动（thrashing）的状态，页面会不断地换入换出，如此多次的读写硬盘开销，性能自然会十分“恐怖”。所以，想要编写出性能高效的程序，首先要保证程序的时间局部性与空间局部性。 多级页表 我们目前为止讨论的只是单页表，但在实际的环境中虚拟空间地址都是很大的（一个32位系统的地址空间有2^32 = 4GB，更别说64位系统了）。在这种情况下，使用一个单页表明显是效率低下的。 常用方法是使用层次结构的页表。假设我们的环境为一个32位的虚拟地址空间，它有如下形式： 虚拟地址空间被分为4KB的页，每个PTE都是4字节。 内存的前2K个页面分配给了代码和数据。 之后的6K个页面还未被分配。 再接下来的1023个页面也未分配，其后的1个页面分配给了用户栈。 下图是为该虚拟地址空间构造的二级页表层次结构（真实情况中多为四级或更多），一级页表（1024个PTE正好覆盖4GB的虚拟地址空间，同时每个PTE只有4字节，这样一级页表与二级页表的大小也正好与一个页面的大小一致都为4KB）的每个PTE负责映射虚拟地址空间中一个4MB的片（chunk），每一片都由1024个连续的页面组成。二级页表中的每个PTE负责映射一个4KB的虚拟内存页面。 这个结构看起来很像是一个B-Tree，这种层次结构有效的减缓了内存要求： 如果一个一级页表的一个PTE是空的，那么相应的二级页表也不会存在。这代表一种巨大的潜在节约（对于一个普通的程序来说，虚拟地址空间的大部分都会是未分配的）。 只有一级页表才总是需要缓存在内存中的，这样虚拟内存系统就可以在需要时创建、页面调入或调出二级页表（只有经常使用的二级页表才会被缓存在内存中），这就减少了内存的压力。 地址翻译的过程 从形式上来说，地址翻译是一个N元素的虚拟地址空间中的元素和一个M元素的物理地址空间中元素之间的映射。 下图为MMU利用页表进行寻址的过程： 页表基址寄存器（PTBR）指向当前页表。一个n位的虚拟地址包含两个部分，一个p位的虚拟页面偏移量（Virtual Page Offset, VPO）和一个（n - p）位的虚拟页号（Virtual Page Number, VPN）。 MMU根据VPN来选择对应的PTE，例如VPN 0代表PTE 0、VPN 1代表PTE 1….因为物理页与虚拟页的大小是一致的，所以物理页面偏移量（Physical Page Offset, PPO）与VPO是相同的。那么之后只要将PTE中的物理页号（Physical Page Number, PPN）与虚拟地址中的VPO串联起来，就能得到相应的物理地址。 多级页表的地址翻译也是如此，只不过因为有多个层次，所以VPN需要分成多段。假设有一个k级页表，虚拟地址会被分割成k个VPN和1个VPO，每个VPN i都是一个到第i级页表的索引。为了构造物理地址，MMU需要访问k个PTE才能拿到对应的PPN。 TLB 页表是被缓存在内存中的，尽管内存的速度相对于硬盘来说已经非常快了，但与CPU还是有所差距。为了防止每次地址翻译操作都需要去访问内存，CPU使用了高速缓存与TLB来缓存PTE。 在最糟糕的情况下（不包括缺页），MMU需要访问内存取得相应的PTE，这个代价大约为几十到几百个周期，如果PTE凑巧缓存在L1高速缓存中（如果L1没有还会从L2中查找，不过我们忽略多级缓冲区的细节），那么性能开销就会下降到1个或2个周期。然而，许多系统甚至需要消除即使这样微小的开销，TLB由此而生。 TLB（Translation Lookaside Buffer, TLB）被称为翻译后备缓冲器或翻译旁路缓冲器，它是MMU中的一个缓冲区，其中每一行都保存着一个由单个PTE组成的块。用于组选择和行匹配的索引与标记字段是从VPN中提取出来的，如果TLB中有T = 2^t个组，那么TLB索引（TLBI）是由VPN的t个最低位组成的，而TLB标记（TLBT）是由VPN中剩余的位组成的。 下图为地址翻译的流程（TLB命中的情况下）： 第一步，CPU将一个虚拟地址交给MMU进行地址翻译。 第二步和第三步，MMU通过TLB取得相应的PTE。 第四步，MMU通过PTE翻译出物理地址并将它发送给高速缓存/内存。 第五步，高速缓存返回数据到CPU（如果缓存命中的话，否则还需要访问内存）。 当TLB未命中时，MMU必须从高速缓存/内存中取出相应的PTE，并将新取得的PTE存放到TLB（如果TLB已满会覆盖一个已经存在的PTE）。 Linux中的虚拟内存系统 Linux为每个进程维护了一个单独的虚拟地址空间。虚拟地址空间分为内核空间与用户空间，用户空间包括代码、数据、堆、共享库以及栈，内核空间包括内核中的代码和数据结构，内核空间的某些区域被映射到所有进程共享的物理页面。Linux也将一组连续的虚拟页面（大小等于内存总量）映射到相应的一组连续的物理页面，这种做法为内核提供了一种便利的方法来访问物理内存中任何特定的位置。 Linux将虚拟内存组织成一些区域（也称为段）的集合，区域的概念允许虚拟地址空间有间隙。一个区域就是已经存在着的已分配的虚拟内存的连续片（chunk）。例如，代码段、数据段、堆、共享库段，以及用户栈都属于不同的区域，每个存在的虚拟页都保存在某个区域中，而不属于任何区域的虚拟页是不存在的，也不能被进程所引用。 内核为系统中的每个进程维护一个单独的任务结构（task_struct）。任务结构中的元素包含或者指向内核运行该进程所需的所有信息（PID、指向用户栈的指针、可执行目标文件的名字、程序计数器等）。 mm_struct：描述了虚拟内存的当前状态。pgd指向一级页表的基址（当内核运行这个进程时，pgd会被存放在CR3控制寄存器，也就是页表基址寄存器中），mmap指向一个vm_area_structs的链表，其中每个vm_area_structs都描述了当前虚拟地址空间的一个区域。 vm_starts：指向这个区域的起始处。 vm_end：指向这个区域的结束处。 vm_prot：描述这个区域内包含的所有页的读写许可权限。 vm_flags：描述这个区域内的页面是与其他进程共享的，还是这个进程私有的以及一些其他信息。 vm_next：指向链表的下一个区域结构。 内存映射 Linux通过将一个虚拟内存区域与一个硬盘上的文件关联起来，以初始化这个虚拟内存区域的内容，这个过程称为内存映射（memory mapping）。这种将虚拟内存系统集成到文件系统的方法可以简单而高效地把程序和数据加载到内存中。 一个区域可以映射到一个普通硬盘文件的连续部分，例如一个可执行目标文件。文件区（section）被分成页大小的片，每一片包含一个虚拟页的初始内容。由于按需页面调度的策略，这些虚拟页面没有实际交换进入物理内存，直到CPU引用的虚拟地址在该区域的范围内。如果区域比文件区要大，那么就用零来填充这个区域的余下部分。 一个区域也可以映射到一个匿名文件，匿名文件是由内核创建的，包含的全是二进制零。当CPU第一次引用这样一个区域内的虚拟页面时，内核就在物理内存中找到一个合适的牺牲页面，如果该页面被修改过，就先将它写回到硬盘，之后用二进制零覆盖牺牲页并更新页表，将这个页面标记为已缓存在内存中的。 简单的来说：普通文件映射就是将一个文件与一块内存建立起映射关系，对该文件进行IO操作可以绕过内核直接在用户态完成（用户态在该虚拟地址区域读写就相当于读写这个文件）。匿名文件映射一般在用户空间需要分配一段内存来存放数据时，由内核创建匿名文件并与内存进行映射，之后用户态就可以通过操作这段虚拟地址来操作内存了。匿名文件映射最熟悉的应用场景就是动态内存分配（malloc()函数）。 Linux很多地方都采用了“懒加载”机制，自然也包括内存映射。不管是普通文件映射还是匿名映射，Linux只会先划分虚拟内存地址。只有当CPU第一次访问该区域内的虚拟地址时，才会真正的与物理内存建立映射关系。 只要虚拟页被初始化了，它就在一个由内核维护的交换文件（swap file）之间换来换去。交换文件又称为交换空间（swap space）或交换区域（swap area）。swap区域不止用于页交换，在物理内存不够的情况下，还会将部分内存数据交换到swap区域（使用硬盘来扩展内存）。 共享对象 虚拟内存系统为每个进程提供了私有的虚拟地址空间，这样可以保证进程之间不会发生错误的读写。但多个进程之间也含有相同的部分，例如每个C程序都使用到了C标准库，如果每个进程都在物理内存中保持这些代码的副本，那会造成很大的内存资源浪费。 内存映射提供了共享对象的机制，来避免内存资源的浪费。一个对象被映射到虚拟内存的一个区域，要么是作为共享对象，要么是作为私有对象的。 如果一个进程将一个共享对象映射到它的虚拟地址空间的一个区域内，那么这个进程对这个区域的任何写操作，对于那些也把这个共享对象映射到它们虚拟内存的其他进程而言，也是可见的。相对的，对一个映射到私有对象的区域的任何写操作，对于其他进程来说是不可见的。一个映射到共享对象的虚拟内存区域叫做共享区域，类似地，也有私有区域。 为了节约内存，私有对象开始的生命周期与共享对象基本上是一致的（在物理内存中只保存私有对象的一份副本），并使用写时复制的技术来应对多个进程的写冲突。 只要没有进程试图写它自己的私有区域，那么多个进程就可以继续共享物理内存中私有对象的一个单独副本。然而，只要有一个进程试图对私有区域的某一页面进行写操作，就会触发一个保护异常。在上图中，进程B试图对私有区域的一个页面进行写操作，该操作触发了保护异常。异常处理程序会在物理内存中创建这个页面的一个新副本，并更新PTE指向这个新的副本，然后恢复这个页的可写权限。 还有一个典型的例子就是fork()函数，该函数用于创建子进程。当fork()函数被当前进程调用时，内核会为新进程创建各种必要的数据结构，并分配给它一个唯一的PID。为了给新进程创建虚拟内存，它复制了当前进程的mm_struct、vm_area_struct和页表的原样副本。并将两个进程的每个页面都标为只读，两个进程中的每个区域都标记为私有区域（写时复制）。 这样，父进程和子进程的虚拟内存空间完全一致，只有当这两个进程中的任一个进行写操作时，再使用写时复制来保证每个进程的虚拟地址空间私有的抽象概念。 动态内存分配 虽然可以使用内存映射（mmap()函数）来创建和删除虚拟内存区域来满足运行时动态内存分配的问题。然而，为了更好的移植性与便利性，还需要一个更高层面的抽象，也就是动态内存分配器（dynamic memory allocator）。 动态内存分配器维护着一个进程的虚拟内存区域，也就是我们所熟悉的“堆（heap）”，内核中还维护着一个指向堆顶的指针brk（break）。动态内存分配器将堆视为一个连续的虚拟内存块（chunk）的集合，每个块有两种状态，已分配和空闲。已分配的块显式地保留为供应用程序使用，空闲块则可以用来进行分配，它的空闲状态直到它显式地被应用程序分配为止。已分配的块要么被应用程序显式释放，要么被垃圾回收器所释放。 本文只讲解动态内存分配的一些概念，关于动态内存分配器的实现已经超出了本文的讨论范围。如果有对它感兴趣的同学，可以去参考dlmalloc的源码，它是由Doug Lea（就是写Java并发包的那位）实现的一个设计巧妙的内存分配器，而且源码中的注释十分多。 内存碎片 造成堆的空间利用率很低的主要原因是一种被称为碎片（fragmentation）的现象，当虽然有未使用的内存但这块内存并不能满足分配请求时，就会产生碎片。有以下两种形式的碎片： 内部碎片：在一个已分配块比有效载荷大时发生。例如，程序请求一个5字（这里我们不纠结字的大小，假设一个字为4字节，堆的大小为16字并且要保证边界双字对齐）的块，内存分配器为了保证空闲块是双字边界对齐的（具体实现中对齐的规定可能略有不同，但对齐是肯定会有的），只好分配一个6字的块。在本例中，已分配块为6字，有效载荷为5字，内部碎片为已分配块减去有效载荷，为1字。 外部碎片：当空闲内存合计起来足够满足一个分配请求，但是没有一个单独的空闲块足够大到可以来处理这个请求时发生。外部碎片难以量化且不可预测，所以分配器通常采用启发式策略来试图维持少量的大空闲块，而不是维持大量的小空闲块。分配器也会根据策略与分配请求的匹配来分割空闲块与合并空闲块（必须相邻）。 空闲链表 分配器将堆组织为一个连续的已分配块和空闲块的序列，该序列被称为空闲链表。空闲链表分为隐式空闲链表与显式空闲链表。 隐式空闲链表，是一个单向链表，并且每个空闲块仅仅是通过头部中的大小字段隐含地连接着的。 显式空闲链表，即是将空闲块组织为某种形式的显式数据结构（为了更加高效地合并与分割空闲块）。例如，将堆组织为一个双向空闲链表，在每个空闲块中，都包含一个前驱节点的指针与后继节点的指针。 查找一个空闲块一般有如下几种策略： 首次适配：从头开始搜索空闲链表，选择第一个遇见的合适的空闲块。它的优点在于趋向于将大的空闲块保留在链表的后面，缺点是它趋向于在靠近链表前部处留下碎片。 下一次适配：每次从上一次查询结束的地方开始进行搜索，直到遇见合适的空闲块。这种策略通常比首次适配效率高，但是内存利用率则要低得多了。 最佳适配：检查每个空闲块，选择适合所需请求大小的最小空闲块。最佳适配的内存利用率是三种策略中最高的，但它需要对堆进行彻底的搜索。 对一个链表进行查找操作的效率是线性的，为了减少分配请求对空闲块匹配的时间，分配器通常采用分离存储（segregated storage）的策略，即是维护多个空闲链表，其中每个链表的块有大致相等的大小。 一种简单的分离存储策略：分配器维护一个空闲链表数组，然后将所有可能的块分成一些等价类（也叫做大小类（size class）），每个大小类代表一个空闲链表，并且每个大小类的空闲链表包含大小相等的块，每个块的大小就是这个大小类中最大元素的大小（例如，某个大小类的范围定义为（17~32），那么这个空闲链表全由大小为32的块组成）。 当有一个分配请求时，我们检查相应的空闲链表。如果链表非空，那么就分配其中第一块的全部。如果链表为空，分配器就向操作系统请求一个固定大小的额外内存片，将这个片分成大小相等的块，然后将这些块链接起来形成新的空闲链表。 要释放一个块，分配器只需要简单地将这个块插入到相应的空闲链表的头部。 垃圾回收 在编写C程序时，一般只能显式地分配与释放堆中的内存（malloc()与free()），程序员不仅需要分配内存，还需要负责内存的释放。 许多现代编程语言都内置了自动内存管理机制（通过引入自动内存管理库也可以让C/C++实现自动内存管理），所谓自动内存管理，就是自动判断不再需要的堆内存（被称为垃圾内存），然后自动释放这些垃圾内存。 自动内存管理的实现是垃圾收集器（garbage collector），它是一种动态内存分配器，它会自动释放应用程序不再需要的已分配块。 垃圾收集器一般采用以下两种（之一）的策略来判断一块堆内存是否为垃圾内存： 引用计数器：在数据的物理空间中添加一个计数器，当有其他数据与其相关时（引用），该计数器加一，反之则减一。通过定期检查计数器的值，只要为0则认为是垃圾内存，可以释放它所占用的已分配块。使用引用计数器，实现简单直接，但缺点也很明显，它无法回收循环引用的两个对象（假设有对象A与对象B，它们2个互相引用，但实际上对象A与对象B都已经是没用的对象了）。 可达性分析：垃圾收集器将堆内存视为一张有向图，然后选出一组根节点（例如，在Java中一般为类加载器、全局变量、运行时常量池中的引用类型变量等），根节点必须是足够“活跃“的对象。然后计算从根节点集合出发的可达路径，只要从根节点出发不可达的节点，都视为垃圾内存。 垃圾收集器进行回收的算法有如下几种： 标记-清除：该算法分为标记（mark）和清除（sweep）两个阶段。首先标记出所有需要回收的对象，然后在标记完成后统一回收所有被标记的对象。标记-清除算法实现简单，但它的效率不高，而且会产生许多内存碎片。 标记-整理：标记-整理与标记-清除算法基本一致，只不过后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉边界以外的内存。 复制：将程序所拥有的内存空间划分为大小相等的两块，每次都只使用其中的一块。当这一块的内存用完了，就把还存活着的对象复制到另一块内存上，然后将已使用过的内存空间进行清理。这种方法不必考虑内存碎片问题，但内存利用率很低。这个比例不是绝对的，像HotSpot虚拟机为了避免浪费，将内存划分为Eden空间与两个Survivor空间，每次都只使用Eden和其中一个Survivor。当回收时，将Eden和Survivor中还存活着的对象一次性地复制到另外一个Survivor空间上，然后清理掉Eden和刚才使用过的Survivor空间。HotSpot虚拟机默认的Eden和Survivor的大小比例为8：1，只有10%的内存空间会被闲置浪费。 分代：分代算法根据对象的存活周期的不同将内存划分为多块，这样就可以对不同的年代采用不同的回收算法。一般分为新生代与老年代，新生代存放的是存活率较低的对象，可以采用复制算法；老年代存放的是存活率较高的对象，如果使用复制算法，那么内存空间会不够用，所以必须使用标记-清除或标记-整理算法。 总结 虚拟内存是对内存的一个抽象。支持虚拟内存的CPU需要通过虚拟寻址的方式来引用内存中的数据。CPU加载一个虚拟地址，然后发送给MMU进行地址翻译。地址翻译需要硬件与操作系统之间紧密合作，MMU借助页表来获得物理地址。 首先，MMU先将虚拟地址发送给TLB以获得PTE（根据VPN寻址）。 如果恰好TLB中缓存了该PTE，那么就返回给MMU，否则MMU需要从高速缓存/内存中获得PTE，然后更新缓存到TLB。 MMU获得了PTE，就可以从PTE中获得对应的PPN，然后结合VPO构造出物理地址。 如果在PTE中发现该虚拟页没有缓存在内存，那么会触发一个缺页异常。缺页异常处理程序会把虚拟页缓存进物理内存，并更新PTE。异常处理程序返回后，CPU会重新加载这个虚拟地址，并进行翻译。 虚拟内存系统简化了内存管理、链接、加载、代码和数据的共享以及访问权限的保护： 简化链接，独立的地址空间允许每个进程的内存映像使用相同的基本格式，而不管代码和数据实际存放在物理内存的何处。 简化加载，虚拟内存使向内存中加载可执行文件和共享对象文件变得更加容易。 简化共享，独立的地址空间为操作系统提供了一个管理用户进程和内核之间共享的一致机制。 访问权限保护，每个虚拟地址都要经过查询PTE的过程，在PTE中设定访问权限的标记位从而简化内存的权限保护。 操作系统通过将虚拟内存与文件系统结合的方式，来初始化虚拟内存区域，这个过程称为内存映射。应用程序显式分配内存的区域叫做堆，通过动态内存分配器来直接操作堆内存。 参考文献 CS:APP3e, Bryant and O’Hallaron Virtual memory - Wikipedia Garbage collection (computer science) - Wikipedia]]></content>
      <categories>
        <category>计算机</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[注解的那点事儿]]></title>
    <url>%2F2017%2F10%2F15%2F2017-10-15-JavaAnnotation%2F</url>
    <content type="text"><![CDATA[什么是注解? 注解是JDK1.5引入的一个语法糖，它主要用来当作元数据，简单的说就是用于解释数据的数据。在Java中，类、方法、变量、参数、包都可以被注解。很多开源框架都使用了注解，例如Spring、MyBatis、Junit。我们平常最常见的注解可能就是@Override了，该注解用来标识一个重写的函数。 注解的作用： 配置文件：替代xml等文本文件格式的配置文件。使用注解作为配置文件可以在代码中实现动态配置，相比外部配置文件，注解的方式会减少很多文本量。但缺点也很明显，更改配置需要对代码进行重新编译，无法像外部配置文件一样进行集中管理（所以现在基本都是外部配置文件+注解混合使用）。 数据的标记：注解可以作为一个标记（例如：被@Override标记的方法代表被重写的方法）。 减少重复代码：注解可以减少重复且乏味的代码。比如我们定义一个@ValidateInt，然后通过反射来获得类中所有成员变量，只要是含有@ValidateInt注解的成员变量，我们就可以对其进行数据的规则校验。 定义一个注解非常简单，只需要遵循以下的语法规则： 12345678910@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.FIELD)@Documentedpublic @interface ValidateInt &#123; // 它们看起来像是定义一个函数，但其实这是注解中的属性 int maxLength(); int minLength();&#125; 我们发现上面的代码在定义注解时也使用了注解，这些注解被称为元注解。作用于注解上的注解称为元注解（元注解其实就是注解的元数据），Java中一共有以下元注解。 @Target：用于描述注解的使用范围（注解可以用在什么地方）。 ElementType.CONSTRUCTOR：构造器。 ElementType.FIELD：成员变量。 ElementType.LOCAL_VARIABLE：局部变量。 ElementType.PACKAGE：包。 ElementType.PARAMETER：参数。 ElementType.METHOD：方法。 ElementType.TYPE：类、接口(包括注解类型) 或enum声明。 @Retention：注解的生命周期，用于表示该注解会在什么时期保留。 RetentionPolicy.RUNTIME：运行时保留，这样就可以通过反射获得了。 RetentionPolicy.CLASS：在class文件中保留。 RetentionPolicy.SOURCE：在源文件中保留。 @Documented：表示该注解会被作为被标注的程序成员的公共API，因此可以被例如javadoc此类的工具文档化。 @Inherited：表示该注解是可被继承的（如果一个使用了@Inherited修饰的annotation类型被用于一个class，则这个annotation将被用于该class的子类）。 了解了这些基础知识之后，接着完成上述定义的@ValidateInt，我们定义一个Cat类然后在它的成员变量中使用@ValidateInt，并通过反射进行数据校验。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class Cat &#123; private String name; @ValidateInt(minLength = 0, maxLength = 10) private int age; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public static void main(String[] args) throws IllegalAccessException &#123; Cat cat = new Cat(); cat.setName("楼楼"); cat.setAge(11); Class&lt;? extends Cat&gt; clazz = cat.getClass(); Field[] fields = clazz.getDeclaredFields(); if (fields != null) &#123; for (Field field : fields) &#123; ValidateInt annotation = field.getDeclaredAnnotation(ValidateInt.class); if (annotation != null) &#123; field.setAccessible(true); int value = field.getInt(cat); if (value &lt; annotation.minLength()) &#123; // .... &#125; else if (value &gt; annotation.maxLength()) &#123; // .... &#125; &#125; &#125; &#125; &#125;&#125; 本文作者为:SylvanasSun(sylvanas.sun@gmail.com)，首发于SylvanasSun’s Blog。原文链接：https://sylvanassun.github.io/2017/10/15/2017-10-15-JavaAnnotation/（转载请务必保留本段声明，并且保留超链接。） 注解的实现 注解其实只是Java的一颗语法糖（语法糖是一种方便程序员使用的语法规则，但它其实并没有表面上那么神奇的功能，只不过是由编译器帮程序员生成那些繁琐的代码）。在Java中这样的语法糖还有很多，例如enum、泛型、forEach等。 通过阅读JLS(Java Language Specification（当你想了解一个语言特性的实现时，最好的方法就是阅读官方规范）发现，注解是一个继承自java.lang.annotation.Annotation接口的特殊接口，原文如下： 1234567891011An annotation type declaration specifies a new annotation type, a special kind of interface type. To distinguish an annotation type declaration from a normal interface declaration, the keyword interface is preceded by an at-sign (@).Note that the at-sign (@) and the keyword interface are distinct tokens. It is possible to separate them with whitespace, but this is discouraged as a matter of style.The rules for annotation modifiers on an annotation type declaration are specified in §9.7.4 and §9.7.5.The Identifier in an annotation type declaration specifies the name of the annotation type.It is a compile-time error if an annotation type has the same simple name as any of its enclosing classes or interfaces.The direct superinterface of every annotation type is java.lang.annotation.Annotation. 123456789101112131415161718192021package java.lang.annotation;/** * The common interface extended by all annotation types. Note that an * interface that manually extends this one does &lt;i&gt;not&lt;/i&gt; define * an annotation type. Also note that this interface does not itself * define an annotation type. * * More information about annotation types can be found in section 9.6 of * &lt;cite&gt;The Java&amp;trade; Language Specification&lt;/cite&gt;. * * The &#123;@link java.lang.reflect.AnnotatedElement&#125; interface discusses * compatibility concerns when evolving an annotation type from being * non-repeatable to being repeatable. * * @author Josh Bloch * @since 1.5 */public interface Annotation &#123; ...&#125; 我们将上节定义的@ValidateInt注解进行反编译来验证这个说法。 123456789101112131415161718192021222324252627282930313233343536373839404142 Last modified Oct 14, 2017; size 479 bytes MD5 checksum 2d9dd2c169fe854db608c7950af3eca7 Compiled from "ValidateInt.java"public interface com.sun.annotation.ValidateInt extends java.lang.annotation.Annotation minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_INTERFACE, ACC_ABSTRACT, ACC_ANNOTATIONConstant pool: #1 = Class #18 // com/sun/annotation/ValidateInt #2 = Class #19 // java/lang/Object #3 = Class #20 // java/lang/annotation/Annotation #4 = Utf8 maxLength #5 = Utf8 ()I #6 = Utf8 minLength #7 = Utf8 SourceFile #8 = Utf8 ValidateInt.java #9 = Utf8 RuntimeVisibleAnnotations #10 = Utf8 Ljava/lang/annotation/Retention; #11 = Utf8 value #12 = Utf8 Ljava/lang/annotation/RetentionPolicy; #13 = Utf8 RUNTIME #14 = Utf8 Ljava/lang/annotation/Target; #15 = Utf8 Ljava/lang/annotation/ElementType; #16 = Utf8 FIELD #17 = Utf8 Ljava/lang/annotation/Documented; #18 = Utf8 com/sun/annotation/ValidateInt #19 = Utf8 java/lang/Object #20 = Utf8 java/lang/annotation/Annotation&#123; public abstract int maxLength(); descriptor: ()I flags: ACC_PUBLIC, ACC_ABSTRACT public abstract int minLength(); descriptor: ()I flags: ACC_PUBLIC, ACC_ABSTRACT&#125;SourceFile: "ValidateInt.java"RuntimeVisibleAnnotations: 0: #10(#11=e#12.#13) 1: #14(#11=[e#15.#16]) 2: #17() public interface com.sun.annotation.ValidateInt extends java.lang.annotation.Annotation，很明显ValidateInt继承自java.lang.annotation.Annotation。 那么，如果注解只是一个接口，又是如何实现对属性的设置呢？这是因为Java使用了动态代理对我们定义的注解接口生成了一个代理类，而对注解的属性设置其实都是在对这个代理类中的变量进行赋值。所以我们才能用反射获得注解中的各种属性。 为了证实注解其实是个动态代理对象，接下来我们使用CLHSDB(Command-Line HotSpot Debugger)来查看JVM的运行时数据。如果有童鞋不了解怎么使用的话，可以参考R大的文章借HSDB来探索HotSpot VM的运行时数据 - Script Ahead, Code Behind - ITeye博客。 10x000000000257f538 com/sun/proxy/$Proxy1 注解的类型为com/sun/proxy/$Proxy1，这正是动态代理生成代理类的默认类型，com/sun/proxy为默认包名，$Proxy是默认的类名，1为自增的编号。 实践-包扫描器 我们在使用Spring的时候，只需要指定一个包名，框架就会去扫描该包下所有带有Spring中的注解的类。实现一个包扫描器很简单，主要思路如下： 先将传入的包名通过类加载器获得项目内的路径。 然后遍历并获得该路径下的所有class文件路径（需要处理为包名的格式）。 得到了class文件的路径就可以使用反射生成Class对象并获得其中的各种信息了。 定义包扫描器接口： 1234567public interface PackageScanner &#123; List&lt;Class&lt;?&gt;&gt; scan(String packageName); List&lt;Class&lt;?&gt;&gt; scan(String packageName, ScannedClassHandler handler);&#125; 函数2需要传入一个ScannedClassHandler接口，该接口是我们定义的回调函数，用于在扫描所有类文件之后执行的处理操作。 123456@FunctionalInterface // 这个注解表示该接口为一个函数接口，用于支持Lambda表达式public interface ScannedClassHandler &#123; void execute(Class&lt;?&gt; clazz);&#125; 我想要包扫描器可以识别和支持不同的文件类型，定义一个枚举类ResourceType： 123456789101112131415161718public enum ResourceType &#123; JAR("jar"), FILE("file"), CLASS_FILE("class"), INVALID("invalid"); private String typeName; public String getTypeName() &#123; return this.typeName; &#125; private ResourceType(String typeName) &#123; this.typeName = typeName; &#125;&#125; PathUtils是一个用来处理路径和包转换等操作的工具类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class PathUtils &#123; private static final String FILE_SEPARATOR = System.getProperty("file.separator"); private static final String CLASS_FILE_SUFFIX = ".class"; private static final String JAR_PROTOCOL = "jar"; private static final String FILE_PROTOCOL = "file"; private PathUtils() &#123; &#125; // 去除后缀名 public static String trimSuffix(String filename) &#123; if (filename == null || "".equals(filename)) return filename; int dotIndex = filename.lastIndexOf("."); if (-1 == dotIndex) return filename; return filename.substring(0, dotIndex); &#125; public static String pathToPackage(String path) &#123; if (path == null || "".equals(path)) return path; if (path.startsWith(FILE_SEPARATOR)) path = path.substring(1); return path.replace(FILE_SEPARATOR, "."); &#125; public static String packageToPath(String packageName) &#123; if (packageName == null || "".equals(packageName)) return packageName; return packageName.replace(".", FILE_SEPARATOR); &#125; /** * 根据URL的协议来判断资源类型 */ public static ResourceType getResourceType(URL url) &#123; String protocol = url.getProtocol(); switch (protocol) &#123; case JAR_PROTOCOL: return ResourceType.JAR; case FILE_PROTOCOL: return ResourceType.FILE; default: return ResourceType.INVALID; &#125; &#125; public static boolean isClassFile(String path) &#123; if (path == null || "".equals(path)) return false; return path.endsWith(CLASS_FILE_SUFFIX); &#125; /** * 抽取URL中的主要路径. * Example: * "file:/com/example/hello" to "/com/example/hello" * "jar:file:/com/example/hello.jar!/" to "/com/example/hello.jar" */ public static String getUrlMainPath(URL url) throws UnsupportedEncodingException &#123; if (url == null) return ""; // 如果不使用URLDecoder解码的话，路径会出现中文乱码问题 String filePath = URLDecoder.decode(url.getFile(), "utf-8"); // if file is not the jar int pos = filePath.indexOf("!"); if (-1 == pos) return filePath; return filePath.substring(5, pos); &#125; public static String concat(Object... args) &#123; if (args == null || args.length == 0) return ""; StringBuilder stringBuilder = new StringBuilder(); for (int i = 0; i &lt; args.length; i++) stringBuilder.append(args[i]); return stringBuilder.toString(); &#125;&#125; 定义了这些辅助类之后，就可以去实现包扫描器了。 12345678910111213141516171819202122232425262728293031323334353637public class SimplePackageScanner implements PackageScanner &#123; protected String packageName; protected String packagePath; protected ClassLoader classLoader; private Logger logger; public SimplePackageScanner() &#123; this.classLoader = Thread.currentThread().getContextClassLoader(); this.logger = LoggerFactory.getLogger(SimplePackageScanner.class); &#125; @Override public List&lt;Class&lt;?&gt;&gt; scan(String packageName) &#123; return this.scan(packageName, null); &#125; @Override public List&lt;Class&lt;?&gt;&gt; scan(String packageName, ScannedClassHandler handler) &#123; this.initPackageNameAndPath(packageName); if (logger.isDebugEnabled()) logger.debug("Start scanning package: &#123;&#125; ....", this.packageName); URL url = this.getResource(this.packagePath); if (url == null) return new ArrayList&lt;&gt;(); return this.parseUrlThenScan(url, handler); &#125; private void initPackageNameAndPath(String packageName) &#123; this.packageName = packageName; this.packagePath = PathUtils.packageToPath(packageName); &#125; &#125; 函数getResource()会根据包名来通过类加载器获得当前项目下的URL对象，如果这个URL为空则直接返回一个空的ArrayList。 12345678protected URL getResource(String packagePath) &#123; URL url = this.classLoader.getResource(packagePath); if (url != null) logger.debug("Get resource: &#123;&#125; success!", packagePath); else logger.debug("Get resource: &#123;&#125; failed,end of scan.", packagePath); return url;&#125; 函数parseUrlThenScan()会解析URL对象并进行扫描，最终返回一个类列表。 1234567891011121314151617181920212223242526272829303132333435 protected List&lt;Class&lt;?&gt;&gt; parseUrlThenScan(URL url, ScannedClassHandler handler) &#123; String urlPath = ""; try &#123; // 先提取出URL中的路径（不含协议名等信息） urlPath = PathUtils.getUrlMainPath(url); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); logger.debug("Get url path failed."); &#125; // 判断URL的类型 ResourceType type = PathUtils.getResourceType(url); List&lt;Class&lt;?&gt;&gt; classList = new ArrayList&lt;&gt;(); try &#123; switch (type) &#123; case FILE: classList = this.getClassListFromFile(urlPath, this.packageName); break; case JAR: classList = this.getClassListFromJar(urlPath); break; default: logger.debug("Unsupported file type."); &#125; &#125; catch (IOException | ClassNotFoundException e) &#123; e.printStackTrace(); logger.debug("Get class list failed."); &#125;// 执行回调函数 this.invokeCallback(classList, handler); logger.debug("End of scan &lt;&#123;&#125;&gt;.", urlPath); return classList; &#125; 函数getClassListFromFile()会扫描路径下的所有class文件，并拼接包名生成Class对象。 1234567891011121314151617181920212223242526protected List&lt;Class&lt;?&gt;&gt; getClassListFromFile(String path, String packageName) throws ClassNotFoundException &#123; File file = new File(path); List&lt;Class&lt;?&gt;&gt; classList = new ArrayList&lt;&gt;(); File[] listFiles = file.listFiles(); if (listFiles != null) &#123; for (File f : listFiles) &#123; if (f.isDirectory()) &#123; // 如果是一个文件夹，则继续递归调用，注意传递的包名 List&lt;Class&lt;?&gt;&gt; list = getClassListFromFile(f.getAbsolutePath(), PathUtils.concat(packageName, ".", f.getName())); classList.addAll(list); &#125; else if (PathUtils.isClassFile(f.getName())) &#123; // 我们不添加名字带有$的class文件，这些都是JVM动态生成的 String className = PathUtils.trimSuffix(f.getName()); if (-1 != className.lastIndexOf("$")) continue; String finalClassName = PathUtils.concat(packageName, ".", className); classList.add(Class.forName(finalClassName)); &#125; &#125; &#125; return classList;&#125; 函数getClassListFromJar()会扫描Jar中的class文件。 1234567891011121314151617protected List&lt;Class&lt;?&gt;&gt; getClassListFromJar(String jarPath) throws IOException, ClassNotFoundException &#123; if (logger.isDebugEnabled()) logger.debug("Start scanning jar: &#123;&#125;", jarPath); JarInputStream jarInputStream = new JarInputStream(new FileInputStream(jarPath)); JarEntry jarEntry = jarInputStream.getNextJarEntry(); List&lt;Class&lt;?&gt;&gt; classList = new ArrayList&lt;&gt;(); while (jarEntry != null) &#123; String name = jarEntry.getName(); if (name.startsWith(this.packageName) &amp;&amp; PathUtils.isClassFile(name)) classList.add(Class.forName(name)); jarEntry = jarInputStream.getNextJarEntry(); &#125; return classList;&#125; 函数invokeCallback()遍历类对象列表，然后执行回调函数。 1234567protected void invokeCallback(List&lt;Class&lt;?&gt;&gt; classList, ScannedClassHandler handler) &#123; if (classList != null &amp;&amp; handler != null) &#123; for (Class&lt;?&gt; clazz : classList) &#123; handler.execute(clazz); &#125; &#125;&#125; 本节中实现的包扫描器源码地址：https://gist.github.com/SylvanasSun/6ab31dcfd9670f29a46917decdba36d1]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>2017</tag>
        <tag>后端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浏览器性能优化-渲染性能]]></title>
    <url>%2F2017%2F10%2F08%2F2017-10-08-BrowserRenderOptimization%2F</url>
    <content type="text"><![CDATA[在浏览器渲染过程与性能优化一文中（建议先去看一下这篇文章再来阅读本文），我们了解与认识了浏览器的关键渲染路径以及如何优化页面的加载速度。在本文中，我们主要关注的是如何提高浏览器的渲染性能（浏览器进行布局计算、绘制像素等操作）与效率。 很多网页都使用了看起来效果非常酷炫的动画与用户进行交互，这些动画效果显著提高了用户的体验，但如果因为性能原因导致动画的每秒帧数太低，反而会让用户体验变得更差（如果一个酷炫的动画效果运行起来总是经常卡顿或者看起来反应很慢，这些都会让用户感觉糟透了）。 一个流畅的动画需要保持在每秒60帧，换算成毫秒浏览器需要在10毫秒左右完成渲染任务（每秒有1000毫秒，1000/60 约等于 16毫秒一帧，但浏览器还有其他工作需要占用时间，所以估算为10毫秒），如果能够理解浏览器的渲染过程并发现性能瓶颈对其优化，可以使你的项目变得具有交互性且动画效果如飘柔般顺滑。 本文作者为: SylvanasSun(sylvanas.sun@gmail.com).转载请务必将本段话置于文章开头处(保留超链接).本文首发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/10/08/2017-10-08-BrowserRenderOptimization/ 像素管道 所谓像素管道其实就是浏览器将渲染树绘制成像素的流程。管道的每个区域都有可能产生卡顿，即管道中的某一区域如果发生变化，浏览器将会进行自动重排，然后重新绘制受影响的区域。 JavaScript：该区域其实指的是实现动画效果的方法，一般使用JavaScript来实现动画，例如JQuery的animate函数、对一个数据集进行排序或动态添加一些DOM节点等。当然，也可以使用其他的方法来实现动画效果，像CSS的Animation、Transition和Transform。 Style：该区域为样式计算阶段，浏览器会根据选择器（就是CSS选择器，如.td）计算出哪些节点应用哪些CSS规则，然后计算出每个节点的最终样式并应用到节点上。 Layout：该区域为布局计算阶段，浏览器会在该过程中根据节点的样式规则来计算它要占据的空间大小以及在屏幕中的位置。 Paint：该区域为绘制阶段，浏览器会先创建绘图调用的列表，然后填充像素。绘制阶段会涉及到文本、颜色、图像、边框和阴影，基本上包括了每个可视部分。绘制一般是在多个图层（用过Photoshop等图片编辑软件的童鞋一定很眼熟图层这个词，这里的图层的含义其实是差不多的）上完成的。 Composite：该区域为合成阶段，浏览器将多个图层按照正确顺序绘制到屏幕上。 假设我们修改了一个几何属性（例如宽度、高度等影响布局的属性），这时Layout阶段受到了影响，浏览器必须检查所有其他区域的元素，然后自动重排页面，任何受到影响的部分都需要重新绘制，并且最终绘制的元素还需要重新进行合成（简单地说就是整个像素管道都要重新执行一遍）。 如果我们只修改了不会影响页面布局的属性，例如背景图片、文字颜色等，那么浏览器会跳过布局阶段，但仍需要重新绘制。 又或者，我们只修改了一个不影响布局也不影响绘制的属性，那么浏览器将跳过布局与绘制阶段，显然这种改动是性能开销最小的。 如果想要知道每个CSS属性将会对哪个阶段产生怎样的影响，请去CSS Triggers，该网站详细地说明了每个CSS属性会影响到哪个阶段。 使用RequestAnimationFrame函数实现动画 我们经常使用JavaScript来实现动画效果，然而时机不当或长时间运行的JavaScript可能就是导致你性能下降的原因。 避免使用setTimeout()或者setInterval()函数来实现动画效果，这种做法的主要问题是回调将会在帧中的某个时间点运行，这可能会刚好在末尾（会丢失帧导致发生卡顿）。 有些第三方库仍在使用setTimeout()&amp;setInterval()函数来实现动画效果，这会产生很多不必要的性能下降，例如老版本的JQuery，如果你使用的是JQuery3，那么不必为此担心，JQuery3已经全面改写了动画模块，采用了requestAnimationFrame()函数来实现动画效果。但如果你使用的是之前版本的JQuery，那么就需要jquery-requestAnimationFrame来将setTimeout()替换为requestAnimationFrame()函数。 读到这里，想必一定会对requestAnimationFrame()产生好奇。要想得到一个流畅的动画，我们希望让视觉变化发生在每一帧的开头，而保证JavaScript在帧开始时运行的方式则是使用requestAnimationFrame()函数，本质上它与setTimeout()没有什么区别，都是在递归调用同一个回调函数来不断更新画面以达到动画的效果，requestAnimationFrame()的使用方法如下： 123456function updateScreen(time) &#123; // 这是你的动画效果函数&#125;// 将你的动画效果函数放入requestAnimationFrame()作为回调函数requestAnimationFrame(updateScreen); 并不是所有浏览器都支持requestAnimationFrame()函数，如IE9（又是万恶的IE），但基本上现代浏览器都会支持这个功能的，如果你需要兼容老旧版本的浏览器，可以使用以下函数。 1234567891011121314151617181920212223242526// 本段代码截取自Paul Irish : https://gist.github.com/paulirish/1579671(function() &#123; var lastTime = 0; var vendors = ['ms', 'moz', 'webkit', 'o']; for(var x = 0; x &lt; vendors.length &amp;&amp; !window.requestAnimationFrame; ++x) &#123; window.requestAnimationFrame = window[vendors[x]+'RequestAnimationFrame']; window.cancelAnimationFrame = window[vendors[x]+'CancelAnimationFrame'] || window[vendors[x]+'CancelRequestAnimationFrame']; &#125; // 如果浏览器不支持，则使用setTimeout() if (!window.requestAnimationFrame) window.requestAnimationFrame = function(callback, element) &#123; var currTime = new Date().getTime(); var timeToCall = Math.max(0, 16 - (currTime - lastTime)); var id = window.setTimeout(function() &#123; callback(currTime + timeToCall); &#125;, timeToCall); lastTime = currTime + timeToCall; return id; &#125;; if (!window.cancelAnimationFrame) window.cancelAnimationFrame = function(id) &#123; clearTimeout(id); &#125;;&#125;()); Web Workers 我们知道JavaScript是单线程的，但浏览器可不是单线程的。JavaScript在浏览器的主线程上运行，这恰好与样式计算、布局等许多其他情况下的渲染操作一起运行，如果JavaScript的运行时间过长，就会阻塞这些后续工作，导致帧丢失。 使用Chrome开发者工具的Timeline功能可以帮助我们查看每个JavaScript脚本的运行时间（包括子脚本），帮助我们发现并突破性能瓶颈。 在找到影响性能的JavaScript脚本后，我们可以通过Web Workers进行优化。Web Workers是HTML5提出的一个标准，它可以让JavaScript脚本运行在后台线程（类似于创建一个子线程），而后台线程不会影响到主线程中的页面。不过，使用Web Workers创建的线程是不能操作DOM树的（这也是Web Workers没有颠覆JavaScript是单线程的原因，JavaScript之所以一直是单线程设计主要也是因为为了避免多个脚本操作DOM树的同步问题，这会提高很多复杂性），所以它只适合于做一些纯计算的工作（数据的排序、遍历等）。 如果你的JavaScript必须要在主线程中执行，那么只能选择另一种方法。将一个大任务分割为多个小任务（每个占用时间不超过几毫秒），并且在每帧的requestAnimationFrame()函数中运行： 123456789101112131415161718192021var taskList = breakBigTaskIntoMicroTasks(monsterTaskList);requestAnimationFrame(processTaskList);function processTaskList(taskStartTime) &#123; var taskFinishTime; do &#123; // 从列表中弹出任务 var nextTask = taskList.pop(); // 执行任务 processTask(nextTask); // 如果有足够的时间进行下一个任务则继续执行 taskFinishTime = window.performance.now(); &#125; while (taskFinishTime - taskStartTime &lt; 3); if (taskList.length &gt; 0) requestAnimationFrame(processTaskList);&#125; 创建一个Web Workers对象很简单，只需要调用Worker()构造器，然后传入指定脚本的URI。现代主流浏览器均支持Web Workers，除了Internet Explorer（又是万恶的IE），所以我们在下面的示例代码中还需要检测浏览器是否兼容。 12345678var myWorker;if (typeof(Worker) !== "undefined") &#123; // 支持Web Workers myWorker = new Worker("worker.js");&#125; else &#123; // 不支持Web Workers&#125; Web Workers与主线程之间通过postMessage()函数来发送信息，使用onmessage()事件处理函数来响应消息（主线程与子线程之间并没有共享数据，只是通过复制数据来交互）。 12345678910111213141516171819main.js: // 在主线程js中发送数据到myWorker绑定的js脚本线程myWorker.postMessage("Hello,World");console.log('Message posted to worker'); worker.js:// onmessage处理函数允许我们在任何时刻，// 一旦接收到消息就可以执行一些代码，代码中消息本身作为事件的data属性进行使用。onmessage = function(data) &#123; console.log("Message received from main script."); console.log("Posting message back to main script."); postMessage("Hello~");&#125;main.js:// 主线程使用onmessage接收消息myWorker.onmessage = function(data) &#123; console.log("Received message: " + data);&#125; 如果你需要从主线程中立刻终止一个运行中的worker，可以调用worker的terminate()函数： 1myWorker.terminate(); myWorker会被立即杀死，不会有任何机会让它继续完成剩下的工作。而在worker线程中也可以调用close()函数进行关闭： 1close(); 有关更多的Web Workers使用方法，请参考Using Web Workers - Web APIs | MDN。 降低样式计算的复杂度 每次修改DOM和CSS都会导致浏览器重新计算样式，在很多情况下还会对页面或页面的一部分重新进行布局计算。 计算样式的第一部分是创建一组匹配选择器（用于计算哪些节点应用哪些样式），第二部分涉及从匹配选择器中获取所有样式规则，并计算出节点的最终样式。 通过降低选择器的复杂性可以提升样式计算的速度。 下面是一个复杂的CSS选择器： 123.box:nth-last-child(-n+1) .title &#123; /* styles */&#125; 浏览器如果想要找到应用该样式的节点，需要先找到有.title类的节点，然后其父节点正好是负n个子元素+1个带.box类的节点。浏览器计算此结果可能需要大量的时间，但我们可以把选择器的预期行为更改为一个类： 123.final-box-title &#123; /* styles */&#125; 我们只是将CSS的命名模块化（降低选择器的复杂性），然后只让浏览器简单地将选择器与节点进行匹配，这样浏览器计算样式的效率会提升许多。 BEM是一种模块化的CSS命名规范，使用这种方法组织CSS不仅结构上十分清晰，也对浏览器的样式查找提供了帮助。 BEM其实就是Block,Element,Modifier，它是一种基于组件的开发方式，其背后的思想就是将用户界面划分为独立的块。这样即使是使用复杂的UI也可以轻松快速地开发，并且模块化的方式可以提高代码的复用性。 Block是一个功能独立的页面组件（可以被重用），Block的命名方式就像写Class名一样。如下面的.button就是代表&lt;button&gt;的Block。 12345.button &#123; background-color: red;&#125;&lt;button class=&quot;button&quot;&gt;I&apos;m a button&lt;/button&gt; Element是一个不能单独使用的Block的复合部分。可以认为Element是Block的子节点。 12345678&lt;!-- `search-form`是一个block --&gt;&lt;form class=&quot;search-form&quot;&gt; &lt;!-- &apos;search-form__input&apos;是&apos;search-form&apos; block中的一个element --&gt; &lt;input class=&quot;search-form__input&quot;&gt; &lt;!-- &apos;search-form__button&apos;是&apos;search-form&apos; block中的一个element --&gt; &lt;button class=&quot;search-form__button&quot;&gt;Search&lt;/button&gt;&lt;/form&gt; Modifier是用于定义Block或Element的外观、状态或行为的实体。假设，我们有了一个新的需求，对button的背景颜色使用绿色，那么我们可以使用Modifier对.button进行一次扩展： 1234567.button &#123; background-color: red;&#125;.button--secondary &#123; background-color: green;&#125; 第一次接触BEM的童鞋可能会对这种命名方式感到奇怪，但BEM重要的是模块化与可维护性的思想，至于命名完全可以按照你所能接受的方式修改。限于篇幅，本文就不再继续探讨BEM了，感兴趣的童鞋可以去看BEM的官方文档。 避免强制同步布局和布局抖动 浏览器每次进行布局计算时几乎总是会作用到整个DOM，如果有大量元素，那么将会需要很长时间才能计算出所有元素的位置与尺寸。 所以我们应当尽量避免在运行时动态地修改几何属性（宽度、高度等），因为这些改动都会导致浏览器重新进行布局计算。如果无法避免，那么要优先使用Flexbox，它会尽量减少布局所需的开销。 强制同步布局就是使用JavaScript强制浏览器提前执行布局。需要先明白一点，在JavaScript运行时，来自上一帧的所有旧布局值都是已知的。 以下代码为例，它在每一帧的开头输出了元素的高度： 12345requestAnimationFrame(logBoxHeight);function logBoxHeight() &#123; console.log(box.offsetHeight);&#125; 但如果在请求高度之前，修改了其样式，就会出现问题，浏览器必须先应用样式，然后进行布局计算，之后才能返回正确的高度。这是不必要的且会产生非常大的开销。 12345function logBoxHeight() &#123; box.classList.add('super-big'); console.log(box.offsetHeight);&#125; 正确的做法，应该利用浏览器可以使用上一帧布局值的特性，然后再执行任何写操作： 12345function logBoxHeight() &#123; console.log(box.offsetHeight); box.classList.add('super-big');&#125; 如果接二连三地发生强制同步布局，那么就会产生布局抖动。以下代码循环处理一组段落，并设置每个段落的宽度以匹配一个名为“box”的元素的宽度。 12345function resizeAllParagraphsToMatchBlockWidth() &#123; for (var i = 0; i &lt; paragraphs.length; i++) &#123; paragraphs[i].style.width = box.offsetWidth + 'px'; &#125;&#125; 这段代码的问题在于每次迭代都会读取box.offsetWidth，然后立即使用此值来更新段落的宽度。在循环的下次迭代中，浏览器必须考虑样式更新这一事实（box.offsetWidth是在上一次迭代中请求的），因此它必须应用样式更改，然后执行布局。这会导致每次迭代都会产生强制同步布局，正确的做法应该先读取值，然后再写入值。 123456789// Read.var width = box.offsetWidth;function resizeAllParagraphsToMatchBlockWidth() &#123; for (var i = 0; i &lt; paragraphs.length; i++) &#123; // Now write. paragraphs[i].style.width = width + 'px'; &#125;&#125; 要想轻松地解决这个问题，可以使用FastDOM进行批量读取与写入，它可以防止强制布局同步与布局抖动。 使用不会触发布局与绘制的属性来实现动画 在像素管道一节中，我们发现有种属性修改后会跳过布局与绘制阶段，这显然会减少不少性能开销。目前只有两种属性符合这个条件：transform和opacity 。 需要注意的是，使用transform和opacity时，更改这些属性所在的元素应处于其自身的图层，所以我们需要将设置动画的元素单独新建一个图层（这样做的好处是该图层上的重绘可以在不影响其他图层上元素的情况下进行处理。如果你用过Photoshop，想必能够理解多图层工作的方便之处）。 创建新图层的最佳方式是使用will-change属性，该属性告知浏览器该元素会有哪些变化，这样浏览器可以在元素属性真正发生变化之前提前做好对应的优化准备工作。 12345678.moving-element &#123; will-change: transform;&#125;// 对于不支持 will-change 但受益于层创建的浏览器，需要使用（滥用）3D 变形来强制创建一个新层.moving-element &#123; transform: translateZ(0);&#125; 但不要认为will-change可以提高性能就随便滥用，使用will-change进行预优化与创建图层都需要额外的内存和管理开销，随便滥用只会得不偿失。 参考文献 Web | Google Developers Using Web Workers - Web APIs | MDN will-change - CSS | MDN Quick start / Methodology / BEM]]></content>
      <categories>
        <category>前端</category>
        <category>浏览器</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>前端</tag>
        <tag>浏览器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浏览器渲染过程与性能优化]]></title>
    <url>%2F2017%2F10%2F03%2F2017-10-03-BrowserCriticalRenderingPath%2F</url>
    <content type="text"><![CDATA[大家都知道万维网的应用层使用了HTTP协议，并且用浏览器作为入口访问网络上的资源。用户在使用浏览器访问一个网站时需要先通过HTTP协议向服务器发送请求，之后服务器返回HTML文件与响应信息。这时，浏览器会根据HTML文件来进行解析与渲染（该阶段还包括向服务器请求非内联的CSS文件与JavaScript文件或者其他资源），最终再将页面呈现在用户面前。 现在知道了网页的渲染都是由浏览器完成的，那么如果一个网站的页面加载速度太慢会导致用户体验不够友好，本文通过详解浏览器渲染页面的过程来引入一些基本的浏览器性能优化方案。让浏览器更快地渲染你的网页并快速响应从而提高用户体验。 本文作者为: SylvanasSun(sylvanas.sun@gmail.com).转载请务必将下面这段话置于文章开头处(保留超链接).本文首发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/10/03/2017-10-03-BrowserCriticalRenderingPath 关键渲染路径 浏览器接收到服务器返回的HTML、CSS和JavaScript字节数据并对其进行解析和转变成像素的渲染过程被称为关键渲染路径。通过优化关键渲染路径即可以缩短浏览器渲染页面的时间。 浏览器在渲染页面前需要先构建出DOM树与CSSOM树（如果没有DOM树和CSSOM树就无法确定页面的结构与样式，所以这两项是必须先构建出来的）。 DOM树全称为Document Object Model文档对象模型，它是HTML和XML文档的编程接口，提供了对文档的结构化表示，并定义了一种可以使程序对该结构进行访问的方式（比如JavaScript就是通过DOM来操作结构、样式和内容）。DOM将文档解析为一个由节点和对象组成的集合，可以说一个WEB页面其实就是一个DOM。 CSSOM树全称为Cascading Style Sheets Object Model层叠样式表对象模型，它与DOM树的含义相差不大，只不过它是CSS的对象集合。 构建DOM树与CSSOM树 浏览器从网络或硬盘中获得HTML字节数据后会经过一个流程将字节解析为DOM树： 编码： 先将HTML的原始字节数据转换为文件指定编码的字符。 令牌化： 然后浏览器会根据HTML规范来将字符串转换成各种令牌（如&lt;html&gt;、&lt;body&gt;这样的标签以及标签中的字符串和属性等都会被转化为令牌，每个令牌具有特殊含义和一组规则）。令牌记录了标签的开始与结束，通过这个特性可以轻松判断一个标签是否为子标签（假设有&lt;html&gt;与&lt;body&gt;两个标签，当&lt;html&gt;标签的令牌还未遇到它的结束令牌&lt;/html&gt;就遇见了&lt;body&gt;标签令牌，那么&lt;body&gt;就是&lt;html&gt;的子标签）。 生成对象： 接下来每个令牌都会被转换成定义其属性和规则的对象（这个对象就是节点对象）。 构建完毕： DOM树构建完成，整个对象集合就像是一棵树形结构。可能有人会疑惑为什么DOM是一个树形结构，这是因为标签之间含有复杂的父子关系，树形结构正好可以诠释这个关系（CSSOS同理，层叠样式也含有父子关系。例如： div p {font-size: 18px}，会先寻找所有p标签并判断它的父标签是否为div之后才会决定要不要采用这个样式进行渲染）。 整个DOM树的构建过程其实就是： 字节 -&gt; 字符 -&gt; 令牌 -&gt; 节点对象 -&gt; 对象模型，下面将通过一个示例HTML代码与配图更形象地解释这个过程。 1234567891011&lt;html&gt; &lt;head&gt; &lt;meta name="viewport" content="width=device-width,initial-scale=1"&gt; &lt;link href="style.css" rel="stylesheet"&gt; &lt;title&gt;Critical Path&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Hello &lt;span&gt;web performance&lt;/span&gt; students!&lt;/p&gt; &lt;div&gt;&lt;img src="awesome-photo.jpg"&gt;&lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 当上述HTML代码遇见&lt;link&gt;标签时，浏览器会发送请求获得该标签中标记的CSS文件（使用内联CSS可以省略请求的步骤提高速度，但没有必要为了这点速度而丢失了模块化与可维护性），style.css中的内容如下： 12345body &#123; font-size: 16px &#125;p &#123; font-weight: bold &#125;span &#123; color: red &#125;p span &#123; display: none &#125;img &#123; float: right &#125; 浏览器获得外部CSS文件的数据后，就会像构建DOM树一样开始构建CSSOM树，这个过程没有什么特别的差别。 如果想要更详细地去体验一下关键渲染路径的构建，可以使用Chrome开发者工具中的Timeline功能，它记录了浏览器从请求页面资源一直到渲染的各种操作过程，甚至还可以录制某一时间段的过程（建议不要去看太大的网站，信息会比较杂乱）。 构建渲染树 在构建了DOM树和CSSOM树之后，浏览器只是拥有了两个互相独立的对象集合，DOM树描述了文档的结构与内容，CSSOM树则描述了对文档应用的样式规则，想要渲染出页面，就需要将DOM树与CSSOM树结合在一起，这就是渲染树。 浏览器会先从DOM树的根节点开始遍历每个可见节点（不可见的节点自然就没必要渲染到页面了，不可见的节点还包括被CSS设置了display: none属性的节点，值得注意的是visibility: hidden属性并不算是不可见属性，它的语义是隐藏元素，但元素仍然占据着布局空间，所以它会被渲染成一个空框）。 对每个可见节点，找到其适配的CSS样式规则并应用。 渲染树构建完成，每个节点都是可见节点并且都含有其内容和对应规则的样式。 渲染树构建完毕后，浏览器得到了每个可见节点的内容与其样式，下一步工作则需要计算每个节点在窗口内的确切位置与大小，也就是布局阶段。 CSS采用了一种叫做盒子模型的思维模型来表示每个节点与其他元素之间的距离，盒子模型包括外边距(Margin)，内边距(Padding)，边框(Border)，内容(Content)。页面中的每个标签其实都是一个个盒子。 布局阶段会从渲染树的根节点开始遍历，然后确定每个节点对象在页面上的确切大小与位置，布局阶段的输出是一个盒子模型，它会精确地捕获每个元素在屏幕内的确切位置与大小，所有相对的测量值也都会被转换为屏幕内的绝对像素值。 1234567891011&lt;html&gt; &lt;head&gt; &lt;meta name="viewport" content="width=device-width,initial-scale=1"&gt; &lt;title&gt;Critial Path: Hello world!&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;div style="width: 50%"&gt; &lt;div style="width: 50%"&gt;Hello world!&lt;/div&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 当Layout布局事件完成后，浏览器会立即发出Paint Setup与Paint事件，开始将渲染树绘制成像素，绘制所需的时间跟CSS样式的复杂度成正比，绘制完成后，用户就可以看到页面的最终呈现效果了。 我们对一个网页发送请求并获得渲染后的页面可能也就经过了1~2秒，但浏览器其实已经做了上述所讲的非常多的工作，总结一下浏览器关键渲染路径的整个过程： 处理HTML标记数据并生成DOM树。 处理CSS标记数据并生成CSSOM树。 将DOM树与CSSOM树合并在一起生成渲染树。 遍历渲染树开始布局，计算每个节点的位置信息。 将每个节点绘制到屏幕。 渲染阻塞的优化方案 浏览器想要渲染一个页面就必须先构建出DOM树与CSSOM树，如果HTML与CSS文件结构非常庞大与复杂，这显然会给页面加载速度带来严重影响。 所谓渲染阻塞资源，即是对该资源发送请求后还需要先构建对应的DOM树或CSSOM树，这种行为显然会延迟渲染操作的开始时间。HTML、CSS、JavaScript都是会对渲染产生阻塞的资源，HTML是必需的（没有DOM还谈何渲染），但还可以从CSS与JavaScript着手优化，尽可能地减少阻塞的产生。 优化CSS 如果可以让CSS资源只在特定条件下使用，这样这些资源就可以在首次加载时先不进行构建CSSOM树，只有在符合特定条件时，才会让浏览器进行阻塞渲染然后构建CSSOM树。 CSS的媒体查询正是用来实现这个功能的，它由媒体类型以及零个或多个检查特定媒体特征状况的表达式组成。 123456789&lt;!-- 没有使用媒体查询，这个css资源会阻塞渲染 --&gt;&lt;link href="style.css" rel="stylesheet"&gt;&lt;!-- all是默认类型，它和不设置媒体查询的效果是一样的 --&gt;&lt;link href="style.css" rel="stylesheet" media="all"&gt;&lt;!-- 动态媒体查询， 将在网页加载时计算。根据网页加载时设备的方向，portrait.css 可能阻塞渲染，也可能不阻塞渲染。--&gt;&lt;link href="portrait.css" rel="stylesheet" media="orientation:portrait"&gt;&lt;!-- 只在打印网页时应用，因此网页首次在浏览器中加载时，它不会阻塞渲染。 --&gt;&lt;link href="print.css" rel="stylesheet" media="print"&gt; 使用媒体查询可以让CSS资源不在首次加载中阻塞渲染，但不管是哪种CSS资源它们的下载请求都不会被忽略，浏览器仍然会先下载CSS文件 优化JavaScript 当浏览器的HTML解析器遇到一个script标记时会暂停构建DOM，然后将控制权移交至JavaScript引擎，这时引擎会开始执行JavaScript脚本，直到执行结束后，浏览器才会从之前中断的地方恢复，然后继续构建DOM。每次去执行JavaScript脚本都会严重地阻塞DOM树的构建，如果JavaScript脚本还操作了CSSOM，而正好这个CSSOM还没有下载和构建，浏览器甚至会延迟脚本执行和构建DOM，直至完成其CSSOM的下载和构建。显而易见，如果对JavaScript的执行位置运用不当，这将会严重影响渲染的速度。 下面代码中的JavaScript脚本并不会生效，这是因为DOM树还没有构建到&lt;p&gt;标签时，JavaScript脚本就已经开始执行了。这也是为什么经常有人在HTML文件的最下方写内联JavaScript代码，又或者使用window.onload()和JQuery中的$(function(){})（这两个函数有一些区别，window.onload()是等待页面完全加载完毕后触发的事件，而$(function(){})在DOM树构建完毕后就会执行）。 1234567891011121314&lt;html&gt; &lt;head&gt; &lt;meta name="viewport" content="width=device-width,initial-scale=1"&gt; &lt;link href="style.css" rel="stylesheet"&gt; &lt;title&gt;Hello,World&lt;/title&gt; &lt;script type="text/javascript"&gt; var p = document.getElementsByTagName('p')[0]; p.textContent = 'SylvanasSun'; &lt;/script&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Hello,World!&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 使用async可以通知浏览器该脚本不需要在引用位置执行，这样浏览器就可以继续构建DOM，JavaScript脚本会在就绪后开始执行，这样将显著提升页面首次加载的性能（async只可以在src标签中使用也就是外部引用的JavaScript文件）。 123&lt;!-- 下面2个用法效果是等价的 --&gt;&lt;script type="text/javascript" src="demo_async.js" async="async"&gt;&lt;/script&gt;&lt;script type="text/javascript" src="demo_async.js" async&gt;&lt;/script&gt; 优化关键渲染路径总结 上文已经完整讲述了浏览器是如何渲染页面的以及渲染之前的准备工作，接下来我们以下面的案例来总结一下优化关键渲染路径的方法。 假设有一个HTML页面，它只引入了一个CSS外部文件： 12345678910&lt;html&gt; &lt;head&gt; &lt;meta name="viewport" content="width=device-width,initial-scale=1"&gt; &lt;link href="style.css" rel="stylesheet"&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Hello &lt;span&gt;web performance&lt;/span&gt; students!&lt;/p&gt; &lt;div&gt;&lt;img src="awesome-photo.jpg"&gt;&lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 它的关键渲染路径如下： 首先浏览器要先对服务器发送请求获得HTML文件，得到HTML文件后开始构建DOM树，在遇见&lt;link&gt;标签时浏览器需要向服务器再次发出请求来获得CSS文件，然后则是继续构建DOM树和CSSOM树，浏览器合并出渲染树，根据渲染树进行布局计算，执行绘制操作，页面渲染完成。 有以下几个用于描述关键渲染路径性能的词汇： 关键资源：可能阻塞网页首次渲染的资源（上图中为2个，HTML文件与外部CSS文件style.css）。 关键路径长度： 获取关键资源所需的往返次数或总时间（上图为2次或以上，一次获取HTML文件，一次获取CSS文件，这个次数基于TCP协议的最大拥塞窗口，一个文件不一定能在一次连接内传输完毕）。 关键字节：所有关键资源文件大小的总和（上图为9KB）。 接下来，案例代码的需求发生了变化，它新增了一个JavaScript文件。 1234567891011&lt;html&gt; &lt;head&gt; &lt;meta name="viewport" content="width=device-width,initial-scale=1"&gt; &lt;link href="style.css" rel="stylesheet"&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Hello &lt;span&gt;web performance&lt;/span&gt; students!&lt;/p&gt; &lt;div&gt;&lt;img src="awesome-photo.jpg"&gt;&lt;/div&gt; &lt;script src="app.js"&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; JavaScript文件阻塞了DOM树的构建，并且在执行JavaScript脚本时还需要先等待构建CSSOM树，上图的关键渲染路径特性如下： 关键资源： 3（HTML、style.css、app.js） 关键路径长度： 2或以上（浏览器会在一次连接中一起下载style.css和app.js） 关键字节：11KB 现在，我们要优化关键渲染路径，首先将&lt;script&gt;标签添加异步属性async，这样浏览器的HTML解析器就不会阻塞这个JavaScript文件了。 1234567891011&lt;html&gt; &lt;head&gt; &lt;meta name="viewport" content="width=device-width,initial-scale=1"&gt; &lt;link href="style.css" rel="stylesheet"&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Hello &lt;span&gt;web performance&lt;/span&gt; students!&lt;/p&gt; &lt;div&gt;&lt;img src="awesome-photo.jpg"&gt;&lt;/div&gt; &lt;script src="app.js" async&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 关键资源：2（app.js为异步加载，不会成为阻塞渲染的资源） 关键路径长度： 2或以上 关键字节： 9KB（app.js不再是关键资源，所以没有算上它的大小） 接下来对CSS进行优化，比如添加上媒体查询。 1234567891011&lt;html&gt; &lt;head&gt; &lt;meta name="viewport" content="width=device-width,initial-scale=1"&gt; &lt;link href="style.css" rel="stylesheet" media="print"&gt; &lt;/head&gt; &lt;body&gt; &lt;p&gt;Hello &lt;span&gt;web performance&lt;/span&gt; students!&lt;/p&gt; &lt;div&gt;&lt;img src="awesome-photo.jpg"&gt;&lt;/div&gt; &lt;script src="app.js" async&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 关键资源：1（app.js为异步加载，style.css只有在打印时才会使用，所以只剩下HTML一个关键资源，也就是说当DOM树构建完毕，浏览器就会开始进行渲染） 关键路径长度：1或以上 关键字节：5KB 优化关键渲染路径就是在对关键资源、关键路径长度和关键字节进行优化。关键资源越少，浏览器在渲染前的准备工作就越少；同样，关键路径长度和关键字节关系到浏览器下载资源的效率，它们越少，浏览器下载资源的速度就越快。 其他优化方案 除了异步加载JavaScript和使用媒体查询外还有很多其他的优化方案可以使页面的首次加载变得更快，这些方案可以综合起来使用，但核心的思想还是针对关键渲染路径进行了优化。 加载部分HTML 服务端在接收到请求时先只响应回HTML的初始部分，后续的HTML内容在需要时再通过AJAX获得。由于服务端只发送了部分HTML文件，这让构建DOM树的工作量减少很多，从而让用户感觉页面的加载速度很快。 注意，这个方法不能用在CSS上，浏览器不允许CSSOM只构建初始部分，否则会无法确定具体的样式。 压缩 通过对外部资源进行压缩可以大幅度地减少浏览器需要下载的资源量，它会减少关键路径长度与关键字节，使页面的加载速度变得更快。 对数据进行压缩其实就是使用更少的位数来对数据进行重编码。如今有非常多的压缩算法，且每一个的作用领域也各不相同，它们的复杂度也不相同，不过在这里我不会讲压缩算法的细节，感兴趣的朋友可以自己Google。 在对HTML、CSS和JavaScript这些文件进行压缩之前，还需要先进行一次冗余压缩。所谓冗余压缩，就是去除多余的字符，例如注释、空格符和换行符。这些字符对于程序员是有用的，毕竟没有格式化的代码可读性是非常恐怖的，但它们对于浏览器是没有任何意义的，去除这些冗余可以减少文件的数据量。在进行完冗余压缩之后，再使用压缩算法进一步对数据本身进行压缩，例如GZIP（GZIP是一个可以作用于任何字节流的通用压缩算法，它会记忆之前已经看到的内容，然后再尝试查找并替换重复的内容。）。 HTTP缓存 通过网络来获取资源通常是缓慢的，如果资源文件过于膨大，浏览器还需要与服务器之间进行多次往返通信才能获得完整的资源文件。缓存可以复用之前获取的资源，既然后端可以使用缓存来减少访问数据库的开销，那前端自然也可以使用缓存来复用资源文件。 浏览器自带了HTTP缓存的功能，只需要确保每个服务器响应的头部都包含了以下的属性： ETag： ETag是一个传递验证令牌，它对资源的更新进行检查，如果资源未发生变化时不会传送任何数据。当浏览器发送一个请求时，会把ETag一起发送到服务器，服务器会根据当前资源核对令牌（ETag通常是对内容进行Hash后得出的一个指纹），如果资源未发生变化，服务器将返回304 Not Modified响应，这时浏览器不必再次下载资源，而是继续复用缓存。 Cache-Control： Cache-Control定义了缓存的策略，它规定在什么条件下可以缓存响应以及可以缓存多久。 no-cache： no-cache表示必须先与服务器确认返回的响应是否发生了变化，然后才能使用该响应来满足后续对同一网址的请求（每次都会根据ETag对服务器发送请求来确认变化，如果未发生变化，浏览器不会下载资源）。 no-store： no-store直接禁止浏览器以及所有中间缓存存储任何版本的返回响应。简单的说，该策略会禁止任何缓存，每次发送请求时，都会完整地下载服务器的响应。 public&amp;private： 如果响应被标记为public，则即使它有关联的HTTP身份验证，甚至响应状态代码通常无法缓存，浏览器也可以缓存响应。如果响应被标记为private，那么这个响应通常只为单个用户缓存，因此不允许任何中间缓存（CDN）对其进行缓存，private一般用在缓存用户私人信息页面。 max-age： max-age定义了从请求时间开始，缓存的最长时间，单位为秒。 资源预加载 Pre-fetching是一种提示浏览器预先加载用户之后可能会使用到的资源的方法。 使用dns-prefetch来提前进行DNS解析，以便之后可以快速地访问另一个主机名（浏览器会在加载网页时对网页中的域名进行解析缓存，这样你在之后的访问时无需进行额外的DNS解析，减少了用户等待时间，提高了页面加载速度）。 1&lt;link rel="dns-prefetch" href="other.hostname.com"&gt; 使用prefetch属性可以预先下载资源，不过它的优先级是最低的。 1&lt;link rel="prefetch" href="/some_other_resource.jpeg"&gt; Chrome允许使用subresource属性指定优先级最高的下载资源（当所有属性为subresource的资源下载完完毕后，才会开始下载属性为prefetch的资源）。 1&lt;link rel="subresource" href="/some_other_resource.js"&gt; prerender可以预先渲染好页面并隐藏起来，之后打开这个页面会跳过渲染阶段直接呈现在用户面前（推荐对用户接下来必须访问的页面进行预渲染，否则得不偿失）。 1&lt;link rel="prerender" href="//domain.com/next_page.html"&gt; 参考文献 Web Fundamentals | Google Developers Flushing the Document Early | High Performance Web Sites Introduction to the DOM - Web APIs | MDN How the Browser Pre-loader Makes Pages Load Faster - Andy Davies]]></content>
      <categories>
        <category>前端</category>
        <category>浏览器</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>前端</tag>
        <tag>浏览器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[教你如何快速实现一个图片爬虫]]></title>
    <url>%2F2017%2F09%2F20%2F2017-09-20-PictureSpider%2F</url>
    <content type="text"><![CDATA[什么是爬虫? 如果是没有接触过爬虫的人可能会有些许疑惑，爬虫是个什么东西呢？其实爬虫的概念很简单，在互联网时代,万维网已然是大量信息的载体，如何有效地利用并提取这些信息是一个巨大的挑战。当我们使用浏览器对某个网站发送请求时，服务器会响应HTML文本并由浏览器来进行渲染显示。爬虫正是利用了这一点，通过程序模拟用户的请求，来获得HTML的内容，并从中提取需要的数据和信息。如果把网络想象成一张蜘蛛网，爬虫程序则像是蜘蛛网上的蜘蛛，不断地爬取数据与信息。 爬虫的概念非常简单易懂，利用python内置的urllib库都可以实现一个简单的爬虫，下面的代码是一个非常简单的爬虫，只要有基本的python知识应该都能看懂。它会收集一个页面中的所有&lt;a&gt;标签(没有做任何规则判断)中的链接，然后顺着这些链接不断地进行深度搜索。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124from bs4 import BeautifulSoupimport urllibimport osfrom datetime import datetime# 网页的实体类,只含有两个属性,url和标题class Page(object): def __init__(self,url,title): self._url = url self._title = title def __str__(self): return '[Url]: %s [Title]: %s' %(self._url,self._title) __repr__ = __str__ @property def url(self): return self._url @property def title(self): return self._title @url.setter def url(self,value): if not isinstance(value,str): raise ValueError('url must be a string!') if value == '': raise ValueError('url must be not empty!') self._url = value @title.setter def title(self,value): if not isinstance(value,str): raise ValueError('title must be a string!') if value == '': raise ValueError('title must be not empty!') self._title = valueclass Spider(object): def __init__(self,init_page): self._init_page = init_page # 种子网页,也就是爬虫的入口 self._pages = [] self._soup = None # BeautifulSoup 一个用来解析HTML的解析器 def crawl(self): start_time = datetime.now() print('[Start Time]: %s' % start_time) start_timestamp = start_time.timestamp() tocrawl = [self._init_page] # 记录将要爬取的网页 crawled = [] # 记录已经爬取过的网页 # 不断循环,直到将这张图搜索完毕 while tocrawl: page = tocrawl.pop() if page not in crawled: self._init_soup(page) self._packaging_to_pages(page) links = self._extract_links() self._union_list(tocrawl,links) crawled.append(page) self._write_to_curdir() end_time = datetime.now() print('[End Time]: %s' % end_time) end_timestamp = end_time.timestamp() print('[Total Time Consuming]: %f.3s' % (start_timestamp - end_timestamp) / 1000) def _init_soup(self,page): page_content = None try: # urllib可以模拟用户请求,获得响应的HTML文本内容 page_content = urllib.request.urlopen(page).read() except: page_content = '' # 初始化BeautifulSoup,参数二是使用到的解析器名字 self._soup = BeautifulSoup(page_content,'lxml') def _extract_links(self): a_tags = self._soup.find_all('a') # 找到所有a标签 links = [] # 收集所有a标签中的链接 for a_tag in a_tags: links.append(a_tag.get('href')) return links def _packaging_to_pages(self,page): title_string = '' try: title_string = self._soup.title.string # 获得title标签中的文本内容 except AttributeError as e : print(e) page_obj = Page(page,title_string) print(page_obj) self._pages.append(page_obj) # 将爬取到的所有信息写入到当前目录下的out.txt文件 def _write_to_curdir(self): cur_path = os.path.join(os.path.abspath('.'),'out.txt') print('Start write to %s' % cur_path) with open(cur_path,'w') as f: f.write(self._pages) # 将dest中的不存在于src的元素合并到src def _union_list(self,src,dest): for dest_val in dest: if dest_val not in src: src.append(dest_val) @property def init_page(self): return self._init_page @property def pages(self): return self._pagesdef test(): spider = Spider('https://sylvanassun.github.io/') spider.crawl()if __name__ == '__main__': test() 但是我们如果想要实现一个性能高效的爬虫，那需要的复杂度也会增长，本文旨在快速实现，所以我们需要借助他人实现的爬虫框架来当做脚手架，在这之上来构建我们的图片爬虫(如果有时间的话当然也鼓励自己造轮子啦)。 本文作者为: SylvanasSun(sylvanas.sun@gmail.com).转载请务必将下面这段话置于文章开头处(保留超链接).本文首发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/09/20/2017-09-20-PictureSpider/ BeautifulSoup BeautifulSoup是一个用于从HTML和XML中提取数据的python库。Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。 利用好BeautifulSoup可以为我们省去许多编写正则表达式的时间，如果当你需要更精准地进行搜索时，BeautifulSoup也支持使用正则表达式进行查询。 BeautifulSoup3已经停止维护了，现在基本使用的都是BeautifulSoup4，安装BeautifulSoup4很简单，只需要执行以下的命令。 1pip install beautifulsoup4 然后从bs4模块中导入BeautifulSoup对象，并创建这个对象。 123from bs4 import BeautifulSoupsoup = BeautifulSoup(body,'lxml') 创建BeautifulSoup对象需要传入两个参数,第一个是需要进行解析的HTML内容，第二个参数为解析器的名字(如果不传入这个参数，BeautifulSoup会默认使用python内置的解析器html.parser)。BeautifulSoup支持多种解析器，有lxml、html5lib、html.parser。 第三方解析器需要用户自己安装，本文中使用的是lxml解析器，安装命令如下（它还需要先安装C语言库）。 1pip install lxml 下面以一个例子演示使用BeautifulSoup的基本方式，如果还想了解更多可以去参考BeautifulSoup文档。 1234567891011121314151617181920212223242526272829303132333435363738394041424344from bs4 import BeautifulSouphtml = """&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse's story&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p class="title" name="dromouse"&gt;&lt;b&gt;The Dormouse's story&lt;/b&gt;&lt;/p&gt;&lt;p class="story"&gt;Once upon a time there were three little sisters; and their names were&lt;a href="http://example.com/elsie" class="sister" id="link1"&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,&lt;a href="http://example.com/lacie" class="sister" id="link2"&gt;Lacie&lt;/a&gt; and&lt;a href="http://example.com/tillie" class="sister" id="link3"&gt;Tillie&lt;/a&gt;;and they lived at the bottom of a well.&lt;/p&gt;&lt;p class="story"&gt;...&lt;/p&gt;"""soup = BeautifulSoup(html,'lxml')# 格式化输出soup中的内容print(soup.prettify())# 可以通过.操作符来访问标签对象title = soup.titleprint(title)p = soup.pprint(p)# 获得title标签中的文本内容,这2个方法得到的结果是一样的print(title.text)print(title.get_text())# 获得head标签的所有子节点,contents返回的是一个列表,children返回的是一个迭代器head = soup.headprint(head.contents)print(head.children)# 获得所有a标签,并输出每个a标签href属性中的内容a_tags = soup.find_all('a')for a_tag in a_tags: print(a_tag['href'])# find函数与find_all一样,只不过返回的是找到的第一个标签 print(soup.find('a')['href'])# 根据属性查找,这2个方法得到的结果是一样的print(soup.find('p',class_='title'))print(soup.find('p',attrs=&#123;'class': 'title'&#125;)) Scrapy Scrapy是一个功能强大的爬虫框架，它已经实现了一个性能高效的爬虫结构，并提供了很多供程序员自定义的配置。使用Scrapy只需要在它的规则上编写我们的爬虫逻辑即可。 首先需要先安装Scrapy,执行命令pip install scrapy。然后再执行命令scrapy startproject 你的项目名来生成Scrapy的基本项目文件夹。生成的项目结构如下。 12345678910你的项目名/ scrapy.cfg 你的项目名/ __init__.py items.py pipelines.py settings.py spiders/ __init__.py ... scrapy.cfg : 项目的配置文件。 items.py：物品模块，用户需要在这个模块中定义数据封装的实体类。 pipelines.py：管道模块，用户需要在这个模块中定义处理数据的逻辑（如存储到数据库等）。 settings.py：这个模块定义了整个项目中的各种配置变量。 spiders/：在这个包中定义用户自己的爬虫模块。 启动Scrapy的爬虫也很简单，只需要执行命令scrapy crawl 你的爬虫名。下面介绍Scrapy中的关键模块的演示案例，如果想要了解有关Scrapy的更多信息，请参考Scrapy官方文档。 items items模块主要是为了将爬取到的非结构化数据封装到一个结构化对象中，自定义的item类必须继承自scrapy.Item，且每个属性都要赋值为scrapy.Field()。 123456import scrapyclass Product(scrapy.Item): name = scrapy.Field() price = scrapy.Field() stock = scrapy.Field() 操作item对象就像操作一个dict对象一样简单。 1234567product = Product()# 对属性赋值product[&apos;name&apos;] = &apos;Sylvanas&apos;product[&apos;price&apos;] = 998# 获得属性print(product[&apos;name&apos;])print(product[&apos;price&apos;]) pipelines 当一个Item经由爬虫封装之后将会到达Pipeline类，你可以定义自己的Pipeline类来决定将Item的处理策略。 每个Pipeline可以实现以下函数。 process_item(item, spider)： 每个Pipeline都会调用此函数来处理Item，这个函数必须返回一个Item，如果在处理过程中遇见错误，可以抛出DropItem异常。 open_spider(spider)： 当spider开始时将会调用此函数，可以利用这个函数进行打开文件等操作。 close_spider(spider)：当spider关闭时将会调用此函数，可以利用这个函数对IO资源进行关闭。 from_crawler(cls, crawler)： 这个函数用于获取settings.py模块中的属性。注意这个函数是一个类方法。 123456789101112131415161718192021from scrapy.exceptions import DropItemclass PricePipeline(object): vat_factor = 1.15 def __init__(self, HELLO): self.HELLO = HELLO def process_item(self, item, spider): if item['price']: if item['price_excludes_vat']: item['price'] = item['price'] * self.vat_factor return item else: raise DropItem("Missing price in %s" % item) @classmethod def from_crawler(cls, crawler): settings = crawler.settings # 从crawler中获得settings return cls(settings['HELLO']) # 返回settings中的属性，将由__init__函数接收 当定义完你的Pipeline后，还需要在settings.py中对你的Pipeline进行设置。 1234ITEM_PIPELINES = &#123; # 后面跟的数字是优先级别 'pipeline类的全路径': 300,&#125; spiders 在spiders模块中，用户可以通过自定义Spider类来制定自己的爬虫逻辑与数据封装策略。每个Spider都必须继承自class scrapy.spider.Spider，这是Scrapy中最简单的爬虫基类，它没有什么特殊功能，Scrapy也提供了其他功能不同的Spider类供用户选择，这里就不多叙述了，可以去参考官方文档。 用户可以通过以下属性来自定义配置Spider: name： 这是Spider的名称，Scrapy需要通过这个属性来定位Spider并启动爬虫，它是唯一且必需的。 allowed_domains： 这个属性规定了Spider允许爬取的域名。 start_urls： Spider开始时将抓取的网页列表。 start_requests()： 该函数是Spider开始抓取时启动的函数，它只会被调用一次，有的网站必须要求用户登录，可以使用这个函数先进行模拟登录。 make_requests_from_url(url)： 该函数接收一个url并返回Request对象。除非重写该函数，否则它会默认以parse(response)函数作为回调函数，并启用dont_filter参数（这个参数是用于过滤重复url的）。 parse(response)： 当请求没有设置回调函数时，则会默认调用parse(response)。 log(message[, level, component])： 用于记录日志。 closed(reason)： 当Spider关闭时调用。 1234567891011121314import scrapyclass MySpider(scrapy.Spider): name = 'example.com' allowed_domains = ['example.com'] start_urls = [ 'http://www.example.com/1.html', 'http://www.example.com/2.html', 'http://www.example.com/3.html', ] def parse(self, response): self.log('A response from %s just arrived!' % response.url) 其他依赖库 Requests Requests也是一个第三方python库，它比python内置的urllib更加简单好用。只需要安装（pip install requests），然后导包后，即可轻松对网站发起请求。 1234567891011121314import requests# 支持http的各种类型请求r = requests.post("http://httpbin.org/post")r = requests.put("http://httpbin.org/put")r = requests.delete("http://httpbin.org/delete")r = requests.head("http://httpbin.org/get")r = requests.options("http://httpbin.org/get")# 获得响应内容r.text # 返回文本r.content # 返回字节r.raw # 返回原始内容r.json() # 返回json 关于更多的参数与内容请参考Requests文档。 BloomFilter BloomFilter是一个用于过滤重复数据的数据结构，我们可以使用它来对重复的url进行过滤。本文使用的BloomFilter来自于python-bloomfilter，其他操作系统用户请使用pip install pybloom命令安装，windows用户请使用pip install pybloom-live（原版对windows不友好）。 分析 介绍了需要的依赖库之后，我们终于可以开始实现自己的图片爬虫了。我们的目标是爬https://www.deviantart.com/网站中的图片，在写爬虫程序之前，还需要先分析一下页面的HTML结构，这样才能针对性地找到图片的源地址。 为了保证爬到的图片的质量，我决定从热门页面开始爬，链接为https://www.deviantart.com/whats-hot/。 打开浏览器的开发者工具后，可以发现每个图片都是由一个a标签组成，每个a标签的class为torpedo-thumb-link，而这个a标签的href正好就是这张图片的详情页面（如果我们从这里就开始爬图片的话，那么爬到的可都只是缩略图）。 进入到详情页后，不要马上爬取当前图片的源地址，因为当前页显示的图片并不是原始格式，我们对图片双击放大之后再使用开发者工具抓到这个图片所在的img标签后，再让爬虫获取这个标签中的源地址。 在获得图片的源地址之后，我的策略是让爬虫继续爬取该页中推荐的更多图片，通过开发者工具，可以发现这些图片都被封装在一个class为tt-crop thumb的div标签中，而该标签里的第一个a子标签正好就是这个图片的详情页链接。 初始配置 在对网页的HTML进行分析之后，可以开始写程序了，首先先用Scrapy的命令来初始化项目。之后在settings.py中做如下配置。 123456789# 这个是网络爬虫协议，爬虫访问网站时都会检查是否有robots.txt文件，# 然后根据文件中的内容选择性地进行爬取，我们这里设置为False即不检查robots.txtROBOTSTXT_OBEY = False# 图片下载的根目录路径IMAGES_STORE = '.'# 图片最大下载数量，当下载的图片达到这个数字时，将会手动关闭爬虫MAXIMUM_IMAGE_NUMBER = 10000 然后定义我们的Item。 12345678import scrapyclass DeviantArtSpiderItem(scrapy.Item): author = scrapy.Field() # 作者名 image_name = scrapy.Field() # 图片名 image_id = scrapy.Field() # 图片id image_src = scrapy.Field() # 图片的源地址 创建自己的spider模块与Spider类。 123456789101112131415161718192021222324252627282930313233import requestsfrom bs4 import BeautifulSoup# this import package is right,if PyCharm give out warning please ignorefrom deviant_art_spider.items import DeviantArtSpiderItemfrom pybloom_live import BloomFilterfrom scrapy.contrib.linkextractors.lxmlhtml import LxmlLinkExtractorfrom scrapy.contrib.spiders import CrawlSpider, Rulefrom scrapy.http import Requestclass DeviantArtImageSpider(CrawlSpider): name = 'deviant_art_image_spider' # 我不想让scrapy帮助过滤所以设置为空 allowed_domains = '' start_urls = ['https://www.deviantart.com/whats-hot/'] rules = ( Rule(LxmlLinkExtractor( allow=&#123;'https://www.deviantart.com/whats-hot/[\?\w+=\d+]*', &#125;), callback='parse_page', # 设置回调函数 follow=True # 允许爬虫不断地跟随链接进行爬取 ), ) headers = &#123; "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36" " (KHTML, like Gecko) Chrome/38.0.2125.111 Safari/537.36", "Referer": "https://www.deviantart.com/" &#125; # 初始化BloomFilter filter = BloomFilter(capacity=15000) DeviantArtImageSpider继承自CrawlSpider，该类是Scrapy最常用的Spider类，它通过Rule类来定义爬取链接的规则，上述代码中使用了正则表达式https://www.deviantart.com/whats-hot/[\?\w+=\d+]*，这个正则表达式将访问每一页的热门页面。 解析热门页面 爬虫启动时将会先访问热门页面，请求得到响应之后会调用回调函数，我们需要在这个回调函数中获取上述分析中得到的&lt;a class = &#39;torpedo-thumb-link&#39;&gt;标签，然后抽取出每张图片的详情页链接。 123456789101112131415161718192021222324252627282930313233 def parse_page(self, response): soup = self._init_soup(response, '[PREPARING PARSE PAGE]') if soup is None: return None # 找到所有class为torpedo-thumb-link的a标签 all_a_tag = soup.find_all('a', class_='torpedo-thumb-link') if all_a_tag is not None and len(all_a_tag) &gt; 0: for a_tag in all_a_tag: # 提取图片详情页，然后对详情页链接发起请求，并设置回调函数 detail_link = a_tag['href'] request = Request( url=detail_link, headers=self.headers, callback=self.parse_detail_page ) # 通过request与response对象来传递Item request.meta['item'] = DeviantArtSpiderItem() yield request else: self.logger.debug('[PARSE FAILED] get &lt;a&gt; tag failed') return None# 初始化BeautifulSoup对象 def _init_soup(self, response, log): url = response.url self.headers['Referer'] = url self.logger.debug(log + ' ' + url) body = requests.get(url, headers=self.headers, timeout=2).content soup = BeautifulSoup(body, 'lxml') if soup is None: self.logger.debug('[PARSE FAILED] read %s body failed' % url) return None return soup 解析详情页 parse_page()函数会不断地发送请求到详情页链接，解析详情页的回调函数需要处理数据封装到Item，还需要提取详情页中更多图片的详情链接然后发送请求。 12345678910111213141516171819202122232425262728293031323334353637 def parse_detail_page(self, response): if response.url in self.filter: self.logger.debug('[REPETITION] already parse url %s ' % response.url) return None soup = self._init_soup(response, '[PREPARING DETAIL PAGE]') if soup is None: return None # 包装Item并返回 yield self.packing_item(response.meta['item'], soup) self.filter.add(response.url) # 继续抓取当前页中的其他图片 all_div_tag = soup.find_all('div', class_='tt-crop thumb') if all_div_tag is not None and len(all_div_tag) &gt; 0: for div_tag in all_div_tag: detail_link = div_tag.find('a')['href'] request = Request( url=detail_link, headers=self.headers, callback=self.parse_detail_page ) request.meta['item'] = DeviantArtSpiderItem() yield request else: self.logger.debug('[PARSE FAILED] get &lt;div&gt; tag failed') return None# 封装数据到Item def packing_item(self, item, soup): self.logger.debug('[PREPARING PACKING ITEM]..........') img = soup.find('img', class_='dev-content-full') img_alt = img['alt'] # alt属性中保存了图片名与作者名 item['image_name'] = img_alt[:img_alt.find('by') - 1] item['author'] = img_alt[img_alt.find('by') + 2:] item['image_id'] = img['data-embed-id'] # data-embed-id属性保存了图片id item['image_src'] = img['src'] self.logger.debug('[PACKING ITEM FINISHED] %s ' % item) return item 处理Item 对于Item的处理，只是简单地将图片命名与下载到本地。我没有使用多进程或者多线程，也没有使用Scrapy自带的ImagePipeline（自由度不高），有兴趣的童鞋可以自己选择实现。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import requestsimport threadingimport osfrom scrapy.exceptions import DropItem, CloseSpiderclass DeviantArtSpiderPipeline(object): def __init__(self, IMAGE_STORE, MAXIMUM_IMAGE_NUMBER): if IMAGE_STORE is None or MAXIMUM_IMAGE_NUMBER is None: raise CloseSpider('Pipeline load settings failed') self.IMAGE_STORE = IMAGE_STORE self.MAXIMUM_IMAGE_NUMBER = MAXIMUM_IMAGE_NUMBER # 记录当前下载的图片数量 self.image_max_counter = 0 # 根据图片数量创建文件夹，每1000张在一个文件夹中 self.dir_counter = 0 def process_item(self, item, spider): if item is None: raise DropItem('Item is null') dir_path = self.make_dir() # 拼接图片名称 image_final_name = item['image_name'] + '-' + item['image_id'] + '-by@' + item['author'] + '.jpg' dest_path = os.path.join(dir_path, image_final_name) self.download_image(item['image_src'], dest_path) self.image_max_counter += 1 if self.image_max_counter &gt;= self.MAXIMUM_IMAGE_NUMBER: raise CloseSpider('Current downloaded image already equal maximum number') return item def make_dir(self): print('[IMAGE_CURRENT NUMBER] %d ' % self.image_max_counter) if self.image_max_counter % 1000 == 0: self.dir_counter += 1 path = os.path.abspath(self.IMAGE_STORE) path = os.path.join(path, 'crawl_images') path = os.path.join(path, 'dir-' + str(self.dir_counter)) if not os.path.exists(path): os.makedirs(path) print('[CREATED DIR] %s ' % path) return path def download_image(self, src, dest): print('[Thread %s] preparing download image.....' % threading.current_thread().name) response = requests.get(src, timeout=2) if response.status_code == 200: with open(dest, 'wb') as f: f.write(response.content) print('[DOWNLOAD FINISHED] from %s to %s ' % (src, dest)) else: raise DropItem('[Thread %s] request image src failed status code = %s' % (threading.current_thread().name, response.status_code)) @classmethod def from_crawler(cls, crawler): settings = crawler.settings return cls(settings['IMAGES_STORE'], settings['MAXIMUM_IMAGE_NUMBER']) 在settings.py中注册该Pipeline 123ITEM_PIPELINES = &#123; 'deviant_art_spider.pipelines.DeviantArtSpiderPipeline': 300,&#125; IP代理池 有些网站会有反爬虫机制，为了解决这个问题，每次请求都使用不同的IP代理，有很多网站提供IP代理服务，我们需要写一个爬虫从云代理中抓取它提供的免费IP代理（免费IP很不稳定，而且我用了代理之后反而各种请求失败了Orz…）。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182import osimport requestsfrom bs4 import BeautifulSoupclass ProxiesSpider(object): def __init__(self, max_page_number=10): self.seed = 'http://www.ip3366.net/free/' self.max_page_number = max_page_number # 最大页数 self.crawled_proxies = [] # 爬到的ip,每个元素都是一个dict self.verified_proxies = [] # 校验过的ip self.headers = &#123; 'Accept': '*/*', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko)' ' Chrome/45.0.2454.101 Safari/537.36', 'Accept-Language': 'zh-CN,zh;q=0.8' &#125; self.tocrawl_url = [] def crawl(self): self.tocrawl_url.append(self.seed) page_counter = 1 while self.tocrawl_url: if page_counter &gt; self.max_page_number: break url = self.tocrawl_url.pop() body = requests.get(url=url, headers=self.headers, params=&#123;'page': page_counter&#125;).content soup = BeautifulSoup(body, 'lxml') if soup is None: print('PARSE PAGE FAILED.......') continue self.parse_page(soup) print('Parse page %s done' % (url + '?page=' + str(page_counter))) page_counter += 1 self.tocrawl_url.append(url) self.verify_proxies() self.download() # 解析页面并封装 def parse_page(self, soup): table = soup.find('table', class_='table table-bordered table-striped') tr_list = table.tbody.find_all('tr') for tr in tr_list: ip = tr.contents[1].text port = tr.contents[3].text protocol = tr.contents[7].text.lower() url = protocol + '://' + ip + ':' + port self.crawled_proxies.append(&#123;url: protocol&#125;) print('Add url %s to crawled_proxies' % url) # 对ip进行校验 def verify_proxies(self): print('Start verify proxies.......') while self.crawled_proxies: self.verify_proxy(self.crawled_proxies.pop()) print('Verify proxies done.....') def verify_proxy(self, proxy): proxies = &#123;&#125; for key in proxy: proxies[str(proxy[key])] = key # requests的proxies的格式必须为 协议 : 地址 try: if requests.get('https://www.deviantart.com/', proxies=proxies, timeout=2).status_code == 200: print('verify proxy success %s ' % proxies) self.verified_proxies.append(proxy) except: print('verify proxy fail %s ' % proxies) # 保存到文件中 def download(self): current_path = os.getcwd() parent_path = os.path.dirname(current_path) with open(parent_path + '\proxies.txt', 'w') as f: for proxy in self.verified_proxies: for key in proxy.keys(): f.write(key + '\n')if __name__ == '__main__': spider = ProxiesSpider() spider.crawl() 得到了IP代理池之后，还要在Scrapy的middlewares.py模块定义代理中间件类。 123456789101112131415161718192021222324252627282930313233343536373839import timefrom scrapy import signalsimport osimport randomclass ProxyMiddleware(object): # 每次请求前从IP代理池中选择一个IP代理并进行设置 def process_request(self, request, spider): proxy = self.get_proxy(self.make_path()) print('Acquire proxy %s ' % proxy) request.meta['proxy'] = proxy # 请求失败，重新设置IP代理 def process_response(self, request, response, spider): if response.status != 200: proxy = self.get_proxy(self.make_path()) print('Response status code is not 200,try reset request proxy %s ' % proxy) request.meta['proxy'] = proxy return request return response def make_path(self): current = os.path.abspath('.') parent = os.path.dirname(current) return os.path.dirname(parent) + '\proxies.txt' # 从IP代理文件中随机获得一个IP代理地址 def get_proxy(self, path): if not os.path.isfile(path): print('[LOADING PROXY] loading proxies failed proxies file is not exist') while True: with open(path, 'r') as f: proxies = f.readlines() if proxies: break else: time.sleep(1) return random.choice(proxies).strip() 最后在settings.py中进行注册。 123456DOWNLOADER_MIDDLEWARES = &#123; # 这个中间件是由scrapy提供的，并且它是必需的 'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware': 543, # 我们自定义的代理中间件 'deviant_art_spider.middlewares.ProxyMiddleware': 540 &#125; End 我们的图片爬虫已经完成了，执行命令scrapy crawl deviant_art_image_spider，然后尽情搜集图片吧！ 想要获得本文中的完整源代码与P站爬虫请点我，顺便求个star… 最近心血来潮想要写爬虫，所以花了点时间过了一遍python语法便匆匆上手了，代码写的有点丑也不够pythonic，各位看官求请吐槽。]]></content>
      <categories>
        <category>后端</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探索计算机的结构与核心概念]]></title>
    <url>%2F2017%2F09%2F08%2F2017-09-08-ComputerStructure%2F</url>
    <content type="text"><![CDATA[在我们的生活与工作中所使用到的计算机都是基于冯诺依曼结构实现的,冯诺依曼结构又称冯诺依曼模型或普林斯顿结构,它是一种将程序指令存储器和数据存储器合并在一起的计算机设计概念结构. 冯诺依曼结构起源于EDVAC(Electronic Discrete Variable Automatic Computer)离散变量自动电子计算机,当时冯诺依曼以技术顾问的身份加入EDVAC项目组,负责总结和详细说明EDVAC的逻辑设计,直到1945年6月发表了一份长达101页的报告,这就是计算机史上著名的”101页报告”,该报告明确规定用二进制替代十进制运算,并将计算机分成五大组件,这一卓越的思想为电子计算机的逻辑结构设计奠定了基础,已成为计算机设计的基本原则. 冯诺依曼结构具有以下特点: 数据由一个贯穿整个结构的总线来进行传输. 存储器是按地址访问、线性编址的空间 指令由操作码和地址码组成 数据以二进制编码 一个冯诺依曼结构的计算机必须有存储器,控制单元,运算单元,输入输出设备. 冯诺依曼结构将CPU与存储器分开的做法也并非十全十美,CPU和内存、硬盘等设备的数据传输速度不匹配成了整体效率的瓶颈,CPU会在等待数据输入的时间中空置,许多技术都是为了解决这个瓶颈,例如DMA(直接内存访问),在CPU中建立高速缓冲区等. 本文作者为: SylvanasSun(sylvanas.sun@gmail.com).转载请务必将下面这段话置于文章开头处(保留超链接).本文首发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/09/08/2017-09-08-ComputerStructure/ 现代计算机结构 现代计算机是基于冯诺依曼结构的电子计算机.所谓电子计算机,就是是一种利用电子学原理,根据一系列指令对数据进行处理的机器. 晶体管是组成现代电子计算机的最原始的部件(集成电路中含有数以亿计的晶体管),它是一种半导体材料(导电性可受控制,范围可从绝缘体至导体之间),晶体管可以通过电流的变化,实现电路的切换,这种特性非常适合组成各种逻辑门(与或非)与表示二进制数据.值得一提的是,早期使用继电器实现逻辑门的计算机体积甚至大到要一整个屋子才能放下. 现代计算机的硬件结构如下图,虽然多了很多其他的硬件但与冯诺依曼结构的概念是一致的: 总线 总线是一组贯穿所有硬件结构的电子管道,它携带数据并负责在各个部件间交互传递.总线传送的数据通常为一个定长的字节块,这个字节块的长度即是总线的位宽,总线位宽越大,数据传输的性能就越高,在32位机器中总线位宽为4个字节,64位机器中为8个字节. 有意思的是总线的英文单词是bus,如果把主板想象成一座城市,那么总线就像是城市中的公共汽车,它按着多种固定线路不停地来回传输数据. I/O设备 I/O(输入/输出)设备是计算机与外部进行联系的桥梁,每个I/O设备都要通过一个控制器或者适配器来与I/O总线相连. 控制器与适配器的区别只在于它们的封装方式,它们的功能都是为了让I/O设备与I/O总线进行连接: 控制器是I/O设备本身或者主板上自带的芯片组 适配器则是插在主板上的外部设备, 在图中,I/O设备包含鼠标、键盘(输入设备)、显示器(输出设备)、磁盘、网络. 内存 内存也叫主存,它是一个临时的存储设备,存储了运行时的数据(程序与程序处理的数据),以供CPU进行处理.内存是由一组DRAM(动态随机存取存储器)芯片组成的,DRAM是RAM(随机存取存储器)的一种,另一种为SRAM(静态随机存取存储器),SRAM比DRAM速度更快,但造价也更贵,通常用来实现为高速缓存区. 32位操作系统中的CPU的最大寻址空间只有2^32字节,换算下来最高内存上限为4GB,但由于CPU还要对BIOS和其他硬件等进行寻址(这些优先级更高),所以用户实际可用的内存只有3GB左右. 64位操作系统的CPU最大寻址空间足足有2^64字节,也就是16EB(1024GB等于1TB,1024TB等于1PB,1024PB等于1EB),这已经是一个无法想象的数字了,不过这也不一定是够用的,毕竟谁又能知道未来的数据量会有多庞大呢? 内存具有以下特点: 随机存取: 当存储器中的数据被写入或读取时,所需要的时间与数据所在的位置无关(从逻辑上,可以把内存看成一个线性的字节数组,每个字节都有其唯一的地址(索引),这些地址是从零开始的). 易失性: 如果电源突然断开,RAM中的数据就会全部丢失(磁盘可以将数据持久化地永久保存下来,就算断电也不会丢失数据). 依赖刷新: RAM使用电容器来存储数据,当电容器充满电之后表示1,未充电则表示0.由于电容器或多或少有漏电的情形,若不作特别处理,电荷会渐渐随时间流失而使数据发生错误.刷新是指重新为电容器充电,弥补流失了的电荷.DRAM的读取即有刷新的功效,但一般的定时刷新并不需要作完整的读取,只需作该芯片的一个列选择,整列的数据即可获得刷新,而同一时间内,所有相关记忆芯片均可同时作同一列选择,因此,在一段期间内逐一做完所有列的刷新,即可完成所有存储器的刷新.需要刷新正好解释了随机存取存储器的易失性. 对静电敏感: RAM与集成电路一样,对环境的静电荷非常敏感,静电会干扰存储器内电容器的电荷,导致数据流失,甚至烧坏电路. CPU Central Processing Unit中央处理单元,简称CPU或处理器,CPU包含了冯诺依曼结构中的控制器与运算器,它是解释或执行存储在内存中的指令的引擎.CPU好比计算机的大脑,从通电开始,直到断电,CPU一直在不断地执行内存中存储的指令.如果没有CPU,那么计算机就会是一台不会动的死机器了. 所谓指令就是进行指定操作的操作码,而指令集架构就是这些操作码的集合,至于微架构是一套用于执行指令集的微处理器设计方法,多个不同微架构的CPU可以使用同一套指令集,一些常见的指令如下: 加载: 从内存中复制数据(多少个字节取决于总线位宽)到寄存器,以覆盖寄存器中原来的内容. 存储: 从寄存器复制数据到内存中的某个位置,以覆盖这个位置上原有的内容. 操作: 把两个在寄存器中的数据复制到ALU,ALU对这2个数据进行算术运算,并将结果存放到一个寄存器中,以覆盖该寄存器中原有的内容. 跳转: 从指令本身中抽取数据(地址),将它复制到程序计数器中,以覆盖程序计数器原有的内容. 下面以一个简单的算术问题1 + 1来大致了解一下CPU的工作流程: 这两个变量首先会被存储在内存中. CPU从内存中读取指令并刷新程序计数器(每执行完一个指令都要刷新程序计数器). CPU执行加载指令,通过总线将这两个变量传输(复制)到寄存器. CPU执行运算指令,从寄存器中复制这两个变量进行算术运算,并将结果存到寄存器. CPU执行存储指令,寄存器通过总线将结果存储回内存(覆盖原有位置). 寄存器 寄存器是CPU中的一个存储部件,可以认为它是容量很小但速度飞快的内存,寄存器是与ALU直接交互的存储设备(不管数据是在内存还是高速缓冲区,最终都要存到寄存器才能与ALU交互). 在CPU架构中,拥有多个寄存器,它们分别拥有各自的用途(指令寄存器,整数寄存器,浮点数寄存器等),且寄存器的数量和它的大小都与指令集架构和机器支持的位宽相关联(例如x86-64指令集架构(64位指令集架构)中支持64位的通用寄存器与64位整数运算,而x86指令集架构只能支持32位和16位). 程序计数器 程序计数器用于指示将要执行的指令序列,并且不断刷新指向新的指令地址,根据CPU的实现不同,程序计数器可能会指向正在运行的指令地址也可能会是下一个指令的地址. 高速缓冲 由于寄存器与内存的速度相差过大,为了避免性能上的浪费,在寄存器与内存之间建立数据的缓存区是很有必要的. 高速缓存是一个比内存更小但更快的存储设备,且使用SRAM实现,现在的CPU一般都配有三级缓存,L1缓存速度最快但存储的容量也最小,L2要比L1慢但存储的容量也更大,以此类推(上一层的存储器作为下一层存储器的高速缓存,也就是说,寄存器就是L1的高速缓存,L1则是L2的高速缓存,L2是L3的高速缓存…)…. 当CPU发起向内存加载数据的请求时,会先从缓存中查找,如果缓存未命中,才会从内存加载数据,并更新缓存.高速缓存之所以如此有效,主要是利用了局部性原理,即最近访问过的内存位置以及周边的内存位置很容易会被再次访问.而高速缓存中就存储着这些经常会被访问的数据. DMA DMA全称为Direct Memory Access直接内存访问,它允许其他硬件可以直接访问内存中的数据,而无需让CPU介入处理.一般会使用到DMA的硬件有显卡、网卡、声卡等. DMA会导致发生缓存不一致的问题,需要额外的进行同步操作保证数据安全.例如,当CPU从内存中读取数据后,会暂时将新数据写入缓存中,但还没有将数据更新回内存,如果在这期间发生了DMA,就会读取到旧的数据. 流水线 流水线又称管线,是现代CPU中必不可少的优化技术,它将指令的处理过程拆分为多个步骤,并通过多个硬件处理单元并行执行这些步骤. 管线的具体执行过程很像工厂中的流水线(指令就像在流水线传送带上的产品,各个硬件处理单元就像是在流水线旁进行操作的工人),因此而得名为流水线. 流水线虽然提高了整体的吞吐量,但也是有其缺点的,这是由于流水线依赖于分支预测,如果CPU预测的分支是错误的,那么整个流水线上的所有指令都要取消,然后重新向流水线填充指令,这项操作是很耗费性能的. 超线程 超线程是一种允许一个CPU执行多个控制流的技术,它复制了CPU中必要的硬件资源(程序计数器、寄存器),来让其在同一时间内处理两个线程的工作. 通过超线程技术,可以让一个CPU核心去执行两个线程,所以一个带有4核(实体核心)的CPU实际上可以执行8个线程(逻辑线程). 多核 多核CPU是指将多个核心(也就是CPU)集成到一个集成电路芯片上.每个核心都可以独立的执行指令,也就是真正意义上的并行执行. 每个核心都拥有独立的寄存器,程序计数器,高速缓存等组件,一般还会有一个所有核心共享的缓存,它是直接与内存连通的缓冲区. 多核CPU与多处理器不同,多处理器是将多个CPU封装在多个独立的集成电路芯片中,而多核CPU是所有核心都封装在同一个集成电路芯片中. 操作系统 操作系统是用于管理计算机硬件与软件的程序,可以把操作系统看成是应用程序与硬件之间插入的一层软件,所有应用程序对硬件的操作尝试都必须通过操作系统. 操作系统需要负责管理与配置内存、调度系统资源的优先次序、管理进程与线程、控制I/O设备、操作网络与管理文件系统等事务.可以说操作系统是整个计算机系统中的灵魂所在. 操作系统的内核是操作系统最核心的地方,它是代码和数据的一个集合.当应用程序需要操作系统的某些操作时,会执行一条系统调用(system call)指令,这时,控制权会被移交到内核,由内核执行被请求的操作并返回到应用程序.大多数系统的交互式操作都需要在内核完成,例如I/O、进程管理等. 虚拟内存 虚拟内存是计算机系统内存管理的一种技术,它为每个进程提供了一个假象,即每个进程都在独占地使用内存(一个连续的地址空间),而实际上,它通常被分割为多个物理内存碎片,还有部分暂时存储在磁盘存储器上,在需要时进行数据交换.使用虚拟内存会使程序的编写更加容易,对真实的物理内存的使用也会更加有效率. 每个进程所能看到的虚拟地址空间大致如上图所示,每个区域都有它专门的作用. 内核虚拟内存: 这个区域是为操作系统内核保留的,它不允许应用程序读写这个区域的内容或者直接调用内核代码定义的函数(只有操作系统内核才有权限). 共享库: 以c语音为例,共享库是用来存放的是像C标准库这样的共享库的代码和数据的区域. 程序代码和数据: 对于所有进程来说,代码都是从同一固定地址开始,紧接着的是其相对应的数据位置.这片区域就是用来存放代码和数据的. 堆: 堆内存是指应用程序在运行时进行分配的内存区域,堆可以在运行时动态地扩展和收缩.像malloc()和free()这样的函数就是在堆内存中进行分配空间与释放,而类似Java这种更高一级的语言提供了自动内存管理和垃圾回收,不需要程序员手动地分配与释放堆内存空间. 栈: 栈同样也是可以动态地扩展和收缩,它是一个后进先出的容器,主要用于函数调用.当一个函数调用时会在栈中分配空间,当调用结束时,这个函数所占用的内存空间会一起释放,无需程序员关心. 进程与线程 进程 进程是操作系统对一个正在运行的程序的一种抽象,它是程序的执行实体,是操作系统对资源进行调度的一个基本单位,同时也是线程的容器. 进程跟虚拟内存一样,也是操作系统提供的一种假象,它让每个程序看上去都是在独占地使用CPU、内存和I/O设备.但其实同一时间只有一个进程在运行,而我们能够边听歌边上网边码代码的原因其实是操作系统在对进程进行切换,一个进程和另一个进程其实是交错执行的,只不过计算机的速度极快,我们无法感受到而已. 操作系统会保持跟踪进程运行所需的所有状态信息,这种状态,被称为上下文(Context),它包含了许多重要的信息,例如程序计数器和寄存器的当前值等.当操作系统需要对当前进程进行切换时(转移到另一个进程),会保存当前进程的上下文,然后恢复新进程的上下文,这时控制权会移交到新进程,新进程会从它上次停下来的地方开始执行,这个过程叫做上下文切换. 操作系统的进程空间可以分为用户空间与内核空间,也就是用户态与内核态.它们的执行权限不同,一般的应用程序是在用户态中运行的,而当应用程序执行系统调用时就需要切换到内核态,由内核执行. 线程 线程是操作系统所能调度的最小单位,它被包含在进程之中,且一个进程中的所有线程共享进程的资源,一个线程一般被指为进程中的一条单一顺序的控制流. 线程都运行在进程的上下文中,虽然线程共享了进程的资源,但每条线程都拥有自己的独立空间,例如函数调用栈、寄存器、线程本地存储. 线程的实现主要有以下三种方式: 使用内核线程实现: 内核线程就是由操作系统内核直接支持的线程,这种线程由内核来完成线程切换调度,内核通过调度器对线程进行调度,并将线程的任务映射到各个处理器上.应用程序一般不会直接使用内核线程,而是使用内核线程的一个接口: 轻量级进程,每个轻量级进程都由一个内核线程支持,所以它们的关系是1:1的.这种线程的实现方式的缺点也很明显,应用程序想要进行任何线程操作都需要进行系统调用,应用程序会在用户态和内核态之间来回切换,消耗的性能资源较多. 使用用户线程实现: 这种方式将线程完全实现在用户空间中,相关的线程操作都在用户态中完成,这样可以避免切换到内核态,提高了性能.但正因为没有借助系统调用,操作系统只负责对进程分配资源,这些复杂的线程操作与线程调度都需要由用户线程自己处理实现,提高了程序的复杂性.这种实现方式下,一个进程对应多个用户线程,它们是1:N的关系. 混合实现: 这是一种将内核线程与用户线程一起使用的实现方式.在这种实现下,即存在用户线程,也存在轻量级进程.用户线程依旧是在用户空间中建立的(相关的线程操作也都是在用户空间中),但使用了轻量级进程来当作用户线程与内核线程之间的桥梁,让内核线程提供线程调度和对处理器的映射.这种实现方式下,用户线程与轻量级进程的数量比例是不定的,它们是N:M的关系. 文件 文件也是一个非常重要的抽象概念,它向应用程序提供了一个统一的视图,来看待系统中可能含有的所有各式各样的I/O设备.计算机文件系统通过文件与树形目录的抽象概念来屏蔽磁盘等物理设备所使用的数据块(chunk),让用户在使用文件的时候无需关心它实际的物理地址,用户也不需要管理磁盘上的空间分配,这些都由文件系统负责. 所谓文件其实也就是一串字节序列,一个文件想要长期存储,就必须要存放于某种存储设备上,如本地磁盘、U盘. 网络 如果用图论的方式来看待网络,其实网络就是一张无向图(需要双向通信),每台计算机都是图中的一个节点(指计算机网络),图的边就是计算机之间互相通信的连接.简单的说,计算机网络其实就是多台计算机进行通信的系统. 网络其实也可以看作是一个I/O设备,当系统从内存中复制一串字节到网络适配器时,数据流经过网络传输到达另一台机器上(这其实就是输出操作),系统也可以读取从其他机器传输过来的数据,并把数据复制到内存中(输入). 互联网(Internet)是计算机网络中的一种(如果按区域划分还有局域网、广域网等),互联网是网络与网络之间组成的巨大的国际网络,这些网络之间以TCP/IP协议相连,连接了全世界上几十亿的设备. 我们日常生活中用浏览器上网浏览网页,其实使用的是万维网(World Wide Web),它是运行在互联网之上提供的一个服务,万维网是一个基于超文本链接组成的系统,并且通过http协议进行访问. OSI模型 OSI模型全称为开放式系统互联通信参考模型(Open System Interconnection Reference Model),是由国际标准化组织提出的一个试图使各种计算机在世界范围内进行互联通信的标准框架. 在OSI模型中,数据经过每一层都会添加该层的协议头(物理层除外),当一个数据从一端发送到另一端时,需要经过层层封装. 应用层: 应用层直接和应用程序通信并提供常见的网络应用服务.常见的应用层协议有:HTTP,HTTPS,FTP,TELNET,SSH,SMTP,POP3等. 表示层: 表示层为不同终端的上层用户提供数据和信息正确的语法表示变换方法.该层定义了数据格式及加解密, 会话层: 会话层负责在数据传输中设置和维护网络中两台电脑之间的通信连接.但会话层不参与具体的传输,它只提供包括访问验证和会话管理在内的建立和维护应用之间通信的机制. 传输层: 传输层将数据封装成数据包,提供端对端的数据通信服务.它还提供面向连接的数据流支持、可靠性、流量控制、多路复用等服务.最著名的传输层协议有TCP与UDP. 网络层: 网络层提供路由和寻址的功能,使两终端系统能够互连且决定最佳路径,并具有一定的拥塞控制和流量控制的能力.网络层将网络表头(包含网络地址等数据)加到数据包中,网络层协议中最出名的就是IP协议. 数据链路层: 数据链路层在两个网络实体之间提供数据链路连接的创建、维持和释放管理.它将数据划分为数据帧从一个节点传输到临近的另一个节点,这些节点是通过MAC(主机的物理地址)来进行标识的. 物理层: 物理层是OSI模型中最低的一层,物理层主要负责传输数据所需要的物理链路创建、维持、拆除，而提供具有机械的,电子的,功能的和规范的特性.简单来说,物理层负责了物理设备之间的通信传输. TCP/IP TCP协议全称为传输控制协议(Transmission Control Protocol),由于它是基于IP协议之上的,所以也有人称作为TCP/IP协议. TCP协议是位于传输层的协议,它与同样位于传输层的UDP协议差别很大,它保证了数据包在传输时的安全性(丢包重传),而UDP则只负责发送数据,不保证数据的安全. TCP为了保证不发生丢包,给每个包标记了一个序号,同时序号也保证了接收端在接收数据包时的顺序.然后接收端对已成功收到的包发回一个相应的确认(ACK)；如果发送端在合理的往返时延(RTT)内未收到确认,那么对应的数据包就被假设为已丢失将会被进行重传.TCP用一个校验和函数来检验数据是否有错误,在发送和接收时都要计算校验和. TCP协议在连接建立与终止时需要经过三次握手与四次挥手,这个机制主要都是为了提高可靠性. 客户端发送SYN（SEQ=x）报文给服务器端,进入SYN_SEND状态,等待服务器端确认. 服务器端收到SYN报文,回应一个SYN （SEQ=y）ACK(ACK=x+1）报文,进入SYN_RECV状态. 客户端收到服务器端的SYN报文,回应一个ACK(ACK=y+1）报文,进入Established状态. 服务器接收到客户端发送的SYN报文,三次握手完成,连接建立. 某一端首先调用close,称该端执行“主动关闭”（active close）.该端发送一个FIN报文,表示数据发送完毕(我们称它为A端). 另一端接收到这个FIN信号执行 “被动关闭”（passive close ),并回应一个ACK报文.(我们称它为B端) 一段时间后,B端没有数据发送的任务了,这时它将调用close关闭套接字,然后向A端发送一个FIN信号. A端接收到FIN信号,开始进行关闭连接,并对B端返回一个ACK. B端接收到来自A端的ACK信号,进行关闭连接,四次挥手完毕. TCP/IP将OSI模型抽象成了四层,下图为以HTTP为例的一个数据发送过程. 分组交换 数据包在网络中进行传输时使用了分组交换.分组交换也称为包交换,它将用户通信的数据划分成多个更小的等长数据段,在每个数据段的前面加上必要的控制信息作为数据段的首部,每个带有首部的数据段就构成了一个分组.首部指明了该分组发送的地址,当交换机收到分组之后,将根据首部中的地址信息将分组转发到目的地,这个过程就是分组交换.能够进行分组交换的通信网被称为分组交换网. 分组交换的本质就是存储转发,它将所接受的分组暂时存储下来,在目的方向路由上排队,当它可以发送信息时,再将信息发送到相应的路由上,完成转发.其存储转发的过程就是分组交换的过程. 数据的表示 计算机编程语言拥有多种数据类型, 例如int、char、double等.但不管是什么类型的数据,在计算机中其实都只是一个字节序列(以8位二进制为一个字节).每个机器中对字节序列的排序不大相同,有一些机器按照从最高有效字节到最低有效字节的顺序存储,这种规则被称为大端法;还有一些机器将最低有效字节排在最前面,这种规则被称为小端法. 计算机使用补码来表示数值,一个数的最高有效位为符号位(以整数为例,整数占有4字节32位,最高位即最左位,剩下31位用于表示数字,所以整数的有效范围为-2^31 ~ 2^31 - 1),如果符号位为1,则代表这个值为负,如果符号位为0,则代表这个值为正.负数的补码即是它的反码(在保持符号位不变的前提下按位取反)+1,正数的补码不需要做其他操作,就是它本身的值. 当将一个较小类型的值强转为较大类型时(如byte强转为int),将会发生符号扩展,较小类型不包含的位会以符号位来进行填充(还是以byte为例,当它强转为int时,高24位会被填充为最高有效位中的数值,如果最高有效位为1,那么高24位都会为1,这时byte原来要表示的值将产生变化,要避免这种情况,可以使用一个低8位为1高24位为0的数,将它与强转后的结果进行&amp;操作,来保留低8位,并消除高24位中的1). 对一个数进行移位操作时,也需要按规则填充丢失的位数.移位操作分为算术移位与逻辑移位,算术移位会填充符号位,而逻辑移位全部填充0. 当进行左移操作时,右边空出的位用0补充,高位左移溢出则舍弃该高位. 当进行右移操作时,左边空出的位用符号位来补充(正数补0,负数补1),右边溢出则舍弃.如果使用逻辑移位(Java中为&gt;&gt;&gt;),左边空出的位会用0来补充. 读到这里,可能有人会有疑问,为什么计算机非得使用补码?这主要因为,计算机中没有减法器只有加法器,而减去一个数其实就是加上一个负数,使用补码进行计算会很方便快速. 我们假设一个指定n为长度的二进制序列,那么它将会有2^n个可能的值,加减法运算都存在上溢出与下溢出的情况,实际上都等价于模(≡) 2^n的加减法运算. 把范围想象成一个时钟,假设现在时针指向数字3,若要得出6小时前时针指向的数字是几,有两种方法: 将时针逆时针拨动6格. 将时针顺时针拨动12 - 6 = 6格. 这里的12就是模,3小时-6小时 = 3小时 + (12 - 6)小时. 例如以下例子,模为2^8 = 256 一个8位无符号整数的值的范围是0到255.因此4+254将上溢出,结果为2: (4 + 254) ≡ 258 ≡ 258 - 256 ≡ 2 一个8位有符号整数的值的范围是−128到127,则126+125将上溢出,结果为-5: (126+125) ≡ 251 ≡ 251 - 256 ≡ -5 浮点数 浮点数是一种对于实数的近似值数值表现法,由一个有效数字（即尾数）加上幂数来表示,通常是乘以某个基数的整数次指数得到.但浮点数计算通常伴随着因为无法精确表示而进行的近似或舍入. 在计算机使用的浮点数被电气电子工程师协会（IEEE）规范化为IEEE-754,任意一个二进制浮点数V都可以表示成下列形式: V = (-1)^s * M * 2^E ${(-1)}^s$表示符号位,当s=0,V为正数;s=1,V为负数. M 表示有效数字,$1≤M&lt;2$. $2^E$表示指数位. 这种表示方式有点类似于科学计数法,在计算机中,通常使用2为基数的幂数来表示.IEEE-754同时还规定了单精度(float)与双精度(double)的区别: 32位的单精度浮点数,最高1位是符号位s,接着的8位是指数E,剩下的23位是有效数字M. 64位的双精度浮点数,最高1位是符号位s,接着的11位是指数E,剩下的52位为有效数字M. 函数调用 当调用一个函数时,系统会在栈上分配一个空间,存放了函数中的局部变量、函数参数、返回地址等,这样的一个结构被称为栈帧. 函数中的数据的存活状态是后进先出的,而栈正好是满足这一特性的数据结构,这也是为什么计算机使用栈来当作函数调用的存储结构. 123456789101112131415161718192021222324252627282930int main() &#123; sayHello(); return 0;&#125;void sayHello() &#123; hello_world();&#125;void hello_world() &#123; print("Hello,World");&#125; main() sayHello() hello_world() print() - main() | +&gt; - sayHello() . | . +&gt; - hello_world() . . | . . +&gt; - print() . . . | . . + &lt;- return from print() . . | . + &lt;- return from hello_world() . | + &lt;- return from sayHello() | - return from main() 在x86-64架构中,栈是向低地址方向生长的,寄存器%rsp指向栈顶,当一个函数被调用时,将会执行pushq指令,栈帧入栈,栈指针减小(向下生长),当函数返回后,将会执行popq指令,栈帧出栈,释放空间,栈指针增加.如果不断有函数进行调用,栈就会不断向下生长,最终会产生Stack Overflow. 计算机编程语言 计算机编程语言是用来定义计算机程序的语言,它以一种标准化的语法规则来向计算机发出指令.最早的编程语言是在计算机发明之前产生的,当时是用来控制提花织布机及自动演奏钢琴的动作.如今已经有上千种不同的编程语言,不管是哪种语言,尽管它们的特性各有不同,但写程序的核心都是条件判断、循环、分支(这些也是机器指令的核心). 编程语言依赖于编译器或解释器(所以也分为编译型语言与解释型语言),如果没有对应的编译器/解释器来对语法与语义进行分析并生成对应的机器语言,那么我们所写的代码其实都只是普通的文本字符(编译器/解释器也会对源代码进行一系列优化提高性能). 编译型语言通过编译器直接将源代码翻译成机器语言并生成一个可执行文件(机器语言是不兼容的,如果要到另一台机器上运行,就需要对源代码重新编译);解释型语言通过解释器动态地翻译源代码并直接执行(性能上会比编译型语言直接运行可执行文件要差);虽然大多数的语言既可被编译又可被解译,但大多数仅在一种情况下能够良好运行. Java的编译机制比较特殊,它将Java源代码编译成JVM字节码(通过虚拟机来达到一次编译在所有平台可用),然后JVM对字节码进行解释执行,但对于较热的代码块(频繁调用的函数等),JVM会通过JIT即时编译技术将这些频繁使用的代码块动态地编译成机器语言,提高程序的性能.]]></content>
      <categories>
        <category>计算机</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>计算机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编码的那点事儿]]></title>
    <url>%2F2017%2F08%2F20%2F2017-08-20-Encode%2F</url>
    <content type="text"><![CDATA[什么是编码? 对于普通人来说,编码总是与一些秘密的东西相关联(加密与解密);对于程序员们来说,编码大多数是指一种用来在机器与人之间传递信息的方式. 但从广义上来讲,编码是从一种信息格式转换为另一种信息格式的过程,解码则是编码的逆向过程.接下来举几个使用到编码的例子: 当我们要把想表达的意思通过一种语言表达出来,其实就是在脑海中对信息进行了一次编码,而对方如果也懂得这门语言,那么就可以用这门语言的解码方法(语法规则)来获得信息(日常的说话交流其实就是在编码与解码). 程序员写程序时,其实就是在将自己的想法通过计算机语言进行编码,而编译器则通过生成抽象语法树,词义分析等操作进行解码,最终交给计算机执行程序(编译器产生的解码结果并不是最终结果,一般为汇编语言,但汇编语言只是CPU指令集的助记符,还需要再进行解码). 计算机只有两种状态(0和1),要想存储和传输多媒体信息,就需要用到编码和解码. 对数据进行压缩,其本质就是以减少自身占用的空间为前提进行重新编码. 了解了编码的含义,我们接下来重点探究Java中的字符编码. 本文作者为: SylvanasSun.转载请务必将下面这段话置于文章开头处(保留超链接).本文首发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/08/20/2017-08-20-Encode/ 常见的字符集 字符集就是字符与二进制的映射表,每一个字符集都有自己的编码规则,每个字符所占用的字节也不同(支持的字符越多每个字符占用的字节也就越多). ASCII : 美国信息交换标准码(American Standard Code for Information Interchange).学过计算机的都知道大名鼎鼎的ASCII码,它是基于拉丁字母的字符集,总共记有128个字符,主要目的是显示英语.其中每个字符占用一个字节(只用到了低7位). ISO-8859-1 : 它是由国际标准化组织(International Standardization Organization)在ASCII基础上制定的8位字符集(仍然是单字节编码).它在ASCII空置的0xA0-0xFF范围内加入了96个字母与符号,支持了欧洲部分国家的语言. GBK : 如果我们想要让电脑上显示汉字就必须要有支持汉字的字符集,GBK就是这样一个支持汉字的字符集,全称为&lt;&lt;汉字内码扩展规范&gt;&gt;,它的编码方式分为单字节与双字节: 00–7F范围内是第一个字节,与ASCII保持一致,之后的双字节中,前一字节是双字节的第一位(范围在81–FE,不包含80和FF),第二字节的一部分在40–7E,其他部分在80–FE.(这里不再介绍GB2313与GB18030,它们都是互相兼容的.) UTF-16 : UTF-16是Unicode(统一码,一种以支持世界上多国语言为目的的通用字符集)的一种实现方式,它把Unicode的抽象码位映射为2~4个字节来表示,UTF-16是变长编码(UTF-32是真正的定长编码),但在最开始以前UTF-16是用来配合UCS-2(UTF-16的子集,它是定长编码,用2个字节表示所有Unicode字符)使用的,主要原因还是因为当时Unicode只有不到65536个字符,2个字节就足以应对一切了.后来,Unicode支持的字符不断膨胀,2个字节已经不够用了,导致一些只支持UCS-2当做内码的产品很尴尬(Java就是其中之一). UTF-8 : UTF-8也是基于Unicode的变长编码表,它使用1~6个字节来为每个字符进行编码(RFC 3629对UTF-8进行了重新规范,只能使用原来Unicode定义的区域,U+0000~U+10FFFF,也就是说最多只有4个字节),UTF-8完全兼容ASCII,它的编码规则如下: 在U+0000~U+007F范围内,只需要一个字节(也就是ASCII字符集中的字符). 在U+0080~U+07FF范围内,需要两个字节(希腊文、阿拉伯文、希伯来文等). 在U+0800~U+FFFF范围内,需要三个字节(亚洲汉字等). 其他的字符使用四个字节. Java中字符的编解码 Java提供了Charset类来完成对字符的编码与解码,主要使用以下函数: public static Charset forName(String charsetName) : 这是一个静态工厂函数,它根据传入的字符集名称来返回对应字符集的Charset类. public final ByteBuffer encode(CharBuffer cb) / public final ByteBuffer encode(String str) : 编码函数,它将传入的字符串或者字符序列进行编码,返回的ByteBuffer是一个字节缓冲区. public final CharBuffer decode(ByteBuffer bb) : 解码函数,将传入的字节序列解码为字符序列. 示例代码 1234567891011121314151617181920212223242526272829303132333435363738private static final String text = "Hello,编码!";private static final Charset ASCII = Charset.forName("ASCII");private static final Charset ISO_8859_1 = Charset.forName("ISO-8859-1");private static final Charset GBK = Charset.forName("GBK");private static final Charset UTF_16 = Charset.forName("UTF-16");private static final Charset UTF_8 = Charset.forName("UTF-8");private static void encodeAndPrint(Charset charset) &#123; System.out.println(charset.name() + ": "); printHex(text.toCharArray(), charset); System.out.println("----------------------------------");&#125;private static void printHex(char[] chars, Charset charset) &#123; System.out.println("ForEach: "); ByteBuffer byteBuffer; byte[] bytes; if (chars != null) &#123; for (char c : chars) &#123; System.out.print("char: " + Integer.toHexString(c) + " "); // 打印出字符编码后对应的字节 byteBuffer = charset.encode(String.valueOf(c)); bytes = byteBuffer.array(); System.out.print("byte: "); if (bytes != null) &#123; for (byte b : bytes) System.out.print(Integer.toHexString(b &amp; 0xFF) + " "); &#125; System.out.println(); &#125; &#125; System.out.println();&#125; 有的读者可能会对以上代码中的b &amp; 0xFF产生疑惑,这是为了解决符号扩展问题.在Java中,如果一个窄类型强转为一个宽类型时,会对多出来的空位进行符号扩展(如果符号位为1,就补1,为0则补0).只有char类型除外,char是没有符号位的,所以它永远都是补0. 代码中调用了函数Integer.toHexString(),变量b在运算之前就已经被强转为了int类型,为了让数值不受到破坏,我们让b对0xFF进行了与运算,0xFF是一个低八位都为1的值(其他位都为0),而byte的有效范围只在低八位,所以结果为前24位(除符号位)都变为了0,低八位保留了原有的值. 如果不做这项操作,那么b又恰好是个负数的话,那这个强转后的int的前24位都会变为1,这个结果显然已经破坏了原有的值. IO中的字符编码 Reader与Writer是Java中负责字符输入与输出的抽象基类,它们的子类实现了在各种场景中的字符输入输出功能. 在使用Reader与Writer进行IO操作时,需要指定字符集,如果不显式指定的话会默认使用当前环境的字符集,但我还是推荐显式指定一致的字符集,这样才不会出现乱码问题(Reader与Writer指定的字符集不一致或更改了环境导致字符集不一致等). 1234567891011121314151617181920212223242526272829303132333435363738394041424344public static void writeChar(String content, String filename, String charset) &#123; OutputStreamWriter writer = null; try &#123; FileOutputStream outputStream = new FileOutputStream(filename); writer = new OutputStreamWriter(outputStream, charset); writer.write(content); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (writer != null) writer.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;public static String readChar(String filename, String charset) &#123; InputStreamReader reader = null; StringBuilder sb = null; try &#123; FileInputStream inputStream = new FileInputStream(filename); reader = new InputStreamReader(inputStream, charset); char[] buf = new char[64]; int count = 0; sb = new StringBuilder(); while ((count = reader.read(buf)) != -1) sb.append(buf, 0, count); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (reader != null) reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return sb.toString();&#125; Web中的字符编码 在Web开发中,乱码也是经常存在的一个问题,主要体现在请求的参数和返回的响应结果,最头疼的是不同的浏览器的默认编码甚至还不一致. Java以Http的请求与响应抽象出了Request和Response两个对象,只要保持请求与响应的编码一致就能避免乱码问题. Request提供了setCharacterEncoding(String encode)函数来改变请求体的编码,一般通过写一个过滤器来统一对所有请求设置编码. 1request.setCharacterEncoding("UTF-8"); Response提供了setCharacterEncoding(String encode)与setHeader(String name,String value)两个函数,它们都可以设置响应的编码. 123response.setCharacterEncoding("UTF-8");// 设置响应头的编码信息,同时也告知了浏览器该如何解码response.setHeader("Content-Type","text/html;charset=UTF-8"); 还有一种更简便的方式,直接使用Spring提供的CharacterEncodingFilter,该过滤器就是用来统一编码的. 12345678910111213141516&lt;filter&gt; &lt;filter-name&gt;charsetFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;charsetFilter&lt;/filter-name&gt; &lt;url-pattern&gt;*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; CharacterEncodingFilter的实现如下: 1234567891011121314151617181920212223242526public class CharacterEncodingFilter extends OncePerRequestFilter &#123; private String encoding; private boolean forceEncoding = false; public CharacterEncodingFilter() &#123; &#125; public void setEncoding(String encoding) &#123; this.encoding = encoding; &#125; public void setForceEncoding(boolean forceEncoding) &#123; this.forceEncoding = forceEncoding; &#125; protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; if(this.encoding != null &amp;&amp; (this.forceEncoding || request.getCharacterEncoding() == null)) &#123; request.setCharacterEncoding(this.encoding); if(this.forceEncoding) &#123; response.setCharacterEncoding(this.encoding); &#125; &#125; filterChain.doFilter(request, response); &#125;&#125; 为什么Char在Java中占用两个字节? 众所周知,在Java中一个char类型占用两个字节,那么这是为什么呢?这是因为Java使用了UTF-16当作内码. 内码(Internal Encoding)就是程序内部所使用的编码,主要在于编程语言实现其char和String类型在内存中使用的内部编码.与之相对的就是外码(External Encoding),它是程序与外部交互时使用的字符编码. 值得一提的是,当初UTF-16是配合UCS-2使用的,后来Unicode支持的字符不断增多,UTF-16也不再只当作一个定长的2字节编码使用了,也就是说,Java中的一个char其实并不一定能代表一个完整的UTF-16字符. String.getBytes()可以将该String的内码转换为指定的外码并返回这个编完码的字节数组(无参数版使用当前平台的默认编码). 12345public static void main(String[] args) throws UnsupportedEncodingException &#123; String text = "码"; byte[] bytes = text.getBytes("UTF-8"); System.out.println(bytes.length); // 输出3&#125; Java还规定char与String类型的序列化是使用UTF-8当作外码的,Java中的Class文件中的字符串常量与符号名也都规定使用UTF-8.这种设计是为了平衡运行时的时间效率与外部存储的空间效率所做的取舍. 在SUN JDK6中,有一条命令-XX:+UseCompressedString.该命令可以让String内部存储字符内容可能用byte[]也可能用char[]: 当整个字符串所有字符处于ASCII字符集范围内时,就使用byte[](使用了ASCII编码)来存储,如果有任一字符超过了ASCII的范围,就退回到使用char[](UTF-16编码)来存储.但是这个功能实现的并不理想,所以没有包含在Open JDK6/Open JDK7/Oracle JDK7等后续版本中. JavaScript也使用了UTF-16作为内码,其实现也广泛应用了CompressedString的思想,主流的JavaScript引擎中都会尽可能使用ASCII内码的字符串,不过这些细节都是对外隐藏的.. 参考文献 ASCII - Wikipedia ISO/IEC 8859-1 - Wikipedia GBK - Wikipedia UTF-16 - Wikipedia UTF-8 - Wikipedia Java 语言中一个字符占几个字节？ - RednaxelaFX的回答]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>2017</tag>
        <tag>后端</tag>
        <tag>编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[B树的那点事儿]]></title>
    <url>%2F2017%2F08%2F13%2F2017-08-13-BTrees%2F</url>
    <content type="text"><![CDATA[概述 B树(B-Tree)是一种自平衡的树,能够保证数据有序.同时它还保证了在查找、插入、删除等操作时性能都能保持在$O(log\;n)$.需要注意的一点是,B-Tree并不是一棵自平衡的二叉查找树,它拥有多个分叉,且为大块数据的读写操作做了优化,同时它也可以用来描述外部存储(支持对保存在磁盘或者网络上的符号表进行外部查找). 在当今的互联网环境下,数据量已经大到无法想象,而能够在巨型数据集合中快速地进行查找操作是非常重要的,而B-Tree的神奇之处正在于: 只需要使用4~5个指向一小块数据的引用即可有效支持在数百亿甚至更多元素的符号表中进行查找和插入等操作. B-Tree的主要应用在于文件系统与数据库系统,例如Mysql中的InnoDB存储引擎就使用到了B-Tree来实现索引. 本文作者为: SylvanasSun.转载请务必将下面这段话置于文章开头处(保留超链接).本文转发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/08/13/2017-08-13-BTrees/ 数据表示 我们使用页来表示一块连续的数据,访问一页的数据需要将它读入本地内存.一个页可能是本地计算机上的一个文件,也可能是服务器上的某个文件的一部分等等.页的访问次数(无论读写)即是外部查找算法的成本模型. 首先,构造一棵B-Tree不会将数据保存在树中,而是会构造一棵由键的副本组成的树,每个副本都关联着一条链接.这种方法能够将索引与符号表进行分离,同时我们还需要遵循以下的规定: 选择一个参数M来构造一棵多向树(M一般为偶数),每个节点最多含有M - 1对键和链接. 每个节点最少含有M / 2对键和链接,根节点例外(它最少可以含有2对). .使用M阶的B-Tree来指定M的值,例如: 在一棵4阶B-Tree中,每个节点都含有至少2对至多3对. B-Tree含有两种不同类型的节点,内部节点与外部节点. 内部节点含有与页相关联的键的副本: 每个键都与一个节点相关联(一条链接),以此节点为根的子树中,所有的键都大于等于与此节点关联的键,但小于原内部节点中更大的键(如果存在的话). 外部节点含有指向实际数据的引用: 每个键都对应着实际的值,外部节点就是一张普通的符号表. 12345678910111213141516171819202122232425262728293031323334353637// max children per B-tree node = M - 1// must be even and greater than 2private static final int M = 4;// root of the B-treeprivate Node root;// height of the B-treeprivate int height;// number of key-value paris int the B-treeprivate int N;// B-tree node data typeprivate static final class Node &#123; private int children_length; private Entry[] children = new Entry[M]; // create a node with k children private Node(int k) &#123; children_length = k; &#125;&#125;// internal nodes : only use key and next// external nodes : only use key and valueprivate static class Entry &#123; private Comparable key; private final Object value; private Node next; private Entry(Comparable key, Object value, Node next) &#123; this.key = key; this.value = value; this.next = next; &#125;&#125; 查找 在B-Tree中进行查找操作每次都会结束于一个外部节点.在查找时,从根节点开始,根据被查找的键来选择当前节点中的适当区间并根据对应的链接从一个节点移动到下一层节点.最终,查找过程会到达树底的一个含有键的页(也就是外部节点),如果被查找的键在该页中,查找命中并结束,如果不在,则查找未命中. 12345678910111213141516171819202122232425262728293031public Value get(Key key) &#123; validateKey(key, "argument key to get() is null."); return search(root, key, height);&#125;private Value search(Node x, Key key, int height) &#123; while (x != null) &#123; Entry[] children = x.children; int children_length = x.children_length; // 当树的高度已经递减为0时,也就到达了树的底部(一个外部节点) // 遍历当前节点的每个键进行比较,如果找到则查找命中返回对应的值. if (height == 0) &#123; for (int j = 0; j &lt; children_length; j++) &#123; if (eq(key, children[j].key)) return (Value) children[j].value; &#125; &#125; else &#123; // 当还是内部节点时,根据键来查找适当的区间 for (int j = 0; j &lt; children_length; j++) &#123; if (j + 1 == children_length || less(key, children[j + 1].key)) &#123; // 找到适当的区间后,移动到下一层节点 x = children[j].next; height--; break; &#125; &#125; &#125; &#125; return null;&#125; 插入 插入操作也要先从根节点不断递归地查找到合适的区间,但需要注意一点,如果查找到的外部节点已经满了怎么办呢? 解决方法也很简单,我们允许被插入的节点暂时”溢出”,然后在递归调用自底向上不断地进行分裂.例如:当M为5时,根节点溢出为6-节点,只需要将它分裂为连接了两个3-节点的2-节点.即将一个M-的父节点k分裂为连接着两个(M / 2)-节点的(k + 1)-节点. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public void put(Key key, Value value) &#123; validateKey(key, "argument key to put() is null."); Node u = insert(root, key, value, height); N++; if (u == null) return; // need to split root Node t = new Node(2); t.children[0] = new Entry(root.children[0].key, null, root); t.children[1] = new Entry(u.children[0].key, null, u); root = t; height++;&#125;private Node insert(Node x, Key key, Value value, int height) &#123; int j; Entry t = new Entry(key, value, null); Entry[] children = x.children; int children_length = x.children_length; // external node if (height == 0) &#123; for (j = 0; j &lt; children_length; j++) &#123; if (less(key, children[j].key)) break; &#125; &#125; else &#123; // internal node for (j = 0; j &lt; children_length; j++) &#123; if (j + 1 == children_length || less(key, children[j + 1].key)) &#123; // 找到合适的区间后继续递归调用 Node u = insert(children[j++].next, key, value, height - 1); // 如果下一层没有进行过分裂操作,直接返回null if (u == null) return null; t.key = u.children[0].key; t.next = u; break; &#125; &#125; &#125; // 将j之后的元素全部右移(为了腾出j的插入位置) for (int i = children_length; i &gt; j; i--) &#123; children[i] = children[i - 1]; &#125; children[j] = t; x.children_length++; if (x.children_length &lt; M) return null; else return split(x); // 如果空间已满,进行分裂&#125; // 将x分裂为两个含有new_length对键的节点private Node split(Node x) &#123; int new_length = M / 2; Node t = new Node(new_length); x.children_length = new_length; for (int j = 0; j &lt; new_length; j++) t.children[j] = x.children[new_length + j]; return t;&#125; 参考文献 Algorithms, 4th Edition by Robert Sedgewick and Kevin Wayne B-tree - Wikipedia]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>Tree</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
        <tag>2017</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[揭秘HTTPS的"秘密"]]></title>
    <url>%2F2017%2F08%2F06%2F2017-08-06-DigestHttps%2F</url>
    <content type="text"><![CDATA[在说https之前,我们先了解一下http,以及为什么要使用https. http(Hyper Text Transfer Protocol)超文本传输协议是一种用于分布式、协作式和超媒体信息系统的应用层协议,它是TCP/IP的上层协议,同时它也是万维网(万维网不等同于互联网,它只是基于互联网的一个服务)的数据通信的基础. http协议是客户端浏览器与其他程序或Web服务器之间交互的应用层通讯协议.但它也有一个致命的缺点:http协议是明文传输协议,在传输信息的过程中并没有进行任何加密,通信的双方也没有任何的认证,这是非常不安全的,如果在通信过程中被中间人进行劫持、监听、篡改,会造成个人隐私泄露等严重的安全问题. 举一个现实中的例子来说,假设小李要给小张寄信,如果信件在运输的过程中没有任何安全保护,那么很可能会被邮递员(也就是中间人)窃取其中的内容,甚至于修改内容. https就是用于解决这样的安全问题的,它的全称为Hypertext Transfer Protocol Secure,它在http的基础上添加了SSL(安全套接字层)层来保证传输数据的安全问题. 本文作者为: SylvanasSun.转载请务必将下面这段话置于文章开头处(保留超链接).本文转发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/08/06/2017-08-06-DigestHttps/ https提供了端对端的加密,而且不仅对数据进行了加密,还对数据完整性提供了保护.不过在讲解https的加密方式之前,我们需要先了解一下加密算法. 对称加密 对称加密的基本思想是: 通信双方使用同一个密钥(或者是两个可以简单地互相推算的密钥)来对明文进行加密与解密. 常见的对称加密算法有DES、3DES、AES、Blowfish、IDEA、RC5、RC6. 对称加密看起来很美好,但是密钥要怎么发送过去呢?如果直接发送过去,被中间人截获了密钥岂不是白费工夫. 非对称加密 非对称加密也叫公开密钥加密,它使用了两个密钥,一个为公钥,一个为私钥,当一个用作于加密的时候,另一个则用作解密. 这两个密钥就算被其他人知道了其中一个也不能凭借它计算出另一个密钥,所以可以公开其中一个密钥(也就是公钥),不公开的密钥为私钥. 如果服务器想发送消息给客户端,只需要用客户端的公钥加密,然后客户端用它自己的私钥进行解密. 常见的非对称加密算法有RSA、DSA、ECDSA、 DH、ECDHE. 我们以DH算法为例,了解一下非对称加密的魅力. Alice要与Bob进行通信,他们协定了一组可以公开的质数$p=23$,$g=5$. Alice选择了一个不公开的秘密数$a=6$,并计算$A = {g^a} \; {mod} \; {p} = {5^6} \; {mod} \; {23} = 8$并发送给Bob. Bob选择了一个不公开的秘密数$b=15$,并计算$B = {g^b} \; {mod} \; {p} = {5^{15}} \; {mod} \; {23} = 19$并发送给Alice Alice 计算$S = {B^a} \; {mod} \; {p} = {19^6} \; {mod} \; {23} = 2$ Bob计算$S = {A^b} \; {mod} \; {p} = {8^{15}} \; {mod} \; {23} = 2$ Alice与Bob得到了同样的值,因此${g^{ab}} \; {mod} \; {p} = {g^{ba}} \; {mod} \; {p}$ 对称加密+非对称加密 尽管非对称加密如此奇妙,但它加解密的效率比对称加密要慢多了.那我们就将对称加密与非对称加密结合起来,取其精华,去其槽粕. 方法很简单,其中一方先自己生成一个对称加密密钥,然后通过非对称加密的方式来发送这个密钥,这样双方之后的通信就可以用对称加密这种高效率的算法进行加解密了. Certificate Authority 对称加密与非对称加密结合使用的方法虽然能够保证了通信过程的安全,但也引发了如下问题: 客户端要如何获取到服务器的公钥? 如果公钥在发送过程被中间人拦截,然后中间人发送自己的公钥给客户端,客户端该如何确认? 解决方法依是通过一个权威的CA(Certificate Authority)证书中心,它来负责颁发证书,这个证书包含了如下等内容: 证书的发布机构. 证书的有效期 公钥 证书所有人 数字签名 数字签名是用来验证数据完整性的,首先将公钥与个人信息用一个Hash算法生成一个消息摘要,Hash算法是不可逆的,且只要内容发生变化,那生成的消息摘要将会截然不同.然后CA再用它的私钥对消息摘要加密,最终形成数字签名. 当客户端接收到证书时,只需要用同样的Hash算法再次生成一个消息摘要,然后用CA的公钥对证书进行解密,之后再对比两个消息摘要就能知道数据有没有被篡改过了. 那么CA的公钥又要从哪里来呢?这似乎陷入了一个鸡生蛋,蛋生鸡的悖论,其实CA也有证书来证明自己,而且CA证书的信用体系就像一棵树的结构,上层节点是信用高的CA同时它也会对底层的CA做信用背书,操作系统中已经内置了一些根证书,所以相当于你已经自动信任了它们(需要注意误安装一些非法或不安全的证书). Https的交互过程 浏览器对服务器发送了一次请求. 服务器发送证书. 浏览器读取证书中的所有人,有效期等信息并进行校验. 浏览器查找操作系统中内置的已经信任的根证书,并对服务器发来的证书进行验证. 如果找不到,浏览器报错,服务器发来的证书是不可信任的. 如果找到,浏览器会从操作系统中取出CA的公钥,然后对服务器发来的证书中的数字签名进行解密. 浏览器使用相同的Hash算法计算出消息摘要,然后对数字签名中的消息摘要进行校对. 如果结果一致,证书合法. 之后浏览器就可以生成对称加密的密钥然后用非对称加密的方式发送给服务器,之后的通信就都是安全的了. 总结 现在国内外的大型网站基本都已经全站启用了Https,虽然相对于Http多了许多用于加密的流程,但为了数据的安全这点牺牲是必要的,Https也将是未来互联网的发展趋势. 参考文献 HTTPS - Wikipedia Public-key cryptography - Wikipedia Diffie–Hellman key exchange - Wikipedia 一个故事讲完https]]></content>
      <categories>
        <category>网络</category>
        <category>http</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>网络</tag>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的闭包之争]]></title>
    <url>%2F2017%2F07%2F30%2F2017-07-30-JavaClosure%2F</url>
    <content type="text"><![CDATA[闭包一直都是Java社区中争论不断的话题,很多语言例如JavaScript,Ruby,Python等都支持闭包这个语言特性,闭包功能强大且灵活,Java并没有显式地支持它,但其实Java中也存在着所谓的”闭包”. 本文作者为: SylvanasSun.转载请务必将下面这段话置于文章开头处(保留超链接).本文转发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/07/30/2017-07-30-JavaClosure/ 闭包 定义一个闭包的要点如下: 一个依赖于外部环境的自由变量的函数. 这个函数能够访问外部环境的自由变量. 也就是说,外部环境持有内部函数所依赖的自由变量,由此对内部函数形成了闭包. 自由变量 那么什么是自由变量呢?自由变量就是在函数自身作用域之外的变量,一个函数$f(x) = x + y$,其中y就是自由变量,它并不是这个函数自身的自变量,而是通过外部环境提供的. 下面以JavaScript的一个闭包为例: 12345function Add(y) &#123; return function(x) &#123; return x + y; &#125;&#125; 对于内部函数function(x)来说,y就是自由变量.而y是函数Add(y)内的参数,所以Add(y)对内部函数function(x)形成了一个闭包. 这个闭包将自由变量y与内部函数绑定在了一起,也就是说,当Add(y)函数执行完毕后,它不会随着函数调用结束后被回收(不能在栈上分配空间). 12var add_function = Add(5); // 这时y=5,并且与返回的内部函数绑定在了一起var result = add_function(10); // x=10,返回最终的结果 10 + 5 = 15 Java中的闭包 Java与JavaScript又或者其他支持闭包的语言不同,它是一个基于类的面向对象语言,也就是说一个方法所用到的自由变量永远都来自于其所在类的实例的. 1234567class AddUtils &#123; private int y = 5; public int add(int x) &#123; retrun x + y; &#125;&#125; 这样一个方法add(x)拥有一个参数x与一个自由变量y,它的返回值也依赖于这个自由变量y.add(x)想要正常工作的话,就必须依赖于AddUtils类的一个实例,不然它无法知道自由变量y的值是多少,也就是自由变量未与add(x)进行绑定. 严格上来说,add(x)中的自由变量应该为this,这是因为y也是通过this关键字来访问的. 所以说,在Java中闭包其实无处不在,只不过我们难以发现而已.但面向对象的语言一般都不把类叫成闭包,这是一种习惯. Java中的内部类就是一种典型的闭包结构. 123456789101112public class Outer &#123; private int y = 5; private class Inner &#123; private int x = 10; public int add() &#123; return x + y; &#125; &#125;&#125; 内部类通过一个指向外部类的引用来访问外部环境中的自由变量,由此形成了一个闭包. 匿名内部类 12345678910111213141516public interface AnonInner() &#123; int add();&#125;public class Outer &#123; public AnonInner getAnonInner(final int x) &#123; final int y = 5; return new AnonInner() &#123; public int add() &#123; return x + y; &#125; &#125; &#125;&#125; getAnonInner(x)方法返回了一个匿名内部类AnonInner,匿名内部类不能显式地声明构造函数,也不能对构造函数传参,且返回的是一个AnonInner接口,但它的add()方法实现中用到了两个自由变量(x与y),也就是说外部方法getAnonInner(x)对这个匿名内部类构成了闭包. 但我们发现自由变量都被加上了final修饰符,这是因为Java对闭包支持的不完整导致的. 对于自由变量的捕获策略有以下两种: capture-by-value: 只需要在创建闭包的地方把捕获的值拷贝一份到对象里即可.Java的匿名内部类和Java 8新的lambda表达式都是这样实现的. capture-by-reference: 把被捕获的局部变量“提升”（hoist）到对象里.C#的匿名函数(匿名委托/lambda表达式)就是这样实现的. Java只实现了capture-by-value,但又没有对外说明这一点,为了以后能进一步扩展成支持capture-by-reference留后路,所以干脆就不允许向被捕获的变量赋值,所以这些自由变量需要强制加上final修饰符(在Jdk8中似乎已经没有这种强制限制了). 参考文献 Java theory and practice: The closures debate 关于对象与闭包的关系的一个有趣小故事 JVM的规范中允许编程语言语义中创建闭包(closure)吗？ - 知乎 为什么Java闭包不能通过返回值之外的方式向外传递值？ - 知乎]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的那点事儿(4)-加权有向图]]></title>
    <url>%2F2017%2F07%2F27%2F2017-07-27-Graph_WeightedDigraph%2F</url>
    <content type="text"><![CDATA[本文作者为: SylvanasSun.转载请务必将下面这段话置于文章开头处(保留超链接).本文转发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/07/27/2017-07-27-Graph_WeightedDigraph 加权有向图 有向图的实现比无向图更加简单,要实现加权有向图只需要在上一章讲到的加权无向图的实现修改一下即可. DirectedEdge 由于有向图的边都是带有方向的,所以下面这个实现提供了from()与to()函数,用于获取代表v-&gt;w的两个顶点. 123456789101112131415161718192021222324252627282930313233343536373839404142public class DirectedEdge &#123; private final int v; private final int w; private final double weight; public DirectedEdge(int v, int w, double weight) &#123; validateVertexes(v, w); if (Double.isNaN(weight)) throw new IllegalArgumentException("Weight " + weight + " is NaN!"); this.v = v; this.w = w; this.weight = weight; &#125; public int from() &#123; return v; &#125; public int to() &#123; return w; &#125; public double weight() &#123; return weight; &#125; public String toString() &#123; return v + "-&gt;" + w + " " + String.format("%5.2f", weight); &#125; private void validateVertexes(int... vertexes) &#123; for (int i = 0; i &lt; vertexes.length; i++) &#123; if (vertexes[i] &lt; 0) throw new IllegalArgumentException("Vertex " + vertexes[i] + " must be positive number!"); &#125; &#125;&#125; EdgeWeightedDigraph 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113public class EdgeWeightedDigraph &#123; private static final String NEWLINE = System.getProperty("line.separator"); // number of vertices in this digraph private final int vertex; // number of edges in this digraph private int edge; // adj[v] = adjacency list for vertex v private Bag&lt;DirectedEdge&gt;[] adj; // indegree[v] = indegree of vertex v private int[] indegree; public EdgeWeightedDigraph(int vertex) &#123; String message = String.format("Vertex %d must be positive number!", vertex); validatePositiveNumber(message, vertex); this.vertex = vertex; this.edge = 0; this.indegree = new int[vertex]; this.adj = (Bag&lt;DirectedEdge&gt;[]) new Bag[vertex]; for (int v = 0; v &lt; vertex; v++) adj[v] = new Bag&lt;&gt;(); &#125; public EdgeWeightedDigraph(Scanner scanner) &#123; this(scanner.nextInt()); int edge = scanner.nextInt(); String message = String.format("Edge %d must be positive number!", edge); validatePositiveNumber(message, edge); for (int i = 0; i &lt; edge; i++) &#123; int v = scanner.nextInt(); int w = scanner.nextInt(); validateVertex(v); validateVertex(w); double weight = scanner.nextDouble(); addEdge(new DirectedEdge(v, w, weight)); &#125; &#125; public int vertex() &#123; return vertex; &#125; public int edge() &#123; return edge; &#125; public void addEdge(DirectedEdge e) &#123; int v = e.from(); int w = e.to(); validateVertex(v); validateVertex(w); adj[v].add(e); indegree[w]++; edge++; &#125; public Iterable&lt;DirectedEdge&gt; adj(int v) &#123; validateVertex(v); return adj[v]; &#125; public int outdegree(int v) &#123; validateVertex(v); return adj[v].size(); &#125; public int indegree(int v) &#123; validateVertex(v); return indegree[v]; &#125; // 在有向图中每条边只会出现一次 // 遍历边集不需要在无向图里那样为了消除重复边而进行复杂的判断 public Iterable&lt;DirectedEdge&gt; edges() &#123; Bag&lt;DirectedEdge&gt; list = new Bag&lt;DirectedEdge&gt;(); for (int v = 0; v &lt; vertex; v++) &#123; for (DirectedEdge e : adj(v)) &#123; list.add(e); &#125; &#125; return list; &#125; public String toString() &#123; StringBuilder s = new StringBuilder(); s.append(vertex + " " + edge + NEWLINE); for (int v = 0; v &lt; vertex; v++) &#123; s.append(v + ": "); for (DirectedEdge e : adj[v]) &#123; s.append(e + " "); &#125; s.append(NEWLINE); &#125; return s.toString(); &#125; private void validatePositiveNumber(String message, int... numbers) &#123; for (int i = 0; i &lt; numbers.length; i++) &#123; if (numbers[i] &lt; 0) throw new IllegalArgumentException(message); &#125; &#125;&#125; 加权有向图的实现与加权无向图区别不大,而且因为有向图中的边只会出现一次,实现代码要比无向图更简单. 本文中的所有完整代码请到我的GitHub中查看 最短路径 “找到一个顶点到达另一个顶点之间的最短路径“是图论研究中的经典算法问题.在加权有向图中,每条有向路径都有一个与之对应的路径权重(路径中所有边的权重之和),要找到一条最短路径其实就是找到路径权重最小的那条路径. 单点最短路径 “从s到目的地v是否存在一条有向路径,如果有,找出最短的那条路径”.类似这样的问题就是单点最短路径问题,它是我们主要研究的问题. 单点最短路径的结果是一棵最短路径树,它是图的一幅子图,包含了从起点到所有可达顶点的最短路径. 从起点到一个顶点可能存在两条长度相等的路径,如果出现这种情况,可以删除其中一条路径的最后一条边,直到从起点到每个顶点都只有一条路径相连. 最短路径的数据结构 要实现最短路径的算法还需要借助以下数据结构: edgeTo[]: 一个由顶点索引的DirectedEdge对象的父链接数组,其中edgeTo[v]的值为树中连接v和它的父节点的边. distTo[]: 一个由顶点索引的double数组,其中distTo[v]代表从起点到v的已知最短路径的长度. 初始化时,edgeTo[s]的值为null(s为起点),distTo[s]的值为0.0,从s到不可达的顶点距离为Double.POSITIVE_INFINITY. 让边松弛 最短路径算法都基于松弛(Relaxation)操作,它在遇到新的边时,通过更新这些信息就可以得到新的最短路径. 假设对边v-&gt;w进行松弛操作,意味着要先检查从s到w的最短路径是否是先从s到v,然后再由v到w(也就是说v-&gt;w是更短的一条路径),如果是,那么就进行更新.由v到达w的最短路径是distTo[v]与e.weight()之和,如果这个值大于distTo[w],称这条边松弛失败,并将它忽略. 松弛操作就像用一根橡皮筋沿着连续两个顶点的路径紧紧展开,放松一条边就像将这条橡皮筋转移到另一条更短的路径上,从而缓解橡皮筋的压力. 1234567891011121314151617181920// 松弛一条边private void relax(DirectedEdge e) &#123; int v = e.from(), w = e.to(); // 如果s-&gt;v-&gt;w的路径更小则进行更新 if (distTo[w] &gt; distTo[v] + e.weight()) &#123; distTo[w] = distTo[v] + e.weight(); edgeTo[w] = e; &#125;&#125;// 松弛一个顶点的所有邻接边private void relax(EdgeWeightedDigraph G, int v) &#123; for (DirectedEdge e : G.adj(v)) &#123; int w = e.to(); if (distTo[w] &gt; distTo[v] + e.weight()) &#123; distTo[w] = distTo[v] + e.weight(); edgeTo[w] = e; &#125; &#125;&#125; Dijkstra算法 Dijkstra算法类似于Prim算法,它将distTo[s]初始化为0.0,distTo[]中的其他元素初始化为Double.POSITIVE_INFINITY.然后将distTo[]中最小的非树顶点放松并加入树中,一直重复直到所有的顶点都在树中或者所有的非树顶点的distTo[]值均为Double.POSITIVE_INFINITY. Dijkstra算法与Prim算法都是用添加边的方式构造一棵树: Prim算法每次添加的是距离树最近的非树顶点. Dijkstra算法每次添加的都是离起点最近的非树顶点. 从上述的步骤我们就能看出,Dijkstra算法需要一个优先队列(也可以用斐波那契堆)来保存需要被放松的顶点并确认下一个被放松的顶点(也就是取出最小的). 如此简单的Dijkstra算法也有其缺点,那就是它只适用于解决权重非负的图. 实现代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public class DijkstraSP &#123; // distTo[v] = distance of shortest s -&gt; v path private double[] distTo; // edgeTo[v] = last edge on shortest s - &gt; v path private DirectedEdge[] edgeTo; // priority queue of vertices private IndexMinPQ&lt;Double&gt; pq; public DijkstraSP(EdgeWeightedDigraph digraph, int s) &#123; validateNegativeWeight(digraph); int vertex = digraph.vertex(); this.distTo = new double[vertex]; this.edgeTo = new DirectedEdge[vertex]; validateVertex(s); for (int v = 0; v &lt; vertex; v++) distTo[v] = Double.POSITIVE_INFINITY; distTo[s] = 0.0; // 将起点放入索引优先队列,并不断地进行松弛 pq = new IndexMinPQ&lt;&gt;(vertex); pq.insert(s, distTo[s]); while (!pq.isEmpty()) &#123; int v = pq.delMin(); // 对权值最小的非树顶点的所有邻接边集进行松弛操作 for (DirectedEdge e : digraph.adj(v)) relax(e); &#125; &#125; // relax edge e and update pq if changed private void relax(DirectedEdge e) &#123; int v = e.from(), w = e.to(); // s -&gt; v -&gt; w的权重 double weight = distTo[v] + e.weight(); if (distTo[w] &gt; weight) &#123; distTo[w] = weight; edgeTo[w] = e; if (pq.contains(w)) pq.decreaseKey(w, weight); else pq.insert(w, weight); &#125; &#125; private void validateNegativeWeight(EdgeWeightedDigraph digraph) &#123; for (DirectedEdge e : digraph.edges()) &#123; if (e.weight() &lt; 0) throw new IllegalArgumentException("Edge " + e + " has negative weight."); &#125; &#125; public double distTo(int v) &#123; validateVertex(v); return distTo[v]; &#125; public boolean hasPathTo(int v) &#123; validateVertex(v); return distTo[v] &lt; Double.POSITIVE_INFINITY; &#125; public Iterable&lt;DirectedEdge&gt; pathTo(int v) &#123; validateVertex(v); if (!hasPathTo(v)) return null; Stack&lt;DirectedEdge&gt; path = new Stack&lt;DirectedEdge&gt;(); for (DirectedEdge e = edgeTo[v]; e != null; e = edgeTo[e.from()]) &#123; path.push(e); &#125; return path; &#125; private void validateVertex(int v) &#123; int V = distTo.length; if (v &lt; 0 || v &gt;= V) throw new IllegalArgumentException("vertex " + v + " is not between 0 and " + (V - 1)); &#125;&#125; 上述的代码也可以用于处理加权无向图,但需要修改传入的对象类型.不管是无向图还是有向图它们对于最短路径问题是等价的. 无环加权有向图中的最短路径算法 如果是处理无环图的情况下,还会有一种比Dijkstra算法更快、更简单的算法.它的特点如下: 能够处理负权重的边. 能够在线性时间内解决单点最短路径问题. 在已知是一张无环图的情况下,它是找出最短路径效率最高的方法. 实现比Dijkstra算法更简单. 只需要将所有顶点按照拓扑排序的顺序来松弛边,就可以得到这个简单高效的算法. 12345678910111213141516171819202122232425262728293031323334353637383940public class AcyclicSP &#123; // distTo[v] = distance of shortest s-&gt;v path private double[] distTo; // edgeTo[v] = last edge on shortest s-&gt;v path private DirectedEdge[] edgeTo; public AcyclicSP(EdgeWeightedDigraph digraph, int s) &#123; int vertex = digraph.vertex(); distTo = new double[vertex]; edgeTo = new DirectedEdge[vertex]; validateVertex(s); for (int v = 0; v &lt; vertex; v++) distTo[v] = Double.POSITIVE_INFINITY; distTo[s] = 0.0; Topological topological = new Topological(digraph); if (!topological.hasOrder()) throw new IllegalArgumentException("Digraph is not acyclic."); // 按照拓扑排序的顺序进行放松操作 for (int v : topological.order()) &#123; for (DirectedEdge e : digraph.adj(v)) relax(e); &#125; &#125; private void relax(DirectedEdge e) &#123; int v = e.from(), w = e.to(); double weight = distTo[v] + e.weight(); if (distTo[w] &gt; weight) &#123; distTo[w] = weight; edgeTo[w] = e; &#125; &#125; &#125; 最长路径 要想找出一条最长路径,只需要把distTo[]的初始化变为Double.NEGATIVE_INFINITY,并更改relax()函数中的不等式的方向. 12345678910111213141516171819202122232425262728 public AcyclicLP(EdgeWeightedDigraph G, int s) &#123; distTo = new double[G.vertex()]; edgeTo = new DirectedEdge[G.vertex()]; validateVertex(s);// 全部初始化为负无穷 for (int v = 0; v &lt; G.vertex(); v++) distTo[v] = Double.NEGATIVE_INFINITY; distTo[s] = 0.0; Topological topological = new Topological(G); if (!topological.hasOrder()) throw new IllegalArgumentException("Digraph is not acyclic."); for (int v : topological.order()) &#123; for (DirectedEdge e : G.adj(v)) relax(e); &#125; &#125; private void relax(DirectedEdge e) &#123; int v = e.from(), w = e.to();// 改变不等式的方向 if (distTo[w] &lt; distTo[v] + e.weight()) &#123; distTo[w] = distTo[v] + e.weight(); edgeTo[w] = e; &#125; &#125; Bellman-Ford算法 我们已经知道了处理权重非负图的Dijkstra算法与处理无环图的算法,但如果遇见既含有环,权重也是负数的加权有向图该怎么办? Bellman-Ford算法就是用于处理有环且含有负权重的加权有向图的,它的原理是对图进行V-1次松弛操作,得到所有可能的最短路径. 要实现Bellman-Ford算法还需要以下数据结构: 队列: 用于保存即将被松弛的顶点. 布尔值数组: 用来标记该顶点是否已经存在于队列中,以防止重复插入. 我们将起点放入队列中,然后进入一个循环,每次循环都会从队列中取出一个顶点并对其进行松弛.为了保证算法在V轮后能够终止,需要能够动态地检测是否存在负权重环,如果找到了这个环则结束运行(也可以用一个变量动态记录轮数). 负权重环的检测 如果存在了一个从起点可达的负权重环,那么队列就永远不可能为空,为了从这个无尽的循环中解脱出来,算法需要能够动态地检测负权重环. Bellman-Ford算法也使用了edgeTo[]来存放最短路径树中的每一条边,我们根据edgeTo[]来复制一幅图并在该图中检测环. 1234567891011 private void findNegativeCycle() &#123; int V = edgeTo.length;// 根据edgeTo[]来创建一幅加权有向图 EdgeWeightedDigraph spt = new EdgeWeightedDigraph(V); for (int v = 0; v &lt; V; v++) if (edgeTo[v] != null) spt.addEdge(edgeTo[v]);// 判断该图有没有环 EdgeWeightedDirectedCycle finder = new EdgeWeightedDirectedCycle(spt); cycle = finder.cycle(); &#125; 实现代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class BellmanFordSP &#123; private double[] distTo; private DirectedEdge[] edgeTo; // 用于标记顶点是否在队列中 private boolean[] onQueue; // 存放下次进行松弛操作的顶点的队列 private Queue&lt;Integer&gt; queue; // 计算松弛操作的轮数 private int cost; // 负权重环 private Iterable&lt;DirectedEdge&gt; cycle; public BellmanFordSP(EdgeWeightedDigraph digraph, int s) &#123; int vertex = digraph.vertex(); this.distTo = new double[vertex]; this.edgeTo = new DirectedEdge[vertex]; this.onQueue = new boolean[vertex]; for (int v = 0; v &lt; vertex; v++) distTo[v] = Double.POSITIVE_INFINITY; distTo[s] = 0.0; // Bellman-Ford algorithm queue = new ArrayDeque&lt;&gt;(); queue.add(s); // 将起点放入队列 onQueue[s] = true; // 标记起点已在队列中 // 当队列为空时或者发现负权重环时结束循环 while (!queue.isEmpty() &amp;&amp; !hasNegativeCycle()) &#123; int v = queue.poll(); onQueue[v] = false; relax(digraph, v); &#125; &#125; private void relax(EdgeWeightedDigraph G, int v) &#123; for (DirectedEdge e : G.adj(v)) &#123; int w = e.to(); double weight = distTo[v] + e.weight(); if (distTo[w] &gt; weight) &#123; distTo[w] = weight; edgeTo[w] = e; // 将不在队列中的顶点w加到队列 if (!onQueue[w]) &#123; queue.add(w); onQueue[w] = true; &#125; &#125; // 动态检测负权重环, if (cost++ % G.vertex() == 0) &#123; findNegativeCycle(); if (hasNegativeCycle()) return; // found a negative cycle &#125; &#125; &#125;&#125; 总结 解决最短路径问题一直都是图论的经典问题,本文中介绍的算法适用于不同的环境,在应用中应该根据不同的环境选择不同的算法. 算法 局限性 路径长度的比较次数(增长的数量级) 空间复杂度 优势 Dijkstra 只能处理正权重 ElogV V 最坏情况下仍有较好的性能 拓扑排序 只适用于无环图 E+V V 实现简单,是无环图情况下的最优算法 Bellman-Ford 不能存在负权重环 E+V,最坏情况为VE V 适用广泛 参考文献 Algorithms, 4th Edition by Robert Sedgewick and Kevin Wayne Dijkstra’s algorithm - Wikipedia Bellman–Ford algorithm - Wikipedia 图的那点事儿 图的那点事儿(1)-无向图 图的那点事儿(2)-有向图 图的那点事儿(3)-加权无向图 图的那点事儿(4)-加权有向图]]></content>
      <categories>
        <category>Algorithms</category>
        <category>Graph</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>2017</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的那点事儿(3)-加权无向图]]></title>
    <url>%2F2017%2F07%2F25%2F2017-07-25-Graph_WeightedUndirectedGraph%2F</url>
    <content type="text"><![CDATA[本文作者为: SylvanasSun.转载请务必将下面这段话置于文章开头处(保留超链接).本文转发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/07/25/2017-07-25-Graph_WeightedUndirectedGraph/ 加权无向图 所谓加权图,即每条边上都有着对应的权重,这个权重是正数也可以是负数,也不一定会和距离成正比.加权无向图的表示方法只需要对无向图的实现进行一下扩展. 在使用邻接矩阵的方法中,可以用边的权重代替布尔值来作为矩阵的元素. 在使用邻接表 的方法中,可以在链表的节点中添加一个权重域. 在使用邻接表的方法中,将边抽象为一个Edge类,它包含了相连的两个顶点和它们的权重,链表中的每个元素都是一个Edge. 我们使用第三种方法来实现加权无向图,它的数据表示如下图: Edge的实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class Edge implements Comparable&lt;Edge&gt; &#123; private final int v; private final int w; private final double weight; public Edge(int v, int w, double weight) &#123; validateVertexes(v, w); if (Double.isNaN(weight)) throw new IllegalArgumentException("Weight is NaN."); this.v = v; this.w = w; this.weight = weight; &#125; private void validateVertexes(int... vertexes) &#123; for (int i = 0; i &lt; vertexes.length; i++) &#123; if (vertexes[i] &lt; 0) &#123; throw new IllegalArgumentException( String.format("Vertex %d must be a nonnegative integer.", vertexes[i])); &#125; &#125; &#125; public double weight() &#123; return weight; &#125; public int either() &#123; return v; &#125; public int other(int vertex) &#123; if (vertex == v) return w; else if (vertex == w) return v; else throw new IllegalArgumentException("Illegal endpoint."); &#125; @Override public int compareTo(Edge that) &#123; return Double.compare(this.weight, that.weight); &#125; @Override public String toString() &#123; return String.format("%d-%d %.5f", v, w, weight); &#125;&#125; Edge类提供了either()与other()两个函数,在两个顶点都未知的情况下,可以调用either()获得顶点v,然后再调用other(v)来获得另一个顶点. 本文中的所有完整代码点我查看 加权无向图的实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116public class EdgeWeightedGraph &#123; private static final String NEWLINE = System.getProperty("line.separator"); private final int vertexes; private int edges; private Bag&lt;Edge&gt;[] adj; public EdgeWeightedGraph(int vertexes) &#123; validateVertexes(vertexes); this.vertexes = vertexes; this.edges = 0; adj = (Bag&lt;Edge&gt;[]) new Bag[vertexes]; for (int i = 0; i &lt; vertexes; i++) adj[i] = new Bag&lt;&gt;(); &#125; public EdgeWeightedGraph(Scanner scanner) &#123; this(scanner.nextInt()); int edges = scanner.nextInt(); validateEdges(edges); for (int i = 0; i &lt; edges; i++) &#123; int v = scanner.nextInt(); int w = scanner.nextInt(); double weight = scanner.nextDouble(); addEdge(new Edge(v, w, weight)); &#125; &#125; public int vertex() &#123; return vertexes; &#125; public int edge() &#123; return edges; &#125; public void addEdge(Edge e) &#123; int v = e.either(); int w = e.other(v); validateVertex(v); validateVertex(w); adj[v].add(e); adj[w].add(e); edges++; &#125; public Iterable&lt;Edge&gt; adj(int v) &#123; validateVertex(v); return adj[v]; &#125; public int degree(int v) &#123; validateVertex(v); return adj[v].size(); &#125; public Iterable&lt;Edge&gt; edges() &#123; Bag&lt;Edge&gt; list = new Bag&lt;Edge&gt;(); for (int v = 0; v &lt; vertexes; v++) &#123; int selfLoops = 0; for (Edge e : adj(v)) &#123; // 只添加一条边 if (e.other(v) &gt; v) &#123; list.add(e); &#125; // 只添加一条自环的边 else if (e.other(v) == v) &#123; if (selfLoops % 2 == 0) list.add(e); selfLoops++; &#125; &#125; &#125; return list; &#125; public String toString() &#123; StringBuilder s = new StringBuilder(); s.append(vertexes + " " + edges + NEWLINE); for (int v = 0; v &lt; vertexes; v++) &#123; s.append(v + ": "); for (Edge e : adj[v]) &#123; s.append(e + " "); &#125; s.append(NEWLINE); &#125; return s.toString(); &#125; private void validateVertexes(int... vertexes) &#123; for (int i = 0; i &lt; vertexes.length; i++) &#123; if (vertexes[i] &lt; 0) &#123; throw new IllegalArgumentException( String.format("Vertex %d must be a nonnegative integer.", vertexes[i])); &#125; &#125; &#125; private void validateEdges(int edges) &#123; if (edges &lt; 0) throw new IllegalArgumentException("Number of edges must be nonnegative."); &#125; // throw an IllegalArgumentException unless &#123;@code 0 &lt;= v &lt; V&#125; private void validateVertex(int v) &#123; if (v &lt; 0 || v &gt;= vertexes) throw new IllegalArgumentException("vertex " + v + " is not between 0 and " + (vertexes - 1)); &#125;&#125; 上述代码是对无向图的扩展,它将邻接表中的元素从整数变为了Edge,函数edges()返回了边的集合,由于是无向图所以每条边会出现两次,需要注意处理. 加权无向图的实现还拥有以下特点: 边的比较: Edge类实现了Comparable接口,它使用了权重来比较两条边的大小,所以加权无向图的自然次序就是权重次序. 自环: 该实现允许存在自环,并且edges()函数中对自环边进行了记录. 平行边: 该实现允许存在平行边,但可以用更复杂的方法来消除平行边,例如只保留平行边中的权重最小者. 最小生成树 最小生成树是加权无向图的重要应用.图的生成树是它的一棵含有其所有顶点的无环连通子图,最小生成树是它的一棵权值(所有边的权值之和)最小的生成树. 在给定的一幅加权无向图$G = (V,E)$中,$(u,v)$代表连接顶点u与顶点v的边,也就是$(u,v) \in E$,而$w(u,v)$代表这条边的权重,若存在T为E的子集,也就是$T \subseteq E$,且为无环图,使得$w(T) = \sum_{(u,v) \in T}w(u,v)$ 的 $w(T)$ 最小,则T为G的最小生成树. 最小生成树在一些情况下可能会存在多个,例如,给定一幅图G,当它的所有边的权重都相同时,那么G的所有生成树都是最小生成树,当所有边的权重互不相同时,将会只有一个最小生成树. 切分定理 切分定理将图中的所有顶点切分为两个集合(两个非空且不重叠的集合),检查两个集合的所有边并识别哪条边应属于图的最小生成树. 一种比较简单的切分方法即通过指定一个顶点集并隐式地认为它的补集为另一个顶点集来指定一个切分. 切分定理也表明了对于每一种切分,权重最小的横切边(一条连接两个属于不同集合的顶点的边)必然属于最小生成树. 切分定理是解决最小生成树问题的所有算法的基础,使用切分定理找到最小生成树的一条边,不断重复直到找到最小生成树的所有边. 这些算法可以说都是贪心算法,算法的每一步都是在找最优解(权值最小的横切边),而解决最小生成树的各种算法不同之处仅在于保存切分和判定权重最小的横切边的方式. Prim算法 Prim算法是用于解决最小生成树的算法之一,算法的每一步都会为一棵生长中的树添加一条边.一开始这棵树只有一个顶点,然后会一直添加到$V - 1$条边,每次总是将下一条连接树中的顶点与不在树中的顶点且权重最小的边加入到树中(也就是由树中顶点所定义的切分中的一条横切边). 实现Prim算法还需要借助以下数据结构: 布尔值数组: 用于记录顶点是否已在树中. 队列: 使用一条队列来保存最小生成树中的边,也可以使用一个由顶点索引的Edge对象的数组. 优先队列: 优先队列用于保存横切边,优先队列的性质可以每次取出权值最小的横切边. 延时实现 当我们连接新加入树中的顶点与其他已经在树中顶点的所有边都失效了(由于两个顶点都已在树中,所以这是一条失效的横切边).我们需要处理这种情况,即使实现对无效边采取忽略(不加入到优先队列中),而延时实现会把无效边留在优先队列中,等到要删除优先队列中的数据时再进行有效性检查. 上图为Prim算法延时实现的轨迹图,它的步骤如下: 将顶点0添加到最小生成树中,将它的邻接表中的所有边添加到优先队列中(将横切边添加到优先队列). 将顶点7和边0-7添加到最小生成树中,将顶点的邻接表中的所有边添加到优先队列中. 将顶点1和边1-7添加到最小生成树中,将顶点的邻接表中的所有边添加到优先队列中. 将顶点2和边0-2添加到最小生成树中,将边2-3和6-2添加到优先队列中,边2-7和1-2失效. 将顶点3和边2-3添加到最小生成树中,将边3-6添加到优先队列之中,边1-3失效. 将顶点5和边5-7添加到最小生成树中,将边4-5添加到优先队列中,边1-5失效. 从优先队列中删除失效边1-3,1-5,2-7. 将顶点4和边4-5添加到最小生成树中,将边6-4添加到优先队列中,边4-7,0-4失效. 从优先队列中删除失效边1-2,4-7,0-4. 将顶点6和边6-2添加到最小生成树中,和顶点6关联的其他边失效. 在添加V个顶点与V - 1条边之后,最小生成树就构造完成了,优先队列中剩余的边都为失效边. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class LazyPrimMST &#123; private final EdgeWeightedGraph graph; // 记录最小生成树的总权重 private double weight; // 存储最小生成树的边 private final Queue&lt;Edge&gt; mst; // 标记这个顶点在树中 private final boolean[] marked; // 存储横切边的优先队列 private final PriorityQueue&lt;Edge&gt; pq; public LazyPrimMST(EdgeWeightedGraph graph) &#123; this.graph = graph; int vertex = graph.vertex(); mst = new ArrayDeque&lt;&gt;(); pq = new PriorityQueue&lt;&gt;(); marked = new boolean[vertex]; for (int v = 0; v &lt; vertex; v++) if (!marked[v]) prim(v); &#125; private void prim(int s) &#123; scanAndPushPQ(s); while (!pq.isEmpty()) &#123; Edge edge = pq.poll(); // 取出权重最小的横切边 int v = edge.either(), w = edge.other(v); assert marked[v] || marked[w]; if (marked[v] &amp;&amp; marked[w]) continue; // 忽略失效边 mst.add(edge); // 添加边到最小生成树中 weight += edge.weight(); // 更新总权重 // 继续将非树顶点加入到树中并更新横切边 if (!marked[v]) scanAndPushPQ(v); if (!marked[w]) scanAndPushPQ(w); &#125; &#125; // 标记顶点到树中,并且添加横切边到优先队列 private void scanAndPushPQ(int v) &#123; assert !marked[v]; marked[v] = true; for (Edge e : graph.adj(v)) if (!marked[e.other(v)]) pq.add(e); &#125; public Iterable&lt;Edge&gt; edges() &#123; return mst; &#125; public double weight() &#123; return weight; &#125;&#125; 即时实现 在即时实现中,将v添加到树中时,对于每个非树顶点w,不需要在优先队列中保存所有从w到树顶点的边,而只需要保存其中权重最小的边,所以在将v添加到树中后,要检查是否需要更新这条权重最小的边(如果v-w的权重更小的话). 也可以认为只会在优先队列中保存每个非树顶点w的一条边(也是权重最小的那条边),将w和树顶点连接起来的其他权重较大的边迟早都会失效,所以没必要在优先队列中保存它们. 要实现即时版的Prim算法,需要使用两个顶点索引的数组edgeTo[]和distTo[]与一个索引优先队列,它们具有以下性质: 如果顶点v不在树中但至少含有一条边和树相连,那么edgeTo[v]是将v和树连接的最短边,distTo[v]为这条边的权重. 所有这类顶点v都保存在索引优先队列中,索引v关联的值是edgeTo[v]的边的权重. 索引优先队列中的最小键即是权重最小的横切边的权重,而和它相关联的顶点v就是下一个将要被添加到树中的顶点. 将顶点0添加到最小生成树之中,将它的邻接表中的所有边添加到优先队列中(这些边是目前唯一已知的横切边). 将顶点7和边0-7添加到最小生成树,将边1-7和5-7添加到优先队列中,将连接顶点4与树的最小边由0-4替换为4-7. 将顶点1和边1-7添加到最小生成树,将边1-3添加到优先队列. 将顶点2和边0-2添加到最小生成树,将连接顶点6与树的最小边由0-6替换为6-2,将连接顶点3与树的最小边由1-3替换为2-3. 将顶点3和边2-3添加到最小生成树. 将顶点5和边5-7添加到最小生成树,将连接顶点4与树的最小边4-7替换为4-5. 将顶点4和边4-5添加到最小生成树. 将顶点6和边6-2添加到最小生成树. 在添加了V - 1条边之后,最小生成树构造完成并且优先队列为空. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class PrimMST &#123; private final EdgeWeightedGraph graph; // 存放最小生成树中的边 private final Edge[] edgeTo; // 每条边对应的权重 private final double[] distTo; private final boolean[] marked; private final IndexMinPQ&lt;Double&gt; pq; public PrimMST(EdgeWeightedGraph graph) &#123; this.graph = graph; int vertex = graph.vertex(); this.edgeTo = new Edge[vertex]; this.marked = new boolean[vertex]; this.pq = new IndexMinPQ&lt;&gt;(vertex); this.distTo = new double[vertex]; // 将权重数组初始化为无穷大 for (int i = 0; i &lt; vertex; i++) distTo[i] = Double.POSITIVE_INFINITY; for (int v = 0; v &lt; vertex; v++) if (!marked[v]) prim(v); &#125; private void prim(int s) &#123; // 将起点设为0.0并加入到优先队列 distTo[s] = 0.0; pq.insert(s, distTo[s]); while (!pq.isEmpty()) &#123; // 取出权重最小的边,优先队列中存的顶点是与树相连的非树顶点, // 同时它也是下一次要加入到树中的顶点 int v = pq.delMin(); scan(v); &#125; &#125; private void scan(int v) &#123; // 将顶点加入到树中 marked[v] = true; for (Edge e : graph.adj(v)) &#123; int w = e.other(v); // 忽略失效边 if (marked[w]) continue; // 如果w与连接树顶点的边的权重小于其他w连接树顶点的边 // 则进行替换更新 if (e.weight() &lt; distTo[w]) &#123; distTo[w] = e.weight(); edgeTo[w] = e; if (pq.contains(w)) pq.decreaseKey(w, distTo[w]); else pq.insert(w, distTo[w]); &#125; &#125; &#125; public Iterable&lt;Edge&gt; edges() &#123; Queue&lt;Edge&gt; mst = new ArrayDeque&lt;&gt;(); for (int v = 0; v &lt; edgeTo.length; v++) &#123; Edge e = edgeTo[v]; if (e != null) &#123; mst.add(e); &#125; &#125; return mst; &#125; public double weight() &#123; double weight = 0.0; for (Edge e : edges()) weight += e.weight(); return weight; &#125;&#125; 不管是延迟实现还是即时实现,Prim算法的规律就是: 在树的生长过程中,都是通过连接一个和新加入的顶点相邻的顶点.当新加入的顶点周围没有非树顶点时,树的生长又会从另一部分开始. Kruskal算法 Kruskal算法的思想是按照边的权重顺序由小到大处理它们,将边添加到最小生成树,加入的边不会与已经在树中的边构成环,直到树中含有V - 1条边为止.这些边会逐渐由一片森林合并为一棵树,也就是我们需要的最小生成树. 与Prim算法的区别 Prim算法是一条边一条边地来构造最小生成树,每一步都会为树中添加一条边. Kruskal算法构造最小生成树也是一条边一条边地添加,但不同的是它寻找的边会连接一片森林中的两棵树.从一片由V棵单顶点的树构成的森林开始并不断地将两棵树合并(可以找到的最短边)直到只剩下一棵树,它就是最小生成树. 实现 要实现Kruskal算法需要借助Union-Find数据结构,它是一种树型的数据结构,用于处理一些不相交集合的合并与查询问题. 关于Union-Find的更多资料可以参考下面的链接: Union-Find简单实现 Disjoint-set data structure - Wikipedia 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class KruskalMST &#123; // 这条队列用于记录最小生成树中的边集 private final Queue&lt;Edge&gt; mst; private double weight; public KruskalMST(EdgeWeightedGraph graph) &#123; this.mst = new ArrayDeque&lt;&gt;(); // 创建一个优先队列,并将图的所有边添加到优先队列中 PriorityQueue&lt;Edge&gt; pq = new PriorityQueue&lt;&gt;(); for (Edge e : graph.edges()) &#123; pq.add(e); &#125; int vertex = graph.vertex(); // 创建一个Union-Find UF uf = new UF(vertex); // 一条一条地添加边到最小生成树,直到添加了 V - 1条边 while (!pq.isEmpty() &amp;&amp; mst.size() &lt; vertex - 1) &#123; // 取出权重最小的边 Edge e = pq.poll(); int v = e.either(); int w = e.other(v); // 如果这条边的两个顶点不在一个分量中(对于union-find数据结构中而言) if (!uf.connected(v, w)) &#123; // 将v和w归并(对于union-find数据结构中而言),然后将边添加进树中,并计算更新权重 uf.union(v, w); mst.add(e); weight += e.weight(); &#125; &#125; &#125; public Iterable&lt;Edge&gt; edges() &#123; return mst; &#125; public double weight() &#123; return weight; &#125;&#125; 上面代码实现的Kruskal算法使用了一条队列来保存最小生成树的边集,一条优先队列来保存还未检查的边,一个Union-Find来判断失效边. 性能比较 算法 空间复杂度 时间复杂度 Prim(延时) E ElogE Prim(即时) V ElogV Kruskal E ElogE 参考文献 Minimum spanning tree - Wikipedia Algorithms, 4th Edition by Robert Sedgewick and Kevin Wayne 图的那点事儿 图的那点事儿(1)-无向图 图的那点事儿(2)-有向图 图的那点事儿(3)-加权无向图 图的那点事儿(4)-加权有向图]]></content>
      <categories>
        <category>Algorithms</category>
        <category>Graph</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>2017</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的那点事儿(2)-有向图]]></title>
    <url>%2F2017%2F07%2F23%2F2017-07-23-Graph_DirectedGraphs%2F</url>
    <content type="text"><![CDATA[本文作者为: SylvanasSun.转载请务必将下面这段话置于文章开头处(保留超链接).本文转发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/07/23/2017-07-23-Graph_DirectedGraphs/ 有向图的性质 有向图与无向图不同,它的边是单向的,每条边所连接的两个顶点都是一个有序对,它们的邻接性是单向的. 在有向图中,一条有向边由第一个顶点指出并指向第二个顶点,一个顶点的出度为由该顶点指出的边的总数;一个顶点的入度为指向该顶点的边的总数. v-&gt;w表示一条由v指向w的边,在一幅有向图中,两个顶点的关系可能有以下四种(特殊图除外): 没有边相连. 存在一条从v到w的边: v-&gt;w. 存在一条从w到v的边: w-&gt;v. 既存在v-&gt;w,也存在w-&gt;v,也就是一条双向边. 当存在从v到w的有向路径时,称顶点w能够由顶点v达到.但在有向图中,由v能够到达w并不意味着由w也能到达v(但每个顶点都是能够到达它自己的). 有向图的实现 有向图的实现与无向图差不多,只不过在边的方向上有所不同.(本文中的所有完整代码可以在我的GitHub中查看) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159public class Digraph implements Graph &#123; private static final String NEWLINE = System.getProperty("line.separator"); // number of vertices in this digraph private final int vertex; // number of edges in this digraph private int edge; // adj[v] = adjacency list for vertex v private Bag&lt;Integer&gt;[] adj; // indegree[v] = indegree of vertex v private int[] indegree; public Digraph(int vertex) &#123; validateVertex(vertex); this.vertex = vertex; this.edge = 0; this.indegree = new int[vertex]; this.adj = (Bag&lt;Integer&gt;[]) new Bag[vertex]; for (int i = 0; i &lt; vertex; i++) adj[i] = new Bag&lt;Integer&gt;(); &#125; public Digraph(Scanner scanner) &#123; if (scanner == null) throw new IllegalArgumentException("Scanner must be not null."); try &#123; int vertex = scanner.nextInt(); validateVertex(vertex); this.vertex = vertex; this.indegree = new int[vertex]; this.adj = (Bag&lt;Integer&gt;[]) new Bag[vertex]; for (int i = 0; i &lt; vertex; i++) adj[i] = new Bag&lt;Integer&gt;(); int edge = scanner.nextInt(); validateEdge(edge); for (int i = 0; i &lt; edge; i++) &#123; int v = scanner.nextInt(); int w = scanner.nextInt(); addEdge(v, w); &#125; &#125; catch (NoSuchElementException e) &#123; throw new IllegalArgumentException("Invalid input format in Digraph constructor", e); &#125; &#125; public Digraph(Digraph digraph) &#123; this(digraph.vertex); this.edge = digraph.edge; for (int v = 0; v &lt; vertex; v++) this.indegree[v] = digraph.indegree(v); for (int v = 0; v &lt; vertex; v++) &#123; Stack&lt;Integer&gt; reverse = new Stack&lt;&gt;(); for (int w : digraph.adj(v)) reverse.push(w); for (int w : reverse) this.adj[v].add(w); &#125; &#125; @Override public int vertex() &#123; return vertex; &#125; @Override public int edge() &#123; return edge; &#125; /** * 注意这里与无向图不同,只在v的邻接表中添加了w */ @Override public void addEdge(int v, int w) &#123; validateVertex(v); validateVertex(w); adj[v].add(w); // w的入度+ 1 indegree[w]++; edge++; &#125; @Override public Iterable&lt;Integer&gt; adj(int v) &#123; validateVertex(v); return adj[v]; &#125; public int indegree(int v) &#123; validateVertex(v); return indegree[v]; &#125; /** * v的出度就是它邻接表中的顶点数 */ public int outdegree(int v) &#123; validateVertex(v); return adj[v].size(); &#125; @Override @Deprecated public int degree(int v) &#123; validateVertex(v); return adj[v].size(); &#125; /** * 它返回该有向图的一个副本,但所有边的方向都会被反转. */ public Digraph reverse() &#123; Digraph reverse = new Digraph(vertex); for (int v = 0; v &lt; vertex; v++) &#123; for (int w : adj[v]) &#123; reverse.addEdge(w, v); &#125; &#125; return reverse; &#125; @Override public String toString() &#123; StringBuilder sb = new StringBuilder(); sb.append(String.format("Vertexes: %s, Edges: %s", vertex, edge)); sb.append(NEWLINE); for (int v = 0; v &lt; vertex; v++) &#123; sb.append(String.format("vertex %d, ", v)); sb.append(String.format("indegree: %d, outdegree: %d", indegree(v), outdegree(v))); sb.append(NEWLINE); sb.append("adjacent point: "); for (int w : adj[v]) sb.append(w).append(" "); sb.append(NEWLINE); &#125; return sb.toString(); &#125; private void validateEdge(int edge) &#123; if (edge &lt; 0) throw new IllegalArgumentException("Number of edges in a Digraph must be nonnegative."); &#125; private void validateVertex(int vertex) &#123; if (vertex &lt; 0) throw new IllegalArgumentException("Number of vertex in a Digraph must be nonnegative."); &#125;&#125; 可达性 对于”是否存在一条从集合中的任意顶点到达给定顶点v的有向路径?”等类似问题,可以使用深度优先搜索或广度优先搜索(与无向图的实现一致,只不过传入的图的类型不同),有向图生成的搜索轨迹甚至要比无向图还要简单. 对于可达性分析的一个典型应用就是内存管理系统.例如,JVM使用多点可达性分析的方法来判断一个对象是否可以进行回收: 所有对象组成一幅有向图,其中有多个Root顶点(它是由JVM自己决定的)作为起点,如果一个对象从Root顶点不可达,那么这个对象就可以进行回收了. 环 在与有向图相关的实际应用中,有向环特别的重要.我们需要知道一幅有向图中是否包含有向环.在任务调度问题或其他许多问题中会不允许存在有向环,所以对于环的检测是很重要的. 使用深度优先搜索解决这个问题并不困难,递归调用隐式使用的栈表示的正是”当前”正在遍历的有向路径,一旦找到了一条边v-&gt;w且w已经存在于栈中,就等于找到了一个环(栈表示的是一条由w到v的有向路径,而v-&gt;w正好补全了这个环). 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class DirectedCycle &#123; private final Digraph digraph; // marked[v] = has vertex v been marked? private final boolean[] marked; // edgeTo[v] = previous vertex on path to v private final int[] edgeTo; // onStack[v] = is vertex on the stack? private final boolean[] onStack; // directed cycle (or null if no such cycle) private Stack&lt;Integer&gt; cycle; public DirectedCycle(Digraph digraph) &#123; this.digraph = digraph; int vertex = digraph.vertex(); this.marked = new boolean[vertex]; this.edgeTo = new int[vertex]; this.onStack = new boolean[vertex]; for (int v = 0; v &lt; vertex; v++) &#123; // 已经找到环时就不再需要继续搜索了 if (!marked[v] &amp;&amp; cycle == null) dfs(v); &#125; &#125; public boolean hasCycle() &#123; return cycle != null; &#125; public Iterable&lt;Integer&gt; cycle() &#123; return cycle; &#125; private void dfs(int vertex) &#123; marked[vertex] = true; onStack[vertex] = true; // 用于模拟递归调用栈 for (int w : digraph.adj(vertex)) &#123; if (cycle != null) return; else if (!marked[w]) &#123; edgeTo[w] = vertex; dfs(w); &#125; else if (onStack[w]) &#123; // 当w已被标记且在栈中时: 找到环 cycle = new Stack&lt;&gt;(); for (int x = vertex; x != w; x = edgeTo[x]) cycle.push(x); cycle.push(w); cycle.push(vertex); assert check(); &#125; &#125; // 这条路径已经到头,从栈中弹出 onStack[vertex] = false; &#125; // certify that digraph has a directed cycle if it reports one private boolean check() &#123; if (hasCycle()) &#123; // verify cycle int first = -1, last = -1; for (int v : cycle()) &#123; if (first == -1) first = v; last = v; &#125; if (first != last) &#123; System.err.printf("cycle begins with %d and ends with %d\n", first, last); return false; &#125; &#125; return true; &#125;&#125; 拓扑排序 拓扑排序等价于计算优先级限制下的调度问题的,所谓优先级限制的调度问题即是在给定一组需要完成的任务与关于任务完成的先后次序的优先级限制,需要在满足限制条件的前提下来安排任务. 拓扑排序需要的是一幅有向无环图,如果这幅图中含有环,那么它肯定不是拓扑有序的(一个带有环的调度问题是无解的). 在学习拓扑排序之前,需要先知道顶点的排序. 顶点排序 使用深度优先搜索来记录顶点排序是一个很好的选择(正好只会访问每个顶点一次),我们借助一些数据结构来保存顶点排序的顺序: 前序: 在递归调用之前将顶点加入队列. 后序: 在递归调用之后将顶点加入队列. 逆后序: 在递归调用之后将顶点压入栈. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114public class DepthFirstOrder &#123; private final Graph graph; // marked[v] = has v been marked in dfs? private final boolean[] marked; // pre[v] = preorder number of v private final int[] pre; // post[v] = postorder number of v private final int[] post; // vertices in preorder private final Queue&lt;Integer&gt; preorder; // vertices in postorder private final Queue&lt;Integer&gt; postorder; // counter or preorder numbering private int preCounter; // counter for postorder numbering private int postCounter; public DepthFirstOrder(Graph graph) &#123; this.graph = graph; int vertex = graph.vertex(); this.pre = new int[vertex]; this.post = new int[vertex]; this.preorder = new ArrayDeque&lt;&gt;(); this.postorder = new ArrayDeque&lt;&gt;(); this.marked = new boolean[vertex]; for (int v = 0; v &lt; vertex; v++) if (!marked[v]) dfs(v); assert check(); &#125; public int pre(int v) &#123; validateVertex(v); return pre[v]; &#125; public int post(int v) &#123; validateVertex(v); return post[v]; &#125; public Iterable&lt;Integer&gt; post() &#123; return postorder; &#125; public Iterable&lt;Integer&gt; pre() &#123; return preorder; &#125; // 逆后序,遍历后序队列并压入栈中 public Iterable&lt;Integer&gt; reversePost() &#123; Stack&lt;Integer&gt; reverse = new Stack&lt;Integer&gt;(); for (int v : postorder) reverse.push(v); return reverse; &#125; private void dfs(int vertex) &#123; marked[vertex] = true; // 前序 pre[vertex] = preCounter++; preorder.add(vertex); for (int w : graph.adj(vertex)) &#123; if (!marked[w]) dfs(w); &#125; // 后序 post[vertex] = postCounter++; postorder.add(vertex); &#125; // check that pre() and post() are consistent with pre(v) and post(v) private boolean check() &#123; // check that post(v) is consistent with post() int r = 0; for (int v : post()) &#123; if (post(v) != r) &#123; System.out.println("post(v) and post() inconsistent"); return false; &#125; r++; &#125; // check that pre(v) is consistent with pre() r = 0; for (int v : pre()) &#123; if (pre(v) != r) &#123; System.out.println("pre(v) and pre() inconsistent"); return false; &#125; r++; &#125; return true; &#125; // throw an IllegalArgumentException unless &#123;@code 0 &lt;= v &lt; V&#125; private void validateVertex(int v) &#123; int V = marked.length; if (v &lt; 0 || v &gt;= V) throw new IllegalArgumentException("vertex " + v + " is not between 0 and " + (V - 1)); &#125;&#125; 拓扑排序的实现 所谓拓扑排序就是无环有向图的逆后序,现在已经知道了如何检测环与顶点排序,那么实现拓扑排序就很简单了. 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Topological &#123; // topological order private Iterable&lt;Integer&gt; order; // rank[v] = position of vertex v in topological order private int[] rank; public Topological(Digraph digraph) &#123; DirectedCycle directedCycle = new DirectedCycle(digraph); // 只有这幅图没有环时,才进行计算拓扑排序 if (!directedCycle.hasCycle()) &#123; DepthFirstOrder depthFirstOrder = new DepthFirstOrder(digraph); // 拓扑排序即是逆后序 order = depthFirstOrder.reversePost(); rank = new int[digraph.vertex()]; int i = 0; for (int v : order) rank[v] = i++; &#125; &#125; public Iterable&lt;Integer&gt; order() &#123; return order; &#125; public boolean hasOrder() &#123; return order != null; &#125; public int rank(int v) &#123; validateVertex(v); if (hasOrder()) return rank[v]; else return -1; &#125; // throw an IllegalArgumentException unless &#123;@code 0 &lt;= v &lt; V&#125; private void validateVertex(int v) &#123; int V = rank.length; if (v &lt; 0 || v &gt;= V) throw new IllegalArgumentException("vertex " + v + " is not between 0 and " + (V - 1)); &#125;&#125; 强连通性 在一幅无向图中,如果有一条路径连接顶点v和w,则它们就是连通的(既可以从w到达v,也可以从v到达w).但在有向图中,如果从顶点v有一条有向路径到达w,则w是从v可达的,但从w到达v的路径可能存在也可能不存在. 强连通性就是两个顶点v和w是互相可达的.有向图中的强连通性具有以下性质: 自反性: 任意顶点v和自己都是强连通性的(有向图中顶点都是自己可达的). 对称性: 如果v和w是强连通的,那么w和v也是强连通的. 传递性: 如果v和w是强连通的且w和x也是强连通的,那么v和x也是强连通的. 强连通性将所有顶点分为了一些等价类,每个等价类都是由相互为强连通的顶点的最大子集组成的.这些子集称为强连通分量,它的定义是基于顶点的,而非边. 一个含有V个顶点的有向图含有1 ~ V个强连通分量.一个强连通图只含有一个强连通分量,而一个有向无环图中则含有V个强连通分量. Kosaraju算法 Kosaraju算法是用于枚举图中每个强连通分量内的所有顶点,它主要有以下步骤: 在给定一幅有向图$G$中,取得它的反向图$G^R$. 利用深度优先搜索得到$G^R$的逆后序排列. 按照上述逆后序的序列进行深度优先搜索 同一个深度优先搜索递归子程序中访问的所有顶点都在同一个强连通分量内. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class KosarajuSharirSCC &#123; private final Digraph digraph; // marked[v] = has vertex v been visited? private final boolean[] marked; // id[v] = id of strong component containing v private final int[] id; // number of strongly-connected components private int count; public KosarajuSharirSCC(Digraph digraph) &#123; this.digraph = digraph; int vertex = digraph.vertex(); // compute reverse postorder of reverse graph DepthFirstOrder depthFirstOrder = new DepthFirstOrder(digraph.reverse()); // run DFS on G, using reverse postorder to guide calculation marked = new boolean[vertex]; id = new int[vertex]; for (int v : depthFirstOrder.reversePost()) &#123; if (!marked[v]) &#123; dfs(v); count++; &#125; &#125; // check that id[] gives strong components assert check(digraph); &#125; private void dfs(int v) &#123; marked[v] = true; id[v] = count; for (int w : digraph.adj(v)) &#123; if (!marked[w]) dfs(w); &#125; &#125; public int count() &#123; return count; &#125; public boolean stronglyConnected(int v, int w) &#123; validateVertex(v); validateVertex(w); return id[v] == id[w]; &#125; public int id(int v) &#123; validateVertex(v); return id[v]; &#125; // does the id[] array contain the strongly connected components? private boolean check(Digraph G) &#123; TransitiveClosure tc = new TransitiveClosure(G); for (int v = 0; v &lt; G.vertex(); v++) &#123; for (int w = 0; w &lt; G.vertex(); w++) &#123; if (stronglyConnected(v, w) != (tc.reachable(v, w) &amp;&amp; tc.reachable(w, v))) return false; &#125; &#125; return true; &#125; // throw an IllegalArgumentException unless &#123;@code 0 &lt;= v &lt; V&#125; private void validateVertex(int v) &#123; int V = marked.length; if (v &lt; 0 || v &gt;= V) throw new IllegalArgumentException("vertex " + v + " is not between 0 and " + (V - 1)); &#125;&#125; 传递闭包 在一幅有向图G中,传递闭包是由相同的一组顶点组成的另一幅有向图,在传递闭包中存在一条从v指向w的边且仅当在G中w是从v可达的. 由于有向图的性质,每个顶点对于自己都是可达的,所以传递闭包会含有V个自环. 通常将传递闭包表示为一个布尔值矩阵,其中v行w列的值为true代表当且仅当w是从v可达的. 传递闭包不适合于处理大型有向图,因为构造函数所需的空间与$V^2$成正比,所需的时间和$V(V+E)$成正比. 123456789101112131415161718192021222324public class TransitiveClosure &#123; private DirectedDFS[] tc; // tc[v] = reachable from v public TransitiveClosure(Digraph G) &#123; tc = new DirectedDFS[G.vertex()]; for (int v = 0; v &lt; G.vertex(); v++) tc[v] = new DirectedDFS(G, v); &#125; public boolean reachable(int v, int w) &#123; validateVertex(v); validateVertex(w); return tc[v].marked(w); &#125; // throw an IllegalArgumentException unless &#123;@code 0 &lt;= v &lt; V&#125; private void validateVertex(int v) &#123; int V = tc.length; if (v &lt; 0 || v &gt;= V) throw new IllegalArgumentException("vertex " + v + " is not between 0 and " + (V - 1)); &#125;&#125; 参考文献 Algorithms, 4th Edition by Robert Sedgewick and Kevin Wayne Kosaraju’s algorithm - Wikipedia Transitive closure - Wikipedia 图的那点事儿 图的那点事儿(1)-无向图 图的那点事儿(2)-有向图 图的那点事儿(3)-加权无向图 图的那点事儿(4)-加权有向图]]></content>
      <categories>
        <category>Algorithms</category>
        <category>Graph</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>2017</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电影中的"FaceMash"算法]]></title>
    <url>%2F2017%2F07%2F19%2F2017-07-19-FaceMash%2F</url>
    <content type="text"><![CDATA[最近在看&lt;&lt;社交网络&gt;&gt;时,发现了一个用于投票排名的算法,自己折腾实现了一下. 在影片中,卷西饰演的扎克伯格在被妹子甩了之后(其实是他自己直男癌),一气之下黑了附近女生宿舍的照片数据库打算做一个FaceMash(通过投票的方式来选出漂亮的女生,同时它也是Facebook的前身,后来这个网站由于流量太大,搞崩了哈佛大学的网络而被强行关闭了),并使用了他的好基友爱德华多用于计算国际象棋排名的算法. 这是一部很好看的电影,如果没有看过我强烈推荐去看一看. 公式 这个算法是用来计算期望胜率的,但影片中其实写的是错误的,正确的公式应该为: $$E_a = \frac{1} {1 + 10 ^ {(R_b - R_a) / 400}}$$ $E_a$就是a的期望胜率. $R_b,R_a$是b与a的Rank分数. 当$R_a,R_b$都相同时,它们的期望胜率都为0.5,即$E_a = \frac{1} {1+10^0} = 0.5$. 电影中只给出了计算期望胜率的算法,但我们还需要一个计算新的Rank分数的算法,公式如下: $$R_n = R_o + K(W - E)$$ $R_n$代表新的Rank,$R_o$自然就是旧的Rank了. K为一个定值,我把它设为10. W是胜负值,胜者为1,败者为0;E就是我们上面计算的期望胜率. 实现 有了这两个核心公式,我们就可以开始实现这个算法了,但在代码实现之前,我们先验证一下公式: 假设有两个女孩A与B,她们的基础Rank都为1400,通过上述的推论我们已经得知,当A,B的分值相同时,她们的期望胜率都为0.5. 如果,我选择了A,则A的胜负值变为1,B的胜负值为0,然后我们套用公式2可以得出: $R_a = 1400 + 10 * (1 - 0.5) = 1405$ $R_b = 1400 + 10 * (0 - 0.5) = 1395$ 由于她们的分数不再相同,所以套用公式1计算现在的期望胜率: $R_a = \frac{1} {1 + 10 ^ {(1395 - 1405) / 400}} \approx 0.51439 $ $R_b = \frac{1} {1 + 10 ^ {(1405 - 1395) / 400}} \approx 0.48561$ 下面是我用C写的一个小程序,它初始化了两个”女孩”,然后根据输入来判断哪个胜出,并动态计算Rank与期望胜率. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#include &lt;stdio.h&gt;#include &lt;math.h&gt;typedef struct &#123; const char *name; int rank; double expect_rate;&#125; girl;const int K = 10;void read_girl(girl g) &#123; printf("Girl name: %s, rank: %d, expect_rate: %.5f\n",g.name,g.rank,g.expect_rate);&#125;void compute_expect_rate(girl *a,girl *b) &#123; int a_rank = a-&gt;rank; int b_rank = b-&gt;rank; // expect rate formula // Ea = 1 / (1 + 10 ^ ((Rb-Ra) / 400)) double a_rank_differ = (double) (b_rank - a_rank) / 400; double a_rank_rate = pow(10,a_rank_differ); a-&gt;expect_rate = 1 / (1 + a_rank_rate); // Eb = 1 / (1 + 10 ^ ((Ra-Rb) / 400)) double b_rank_differ = (double) (a_rank - b_rank) / 400; double b_rank_rate = pow(10,b_rank_differ); b-&gt;expect_rate = 1 / (1 + b_rank_rate);&#125;// new rank formula: Rn = Ro + K(W - E)void compute_rank(girl *a,girl *b,int a_win_rate,int b_win_rate) &#123; a-&gt;rank = a-&gt;rank + K * (a_win_rate - a-&gt;expect_rate); b-&gt;rank = b-&gt;rank + K * (b_win_rate - b-&gt;expect_rate);&#125;int main(int argc,char *argv[]) &#123; char a_girl_name[20]; char b_girl_name[20]; girl a = &#123;.name = "A Gril",.rank = 1400&#125;; girl b = &#123;.name = "B Gril",.rank = 1400&#125;; compute_expect_rate(&amp;a,&amp;b); read_girl(a); read_girl(b); while (1) &#123; char choice[2]; printf("Choice A or B?\n"); scanf("%s",choice); if (choice[0] == 'A') &#123; compute_rank(&amp;a,&amp;b,1,0); compute_expect_rate(&amp;a,&amp;b); &#125; else if (choice[0] == 'B') &#123; compute_rank(&amp;a,&amp;b,0,1); compute_expect_rate(&amp;a,&amp;b); &#125; else &#123; printf("Invalid choice!\n"); break; &#125; read_girl(a); read_girl(b); &#125;&#125; 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>Algorithms</category>
        <category>Other</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的那点事儿(1)-无向图]]></title>
    <url>%2F2017%2F07%2F18%2F2017-07-18-Graph_UndirectedGraph%2F</url>
    <content type="text"><![CDATA[在数学中,一个图(Graph)是表示物件与物件之间关系的方法,是图论的基本研究对象.一个图是由顶点(Vertex)与连接这些顶点的边(Edge)组成的. 图论作为数学领域中的一个重要分支已经有数百年的历史了.人们发现了图的许多重要而实用的性质,发明了许多重要的算法,给你一个图(Graph)你可以联想到许多问题: 两个顶点之间是否存在一条链接?如果存在,两个顶点之间最短的连接又是哪一条?…. 在生活中,到处都可以发现图论的应用: 地图: 在使用地图中,我们经常会想知道”从xx到xx的最短路线”这样的问题,要回答这些问题,就需要把地图抽象成一个图(Graph),十字路口就是顶点,公路就是边. 互联网: 整个互联网其实就是一张图,它的顶点为网页,边为超链接.而图论可以帮助我们在网络上定位信息. 任务调度: 当一些任务拥有优先级限制且需要满足前置条件时,如何在满足条件的情况下用最少的时间完成就需要用到图论. 社交网络: 在使用社交网站时,你就是一个顶点,你和你的朋友建立的关系则是边.分析这些社交网络的性质也是图论的一个重要应用. 图就是由一组顶点和一组能够将两个顶点相连的边组成的. 基本术语 相邻: 当两个顶点通过一条边相连接时,这两个顶点即为相邻的(也可以说这条边依附于这两个顶点). 度数: 某个顶点的度数即为依附于它的边的总数. 阶: 图G中的顶点集合V的大小称为G的阶. 自环: 一条连接一个顶点和其自身的边. 平行边: 连接同一对顶点的两条边称为平行边. 桥: 如果去掉一条边会使整个图变成非连通图,则该边称为桥. 路径: 当顶点v到顶点w是连通时,我们用v-&gt;x-&gt;y-&gt;w为一条v到w的路径,用v-&gt;x-&gt;y-&gt;v表示一条环. 子图: 也称作连通分量,它由一张图的所有边的一个子集组成的图(以及依附的所有顶点). 连通图: 连通图是一个整体,而非连通图则包含两个或多个连通分量. 稀疏图: 如果一张图中不同的边的数量在顶点总数V的一个小的常数倍内,那么该图就为稀疏图,否则为稠密图. 简单图与多重图: 含有平行边与自环的图称为多重图,而不含有平行边和自环的图称为简单图. 树 树是一张无环连通图,互不相连的树组成的集合称为森林.连通图的生成树是它的一张子图,它含有图中的所有顶点且是一棵树.图的生成树森林是它的所有连通分量的生成树的集合. 图G只要满足以下性质,那么它就是一棵树: G有V-1条边且不含有环. G有V-1条边且是连通的. G是连通的,但删除任意一条边都会使它不再连通. G是无环图,但添加任意一条边都会产生一条环. G中的任意一对顶点之间仅存在一条简单路径(一条没有重复顶点的路径). 二分图 二分图是一种能够将所有顶点分为两部分的图,其中图的每条边所连接的两个顶点都分别属于不同的部分. 设G = (V,E)为一张无向图,如果顶点V可以分割为两个互不相交的子集(U,V),且图中的每条边(x,y)所关联的两个顶点x,y分别属于这两个不同的顶点集合(x in U , y in V),则G为二分图. 也可以将(U,V)当做一张着色图: U中的所有顶点为蓝色,V中的所有顶点为绿色,每条边所关联的两个顶点颜色不同. 无向图 无向图是一种最简单的图模型,它的每条边都没有方向. 图的表示方法 实现一张图的API需要满足以下两个要求: 必须为可能在应用中碰到的各种类型的图预留出足够的空间. 图的实现一定要足够快(因为这是所有处理图的算法的基础结构). 有以下三种数据结构能够用来表示一张图: 邻接矩阵: 使用一个V * V的布尔矩阵.当顶点v和顶点w之间有相连接的边时,将v行w列的元素设为true,否则为false.这种方法不符合第一个条件,当图的顶点非常多时,邻接矩阵所需的空间将会非常大.且它无法表示平行边. 边的数组: 使用一个Edge类,它含有两个int成员变量来表示所依附的顶点.这种方法简单直接但不满足第二个条件(要实现查询邻接点的函数需要检查图中的所有边). 邻接表数组: 使用一个顶点为索引的链表数组,其中的每个元素都是和该顶点相邻的顶点列表(邻接点).这种方法同时满足了两个条件,我们会使用这种方法来实现图的数据结构. 实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138public interface Graph &#123; int vertex(); int edge(); void addEdge(int v, int w); Iterable&lt;Integer&gt; adj(int v); int degree(int v); String toString();&#125;public class UndirectedGraph implements Graph &#123; private static final String NEW_LINE_SEPARATOR = System.getProperty("line.separator"); private final int vertex; // 顶点 private int edge; // 边 private final Bag&lt;Integer&gt;[] adjacent; // 邻接表数组,Bag是一个没有实现删除操作的Stack public UndirectedGraph(int vertex) &#123; checkVertex(vertex); this.vertex = vertex; this.edge = 0; this.adjacent = (Bag&lt;Integer&gt;[]) new Bag[vertex]; for (int v = 0; v &lt; vertex; v++) adjacent[v] = new Bag&lt;Integer&gt;(); &#125; // 读取一个文件并初始化为无向图 public UndirectedGraph(Scanner scanner) &#123; if (scanner == null) throw new IllegalArgumentException("Specified input stream must not null!"); try &#123; // 文件的第一行为顶点数 this.vertex = scanner.nextInt(); checkVertex(this.vertex); // 文件的第二行为边数 int edge = scanner.nextInt(); checkEdge(this.edge); this.adjacent = (Bag&lt;Integer&gt;[]) new Bag[this.vertex]; for (int v = 0; v &lt; this.vertex; v++) adjacent[v] = new Bag&lt;Integer&gt;(); // 文件的剩余行为相连的顶点对 for (int i = 0; i &lt; edge; i++) &#123; int v = scanner.nextInt(); int w = scanner.nextInt(); addEdge(v, w); &#125; &#125; catch (NoSuchElementException e) &#123; throw new IllegalArgumentException("Invalid input format in Undirected Graph constructor", e); &#125; &#125; public UndirectedGraph(Graph graph) &#123; this(graph.vertex()); this.edge = graph.edge(); for (int v = 0; v &lt; this.vertex; v++) &#123; // reverse so that adjacency list is in same order as original Stack&lt;Integer&gt; stack = new Stack&lt;Integer&gt;(); for (int w : graph.adj(v)) stack.push(w); for (int w : stack) adjacent[v].add(w); &#125; &#125; private void checkVertex(int vertex) &#123; if (vertex &lt;= 0) throw new IllegalArgumentException("Number of vertices must be positive number!"); &#125; private void checkEdge(int edge) &#123; if (edge &lt; 0) throw new IllegalArgumentException("Number of edges must be positive number!"); &#125; public int vertex() &#123; return vertex; &#125; public int edge() &#123; return edge; &#125; // 添加一条连接v和w的边,由于是无向图所以这条边会出现两次 public void addEdge(int v, int w) &#123; validateVertex(v); validateVertex(w); adjacent[v].add(w); adjacent[w].add(v); edge++; &#125; public Iterable&lt;Integer&gt; adj(int v) &#123; validateVertex(v); return adjacent[v]; &#125; public int degree(int v) &#123; validateVertex(v); return adjacent[v].size(); &#125; private void validateVertex(int vertex) &#123; if (vertex &lt; 0 || vertex &gt;= this.vertex) throw new IllegalArgumentException("Vertex " + vertex + " is not between 0 and " + (this.vertex - 1)); &#125; @Override public String toString() &#123; StringBuilder sb = new StringBuilder(); sb.append("Vertices: ").append(vertex).append(" Edges: ").append(edge).append(NEW_LINE_SEPARATOR); for (int v = 0; v &lt; vertex; v++) &#123; sb.append(v).append(": "); for (int w : adjacent[v]) sb.append(w).append(" "); sb.append(NEW_LINE_SEPARATOR); &#125; return sb.toString(); &#125; public static void main(String[] args) throws FileNotFoundException &#123; InputStream inputStream = UndirectedGraph.class.getResourceAsStream("/graph_file/C4_1_UndirectedGraphs/" + args[0]); Scanner scanner = new Scanner(inputStream, "UTF-8"); Graph graph = new UndirectedGraph(scanner); System.out.println(graph); &#125;&#125; 上面的这个实现拥有以下特点: 使用的空间和V + E成正比. 添加一条边所需的时间为常数. 遍历顶点v的所有邻接点所需的时间和v的度数成正比(处理每个邻接点所需的时间为常数). 边的插入顺序决定了邻接表中顶点的出现顺序. 支持平行边与自环. 不支持添加或删除顶点的操作(如果想要支持这些操作需要使用一个符号表来代替由顶点索引构成的数组). 不支持删除边的操作(如果想要支持这个操作需要使用一个SET来代替Bag来实现邻接表,这种方法也叫邻接集). 每种图实现的性能复杂度如下表: 数据结构 所需空间 添加一条边v - w 检查w和v是否相邻 遍历v的所有邻接点 边的数组 E 1 E E 邻接矩阵 V^2 1 1 V 邻接表 E+V 1 degree(V) degree(V) 邻接集 E+V logV logV logV+degree(V) 本文中的所有完整代码可以到我的GitHub中查看. 深度优先搜索 处理图的基本问题: v 到 w是否是相连的?. 深度优先搜索就是用于解决这样问题的,它会沿着图的边寻找和起点连通的所有顶点. 如其名一样,深度优先搜素就是沿着图的深度来遍历顶点,它类似于走迷宫,会沿着一条路径一直走,直到走到尽头时再回退到上一个路口.为了防止迷路,还需要使用工具来标记已走过的路口(在我们的代码实现中使用一个布尔数组来进行标记). 递归实现 使用递归方法来实现深度优先搜索会很简洁,当遇到一个顶点时: 将它标记为已访问. 递归地访问它的所有没有被访问过的邻接点. 1234567891011121314151617181920212223242526272829303132333435363738394041public class DepthFirstSearch &#123; private final boolean[] marked; // 标记已访问过的顶点 private int count; // 记录起点连通的顶点数 private final Graph graph; public DepthFirstSearch(Graph graph, int originPoint) &#123; this.graph = graph; this.count = 0; this.marked = new boolean[graph.vertex()]; validateVertex(originPoint); // 从起点开始进行深度优先搜索 depthSearch(originPoint); &#125; public int count() &#123; return count; &#125; public boolean marked(int vertex) &#123; validateVertex(vertex); return marked[vertex]; &#125; private void depthSearch(int vertex) &#123; marked[vertex] = true; count++; for (int adj : graph.adj(vertex)) &#123; // 遍历邻接点,如果未访问则递归调用 if (!marked[adj]) depthSearch(adj); &#125; &#125; private void validateVertex(int vertex) &#123; int length = marked.length; if (vertex &lt; 0 || vertex &gt;= length) throw new IllegalArgumentException("Vertex " + vertex + " is not between 0 and " + (length - 1)); &#125;&#125; 非递归实现 如果是了解JVM中函数调用的小伙伴们应该知道,函数都会封装成一个个栈帧然后压入虚拟机栈,上述的递归实现其实就是在隐式的使用到了栈,要想实现非递归,我们需要显式使用栈这个数据结构. 12345678910111213141516171819202122232425262728293031323334353637383940414243public class NonrecursiveDFS &#123; private final boolean[] marked; private final Iterator&lt;Integer&gt;[] adj; public NonrecursiveDFS(Graph graph, int originPoint) &#123; int vertex = graph.vertex(); this.marked = new boolean[vertex]; validateVertex(originPoint); // 取出所有顶点的邻接表迭代器 adj = (Iterator&lt;Integer&gt;[]) new Iterator[vertex]; for (int v = 0; v &lt; vertex; v++) adj[v] = graph.adj(v).iterator(); dfs(originPoint); &#125; private void dfs(int originPoint) &#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); // 标记起点并放入栈 marked[originPoint] = true; stack.push(originPoint); while (!stack.isEmpty()) &#123; Integer v = stack.peek(); // 遍历栈顶顶点的邻接点 if (adj[v].hasNext()) &#123; int w = adj[v].next(); // 如果未被访问,进行标记并放入栈中 if (!marked[w]) &#123; marked[w] = true; stack.push(w); &#125; &#125; else &#123; // 当栈顶顶点的所有邻接点已经遍历完时,弹出栈 stack.pop(); &#125; &#125; &#125;&#125; 寻找路径 在图的应用中,找出v-w的可达路径也是常见的问题之一. 我们基于深度优先搜索实现寻找路径,并添加一个edgeTo[]整形数组来记录路径.例如,在由边v-w第一次访问任意w时,将edgeTo[w]设为v来记录这条路径(v-w是从起点到w的路径上最后一条已知的边).这样搜索到的路径就是一颗以起点为根的树,edgeTo[]是一颗由父链接表示的树. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class DepthFirstPaths &#123; private final Graph graph; private final boolean[] marked; private final int[] edgeTo; // 用于记录路径 private final int originPoint; public DepthFirstPaths(Graph graph, int originPoint) &#123; int vertex = graph.vertex(); this.graph = graph; this.originPoint = originPoint; this.marked = new boolean[vertex]; this.edgeTo = new int[vertex]; validateVertex(originPoint); dfs(originPoint); &#125; public boolean hasPathTo(int vertex) &#123; validateVertex(vertex); return marked[vertex]; &#125; public Iterable&lt;Integer&gt; pathTo(int vertex) &#123; validateVertex(vertex); Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); // 从指定顶点处向上遍历路径(直到起点) for (int x = vertex; x != originPoint; x = edgeTo[x]) stack.push(x); stack.push(originPoint); return stack; &#125; private void dfs(int vertex) &#123; marked[vertex] = true; for (int adj : graph.adj(vertex)) &#123; if (!marked[adj]) &#123; marked[adj] = true; // edgeTo[w] = v,记录了父链接 edgeTo[adj] = vertex; dfs(adj); &#125; &#125; &#125; &#125; 广度优先搜索 对于寻找一条最短路径,深度优先搜索没有什么作为,因为它遍历整个图的顺序和找出最短路径的目标没有任何关系.这种问题就需要用到广度优先搜索. 广度优先搜索是沿着宽度来进行搜索的.例如,要找到s到v的最短路径,从s开始,在所有由一条边就可以到达的顶点中寻找v,如果找不到就继续在与s距离两条边的所有顶点中寻找v,以此类推. 如果说深度优先搜索是一个人在走迷宫,那么广度优先搜索就是一群人一起朝着各个方向去走迷宫. 在广度优先搜索中,我们使用一个队列来保存所有已被标记过但邻接表还未被检查过的顶点.先将起点放入队列,然后重复以下步骤直到队列为空: 取出队列中的下一个顶点并标记. 将它相邻的所有未被标记过的顶点加入队列. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111public class BreadthFirstPaths &#123; private static final int INFINITY = Integer.MAX_VALUE; private final Graph graph; private final boolean[] marked; private final int[] edgeTo; private final int[] distTo; // 记录路径中经过的顶点数,起点为0,需要全部初始化为无穷大 public BreadthFirstPaths(Graph graph, int originPoint) &#123; this.graph = graph; int vertex = graph.vertex(); marked = new boolean[vertex]; edgeTo = new int[vertex]; distTo = new int[vertex]; for (int i = 0; i &lt; vertex; i++) distTo[i] = INFINITY; validateVertex(originPoint); bfs(originPoint); &#125; // 以一组顶点为起点 public BreadthFirstPaths(Graph graph, Iterable&lt;Integer&gt; sources) &#123; this.graph = graph; int vertex = graph.vertex(); this.marked = new boolean[vertex]; this.edgeTo = new int[vertex]; this.distTo = new int[vertex]; for (int i = 0; i &lt; vertex; i++) distTo[i] = INFINITY; validateVertices(sources); bfs(sources); &#125; public boolean hasPathTo(int vertex) &#123; validateVertex(vertex); return marked[vertex]; &#125; public int distTo(int vertex) &#123; validateVertex(vertex); return distTo[vertex]; &#125; public Iterable&lt;Integer&gt; pathTo(int vertex) &#123; validateVertex(vertex); Stack&lt;Integer&gt; path = new Stack&lt;&gt;(); int x; // 这里使用distTo[x] != 0来判断是否为起点 for (x = vertex; distTo[x] != 0; x = edgeTo[x]) path.push(x); path.push(x); return path; &#125; private void bfs(int vertex) &#123; Queue&lt;Integer&gt; queue = new ArrayDeque&lt;&gt;(); marked[vertex] = true; distTo[vertex] = 0; queue.add(vertex); searchAndMarkAdjacent(queue); &#125; private void bfs(Iterable&lt;Integer&gt; sources) &#123; Queue&lt;Integer&gt; queue = new ArrayDeque&lt;&gt;(); for (int v : sources) &#123; marked[v] = true; distTo[v] = 0; queue.add(v); &#125; searchAndMarkAdjacent(queue); &#125; // 广度优先搜索 private void searchAndMarkAdjacent(Queue&lt;Integer&gt; queue) &#123; while (!queue.isEmpty()) &#123; Integer v = queue.remove(); for (int adj : graph.adj(v)) &#123; // 将未标记过的邻接点加入队列并进行标记等操作 if (!marked[adj]) &#123; marked[adj] = true; edgeTo[adj] = v; distTo[adj] = distTo[v] + 1; queue.add(adj); &#125; &#125; &#125; &#125; private void validateVertex(int vertex) &#123; int length = marked.length; if (vertex &lt; 0 || vertex &gt;= length) throw new IllegalArgumentException("Vertex " + vertex + " is not between 0 and " + (length - 1)); &#125; private void validateVertices(Iterable&lt;Integer&gt; vertices) &#123; if (vertices == null) throw new IllegalArgumentException("Vertices is null."); int length = marked.length; for (int v : vertices) &#123; if (v &lt; 0 || v &gt;= length) throw new IllegalArgumentException("Vertex " + v + " is not between 0 and " + (length - 1)); &#125; &#125;&#125; 不管是深度优先搜索还是广度优先搜索,它们都是先将起点存入一个数据结构中,然后重复以下步骤直到数据结构被清空: 取其中的下一个顶点并标记它. 将它的所有相邻而又未被标记的顶点放入数据结构中. 这两种算法的不同之处仅在于从数据结构中获取下一个顶点的规则(对于广度优先搜索来说是最早加入的顶点,对于深度优先搜索来说是最晚加入的顶点). 深度优先搜索的方式是不断寻找离起点更远的顶点,直到碰见死胡同时才返回近处顶点. 广度优先搜索的方式是先覆盖起点附近的顶点,只有当邻接的所有顶点都被访问过之后才继续前进. 深度优先搜素的路径通常长且曲折,广度优先搜索的路径则短而直接.但不管是使用哪种算法,所有与起点连通的顶点和边都会被访问到. 连通分量 深度优先搜索的一个重要应用就是寻找出一幅图中的所有连通分量. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class ConnectedComponent &#123; private final Graph graph; private final boolean[] marked; // 顶点与它们所属的连通分量进行关联的数组 private final int[] id; // 记录每个连通分量中有多少顶点的数组 private final int[] size; // 连通分量数 private int count; public ConnectedComponent(Graph graph) &#123; this.graph = graph; int vertex = graph.vertex(); this.marked = new boolean[vertex]; this.id = new int[vertex]; this.size = new int[vertex]; for (int v = 0; v &lt; vertex; v++) &#123; if (!marked[v]) &#123; dfs(v); count++; // 一张连通图遍历完毕后,连通分量数 + 1 &#125; &#125; &#125; public int id(int vertex) &#123; validateVertex(vertex); return id[vertex]; &#125; public int size(int vertex) &#123; validateVertex(vertex); return size[id[vertex]]; &#125; public int count() &#123; return count; &#125; // 两个顶点是否处于一个连通分量中 public boolean connected(int v, int w) &#123; validateVertex(v); validateVertex(w); return id[v] == id[w]; &#125; private void dfs(int vertex) &#123; marked[vertex] = true; id[vertex] = count; size[count]++; for (int adj : graph.adj(vertex)) &#123; if (!marked[adj]) dfs(adj); &#125; &#125; &#125; 检测环与双色问题 深度优先搜索的应用远不于此,它还可以用来检测是否有环以及双色问题. 检测环 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public class Cyclic &#123; private final Graph graph; private boolean[] marked ; private int[] edgeTo; // 如果存在环则返回这条环路径 private Stack&lt;Integer&gt; cyclic; public Cyclic(Graph graph) &#123; this.graph = graph; // 先检测是否有自环 if (hasSelfLoop()) return; // 再检测是否有平行边 if (hasParallelEdges()) return; int vertex = graph.vertex(); this.marked = new boolean[vertex]; this.edgeTo = new int[vertex]; for (int v = 0; v &lt; vertex; v++) &#123; if (!marked[v]) dfs(v, -1); &#125; &#125; public boolean hasCyclic() &#123; return cyclic != null; &#125; public Iterable&lt;Integer&gt; cyclic() &#123; return cyclic; &#125; private boolean hasSelfLoop() &#123; for (int v = 0; v &lt; graph.vertex(); v++) &#123; for (int w : graph.adj(v)) &#123; // 如果v与w是同一个顶点,则代表有自环 if (v == w) &#123; cyclic = new Stack&lt;&gt;(); cyclic.push(v); cyclic.push(v); return true; &#125; &#125; &#125; return false; &#125; private boolean hasParallelEdges() &#123; int vertex = graph.vertex(); boolean[] marked = new boolean[vertex]; for (int v = 0; v &lt; vertex; v++) &#123; // check for parallel edges incident to v for (int w : graph.adj(v)) &#123; if (marked[w]) &#123; cyclic = new Stack&lt;&gt;(); cyclic.push(v); cyclic.push(w); cyclic.push(v); return true; &#125; marked[w] = true; &#125; // reset so marked[v] = false for all v for (int w : graph.adj(v)) marked[w] = false; &#125; return false; &#125; private void dfs(int v, int u) &#123; marked[v] = true; for (int w : graph.adj(v)) &#123; if (cyclic != null) return; if (!marked[w]) &#123; edgeTo[w] = v; dfs(w, v); &#125; else if (w != u) &#123; // check for cycle (but disregard reverse of edge leading to v) cyclic = new Stack&lt;&gt;(); for (int x = v; x != w; x = edgeTo[x]) cyclic.push(x); cyclic.push(w); cyclic.push(v); &#125; &#125; &#125;&#125; 检测双色 12345678910111213141516171819202122232425262728293031323334353637383940414243public class TwoColor &#123; private final Graph graph; private final boolean[] marked; private final boolean[] color; private boolean isTwoColorable = true; public TwoColor(Graph graph) &#123; this.graph = graph; int vertex = graph.vertex(); this.marked = new boolean[vertex]; this.color = new boolean[vertex]; for (int v = 0; v &lt; vertex; v++) &#123; if (!marked[v]) dfs(v); &#125; &#125; public boolean isBipartite() &#123; return isTwoColorable; &#125; private void dfs(int v) &#123; marked[v] = true; for (int w : graph.adj(v)) &#123; if (!marked[w]) &#123; // 将未被访问过的邻接点w设为v的反色 color[w] = !color[v]; dfs(w); &#125; else if (color[w] == color[v]) &#123; // 如果w已被访问且颜色与v相同,则代表这不是一张双色图 isTwoColorable = false; &#125; &#125; &#125;&#125; 符号图 在很多应用中,是使用字符串而非整数来表示顶点的,为了适应这种需求,需要拥有以下性质的输入格式: 顶点名是字符串. 用指定的分隔符来隔开顶点名 每一行都表示一组边的集合,每一条边都连接着这一行的第一个名称表示的顶点和其他名称所表示的顶点. 顶点集V与边集E都是隐式定义的. 要实现符号图还需要借助以下数据结构: 一个符号表,我这里使用的是TreeMap即红黑树,它的Key为String(顶点名),Value为Integer(顶点索引). 一个字符串数组,它用来与符号表作反向索引,保存每个顶点索引所对应的顶点名. 一个Graph对象,我们使用索引来生成这张图对象. 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class SymbolGraph &#123; private TreeMap&lt;String, Integer&gt; symbolTable; // string -&gt; index private String[] keys; // index -&gt; string private Graph graph; public SymbolGraph(String filename, String delimiter) &#123; symbolTable = new TreeMap&lt;&gt;(); // 第一次读取文件 String filePath = "/graph_file/C4_1_UndirectedGraphs/" + filename; InputStream inputStream = SymbolGraph.class.getResourceAsStream(filePath); Scanner scanner = new Scanner(inputStream, "UTF-8"); // 初始化符号表 while (scanner.hasNextLine()) &#123; String[] s = scanner.nextLine().split(delimiter); for (String key : s) &#123; if (!symbolTable.containsKey(key)) symbolTable.put(key, symbolTable.size()); &#125; &#125; System.out.printf("Done reading %s!\n", filename); // 初始化反向索引 keys = new String[symbolTable.size()]; for (String name : symbolTable.keySet()) keys[symbolTable.get(name)] = name; // 第二次读取文件,并生成图 graph = new UndirectedGraph(symbolTable.size()); Scanner create_graph_scanner = new Scanner(SymbolGraph.class.getResourceAsStream(filePath)); while (create_graph_scanner.hasNextLine()) &#123; String[] s = create_graph_scanner.nextLine().split(delimiter); // 将第一行的第一个顶点与其他顶点相连 int v = symbolTable.get(s[0]); for (int i = 1; i &lt; s.length; i++) &#123; int w = symbolTable.get(s[i]); graph.addEdge(v, w); &#125; &#125; &#125; public boolean contains(String s) &#123; return symbolTable.containsKey(s); &#125; public int indexOf(String s) &#123; return symbolTable.get(s); &#125; public String nameOf(int v) &#123; validateVertex(v); return keys[v]; &#125; public Graph graph() &#123; return graph; &#125;&#125; 参考资料 Graph (discrete mathematics) - Wikipedia Bipartite graph - Wikipedia Algorithms, 4th Edition by Robert Sedgewick and Kevin Wayne 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接. 图的那点事儿 图的那点事儿(1)-无向图 图的那点事儿(2)-有向图 图的那点事儿(3)-加权无向图 图的那点事儿(4)-加权有向图]]></content>
      <categories>
        <category>Algorithms</category>
        <category>Graph</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>2017</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是动态规划?]]></title>
    <url>%2F2017%2F06%2F27%2F2017-06-27-DynamicProgramming%2F</url>
    <content type="text"><![CDATA[概述 动态规划(Dynamic Programming)是一种分阶段求解决策问题的数学思想,它通过把原问题分解为简单的子问题来解决复杂问题.动态规划在很多领域都有着广泛的应用,例如管理学,经济学,数学,生物学. 动态规划适用于解决带有最优子结构和子问题重叠性质的问题. 最优子结构 : 即是局部最优解能够决定全局最优解(也可以认为是问题可以被分解为子问题来解决),如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构性质. 子问题重叠 : 即是当使用递归进行自顶向下的求解时,每次产生的子问题不总是新的问题,而是已经被重复计算过的问题.动态规划利用了这种性质,使用一个集合将已经计算过的结果放入其中,当再次遇见重复的问题时,只需要从集合中取出对应的结果. 动态规划与分治算法的区别 相信了解过分治算法的同学会发现,动态规划与分治算法很相似,下面我们例举出一些它们的相同之处与不同之处. 相同点 分治算法与动态规划都是将一个复杂问题分解为简单的子问题. 分治算法与动态规划都只能解决带有最优子结构性质的问题. 不同点 分治算法一般都是使用递归自顶向下实现,动态规划使用迭代自底向上实现或带有记忆功能的递归实现. 动态规划解决带有子问题重叠性质的问题效率更加高效. 分治算法分解的子问题是相对独立的. 动态规划分解的子问题是互相带有关联且有重叠的. 斐波那契数列 斐波那契数列就很适合使用动态规划来求解,它在数学上是使用递归来定义的,公式为F(n) = F(n-1) + F(n-2). 普通递归实现一个最简单的实现如下. 12345678910public int fibonacci(int n) &#123; if (n &lt; 1) return 0; if (n == 1) return 1; if (n == 2) return 2; return fibonacci(n - 1) + fibonacci(n - 2);&#125; 但这种算法并不高效,它做了很多重复计算,它的时间复杂度为O(2^n). 动态规划递归实现使用动态规划来将重复计算的结果具有”记忆性”,就可以将时间复杂度降低为O(n). 12345678910111213141516public int fibonacci(int n) &#123; if (n &lt; 1) return 0; if (n == 1) return 1; if (n == 2) return 2; // 判断当前n的结果是否已经被计算,如果map存在n则代表该结果已经计算过了 if (map.containsKey(n)) return map.get(n); int value = fibonacci(n - 1) + fibonacci(n - 2); map.put(n, value); return value;&#125; 虽然降低了时间复杂度,但需要维护一个集合用于存放计算结果,导致空间复杂度提升了. 动态规划迭代实现通过观察斐波那契数列的规律,发现n只依赖于前2种状态,所以我们可以自底向上地迭代实现. 123456789101112131415161718192021public int fibonacci(int n) &#123; if (n &lt; 1) return 0; if (n == 1) return 1; if (n == 2) return 2; // 使用变量a,b来保存上次迭代和上上次迭代的结果 int a = 1; int b = 2; int temp = 0; for (int i = 3; i &lt;= n; i++) &#123; temp = a + b; a = b; b = temp; &#125; return temp;&#125; 这样不仅时间复杂度得到了优化,也不需要额外的空间复杂度. 参考资料 Wikipedia 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>Algorithms</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>2017</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[红黑树那点事儿]]></title>
    <url>%2F2017%2F06%2F16%2F2017-06-16-RedBlackTree%2F</url>
    <content type="text"><![CDATA[概述 红黑树是一种自平衡二叉查找树,它相对于二叉查找树性能会更加高效(查找、删除、添加等操作需要O(log n),其中n为树中元素的个数),但实现较为复杂(需要保持自身的平衡). 性质 红黑树与二叉查找树不同,它的节点多了一个颜色属性,每个节点非黑即红,这也是它名字的由来. 红黑树的节点定义如以下代码: 123456789101112131415161718private static final boolean RED = true;private static final boolean BLACK = false;private Node root;private class Node &#123; private int size = 0; private boolean color = RED; //颜色 private Node parent, left, right; private int orderStatus = 0; private K key; private V value; public Node(K key, V value) &#123; this.key = key; this.value = value; this.size = 1; &#125;&#125; 完整的代码我已经放在了我的Gist中,点击查看完整代码. 红黑树需要保证以下性质: 每个节点的颜色非黑即红. 根节点的颜色为黑色. 所有叶子节点都为黑色(即NIL节点). 每个红色节点的两个子节点都必须为黑色(不能有两个连续的红节点). 从任一节点到其叶子的所有简单路径包含相同数量的黑色节点. 插入 红黑树的查找操作与二叉查找树一致(因为查找不会影响树的结构),而插入与删除操作需要在最后对树进行调整. 我们将新的节点的颜色设为红色(如果设为黑色会使根节点到叶子的一条路径上多了一个黑节点,违反了性质5,这个是很难调整的). 现在我们假设新节点为N,它的父节点为P(且P为G的左节点,如果为右节点则与其操作互为镜像),祖父节点为G,叔叔节点为U.插入一个节点会有以下种情况. 情况1N位于根,它没有父节点与子节点,这时候只需要把它重新设置为黑色即可,无需其他调整. 情况2P的颜色为黑色,这种情况下保持了性质4(N只有两个叶子节点,它们都为黑色)与性质5(N是一个红色节点,不会对其造成影响)的有效,所以无需调整. 情况3如果P与U都为红色,我们可以将它们两个重新绘制为黑色,然后将G绘制为红色(保持性质5),最后再从G开始继续向上进行调整. 情况4P为红色,U为黑色,且N为P的左子节点,这种情况下,我们需要在G处进行一次右旋转,结果满足了性质4与性质5,因为通过这三个节点中任何一个的所有路径以前都通过祖父节点G，现在它们都通过以前的父节点P. 关于旋转操作,可以查看这篇文章《Algorithms,4th Edition》读书笔记-红黑二叉查找树. 情况5P为红色,U为黑色,且N为P的右子节点,我们需要先在P处进行一次左旋转,这样就又回到了情况4. 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 private void fixAfterInsertion(Node x) &#123; while (x != null &amp;&amp; x != root &amp;&amp; colorOf(parentOf(x)) == RED) &#123; if (parentOf(x) == grandpaOf(x).left) &#123; x = parentIsLeftNode(x); &#125; else &#123; x = parentIsRightNode(x); &#125; fixSize(x); &#125; setColor(root, BLACK); &#125; private Node parentIsLeftNode(Node x) &#123; Node xUncle = grandpaOf(x).right;// 情况3 if (colorOf(xUncle) == RED) &#123; x = uncleColorIsRed(x, xUncle); &#125; else &#123; // 情况5 if (x == parentOf(x).right) &#123; x = parentOf(x); rotateLeft(x); &#125; // 情况4 rotateRight(grandpaOf(x)); &#125; return x; &#125; private Node parentIsRightNode(Node x) &#123; Node xUncle = grandpaOf(x).left; if (colorOf(xUncle) == RED) &#123; x = uncleColorIsRed(x, xUncle); &#125; else &#123; if (x == parentOf(x).left) &#123; x = parentOf(x); rotateRight(x); &#125; rotateLeft(grandpaOf(x)); &#125; return x; &#125; private Node uncleColorIsRed(Node x, Node xUncle) &#123; setColor(parentOf(x), BLACK); setColor(xUncle, BLACK); setColor(grandpaOf(x), RED); x = grandpaOf(x); return x; &#125; 删除我们只考虑删除节点只有一个子节点的情况,且只有后继节点与删除节点都为黑色(如果删除节点为红色,从根节点到叶子节点的每条路径上少了一个红色节点并不会违反红黑树的性质,而如果后继节点为红色,只需要将它重新绘制为黑色即可). 先将删除节点替换为后继节点,且后继节点定义为N,它的兄弟节点为S. 情况1N为新的根节点,在这种情况下只需要把根节点保持为黑色即可. 情况2S为红色,只需要在P进行一次左旋转,接下来则继续按以下情况进行处理(尽管路径上的黑色节点数量没有改变,但N有了一个黑色的兄弟节点与红色的父节点). 情况3S和它的子节点都是黑色的,而P为红色.这种情况下只需要将S与P的颜色进行交换 情况4S和它的子节点都是黑色的,这种情况下需要把S重新绘制为红色.这时不通过N的路径都将少一个黑色节点(通过N的路径因为删除节点是黑色的也都少了一个黑色节点),这让它们平衡了起来. 但现在通过P的路径比不通过P的路径都少了一个黑色节点,所以还需要在P上继续进行调整. 情况5S为黑色,它的左子节点为红色,右子节点为黑色.这种情况下,我们在S上做右旋转,这样S的左儿子成为S的父亲和N的新兄弟。我们接着交换S和它的新父亲的颜色。所有路径仍有同样数目的黑色节点，但是现在N有了一个右儿子是红色的黑色兄弟，所以我们进入了情况6。N和P都不受这个变换的影响。 情况6S是黑色，它的右子节点是红色,我们在N的父亲P上做左旋转.这样S成为N的父亲和S的右儿子的父亲。我们接着交换N的父亲和S的颜色，并使S的右儿子为黑色。子树在它的根上的仍是同样的颜色,但是,N现在增加了一个黑色祖先.所以,通过N的路径都增加了一个黑色节点.此时,如果一个路径不通过N,则有两种可能性: 它通过N的新兄弟.那么它以前和现在都必定通过S和N的父亲,而它们只是交换了颜色.所以路径保持了同样数目的黑色节点. 它通过N的新叔父,S的右儿子.那么它以前通过S、S的父亲和S的右儿子,但是现在只通过S,它被假定为它以前的父亲的颜色,和S的右儿子,它被从红色改变为黑色.合成效果是这个路径通过了同样数目的黑色节点. 在任何情况下,在这些路径上的黑色节点数目都没有改变.所以我们恢复了性质4.在示意图中的白色节点可以是红色或黑色,但是在变换前后都必须指定相同的颜色. 代码12345678910111213141516171819202122232425262728293031323334353637383940 private void fixAfterDeletion(Node x) &#123; while (x != null &amp;&amp; x != root &amp;&amp; colorOf(x) == BLACK) &#123; if (x == parentOf(x).left) &#123; x = successorIsLeftNode(x); &#125; else &#123; x = successorIsRightNode(x); &#125; &#125; setColor(x, BLACK); &#125; private Node successorIsLeftNode(Node x) &#123; Node brother = parentOf(x).right;// 情况2 if (colorOf(brother) == RED) &#123; rotateLeft(parentOf(x)); brother = parentOf(x).right; &#125;// 情况3,4 if (colorOf(brother.left) == BLACK &amp;&amp; colorOf(brother.right) == BLACK) &#123; x = brotherChildrenColorIsBlack(x, brother); &#125; else &#123; // 情况5 if (colorOf(brother.right) == BLACK) &#123; rotateRight(brother); brother = parentOf(x).right; &#125; // 情况6 setColor(brother.right, BLACK); rotateLeft(parentOf(x)); x = root; &#125; return x; &#125; private Node brotherChildrenColorIsBlack(Node x, Node brother) &#123; setColor(brother, RED); x = parentOf(x); return x; &#125; 参考资料 Wikipedia 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>Tree</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
        <tag>2017</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出排序算法(3)-快速排序]]></title>
    <url>%2F2017%2F06%2F14%2F2017-06-14-sort_algorithms_qucikSort%2F</url>
    <content type="text"><![CDATA[概述 快速排序与归并排序一样也是基于分治算法的排序算法.所以它的实现方法也与其他的分治算法一样,需要进行分解子任务,处理子任务,归并子任务这些步骤. 但快速排序与归并排序不同,它是一种原地排序算法(不需要额外的辅助数组),且快速排序不使用中间值来分解任务,而是使用划分函数. 算法过程 从数组中挑选出一个值,作为基准值 k. 重新排序序列,将所有小于k的值放到k前面,所有大于k的值放到k后面(也可以理解为将数组a切分为两个子数组a[begin...k-1],a[k+1...end],其中前一个子数组都小于k,后一个子数组都大于k). 递归地将两个子数组进行快速排序(递归到最底部时,子数组的大小是零或一,也就是已经排序好了.). 划分函数 划分函数就是上述步骤中的第二步,它将数组根据基准值进行重排序.根据基准值选择的位置不同,划分函数也有不同的实现方法,不过其根本思想都是将小于基准值的值放到前面,大于基准值的值放到后面. 使用末尾元素作为基准值 123456789101112131415161718// 使用末尾元素作为基准值来进行切分private static int partitionUseEnd(Comparable[] a, int begin, int end) &#123; Comparable pivot = a[end]; // 基准值,切分后的数组应满足左边都小于基准,右边都大于基准 int i = begin - 1; for (int j = begin; j &lt; end; j++) &#123; // 如果j小于基准值则与i交换 if (less(a[j], pivot)) &#123; i++; swap(a, i, j); &#125; &#125; // 将基准值交换到正确的位置上 int pivotLocation = i + 1; swap(a, pivotLocation, end); return pivotLocation;&#125; 使用首元素作为基准值 1234567891011121314151617181920212223242526272829// 使用首元素作为基准值来进行切分private static int partitionUseBegin(Comparable[] a, int begin, int end) &#123; Comparable pivot = a[begin]; int i = begin; int j = end + 1; while (true) &#123; // 从左向右扫描,直到找出一个大于等于基准的值 while (less(a[++i], pivot)) &#123; if (i &gt;= end) break; &#125; // 从右向左扫描,直到找出一个小于等于基准的值 while (less(pivot, a[--j])) &#123; if (j &lt;= begin) break; &#125; // 如果指针i与j发生碰撞则结束循环 if (i &gt;= j) break; // 将左边大于小于基准的值与右边小于等于基准的值进行交换 swap(a, i, j); &#125; // 将基准值交换到正确的位置上 swap(a, begin, j); return j;&#125; 代码实现 了解了划分函数的实现,剩下就只需要递归地调用快速排序不断地分解子任务即可. 注意,快速排序与归并排序不同,它不需要进行归并(划分后就已经是有序的了),并且是先进行划分函数,再分解任务. 123456789101112public static void sort(Comparable[] a) &#123; sort(a, 0, a.length - 1);&#125;private static void sort(Comparable[] a, int begin, int end) &#123; if (begin &gt;= end) return; int k = partitionUseEnd(a, begin, end); sort(a, begin, k - 1); sort(a, k + 1, end);&#125; 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>Algorithms</category>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>排序算法</tag>
        <tag>2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出排序算法(2)-归并排序]]></title>
    <url>%2F2017%2F06%2F12%2F2017-06-12-sort_algorithmes_mergeSort%2F</url>
    <content type="text"><![CDATA[概述 归并排序是基于分治算法实现的一种排序算法,它将数组分割为两个子数组,然后对子数组进行排序,最终将子数组归并为有序的数组. 归并排序的时间复杂度为O(n log n),空间复杂度为O(1),并且它是稳定的排序算法(所谓稳定即是不影响值相等元素的相对次序). 算法过程 首先,归并排序需要将一个大小为n个元素的数组分解为各包含n/2个元素的子数组(这个分解的过程会不断进行,直到子数组元素个数为1). 当子数组的元素个数为1时,代表这个子数组已经有序,开始两两归并(将两个个数为1的子数组归并为一个个数为2的子数组,不断归并,直到所有子数组个数为2,然后继续将两个个数为2的子数组归并为一个个数为4的子数组….以此类推). 不断重复步骤2,直到整个数组有序. 归并 通过以上的了解,我们发现归并排序中最重要的步骤就是归并. 采用类似洗牌的方式来理解这个过程.想象辅助数组为一个空牌堆,两个子数组为两堆牌a和b.我们从a堆与b堆中各取出一张牌进行比较,然后将较小的牌放入空牌堆中,不断重复比较直到任一牌堆为空.最后,再将未空的牌堆全部放入空牌堆中. 1234567891011121314151617181920212223242526// 将两个子序列进行归并private static void merge(Comparable[] a, int lo, int mid, int hi) &#123; Comparable[] aux = new Comparable[a.length]; // 辅助数组 int i = lo, j = mid + 1; int count = lo; // 对[lo...mid] 与 [mid+1...hi] 两个子序列的首元素进行比较,将较小的元素放入辅助数组 while (i &lt;= mid &amp;&amp; j &lt;= hi) &#123; if (less(a[i], a[j])) aux[count++] = a[i++]; else aux[count++] = a[j++]; &#125; //将[lo...mid] 与 [mid+1...hi] 两个子序列中剩余的元素放入辅助数组 while (i &lt;= mid) &#123; aux[count++] = a[i++]; &#125; while (j &lt;= hi) &#123; aux[count++] = a[j++]; &#125; // 将辅助数组中的元素复制到源数组中 for (int k = lo; k &lt;= hi; k++) &#123; a[k] = aux[k]; &#125;&#125; 递归实现 只要理解了归并的过程,剩下就很容易实现了.归并排序的递归实现如下. 12345678910111213141516 public static void sort(Comparable[] a) &#123; sort(a, 0, a.length - 1); &#125; // 递归实现归并排序 private static void sort(Comparable[] a, int lo, int hi) &#123; if (lo &gt;= hi) return; int mid = (lo + hi) &gt;&gt;&gt; 1; // (lo + hi) / 2// 分解数组 sort(a, lo, mid); sort(a, mid + 1, hi);// 归并 merge(a, lo, mid, hi); &#125; 非递归实现 我们已经知道了归并排序中最小子数组的元素个数为1,非递归实现只需要从1开始自底向上地归并即可(递归实现的真实计算过程也是如此,这是由于递归调用是后进先出的). 123456789101112131415161718192021222324252627282930313233343536373839404142434445 // 非递归实现归并排序 private static void sortUnRecursive(Comparable[] a) &#123; int len = 1; // 自底向上实现归并排序,子序列的最小粒度为1 while (len &lt; a.length) &#123; for (int i = 0; i &lt; a.length; i += len &lt;&lt; 1) &#123; merge(a, i, len); &#125; len = len &lt;&lt; 1; // 子序列规模每次迭代时乘2 &#125; &#125;// 与递归实现的归并函数不同,需要注意边界检查 private static void merge(Comparable[] a, int lo, int hi) &#123; int length = a.length; Comparable[] aux = new Comparable[length]; int count = lo; // 子数组1 int i = lo; int i_bound = lo + hi; // 子数组2 int j = i_bound; int j_bound = j + hi; // 注意j的边界检查 while (i &lt; i_bound &amp;&amp; j &lt; j_bound &amp;&amp; j &lt; length) &#123; if (less(a[i], a[j])) aux[count++] = a[i++]; else aux[count++] = a[j++]; &#125; // i和j都有可能越界 while (i &lt; i_bound &amp;&amp; i &lt; length) &#123; aux[count++] = a[i++]; &#125; while (j &lt; j_bound &amp;&amp; j &lt; length) &#123; aux[count++] = a[j++]; &#125; int k = lo; while (k &lt; j &amp;&amp; k &lt; length) &#123; a[k] = aux[k]; k++; &#125; &#125; 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>Algorithms</category>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>排序算法</tag>
        <tag>2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出排序算法(1)-堆排序]]></title>
    <url>%2F2017%2F06%2F09%2F2017-06-09-sort_algorithms_heapSort%2F</url>
    <content type="text"><![CDATA[概述 堆排序即是利用堆这个数据结构来完成排序的.所以,要想理解堆排序就要先了解堆. 堆 堆(Heap)是一种数据结构,它可以被看做是一棵树的数组对象.一个二叉堆拥有以下性质. 父节点k的左子节点在数组中的索引位置为2 * k + 1. 父节点k的右子节点在数组中的索引位置为2 * k + 2. 子节点i的父节点在数组中的索引位置为(i - 1) / 2. 父节点k的任意子节点都必须小于(或大于)k. 根节点必须是最大节点(或最小节点). 最大堆代码实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class MaxHeap&lt;T extends Comparable&gt; &#123; T[] heap; private MaxHeap() &#123; &#125; public MaxHeap(T[] heap) &#123; this.heap = heap; buildHeap(); &#125; /** * 自底向上构建堆 */ private void buildHeap() &#123; int length = heap.length; // 当堆为空或者长度为1时不需要任何操作 if (length &lt;= 1) return; int root = (length - 2) &gt;&gt;&gt; 1; // (i - 1) / 2 while (root &gt;= 0) &#123; heapify(heap, length, root); root--; &#125; &#125; /** * 调整堆的结构 * * @param heap 堆 * @param length 堆的长度 * @param root 根节点索引 */ public void heapify(T[] heap, int length, int root) &#123; if (root &gt;= length) return; int largest = root; // 表示root,left,right中最大值的变量 int left = (root &lt;&lt; 1) + 1; // 左子节点,root * 2 + 1 int right = left + 1; // 右子节点,root * 2 + 2 // 找出最大值 if (left &lt; length &amp;&amp; greater(heap[left], heap[largest])) largest = left; if (right &lt; length &amp;&amp; greater(heap[right], heap[largest])) largest = right; // 如果largest发生变化,将largest与root交换 if (largest != root) &#123; T t = heap[root]; heap[root] = heap[largest]; heap[largest] = t; // 继续向下调整堆 heapify(heap, length, largest); &#125; &#125; private boolean greater(Comparable a, Comparable b) &#123; return a.compareTo(b) &gt; 0; &#125;&#125; 优先队列 普通的队列是基于先进先出的,也就是说最先入队的元素永远是在第一位,而优先队列中的每一个元素都是拥有优先级的,优先级最高的元素永远在第一位. 优先队列也是贪心算法的体现,所谓的贪心算法即是在问题求解的每一步中总是选择当前最好的结果. 堆就是用于实现优先队列的,因为堆的性质与优先队列十分吻合. 添加 往优先队列中添加元素时,我们只需要将元素添加到数组末尾并调整堆(以下例子均是以最大堆为例). 1234567891011121314151617181920212223242526272829 public boolean add(T t) &#123; if (t == null) throw new NullPointerException(); if (size == queue.length) resize(queue.length * 2); int i = size; // 如果当前队列为空,则不需要进行堆调整直接插入元素即可 if (i == 0) queue[0] = t; else swim(i, t); size++; return true; &#125;// 上浮调整 private void swim(int i, T t) &#123; Comparable&lt;? super T&gt; key = (Comparable) t; while (i &gt; 0) &#123; int parent = (i - 1) &gt;&gt;&gt; 1; T p = (T) queue[parent]; // 如果key小于他的父节点(符合最大堆规则)则结束调整 if (key.compareTo(p) &lt; 0) break; queue[i] = p; i = parent; &#125; queue[i] = key; &#125; 删除 删除操作要稍微麻烦一点,将优先队列中末尾的元素放到队头并进行堆调整. 12345678910111213141516171819202122232425262728293031323334 public T poll() &#123; if (isEmpty()) return null; int s = --size; Object result = queue[0]; Object end = queue[s]; queue[s] = null; if (s != 0) sink(0, (T) end); if (size &lt;= queue.length / 4) resize(queue.length / 2); return (T) result; &#125;// 下沉调整 private void sink(int i, T t) &#123; Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;) t; int half = size &gt;&gt;&gt; 1; while (i &lt; half) &#123; int child = (i &lt;&lt; 1) + 1; // 左子节点 int right = child + 1; // 右子节点 T max = (T) queue[child]; // find maximum element if (right &lt; size &amp;&amp; ((Comparable&lt;? super T&gt;) max).compareTo((T) queue[right]) &lt; 0) max = (T) queue[child = right]; // key大于它的最大子节点(符合最大堆规则)则结束调整 if (key.compareTo(max) &gt; 0) break; queue[i] = max; i = child; &#125; queue[i] = key; &#125; 点击查看优先队列完整代码 堆排序 实现堆排序有两种方法,一种是使用优先队列,另一种是直接使用堆. 直接使用堆实现堆排序 123456789101112// 使用最大堆实现堆排序private static void maxHeapSort(Comparable[] a) &#123; MaxHeap&lt;Comparable&gt; maxHeap = new MaxHeap&lt;&gt;(a); //不断地将最大堆中顶端元素(最大值)与最底部的元素(最小值)交换 for (int i = a.length - 1; i &gt; 0; i--) &#123; Comparable largest = a[0]; a[0] = a[i]; a[i] = largest; // 堆减少,并调整新的堆 maxHeap.heapify(a, i, 0); &#125;&#125; 使用优先队列实现堆排序12345678910// 使用优先队列实现堆排序private static void pqSort(Comparable[] a) &#123; MinPriorityQueue&lt;Comparable&gt; priorityQueue = new MinPriorityQueue&lt;&gt;(); for (int i = 0; i &lt; a.length; i++) &#123; priorityQueue.add(a[i]); &#125; for (int i = 0; i &lt; a.length; i++) &#123; a[i] = priorityQueue.poll(); &#125;&#125; 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>Algorithms</category>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>排序算法</tag>
        <tag>2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IoC与AOP的那点事儿]]></title>
    <url>%2F2017%2F06%2F07%2F2017-06-07-IoC%26AOP%2F</url>
    <content type="text"><![CDATA[IoC 控制反转(Inversion of Control)是OOP中的一种设计原则,也是Spring框架的核心.大多数应用程序的业务逻辑代码都需要两个或多个类进行合作完成的,通过IoC则可以减少它们之间的耦合度. 实现方法 IoC的主要实现方法有两种,依赖注入与依赖查找. 依赖注入 : 应用程序被动的接收对象,IoC容器通过类型或名称等信息来判断将不同的对象注入到不同的属性中. 依赖注入主要有以下的方式: 基于set方法 : 实现特定属性的public set()方法,来让IoC容器调用注入所依赖类型的对象. 基于接口 : 实现特定接口以供IoC容器注入所依赖类型的对象. 基于构造函数 : 实现特定参数的构造函数,在创建对象时来让IoC容器注入所依赖类型的对象. 基于注解 : 通过Java的注解机制来让IoC容器注入所依赖类型的对象,例如Spring框架中的@Autowired. 依赖查找 : 它相对于依赖注入而言是一种更为主动的方法,它会在需要的时候通过调用框架提供的方法来获取对象,获取时需要提供相关的配置文件路径、key等信息来确定获取对象的状态. IoC的思想 在传统实现中,我们都是通过应用程序自己来管理依赖的创建,例如下代码. 123456789public class Person &#123; // 由Person自己管理Food类的创建 public void eat() &#123; Food food = new Chicken(); System.out.println("I am eating " + food.getName() + "..."); &#125;&#125; 而IoC则是通过一个第三方容器来管理并维护这些被依赖对象,应用程序只需要接收并使用IoC容器注入的对象而不需要关注其他事情. 1234567891011121314public class Person &#123; private Food food; // 通过set注入 public void setFood(Food food) &#123; this.food = food; &#125; // Person不需要关注Food,只管使用即可 public void eat() &#123; System.out.println("I am eating " + this.food.getName() + "..."); &#125;&#125; 通过以上的例子我们能够发现,控制反转其实就是对象控制权的转移,应用程序将对象的控制权转移给了第三方容器并通过它来管理这些被依赖对象,完成了应用程序与被依赖对象的解耦. AOP AOP(Aspect-Oriented Programming)即面向方面编程.它是一种在运行时,动态地将代码切入到类的指定方法、指定位置上的编程思想.用于切入到指定类指定方法的代码片段叫做切面,而切入到哪些类中的哪些方法叫做切入点. AOP是OOP的有益补充,OOP从横向上区分出了一个个类,AOP则从纵向上向指定类的指定方法中动态地切入代码.它使OOP变得更加立体. Java中的动态代理或CGLib就是AOP的体现. 案例分析 在OOP中,我们使用封装的特性来将不同职责的代码抽象到不同的类中.但是在分散代码的同时,也增加了代码的重复性. 例如,我们需要在两个或多个类中的方法都记录日志或执行时间,可能这些代码是完全一致的,但因为类与类无法联系造成代码重复. 12345678910111213141516171819202122232425public class A &#123; public void something () &#123; // 业务逻辑... recordLog(); &#125; private void recordLog() &#123; // 记录日志... &#125;&#125;public class B &#123; public void something () &#123; // 业务逻辑... recordLog(); &#125; private void recordLog() &#123; // 记录日志... &#125;&#125; 接下来,我们采取两种不同方案来改进这段代码. 将重复代码抽离到一个类中 12345678910111213141516171819202122232425public class A &#123; public void something () &#123; // 业务逻辑... Report.recordLog(); &#125;&#125;public class B &#123; public void something () &#123; // 业务逻辑... Report.recordLog(); &#125;&#125;public class Report &#123; public static void recordLog (String ...messages) &#123; // 记录日志... &#125;&#125; 这样看似解决了问题,但类之间已经耦合了.并且当这些外围业务代码(日志,权限校验等)越来越多时,它们的侵入(与核心业务代码混在一起)会使代码的整洁度变得混乱不堪. 使用AOP分离外围业务代码 我们使用AspectJ,它是一个AOP框架,扩展了Java语言,并定义了AOP语法(通过它实现的编译器). 使用AspectJ需要先安装并将lib中aspectjrt.jar添加进入classpath,下载地址. 12345678910111213141516171819202122232425262728public class Something &#123; public void say() &#123; System.out.println("Say something..."); &#125; public static void main(String[] args) &#123; Something something = new Something(); something.say(); &#125;&#125;public aspect SomethingAspect &#123; /** * 切入点,切入到Something.say() */ pointcut recordLog():call(* com.sun.sylvanas.application.hello_aop.Something.say(..)); /** * 在方法执行后执行 */ after():recordLog() &#123; System.out.println("[AFTER] Record log..."); &#125;&#125; AOP解决了代码的重复并将这些外围业务代码抽离到一个切面中,我们可以动态地将切面切入到切入点. 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈如何实现一个非阻塞的线程安全的集合]]></title>
    <url>%2F2017%2F05%2F29%2F2017-5-29-cas_concurrent_stack%2F</url>
    <content type="text"><![CDATA[概述 众所周知,想要在java中要实现一个线程安全的类有很多方法.最简单直接的即是使用synchronized关键字或ReentrantLock. 但是,这两种同步方法都是基于锁的,基于锁的同步方法是阻塞的,即未争夺到锁的线程需要阻塞等待(或挂起)直到锁可用. 这种方法具有一些明显的缺点: 被阻塞的线程无法去做任何其他事情,如果这个线程是优先级较高的线程甚至会发生非常不好的结果(优先级倒置). 由于java的线程模型是基于内核线程实现的,挂起恢复线程需要来回地切换到内核态,性能开销很大. 当两个(或多个)线程都阻塞着等待另一方释放锁时,将会引发死锁. 那么有非阻塞的方法来实现同步吗?(volatile关键字也是非阻塞的,但它只保证了数据的可见性与有序性,并不保证原子性) 有!在jdk5中,java增加了大量的原子类来保证无锁下的操作原子性,可以说java.util.concurrent包下的所有类都几乎用到了这些原子类. CAS 这些原子类都是基于CAS实现的,CAS即是Compare And Swap,它的原理简单来讲就是在更新新值之前先去比较原值有没有发生变化,如果没发生变化则进行更新. java中的CAS是通过Unsafe类中的本地方法实现的,而这些本地方法需要通过现代处理器提供的CAS指令实现(在Intel处理器中该指令为cmpxchg). 所以我们发现,CAS操作的原子性是由处理器来保证的. 比较的过程在CAS操作中包含了三个数,V(内存位置),A(预期值),B(新值). 首先会将V与A进行匹配. 如果两个值相等,则使用B作为新值进行更新. 如果不相等,则不进行更新操作(一般的补救措施是继续进行请求). 与锁相比的优点 CAS操作是无锁的实现,所以它不会发生死锁情况. 虽然CAS操作失败需要不断的进行请求重试,但相对于不断地挂起或恢复线程来说,性能开销要低得多. CAS的粒度更细,操作也更加轻量与灵活. ConcurrentStack 我们通过实现一个简单的ConcurentStack来看看CAS操作是如何保证线程安全的. 完整代码请从作者的Gist中获取 节点的实现12345678910111213141516171819public class ConcurrentStack&lt;E&gt; implements Iterable&lt;E&gt; &#123; private AtomicReference&lt;Node&lt;E&gt;&gt; head = new AtomicReference&lt;&gt;(null); private AtomicInteger size = new AtomicInteger(0); /** * This internal static class represents the nodes in the stack. */ private static class Node&lt;E&gt; &#123; private final E value; private volatile Node&lt;E&gt; next; private Node(E value, Node&lt;E&gt; next) &#123; this.value = value; this.next = next; &#125; &#125;&#125; pushpush函数主要是通过观察头节点(这里的头节点即是V),然后构建一个新的节点(它代表B)放于栈顶,如果V没有发生变化,则进行更新.如果发生了变化(被其他线程修改),就重新尝试进行CAS操作. 12345678910111213141516171819202122/** * Insert a new element to the this stack. * * @return if &#123;@code true&#125; insert success,&#123;@code false&#125; otherwise * @throws IllegalArgumentException if &#123;@code value&#125; is null */public boolean put(E value) &#123; if (value == null) throw new IllegalArgumentException(); return putAndReturnResult(value);&#125;private boolean putAndReturnResult(E value) &#123; Node&lt;E&gt; oldNode; Node&lt;E&gt; newNode; do &#123; oldNode = head.get(); newNode = new Node&lt;E&gt;(value, oldNode); &#125; while (!head.compareAndSet(oldNode, newNode)); sizePlusOne(); return true;&#125; poppop函数中的CAS操作的思想基本与push函数一致. 1234567891011121314151617181920212223/** * Return the element of stack top and remove this element. * * @throws NullPointerException if this stack is empty */public E pop() &#123; if (isEmpty()) throw new NullPointerException(); return removeAndReturnElement();&#125;private E removeAndReturnElement() &#123; Node&lt;E&gt; oldNode; Node&lt;E&gt; newNode; E result; do &#123; oldNode = head.get(); newNode = oldNode.next; result = oldNode.value; &#125; while (!head.compareAndSet(oldNode, newNode)); sizeMinusOne(); return result;&#125; end 非阻塞的算法实现的复杂度要比阻塞算法复杂的多,但它能带来更少的性能开销,在jdk中,很多线程安全类都是在尽量地避免使用锁的基础上来实现线程安全. 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>后端</category>
        <category>多线程</category>
        <category>CAS</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>多线程</tag>
        <tag>CAS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Algorithms,4th Edition》读书笔记-散列表]]></title>
    <url>%2F2017%2F04%2F13%2F2017-4-13-hash_table%2F</url>
    <content type="text"><![CDATA[概述 散列表(Hash Table,也叫哈希表),它是根据键而直接访问在内存存储位置的数据结构.也可以说是用一个数组来实现的无序的符号表,将键作为数组的索引而数组中键i处存储的就是它对应的值. 散列表通过散列函数将键转化为数组的索引来访问数组中的键值对. 在散列表的算法中,最重要的两个操作如下. 使用散列函数将被查找的键转化为数组的一个索引. 处理散列表中的碰撞冲突问题. 性质 若关键字为k,则其值存放于的存储位置上.由此,不需要比较便可直接取得所查记录.称这个对应关系为散列函数,按照这个思想建立的符合表为散列表. 对不同的键可能会得到同一个散列地址,即,而,这种现象被称为碰撞冲突.具有相同函数值的键对该散列函数来说称做同义词.综上所述,根据散列函数和处理碰撞冲突的方法将一组键映射到一个有限的连续的地址集(区间)上,这种表称为散列表,这一映射过程称为散列,所得的存储位置称为散列地址. 若对于键集合中的任一个键,经散列函数映射到地址集合中任何一个地址的概率是相等的,则这个散列函数被称为均匀散列函数,它可以减少碰撞冲突. 散列函数 散列函数用于将键转化为数组的索引.如果我们有一个能够保存M个键值对的数组,那么我们就需要一个能够将任意键转化为该数组范围内的索引([0,M-1]范围内的整数)的散列函数 散列函数与键的类型有关,对于每种类型的键都需要一个与之对应的散列函数. 实现散列函数的几种方法 直接定址法 : 取key或者key的某个线性函数值为散列地址.即或其中a,b为常数(这种散列函数叫做自身函数). 数字分析法 : 假设key是以r为基的数,并且散列表中可能出现的key都是事先知道的,则可取key的若干数位组成散列地址. 平方取中法 : 取key平方后的中间几位为散列地址.通常在选定散列函数时不一定能知道key的全部情况,取其中的哪几位也不一定合适,而一个数平方后的中间几位数和数的每一位都相关,由此使随机分布的key得到的散列地址也是随机的.取的位数由表长决定. 折叠法 : 将key分割成位数相同的几部分(最后一部分的位数可以不同),然后取这几部分的叠加和(舍去进位)作为散列地址. 除留余数法 : 取key被某个不大于散列表长度m的数p除后所得的余数为散列地址.即,.不仅可以对key直接取模，也可在折叠法、平方取中法等运算之后取模。对p的选择很重要，一般取素数或m，若p选择不好，容易产生碰撞冲突. 正整数将正整数散列一般使用的是除留余数法.我们选择大小为素数M的数组,对于任意正整数k,计算k除以M的余数(即k%M).它能够有效地将key散布在0到M-1的范围内. 如果M不是素数,可能无法利用key中包含的所有信息,这可能导致无法均匀地散列散列值. 浮点数对浮点数进行散列一般是将key表示为二进制数然后再使用除留余数法. 字符串除留余数法也可以处理较长的key,例如字符串,我们只需将它们当成大整数即可. 12345int hash = 0;for (int i = 0; i &lt; s.length(); i++) &#123; hash = (R * hash + s.charAt(i)) % M;&#125; Java的charAt()函数能够返回一个char值,即一个非负16位整数.如果R比任何字符的值都大,这种计算相当于将字符串当作一个N位的R进制值,将它除以M并取余.只要R足够小,不造成溢出,那么结果就能够落在0至M-1之间.可以使用一个较小的素数,例如31. 组合键如果key的类型含有多个整型变量,我们可以和字符串类型一样将它们混合起来. 例如,key的类型为Date,其中含有几个整型的域 : day(两个数字表示的日),month(两个数字表示的月),year(四个数字表示的年).我们可以这样计算它的散列值: 1int hash = (((day * R + month) % M) * R + year) % M; Java中的约定在Java中如果要为自定义的数据类型定义散列函数,需要同时重写hashCode()和equals()两个函数,并要遵守以下规则. hashCode()与equals()的结果必须保持一致性.即a.equals(b)返回true,则a.hashCode()的返回值也必然和b.hashCode()的返回值相同. 但如果两个对象的hashCode()函数的返回值相同,这两个对象也有可能不同,还需要用equals()函数进行判断. 一个使用除留余数法的简单散列函数如下,它会将符号位屏蔽(将一个32位整数变为一个31位非负整数). 123private int hash(Key key) &#123; return (key.hashCode() &amp; 0x7fffffff) % M;&#125; 软缓存由于散列函数的计算有可能会很耗时,我们可以进行缓存优化,将每个key的散列值缓存起来(可以在每个key中使用一个hash变量来保存它的hashCode()的返回值). 当第一次调用hashCode()时,需要计算对象的散列值,但之后对hashCode()方法的调用会直接返回hash变量的值. 总结总之,要想实现一个优秀的散列函数需要满足以下的条件. 一致性,等价的key必然产生相等的散列值. .高效性,计算简便. 均匀性,均匀地散列所有的key. 基于拉链法的散列表 拉链法是解决碰撞冲突的一种策略,它的核心思想是 : 将大小为M的数组中的每个元素指向一条链表,链表中的每个节点都存储了散列值为该元素的索引的键值对. 拉链法的实现一般分为以下两种: 使用一个原始的链表数据类型来表示数组中的每个元素. 使用一个符号表实现来表示数组中的每个元素(这个方法实现简单但效率偏低). 12345678910111213141516171819202122232425262728public class SeparateChainingHashST&lt;K, V&gt; &#123; private static final int INIT_CAPACITY = 4; private int n; // the number of key-value pairs in the symbol table private int m; // the number of size of separate chaining table private Node&lt;K, V&gt;[] table; // array of linked-list symbol tables private class Node&lt;K, V&gt; &#123; private K key; private V value; private Node&lt;K,V&gt; next; public Node() &#123; &#125; public Node(K key, V value, Node next) &#123; this.key = key; this.value = value; this.next = next; &#125; &#125; private int hash(K key) &#123; return ((key.hashCode()) &amp; 0x7fffffff) % m; &#125;&#125; 查找、插入、删除基于拉链法的散列表的查找、插入、删除算法基本分为两步: 首先根据散列值找到对应的链表. 然后沿着这条链表进行相应的操作. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public V get(K key) &#123; if (key == null) throw new IllegalArgumentException("called get() with key is null."); int i = hash(key); Node x = table[i]; while (x != null) &#123; if (key.equals(x.key)) return (V) x.value; x = x.next; &#125; return null;&#125; public void put(K key, V value) &#123; if (key == null) throw new IllegalArgumentException("called put() with key is null."); if (value == null) &#123; remove(key); return; &#125; // double table size if average length of list &gt;= 10 if (n &gt;= 10 * m) resize(2 * m); int i = hash(key); Node x = table[i]; Node p = null; while (x != null) &#123; if (key.equals(x.key)) &#123; x.value = value; return; &#125; p = x; x = x.next; &#125; if (p == null) &#123; table[i] = new Node(key, value, null); n++; &#125; else &#123; p.next = new Node(key, value, null); n++; &#125;&#125; public V remove(K key) &#123; if (key == null) throw new IllegalArgumentException("called remove() with key is null."); if (isEmpty()) throw new NoSuchElementException("called remove() with empty symbol table."); if (!contains(key)) return null; int i = hash(key); Node x = table[i]; Node p = null; V oldValue = null; while (x != null) &#123; if (key.equals(x.key)) &#123; oldValue = (V) x.value; if (p == null) &#123; table[i] = x.next; &#125; else &#123; p.next = x.next; &#125; n--; break; &#125; p = x; x = x.next; &#125; // halve table size if average length of list &lt;= 2 if (m &gt; INIT_CAPACITY &amp;&amp; n &lt;= 2 * m) resize(m / 2); return oldValue;&#125; 基于线性探测法的散列表 解决碰撞冲突的另一种策略是使用线性探测法.它的核心思想是: 使用大小为M的数组保存N个键值对,其中M&gt;N.这种方法需要依靠数组中的空位来解决碰撞冲突,基于这种策略的所有方法被统称为开放地址散列表. 开放地址散列表中最简单的方法就是线性探测法: 当发生碰撞冲突时,我们直接检查散列表中的下一个位置(将索引值加1).它可能会产生三种结果: 命中,该位置的key和被查找的key相同. 未命中,key为空(该位置没有key). 继续查找,该位置的key和被查找的key不同. 我们使用散列函数找到key在数组中的索引,检查其中的key和被查找的key是否相同.如果不同则继续查找(将索引值加1,到达数组结尾时折回数组的开头),直到找到该key或者遇到一个空元素. 开放地址散列表的核心思想是: 与其将内存用作链表,不如将它们作为在散列表的空元素(这些空元素可以作为查找结束的标识). 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class LinearProbingHashST&lt;K, V&gt; &#123; private static final int INIT_CAPACITY = 4; private int n; // the number of key-value pairs in the symbol table private int m; // the number of size of linear probing table private K[] keys; // the keys private V[] vals; // the values /** * Initializes an empty symbol table. */ public LinearProbingHashST() &#123; this(INIT_CAPACITY); &#125; /** * Initializes an empty symbol table with the specified initial capacity. * * @param capacity the initial capacity */ public LinearProbingHashST(int capacity) &#123; m = capacity; n = 0; keys = (K[]) new Object[m]; vals = (V[]) new Object[m]; &#125; public V get(K key) &#123; if (key == null) throw new IllegalArgumentException("called get() with key is null."); for (int i = hash(key); keys[i] != null; i = (i + 1) % m) &#123; if (keys[i].equals(key)) return vals[i]; &#125; return null; &#125; public void put(K key, V value) &#123; if (key == null) throw new IllegalArgumentException("called put() with key is null."); if (value == null) &#123; delete(key); return; &#125; // double table size if 50% full if (n &gt;= m / 2) resize(2 * m); int i; for (i = hash(key); keys[i] != null; i = (i + 1) % m) &#123; if (keys[i].equals(key)) &#123; vals[i] = value; return; &#125; &#125; keys[i] = key; vals[i] = value; n++; &#125; &#125; 删除基于线性探测法的散列表的删除操作较为复杂,我们不能直接将key所在的位置设为null,这样会使在此位置之后的元素无法被查找到. 因此,我们需要将被删除键的右侧的所有键重新插入到散列表中. 1234567891011121314151617181920212223242526272829303132333435363738public V delete(K key) &#123; if (key == null) throw new IllegalArgumentException("called delete() with key is null."); if (isEmpty()) throw new NoSuchElementException("called delete() with empty symbol table."); if (!contains(key)) return null; // find position i of key int i = hash(key); while (!key.equals(keys[i])) &#123; i = (i + 1) % m; &#125; V oldValue = vals[i]; // delete key and associated value keys[i] = null; vals[i] = null; // rehash all keys in same cluster i = (i + 1) % m; while (keys[i] != null) &#123; // delete keys[i] an vals[i] and reinsert K keyToRehash = keys[i]; V valToRehash = vals[i]; keys[i] = null; vals[i] = null; n--; put(keyToRehash, valToRehash); i = (i + 1) % m; &#125; n--; // halves size of array if it's 12.5% full or less if (n &gt; 0 &amp;&amp; n &lt;= m / 8) resize(m / 2); assert check(); return oldValue;&#125; 键簇线性探测法的平均成本取决于元素在插入数组后聚集成的一组连续的条目,也叫作键簇. 显然,短小的键簇才能保证较高的效率.随着插入的key越来越多,这个要求会很难满足,较长的键簇会越来越多.长键簇的可能性要比短键簇更大,因为新键的散列值无论落在键簇的任何位置都会使它的长度加1. 总结 散列表使用了适度的空间和时间并在这两个极端之间找到了一种平衡,所以它可以在一般应用中实现拥有(均摊后)常数级别的查找和插入操作的符号表. 但散列表是很难实现有序操作的,这是因为散列最主要的目的在于均匀地将键散布开来,因此在计算散列后键的顺序信息就已经丢失了. 同时,散列表的性能也依赖于α=N/M的比值,其中α称为散列表的使用率.对于拉链法来说,α是每条链表的长度,因此一般大于1.对于线性探测法来说,α是表中已被占用的空间的比例,它是不可能大于1的. 散列表的性能虽然高效,但它也有以下的局限性: 每种类型的键都需要一个优秀的散列函数. 性能保证来自于散列函数的质量. 散列函数的计算可能复杂而且昂贵. 难以支持有序性相关的操作. end Author : SylvanasSun Email : sylvanassun_xtz@163.com 文中的完整实现代码见我的GitHub &amp; Gist 文中参考资料引用自&lt;&gt; &amp; Wikepedia]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>HashTable</category>
      </categories>
      <tags>
        <tag>HashTable</tag>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
        <tag>2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[平衡查找树之AVL树]]></title>
    <url>%2F2017%2F04%2F08%2F2017-4-08-avl_tree%2F</url>
    <content type="text"><![CDATA[概述 AVL树得名于它的发明者G.M. Adelson-Velsky和E.M. Landis,它是最先发明的自平衡二叉查找树. 在AVL树中任何节点的两个子树的高度最大差别为一.并且,查找、插入、删除等操作在平均和最坏情况下都是O(log n). AVL树的基本操作都与二叉查找树的算法一致,只有在插入、删除等这种会改变树的平衡性的操作需要使用一些旋转操作来修正树的平衡性. 平衡因子 节点的平衡因子一般是它的左子树的高度减去它的右子树的高度(相反也可以).带有平衡因子为1、0或-1的节点被认为是平衡的.带有平衡因子为-2或2的节点被认为是不平衡的. 计算树的高度与平衡因子的代码如下. 12345678910111213141516171819202122// calculate node x depthprivate int calcDepth(Node x) &#123; int depth = 0; if (x.left != null) depth = x.left.depth; if (x.right != null &amp;&amp; x.right.depth &gt; depth) depth = x.right.depth; // parent + left or right depth depth++; return depth;&#125;// calculate node x balance(left.depth - right.depth)private int calcBalance(Node x) &#123; int leftDepth = 0; int rightDepth = 0; if (x.left != null) leftDepth = x.left.depth; if (x.right != null) rightDepth = x.right.depth; return leftDepth - rightDepth;&#125; 旋转 旋转操作是用于修复树的平衡性的,它保证了树的有序性与平衡性(旋转操作的具体讲解可以参考《Algorithms,4th Edition》读书笔记-红黑二叉查找树). 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051private Node rotateLeft(Node x) &#123; Node t = x.right; x.right = t.left; t.left = x; if (x.parent != null) &#123; t.parent = x.parent; if (x.parent.left == x) x.parent.left = t; else x.parent.right = t; &#125; else &#123; t.parent = null; root = t; &#125; x.parent = t; // calculate depth and balance x.depth = calcDepth(x); x.balance = calcBalance(x); t.depth = calcDepth(t); t.balance = calcBalance(t); // calculate size t.size = x.size; x.size = 1 + size(x.left) + size(x.right); return t;&#125;private Node rotateRight(Node x) &#123; Node t = x.left; x.left = t.right; t.right = x; if (x.parent != null) &#123; t.parent = x.parent; if (x.parent.left == x) x.parent.left = t; else x.parent.right = t; &#125; else &#123; t.parent = null; root = t; &#125; x.parent = t; // calculate depth and balance x.depth = calcDepth(x); x.balance = calcBalance(x); t.depth = calcDepth(t); t.balance = calcBalance(t); // calculate size t.size = x.size; x.size = 1 + size(x.left) + size(x.right); return t;&#125; 平衡修正 当一个节点被认为是不平衡的时候,我们需要使用一些旋转操作来修正树的平衡,一般有以下情况需要进行旋转. 例如当前节点为x,对x进行平衡修正需要进行以下判断. 当x的平衡因子大于等于2时(左子树高度偏高),对其进行右旋转. 当x的左子树的平衡因子等于-1时(左子树的右子节点高度偏高),对x的左子树进行左旋转. 当x的平衡因子小于等于-2时(右子树高度偏高),对其进行左旋转. 当x的右子树的平衡因子等于1时(右子树的左子节点高度偏高),对x的右子树进行右旋转. 123456789101112131415161718192021222324private void balance(Node x) &#123; while (x != null) &#123; x.depth = calcDepth(x); x.balance = calcBalance(x); // if x left subtree high,rotateRight if (x.balance &gt;= 2) &#123; // if x.left.right high,rotateLeft if (x.left != null &amp;&amp; x.left.balance == -1) &#123; x.left = rotateLeft(x.left); &#125; x = rotateRight(x); &#125; // if x right subtree high,rotateLeft if (x.balance &lt;= -2) &#123; // if x.right.left high,rotateRight if (x.right != null &amp;&amp; x.right.balance == 1) &#123; x.right = rotateRight(x.right); &#125; x = rotateLeft(x); &#125; x.size = 1 + size(x.left) + size(x.right); x = x.parent; &#125;&#125; 插入 AVL树的插入和删除与二分查找树的算法一致,只不过在完成插入后需要自底向上的修复平衡性. 123456789101112131415161718192021222324252627282930313233343536373839public void put(K key, V value) &#123; if (key == null) throw new IllegalArgumentException("called put() with key is null."); if (value == null) &#123; remove(key); return; &#125; put(root, key, value);&#125;private void put(Node x, K key, V value) &#123; Node parent = x; int cmp = 0; while (x != null) &#123; parent = x; cmp = key.compareTo(x.key); if (cmp &lt; 0) &#123; x = x.left; &#125; else if (cmp &gt; 0) &#123; x = x.right; &#125; else &#123; x.value = value; return; &#125; &#125; // if not find key,create new node x = new Node(key, value, 1, parent); if (parent != null) &#123; if (cmp &lt; 0) parent.left = x; else parent.right = x; &#125; else &#123; root = x; &#125; // fixup balance balance(x);&#125; end Author : SylvanasSun Email : sylvanassun_xtz@163.com 文中的完整实现代码见我的GitHub &amp; Gist 本文参考资料引用自Wikipedia]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>Tree</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
        <tag>2017</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Algorithms,4th Edition》读书笔记-红黑二叉查找树]]></title>
    <url>%2F2017%2F03%2F30%2F2017-3-30-red_black_binary_search_tree%2F</url>
    <content type="text"><![CDATA[红黑二叉查找树是2-3查找树的简单表示方式,它的代码量并不大,并且保证了平衡性.阅读本文前需先了解 《Algorithms,4th Edition》读书笔记-2-3查找树 概述 红黑树是一种自平衡的二叉查找树,它的基本思想是用标准的二叉查找树(完全由2-节点构成)和一些额外的信息(替换3-节点)来表示2-3树. 可以说红黑树是2-3树的一种等同. 红黑树中的链接可以分为两种类型: 红链接 : 它将两个2-节点连接起来构成一个3-节点(也可以说是将3-节点表示为由一条红色左链接(两个2-节点其中之一是另一个的左子节点)相连的两个2-节点). 黑链接 : 表示2-3树中的普通链接. 这种表示方式带来的优点如下: 无需修改就可以直接使用标准的二叉查找树中的查找方法(其他与链接颜色不关联的方法也可以直接使用). 对于任意的2-3树,只要对节点进行转换,我们都可以立即派生出一棵对应的二叉查找树. 红黑树的性质 红黑树是含有红黑链接并满足下列条件的二叉查找树(满足这些条件的红黑树才是与相应的2-3树一一对应的). 红链接均为左链接(这条仅限于偏向左红链接实现的红黑树). 每个节点不是红色就是黑色的. 没有任何一个节点同时和两条红链接相连(不可以有两条连续的红链接). 该树是完美黑色平衡的,即任意空链接到根节点的路径上的黑链接数量相同. 根节点是黑色的. 所有叶子节点(即null节点)的颜色是黑色的. 与2-3树的对应关系 假如我们将一棵红黑树中的红链接画平,我们会发现所有的空链接到根节点的距离都将是相同的.如果再把由红链接相连的节点合并,得到的就是一棵2-3树. 相对的,如果将一棵2-3树中的3-节点画作由红色左链接相连的两个2-节点,那么不会存在能够和两条红链接相连的节点,且树必然是完美黑色平衡的,因为黑链接就是2-3树中的普通链接,根据定义这些链接必然是完美平衡的. 通过这些结论,我们可以发现红黑树即是二叉查找树,也是2-3树. 节点的实现 我们使用boolean类型的变量color来表示链接的颜色.如果指向它的链接为红色,则color变量为true,黑色则为false(空链接也为黑色). 并且定义了一个isRed()函数用于判断链接的颜色. 这里节点的颜色指的是指向该节点的链接的颜色. 12345678910111213141516171819202122232425 private static final boolean RED = true; private static final boolean BLACK = false; private Node root; // root node private class Node &#123; private K key; private V value; private Node left, right; // links to left and right subtress private boolean color; // color of parent link private int size; // subtree count public Node(K key, V value, boolean color, int size) &#123; this.key = key; this.value = value; this.color = color; this.size = size; &#125; &#125; // node x is red? if x is null return false. private boolean isRed(Node x) &#123; if (x == null) return false; return x.color == RED; &#125; 旋转 当我们在实现某些操作时,可能会产生一些红色右链接或者两条连续的红色左链接.这时就需要在操作完成前进行旋转操作来修复红黑树的平衡性(旋转操作会改变红链接的指向). 旋转操作保证了红黑树的两个重要性质 : 有序性和完美平衡性. 左旋转假设当前有一条红色右链接需要被修正旋转为左链接.这个操作叫做左旋转. 左旋转函数接受一条指向红黑树中的某个节点的链接作为参数.然后会对树进行必要的调整并返回一个指向包含同一组键的子树且其左链接为红色的根节点的链接. 也可以认为是将用两个键中的较小者作为根节点变为将较大者作为根节点(右旋转中逻辑相反). 旋转操作返回的链接可能是左链接也可能是右链接,这个链接可能是红色也可能是黑色的(在实现中我们使用x.color = h.color保留了它原本的颜色).这可能会产生两条连续的红链接,但算法会在后续操作中继续使用旋转操作修正这种情况. 旋转操作只影响了根节点(返回的节点的子树中的所有键和旋转前都相同,只有根节点发生了变化). 具体的实现如下图: 右旋转实现右旋转的逻辑基本与左旋转相同,只需要将left和right互换即可. 颜色转换 颜色转换操作也是用于保证红黑树的性质的.它将父节点的颜色由黑变红,将子节点的颜色由红变黑. 这项操作与旋转操作一样是局部变换,不会影响整棵树的黑色平衡性. 根节点总是为黑颜色转换可能会使根节点变为红色,但红色的根节点说明根节点是一个3-节点的一部分,实际情况并不是这样的.所以我们需要将根节点设为黑色. 每当根节点由红变黑时,树的黑链接高度就会加1. 插入 在红黑树中实现插入操作是比较复杂的,因为需要保持红黑树的平衡性.但只要利用好左旋转、右旋转、颜色转换这三个辅助操作,就能够保证插入操作后树的平衡性. 向单个2-节点中插入新键当一棵只含有一个键的红黑树只含有一个2-节点时,插入另一个键后需要马上进行旋转操作修正树的平衡性. 如果新键小于老键,只需要新增一个红色的节点即可(这时,新的红黑树等价于一个3-节点). 如果新键大于老键,那么新增的红色节点将会产生一条红色的右链接,这时就需要使用左旋转修正根节点的链接. 以上两种情况最终的结果均为一棵等价于单个3-节点的红黑树,它含有两个键,一条红链接,树的黑链接高度为1. 向树底部的2-节点插入新键和二叉查找树一样,向红黑树中插入一个新键会在树的底部新增一个节点,但在红黑树中总是用红链接将新节点和它的父节点相连. 如果它的父节点是一个2-节点,那么上一节讨论的方法依然适用. 如果指向新节点的是父节点的左链接,那么父节点就直接成为一个3-节点. 如果指向新节点的是父节点的右链接,那么就需要一次左旋转进行修正. 向一棵双键树(一个3-节点)中插入新键当向一个3-节点中插入新键时,会发生以下三种情况且每种情况都会产生一个同时连接到两条红链接的节点,我们需要修正这一点. 如果新键大于原树中的两个键 : 这是最容易处理的一种情况,这个键会被连接到3-节点的右链接.此时树是平衡的,根节点为中间大小的键,它有两条红链接分别和较小和较大的节点相连.只需要把这两条链接的颜色都由红变黑,那么就可以得到一棵由三个节点组成、高度为2的平衡树(其他两种情况最终也会转化为这样的树). 如果新键小于原树中的两个键 : 这个键会被连接到最左边的空链接,这样就产生了两条连续的红链接.此时只需要将上层的红链接右旋转即可得到第一种情况(中值键为根节点并和其他两个节点用红链接相连). 如果新键介于原树中的两个键之间 : 这种情况依然会产生两条连续的红链接:一条红色左链接接一条红色右链接.此时只需要将下层的红链接左旋转即可得到第二种情况(两条连续的红色左链接). 通过以上这三种情况可以总结出 : 我们只需要通过0次、1次、2次旋转以及颜色转换就可以完成对红黑树的修正. 将红链接向上传递当每次旋转操作之后都会进行颜色转换,它会使得中间节点变为红色.从父节点的角度来看,处理这样一个红色节点的方式和处理一个新插入的红色节点完全相同(继续将红链接转移到中间节点). 这个操作对应于2-3树中向3-节点进行插入的操作 : 即在一个3-节点下插入新键,需要创建一个临时的4-节点,将其分解并将中间键插入父节点(在红黑树中,是将红链接由中间键传递给它的父节点).重复这个过程,直至遇到一个2-节点或者根节点. 当根节点变为红色时需要将根节点的颜色转换为黑色(对应2-3树中的根节点分解). 实现插入操作的实现除了每次递归调用之后的对平衡性修正的操作,其他与二叉查找树中的插入操作没什么不同. 1234567891011121314151617181920212223242526272829public void put(K key, V val) &#123; if (key == null) throw new IllegalArgumentException("first argument to put() is null"); if (val == null) &#123; delete(key); return; &#125; root = put(root, key, val); root.color = BLACK; assert check();&#125;// insert the key-value pair in the subtree rooted at hprivate Node put(Node h, K key, V val) &#123; if (h == null) return new Node(key, val, RED, 1); int cmp = key.compareTo(h.key); if (cmp &lt; 0) h.left = put(h.left, key, val); else if (cmp &gt; 0) h.right = put(h.right, key, val); else h.value = val; // fix-up any right-leaning links if (isRed(h.right) &amp;&amp; !isRed(h.left)) h = rotateLeft(h); if (isRed(h.left) &amp;&amp; isRed(h.left.left)) h = rotateRight(h); if (isRed(h.left) &amp;&amp; isRed(h.right)) flipColors(h); h.size = size(h.left) + size(h.right) + 1; return h;&#125; 总结只要在沿着插入点到根节点的路径向上移动时在所经过的每个节点中顺序完成以下操作,就能够实现红黑树的插入操作. 如果右子节点是红色的而左子节点是黑色的,那么进行左旋转. 如果左子节点是红色的而且它的左子节点也是红色的,那么进行右旋转. 如果左右子节点都是红色的,那么进行颜色转换. 删除 删除操作也需要定义一系列局部变换来在删除一个节点的同时保持树的完美平衡性.然而,这个过程要比插入操作还要复杂,它不仅要在(为了删除一个节点而)构造临时4-节点时沿着查找路径向下进行变换,还要在分解遗留的4-节点时沿着查找路径向上进行变换(同插入操作). 自顶向下的2-3-4树2-3-4树是一种允许存在4-节点的树.它的插入算法就是一种沿着查找路径既能向上也能向下进行变换的算法. 沿查找路径向下进行变换(向下变换与2-3树中分解4-节点所进行的变换完全相同)是为了保证当前节点不是4-节点(这样树的底部才有足够的空间插入新的键). 沿查找路径向上进行变换是为了将之前创建的4-节点配平. 如果根节点是一个4-节点,就将它分解成三个2-节点,树的高度加1. 如果在向下查找的过程中,遇到了一个父节点为2-节点的4-节点,就将4-节点分解为两个2-节点并将中间键传递给它的父节点(这时父节点变为了一个3-节点). 如果遇到了一个父节点为3-节点的4-节点,将4-节点分解为两个2-节点并将中间键传递给它的父节点(这时父节点变为了一个4-节点). 不必担心遇见父节点为4-节点的4-节点,算法本身保证了不会出现这种情况,到达树的底部之后,只会遇到2-节点或者3-节点. 如果要使用红黑树来实现这个算法,需要以下步骤 : 将4-节点表示为由三个2-节点组成的一棵平衡的子树,根节点和两个子节点都用红链接相连. 在向下的过程中分解所有4-节点并进行颜色转换. 在向上的过程中使用旋转将4-节点配平. 只需要将插入一节中的put()实现方法里的flipColors语句(及其if语句)移动到递归调用之前(null判断和比较操作之间)就能实现2-3-4树的插入操作. 删除最小键从2-节点中删除一个键会留下一个空节点,一般会将它替换为一个空链接,但这样会破坏树的完美平衡性.所以在删除操作中,为了避免删除一个2-节点,我们沿着左链接向下进行变换时,需要确保当前节点不是2-节点. 根节点可能有以下两种情况: 如果根节点是一个2-节点且它的两个子节点都是2-节点,可以直接将这三个节点变成一个4-节点. 否则,需要保证根节点的左子节点不是2-节点,必要时可以从它右侧的兄弟节点借走一个键. 在沿着左链接向下的过程中,保证以下情况之一成立: 如果当前节点的左子节点不是2-节点. 如果当前节点的左子节点是2-节点而它的兄弟节点不是2-节点,将左子节点的兄弟节点中的一个键移动到左子节点中 如果当前节点的左子节点和它的兄弟节点都是2-节点,将左子节点、父节点中的最小键和左子节点最近的兄弟节点合并为一个4-节点,使父节点由3-节点变为2-节点(或是从4-节点变为3-节点). 只要保证了以上的条件,我们最终能够得到一个含有最小键的3-节点或4-节点(然后进行删除即可),之后再不断向上分解所有临时的4-节点. 代码实现在删除操作中,颜色转换的操作与插入操作中的实现略微有些不同(需要将父节点设为黑,而将两个子节点设为红). 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051private void flipColors(Node h) &#123; h.color = !h.color; h.left.color = !h.left.color; h.right.color = !h.right.color; &#125; // restore red-black tree invariant private Node balance(Node h) &#123; if (isRed(h.right)) h = rotateLeft(h); if (isRed(h.left) &amp;&amp; isRed(h.left.left)) h = rotateRight(h); if (isRed(h.left) &amp;&amp; isRed(h.right)) flipColors(h); h.size = size(h.left) + size(h.right) + 1; return h; &#125; // Assuming that h is red and both h.left and h.left.left // are black, make h.left or one of its children red. private Node moveRedLeft(Node h) &#123; flipColors(h); if (isRed(h.right.left)) &#123; h.right = rotateRight(h.right); h = rotateLeft(h); flipColors(h); &#125; return h; &#125; public void deleteMin() &#123; if (isEmpty()) throw new NoSuchElementException("RedBlackBST underflow."); // if both children of root are black, set root to red if (!isRed(root.left) &amp;&amp; !isRed(root.right)) root.color = RED; root = deleteMin(root); if (!isEmpty()) root.color = BLACK; &#125; // delete the key-value pair with the minimum key rooted at h private Node deleteMin(Node h) &#123; if (h.left == null) return null; if (!isRed(h.left) &amp;&amp; !isRed(h.left.left)) h = moveRedLeft(h); h.left = deleteMin(h.left); return balance(h); &#125; 删除最大键1234567891011121314151617181920212223242526272829303132333435363738// Assuming that h is red and both h.right and h.right.left// are black, make h.right or one of its children red.private Node moveRedRight(Node h) &#123; flipColors(h); if (isRed(h.left.left)) &#123; h = rotateRight(h); flipColors(h); &#125; return h;&#125; public void deleteMax() &#123; if (isEmpty()) throw new NoSuchElementException("RedBlackBST underflow."); // if both children of root are black, set root to red if (!isRed(root.left) &amp;&amp; !isRed(root.right)) root.color = RED; root = deleteMax(root); if (!isEmpty()) root.color = BLACK;&#125;// delete the key-value pair with the maximum key rooted at hprivate Node deleteMax(Node h) &#123; if (isRed(h.left)) h = rotateRight(h); if (h.right == null) return null; if (!isRed(h.right) &amp;&amp; !isRed(h.right.left)) h = moveRedRight(h); h.right = deleteMax(h.right); return balance(h);&#125; 删除操作同样也需要像删除最小键那样在查找路径上进行变换来保证查找过程中任意当前节点均不是2-节点.如果目标键在树的底部,可以直接删除它;如果不在,则需要将它和它的后继节点交换. 在删除操作之后需要向上变换分解余下的4-节点. 12345678910111213141516171819202122232425262728293031323334public void delete(K key) &#123; if (key == null) throw new IllegalArgumentException("called delete() with key is null."); if (!contains(key)) return; // if both children of root are black, set root to red if (!isRed(root.left) &amp;&amp; !isRed(root.right)) root.color = RED; root = delete(root, key); if (!isEmpty()) root.color = BLACK;&#125;// delete the key-value pair with the given key rooted at hprivate Node delete(Node h, K key) &#123; if (key.compareTo(h.key) &lt; 0) &#123; if (!isRed(h.left) &amp;&amp; !isRed(h.left.left)) h = moveRedLeft(h); h.left = delete(h.left, key); &#125; else &#123; if (isRed(h.left)) h = rotateRight(h); if (key.compareTo(h.key) == 0 &amp;&amp; (h.right == null)) return null; if (!isRed(h.right) &amp;&amp; !isRed(h.right.left)) h = moveRedRight(h); if (key.compareTo(h.key) == 0) &#123; Node x = min(h.right); h.key = x.key; h.value = x.value; h.right = deleteMin(h.right); &#125; else h.right = delete(h.right, key); &#125; return balance(h);&#125; 总结 无论键的插入顺序如何,红黑树都几乎是完美平衡的,基于它实现的有序符号表操作的运行时间均为对数级别(除了范围查询). 在红黑树的实现中复杂的代码仅限于put()和delete()方法,像get()这些不会涉及检查颜色的方法与二叉查找树中的实现一致(因为这些操作与平衡性无关). end Author : SylvanasSun Email : sylvanassun_xtz@163.com 文中的完整实现代码见我的GitHub &amp; Gist 本文参考资料引用自《Algorithms,4th Editio》]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>Tree</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
        <tag>2017</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Algorithms,4th Edition》读书笔记-2-3查找树]]></title>
    <url>%2F2017%2F03%2F28%2F2017-3-28-2_3tree%2F</url>
    <content type="text"><![CDATA[概述 由于二叉查找树的性能与树的高度(即根节点到底部节点的深度)相关,因此当高度较大时,二叉查找树的性能就会下降.为了更高效的性能,平衡查找树应运而生,它能保证无论键的插入顺序如何,树的高度都将是总键数的对数. 2-3查找树就是平衡树的一种. 性质 2-3查找树允许树中的一个节点保存多个键.我们可以将二叉查找树中的节点称为2-节点,而在2-3查找树中引入了3-节点,它含有两个键和三条链接. 一棵2-3查找树由以下节点组成: 2-节点 : 含有一个键(及其对应的值)和两条链接,左链接指向的2-3查找树中的键都小于该节点,右链接指向的2-3查找树中的键都大于该节点. 3-节点 : 含有两个键(及其对应的值)和三条链接,左链接指向的2-3查找树中的键都小于该节点,中链接指向的2-3查找树中的键都位于该节点的两个键之间,右链接指向的2-3查找树中的键都大于该节点. 一棵完美平衡的2-3查找树中的所有空链接到根节点的距离都应该是相同的. 查找 2-3查找树的查找算法与二叉查找树基本相似. 首先,要判断一个键需要先将它和根节点中的键进行比较. 如果它和其中任意一个相等,查找命中. 否则,根据比较的结果找到指向相应区间的链接,并在其指向的子树中递归地继续查找. 如果最后指向空链接,查找未命中. 插入 由于2-3查找树需要保持完美平衡性,所以它的插入算法并不像二叉查找树那么简单. 它的插入算法基本思想是 : 一直向上不断分解临时的4-节点并将中键插入更高层的父节点中,直至遇到一个2-节点并将它替换为一个不需要继续分解的3-节点,或是到达3-节点的根(分解根节点) 向2-节点中插入新键如果未命中的查找结束于一个2-节点,只需要把这个2-节点替换为一个3-节点,将要插入的键保存在其中即可. 向一棵只含有一个3-节点的树中插入新键如果我们需要向一棵只含有一个3-节点的树中插入一个新键(这棵树中唯一的节点已经没有可插入新键的空间了). 先临时将新键存入该节点中,使之成为一个4-节点(它扩展了以前的节点并含有3个键和4条链接). 将4-节点分解为一棵由3个2-节点组成的2-3查找树,其中一个节点(根)含有中键,一个节点含有3个键中的最小者(和根节点的左链接相连),一个节点含有3个键中的最大者(和根节点的右链接相连). 这时,这棵树既是一棵含有3个节点的二叉查找树,同时也是一棵完美平衡的2-3查找树. 向一个父节点为2-节点的3-节点中插入新键如果未命中的查找结束于一个3-节点,而它的父节点是一个2-节点.这种情况下,我们需要在维持树的完美平衡性的前提下为新键腾出空间. 构造一个临时的4-节点并将其分解(此时并不会为中键创建一个新节点). 将中键移动至父节点中(可以看做将指向3-节点的一条链接替换为新父节点中的原中键左右两边的两条链接,并分别指向两个新的2-节点). 向一个父节点为3-节点的3-节点中插入新键如果未命中的查找结束于一个父节点为3-节点且它本身也是一个3-节点时.我们可以构造一个临时的4-节点并分解它.将中键插入到它的父节点中. 但由于它的父节点也是一个3-节点,所以需要再用这个中键构造一个新的临时4-节点,然后在这个节点上进行相同的变换,即分解这个父节点并将它的中键插入到它的父节点中. 重复相同的变换直到遇到一个2-节点(将2-节点替换为一个3-节点)或者到达根节点(分解根节点). 分解根节点如果从插入节点到根节点的路径上全部都是3-节点.那么根节点最终会变成一个临时的4-节点,这时可以将4-节点分解为3个2-节点,同时树高加1(仍然保持了树的完美平衡性,因为它变换的是根节点). 具体实现 关于如何使用一个简单的数据结构来表达实现2-3查找树可以见此文 &lt;&gt;读书笔记-红黑二叉查找树 总结 2-3查找树的根本在于插入操作中的变换操作都是局部的,除了相关的节点和链接之外不必修改或者检查树的其他部分. 每次变换都会将4-节点中的一个键移动至它的父节点中,并重构相应的链接而不必涉及树的其他部分.且保持了树的完美平衡性,例如在变换之前根节点到所有空链接的路径长度为h,那么变换之后该长度仍然为h.只有进行根节点分解时,所有空链接到根节点的路径长度才会加1. 通过这些我们可以总结得出: 2-3查找树的生长是由下向上的. (标准的二叉查找树则是由上向下生长的) end Author : SylvanasSun Email : sylvanassun_xtz@163.com 本文参考资料引用自&lt;&gt;]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>Tree</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
        <tag>2017</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Algorithms,4th Edition》读书笔记-二叉查找树]]></title>
    <url>%2F2017%2F03%2F26%2F2017-3-26-binary_search_tree%2F</url>
    <content type="text"><![CDATA[概述 二叉查找树是一颗有序的二叉树,它有以下性质: 若任意节点的左子树不为空,则左子树上所有节点的值均小于它的根节点的值. 若任意节点的右子树不为空,则右子树上所有节点的值均大于它的根节点的值. 可以将每个链接看做指向另一颗二叉查找树,而这棵树的根节点就是被指向的节点. 没有键值相等的节点. 使用二叉查找树实现的符号表结合了链表插入的灵活性和有序数组查找的高效性.通常采取二叉链表作为存储结构. 每个节点都含有一个键、一个值、一条左链接、一条右链接和一个节点计数器(用于统计其所有子节点数).左链接指向一棵由小于该节点的所有键组成的二叉查找树,右链接指向一棵由大于该节点的所有键组成的二叉查找树. 如果将一棵二叉查找树的所有键投影到一条直线上,保证一个节点的左子树中的键出现在它的左边,右子树种的键出现在它的右边,那么我们一定可以得到一条有序的键列. 可以说二叉查找树和快速排序很相似.树的根节点就是快速排序中的第一个基准数(切分元素),左侧的键都比它小,右侧的键都比它大. 基本实现 1234567891011121314151617181920212223242526272829303132333435363738394041public class BinarySearchTree&lt;K extends Comparable&lt;K&gt;, V&gt; &#123; private Node root; // root node private class Node &#123; private K key; private V value; private Node left, right; // left and right subtree private int size; // number of nodes in subtree public Node(K key, V value, int size) &#123; this.key = key; this.value = value; this.size = size; &#125; &#125; /** * Returns true if this symbol table is empty. * * @return &#123;@code true&#125; is this symbol table is empty, &#123;@code false&#125; otherwise. */ public boolean isEmpty() &#123; return size() == 0; &#125; /** * Returns the number of key-value pairs in this symbol table. * * @return the number of key-value pairs in this symbol table. */ public int size() &#123; return size(root); &#125; // return number of key-value pairs in binary search tree rooted at x private int size(Node x) &#123; if (x == null) return 0; else return x.size; &#125;&#125; 在以上代码中,使用私有嵌套类Node来表示一个二叉链表,每个Node对象都是一棵含有N个节点的子树的根节点.变量root指向二叉查找树的根节点(这棵树包含了符号表中的所有键值对). 查找与插入 查找1234567891011121314151617181920212223242526272829/** * Returns the value associated with the given key. * * @param key the key * @return the value associated with the given key if the key is in the symbol table * and &#123;@code null&#125; if the key is not in the symbol table. * @throws IllegalArgumentException if &#123;@code key&#125; is &#123;@code null&#125; */ public V get(K key) &#123; if (key == null) throw new IllegalArgumentException("called get() with a null key."); return get(root, key); &#125; private V get(Node x, K key) &#123; if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp &lt; 0) &#123; // if key &lt; x.key , search left subtree return get(x.left, key); &#125; else if (cmp &gt; 0) &#123; // if key &gt; x.key , search right subtree return get(x.right, key); &#125; else &#123; // hit target return x.value; &#125; &#125; 在二叉查找树中实现查找操作是很简单而简洁的,这也是二叉查找树的特性之一.-如果树为空,就返回null,如果被查找的键小于根节点的键,我们就继续在左子树中查找,否则在右子树中查找. 插入插入操作的逻辑与查找差不多,只不过需要在判定树为空时,返回一个含有该键值对的新节点,还需要重置搜索路径上每个父节点指向子节点的链接,并增加路径上每个节点中的子节点计数器的值. 1234567891011121314151617181920212223242526272829303132333435363738/** * Inserts the specified key-value pair into the symbol table. * overwriting the old value with the new value if the symbol table already contains * the specified key. * Deletes the specified key (and its associated value) from this symbol table * if the specified value is &#123;@code null&#125;. * * @param key the key * @param value the value * @throws IllegalArgumentException if &#123;@code key&#125; is &#123;@code null&#125;. */ public void put(K key, V value) &#123; if (key == null) throw new IllegalArgumentException("called put() with a null key."); if (value == null) &#123; delete(key); return; &#125; root = put(root, key, value); assert check(); &#125; private Node put(Node x, K key, V value) &#123; // if tree is empty, return a new node. if (x == null) return new Node(key, value, 1); int cmp = key.compareTo(x.key); if (cmp &lt; 0) &#123; x.left = put(x.left, key, value); &#125; else if (cmp &gt; 0) &#123; x.right = put(x.right, key, value); &#125; else &#123; // hit target,overwriting old value with the new value x.value = value; &#125; x.size = 1 + size(x.left) + size(x.right); // compute subtree node size return x; &#125; 最大键和最小键 如果根节点的左链接为空,那么一棵二叉查找树中最小的键就是根节点. 如果左链接非空,那么树中的最小键就是左子树中的最小键.找出最大键的逻辑也是类似的,只是变为查找右子树而已. 123456789101112131415161718192021222324252627282930313233/** * Returns the smallest key in the symbol table. * * @return the smallest key in the symbol table. * @throws NoSuchElementException if the symbol table is empty */ public K min() &#123; if (isEmpty()) throw new NoSuchElementException("called min() with empty symbol table."); return min(root).key; &#125; private Node min(Node x) &#123; if (x.left == null) return x; else return min(x.left); &#125; /** * Returns the largest key in the symbol table. * * @return the largest key in the symbol table. * @throws NoSuchElementException if the symbol table is empty */ public K max() &#123; if (isEmpty()) throw new NoSuchElementException("called max() with empty symbol table."); return max(root).key; &#125; private Node max(Node x) &#123; if (x.right == null) return x; else return max(x.right); &#125; 向上取整和向下取整 如果给定的键key小于二叉查找树的根节点的键,那么小于等于key的最大键floor(key)一定在根节点的左子树中. 如果给定的键key大于二叉查找树的根节点,那么只有当根节点右子树中存在小于等于key的节点时,小于等于key的最大键才会出现在右子树中,否则根节点就是小于等于key的最大键.(将左变为右,小于变为大于就是向上取整的实现逻辑) 12345678910111213141516171819202122232425262728293031323334353637383940414243public K floor(K key) &#123; if (key == null) throw new IllegalArgumentException("called floor() with a null key."); if (isEmpty()) throw new NoSuchElementException("called floor() with empty symbol table."); Node x = floor(root, key); if (x == null) return null; else return x.key; &#125; private Node floor(Node x, K key) &#123; if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp == 0) return x; if (cmp &lt; 0) return floor(x.left, key); Node t = floor(x.right, key); if (t != null) return t; else return x; &#125;public K ceiling(K key) &#123; if (key == null) throw new IllegalArgumentException("called ceiling() with a null key."); if (isEmpty()) throw new NoSuchElementException("called ceiling() with empty symbol table."); Node x = ceiling(root, key); if (x == null) return null; else return x.key; &#125; private Node ceiling(Node x, K key) &#123; if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp == 0) return x; if (cmp &lt; 0) &#123; Node t = ceiling(x.left, key); if (t != null) return t; else return x; &#125; return ceiling(x.right, key); &#125; 选择和排名 select假设我们想找到排名为k的键(即树中正好有k个小于它的键). 如果左子树中的节点数t大于k,那么我们就继续递归地在左子树中查找排名为k的键. 如果t等于k,我们就返回根节点中的键. 如果t小于k,我们就递归地在右子树中查找排名为(k-t-1)的键. 123456789101112131415public K select(int k) &#123; if (k &lt; 0 || k &gt;= size()) throw new IllegalArgumentException("called select() with invalid argument: " + k); return select(root, k).key;&#125;private Node select(Node x, int k) &#123; if (x == null) return null; int t = size(x.left); // if left subtree node size greater than k,in left subtree search if (t &gt; k) return select(x.left, k); // otherwise,in right subtree search else if (t &lt; k) return select(x.right, k - t - 1); else return x;&#125; rankrank()函数是select()函数的逆函数,它会返回指定键的排名. 如果给定的键和根节点的键相等,就返回左子树中的节点总数t. 如果给定的键小于根节点,就返回该键在左子树中的排名(递归计算). 如果给定的键大于根节点,就返回t+1(根节点)加上它在右子树中的排名(递归计算). 1234567891011121314151617public int rank(K key) &#123; if (key == null) throw new IllegalArgumentException("called rank() with a null key."); return rank(root, key);&#125;private int rank(Node x, K key) &#123; if (x == null) return 0; int cmp = key.compareTo(x.key); if (cmp &lt; 0) return rank(x.left, key); else if (cmp &gt; 0) return 1 + size(x.left) + rank(x.right, key); else return size(x.left);&#125; 删除最大键和删除最小键 对于删除最小键,需要不断深入根节点的左子树直至遇见一个空链接,然后将指向该节点的链接指向该节点的右子树(只需要在递归调用中返回它的右链接即可).此时已经没有任何链接指向要被删除的节点,因此它会被垃圾回收器gc掉. 删除最大键与其逻辑相似,只是方向相反. 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Removes the smallest key and associated value from the symbol table. * * @throws NoSuchElementException if the symbol table is empty. */public void deleteMin() &#123; if (isEmpty()) throw new NoSuchElementException("Symbol table underflow."); root = deleteMin(root); assert check();&#125;private Node deleteMin(Node x) &#123; // if the left link is empty,will link to the node of the right subtree. if (x.left == null) return x.right; x.left = deleteMin(x.left); x.size = size(x.left) + size(x.right) + 1; return x;&#125;/** * Removes the largest key and associated value from the symbol table. * * @throws NoSuchElementException if the symbol table is empty */public void deleteMax() &#123; if (isEmpty()) throw new NoSuchElementException("Symbol table underflow."); root = deleteMax(root); assert check();&#125;private Node deleteMax(Node x) &#123; // if the right link is empty,will link to the node of the left subtree. if (x.right == null) return x.left; x.right = deleteMax(x.right); x.size = size(x.left) + size(x.right) + 1; return x;&#125; 删除 删除操作是二叉查找树中较为复杂的操作,假设我们要删除节点x(它是一个拥有两个子节点的节点),基本的实现逻辑如下. 在删除节点x后用它的后继节点填补它的位置. 找出x的右子树中的最小节点(这样替换仍能保证树的有序性,因为x.key和它的后继节点的键之间不存在其他的键)做为x的后继节点. 将由此节点到根节点的路径上的所有节点的计数器减1(这里计数器的值仍然会被设为其所有子树中的节点总数加1). 具体的过程如以下例子: 将指向即将被删除的节点的链接保存为t. 将x指向它的后继节点min(t.right). 将x的右链接(原本指向一棵所有节点都大于x.key的二叉查找树)指向deleteMin(t.right),也就是在删除后所有的节点仍然都大于x.key的子二叉查找树. 将x的左链接(本为空)指向t.left(其下所有的键都小于被删除的节点和它的后继节点). 以上的实现逻辑有一个缺点,即是在某些实际应用场景下会产生性能问题,主要原因在于后继节点是一个随意的决定,且没有考虑树的对称性. 12345678910111213141516171819202122232425262728public void delete(K key) &#123; if (key == null) throw new IllegalArgumentException("called delete() with a null key."); root = delete(root, key); assert check();&#125;private Node delete(Node x, K key) &#123; if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp &lt; 0) x.left = delete(x.left, key); else if (cmp &gt; 0) x.right = delete(x.right, key); else &#123; if (x.right == null) return x.left; if (x.left == null) return x.right; Node t = x; x = min(t.right); x.right = deleteMin(t.right); x.left = t.left; &#125; x.size = size(x.left) + size(x.right) + 1; return x;&#125; 范围查找 实现一个范围查找的思路可以是:将所有落在给定范围以内的键放入一个队列Queue并跳过那些不可能含有所查找键的子树. 12345678910111213141516171819202122public Iterable&lt;K&gt; keys(K lo, K hi) &#123; if (lo == null) throw new IllegalArgumentException("called keys(lo,hi) first argument is null."); if (hi == null) throw new IllegalArgumentException("called keys(lo,hi) second argument is null."); Queue&lt;K&gt; queue = new Queue&lt;K&gt;(); keys(root, queue, lo, hi); return queue;&#125;private void keys(Node x, Queue&lt;K&gt; queue, K lo, K hi) &#123; if (x == null) return; int cmp_lo = lo.compareTo(x.key); int cmp_hi = hi.compareTo(x.key); if (cmp_lo &lt; 0) keys(x.left, queue, lo, hi); if (cmp_lo &lt;= 0 &amp;&amp; cmp_hi &gt;= 0) queue.enqueue(x.key); if (cmp_hi &gt; 0) keys(x.right, queue, lo, hi);&#125; 总结 二叉查找树的高度决定了它在最坏情况下的运行效率,但由于键的插入顺序不会是永远随机的,所以树的某一端高度可能会非常深,解决这个问题可以使用平衡二叉查找树,它能保证无论键的插入顺序如何,树的高度都将是总键数的对数. 本文中的实现皆采用递归的方式是为了提高可读性,二叉查找树可以使用非递归的方式实现且效率会更高. 二叉查找树结合了链表插入操作的灵活性和有序数组查找操作的高效性,且还有很多种优化的改进方案(例如平衡二叉查找树),总体来说二叉查找树是一种比较好的动态查找方法. end Author : SylvanasSun Email : sylvanassun_xtz@163.com 文中的完整实现代码见我的GitHub &amp; Gist 本文参考资料引用自&lt;&gt; &amp; WikiPedia]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>Tree</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
        <tag>2017</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈几个常用的排序算法]]></title>
    <url>%2F2017%2F03%2F20%2F2017-3-20-sorting_algorithm%2F</url>
    <content type="text"><![CDATA[最近在读&lt;&gt;时,了解到了很多常用的排序算法,故写一篇读书笔记记录下这些排序算法的思路和实现. 冒泡排序 冒泡排序是一种非常简单的初级排序算法,它每次比较相邻的两个元素,如果顺序错误就进行交换.由于最小的元素是经由不断交换慢慢浮到顶端的,所以叫做冒泡排序. 冒泡排序对n个元素需要O(n^2)次的比较次数,所以它对规模较大的数组进行排序是效率低下的. 运行过程 比较相邻的两个元素,如果第二个元素小于第一个元素,则进行交换(降序则相反). 对每一对相邻元素作同样的工作,从开始第一对直到最后一对.完成后,最后的元素将是最大的元素. 针对所有的元素重复以上步骤,除了最后一个元素. 持续地对每次越来越少的元素重复以上步骤,直到整个数组有序(即没有任何一对元素需要比较). 代码实现12345678910 // less与exch函数见完整代码public static void sort(Comparable[] a) &#123; for (int i = 0; i &lt; a.length - 1; i++) &#123; for (int j = 0; j &lt; a.length - 1 - i; j++) &#123; if (less(a[j + 1], a[j])) &#123; exch(a, j, j + 1); &#125; &#125; &#125;&#125; 完整代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * Bubble Sort * * @author SylvanasSun * */public class Bubble &#123; // This class should not be instantiated. private Bubble() &#123; &#125; /** * Rearranges the array in ascending order, using the natural order. * * @param a * a the array to be sorted */ public static void sort(Comparable[] a) &#123; for (int i = 0; i &lt; a.length - 1; i++) &#123; for (int j = 0; j &lt; a.length - 1 - i; j++) &#123; if (less(a[j + 1], a[j])) &#123; exch(a, j, j + 1); &#125; &#125; &#125; &#125; /** * Rearranges the array in ascending order, using a comparator. * * @param a * a the arry to be sorted * @param comparator * comparator the comparator specifying the order */ public static void sort(Object[] a, Comparator comparator) &#123; for (int i = 0; i &lt; a.length - 1; i++) &#123; for (int j = 0; j &lt; a.length - 1 - i; j++) &#123; if (less(comparator, a[j + 1], a[j])) &#123; exch(a, j, j + 1); &#125; &#125; &#125; &#125; // a &lt; b ? private static boolean less(Comparable a, Comparable b) &#123; return a.compareTo(b) &lt; 0; &#125; // a &lt; b ? private static boolean less(Comparator comparator, Object a, Object b) &#123; return comparator.compare(a, b) &lt; 0; &#125; // exchange a[i] and a[j] private static void exch(Object[] a, int i, int j) &#123; Object temp = a[i]; a[i] = a[j]; a[j] = temp; &#125; // print array elements to console public static void print(Comparable[] a) &#123; for (int i = 0; i &lt; a.length; i++) &#123; System.out.print(a[i] + " "); &#125; &#125; // test public static void main(String[] args) &#123; String[] s = new Scanner(System.in).nextLine().split("\\s+"); Bubble.sort(s); Bubble.print(s); &#125;&#125; 选择排序 选择排序也是一种非常简单直观的初级排序算法,它的思想是不断地选择剩余元素之中的最小者. 它有以下两个特点. 运行时间与输入模型无关 在选择排序中,为了找出最小元素而扫描一遍数组并不能为下一轮扫描提供什么信息,即使输入是一个已经有序的数组或者是主键全部相等的数组和一个元素随机排列无序的数组所用的排序时间是一样长的. 数据移动是最少的 如果元素处于正确的位置上,则它不会被移动.选择排序每次交换一对元素，它们当中至少有一个将被移到其最终位置上，因此对n个元素的表进行排序总共进行至多n-1次交换. 运行过程 首先,找到数组中最小的那个元素 其次,将它和数组的第一个元素交换位置(如果第一个元素就是最小元素则它就和自己交换) 再次,在剩下的元素中找到最小的元素,将它与数组第二个元素交换位置.如此往复,直到整个数组有序. 代码实现12345678910public static void sort(Comparable[] a) &#123; for (int i = 0; i &lt; a.length; i++) &#123; int min = i; // the smallest element index for (int j = i + 1; j &lt; a.length; j++) &#123; if (less(a[j], a[min])) min = j; exch(a, i, min); &#125; &#125; &#125; 插入排序 插入排序与选择排序一样,当前索引左边的所有元素都是有序的,但它们的最终位置并不是确定的.它构建了一个有序序列,对于未排序的元素,在有序序列中从后向前扫描,找到相应的位置并插入. 插入排序所需的时间取决于输入模型中元素的初始顺序.当输入模型是一个部分有序的数组时,插入排序的效率会高很多. 因此插入排序对于部分有序的数组十分高效,也很适合小规模的数组. 运行过程 从第一个元素开始,该元素可以认为已是有序的 取出下一个元素,在有序序列中从后向前进行扫描 如果该元素(已排序)大于新元素,则将该元素移到下一位置(右移) 重复步骤3,直到找到已排序的元素小于或者等于新元素的位置 将新元素插入到该位置后 重复步骤2~5 代码实现12345678public static void sort(Comparable[] a) &#123; for (int i = 0; i &lt; a.length; i++) &#123; // a[i] insert to a[i-1]、a[i-2]、a[i-3]... for (int j = i; j &gt; 0 &amp;&amp; less(a[j], a[j - 1]); j--) &#123; exch(a, j, j - 1); &#125; &#125; &#125; 优化插入排序还有很多可以优化的地方,这里例举两个案例. 采用二分查找法来减少比较操作的次数.123456789101112131415161718192021public static void sort(Comparable[] a) &#123; int length = a.length; for (int i = 1; i &lt; length; i++) &#123; // binary search to determine index j at which to insert a[i] Comparable v = a[i]; int lo = 0, hi = i; while (lo &lt; hi) &#123; int mid = lo + (hi - lo) / 2; if (less(v, a[mid])) hi = mid; else lo = mid + 1; &#125; // insertion sort with "half exchanges" // (insert a[i] at index j and shift a[j], ..., a[i-1] to right) for (int j = i; j &gt; lo; --j) a[j] = a[j - 1]; a[lo] = v; &#125;&#125; 在内循环中将较大的元素都向右移动而不总是交换两个元素(访问数组的次数能够减半)12345678910111213141516171819202122232425public static void sort(Comparable[] a) &#123; int length = a.length; // put smallest element in position to serve as sentinel int exchanges = 0; for (int i = length - 1; i &gt; 0; i--) &#123; if (less(a[i], a[i - 1])) &#123; exch(a, i, i - 1); exchanges++; &#125; &#125; if (exchanges == 0) return; // insertion sort with half-exchanges for (int i = 2; i &lt; length; i++) &#123; Comparable v = a[i]; int j = i; while (less(v, a[j - 1])) &#123; a[j] = a[j - 1]; j--; &#125; a[j] = v; &#125;&#125; 希尔排序 希尔排序,也称递减增量排序算法,它是基于插入排序的一种更高效的改进版本. 由于插入排序对于大规模乱序数组效率并不高,因为它只会交换相邻的元素,因此元素只能一点一点地从数组的一端移动到另一端. 而希尔排序为了加快速度简单地改进了插入排序,交换不相邻的元素以对数组的局部进行排序,并最终用插入排序将局部有序的数组排序. 希尔排序的思想是使数组中任意间隔为h的元素都是有序的,可以说一个h有序的数组就是h个互相独立的有序数组编织在一起组成的一个数组. 代码实现12345678910111213141516public static void sort(Comparable[] a) &#123; int h = 1; while (h &lt; a.length / 3) &#123; // h sequence 1,4,13,40,121,364,1093,... h = h * 3 + 1; &#125; while (h &gt;= 1) &#123; for (int i = h; i &lt; a.length; i++) &#123; // a[i] insert to a[i-h],a[i-2*h],a[i-3*h]... for (int j = i; j &gt;= h &amp;&amp; less(a[j], a[j - h]); j -= h) &#123; exch(a, j, j - h); &#125; &#125; h = h / 3; &#125; &#125; 归并排序 归并排序是分治算法的典型应用.所谓归并即是将两个有序的数组归并成一个更大的有序数组. 它有一个主要的缺点就是它需要额外的空间(辅助数组)并且所需的额外空间和N成正比. 合并过程 申请空间,使其大小为两个已有序序列之和,该空间用于存放合并后的序列 声明两个指针,最初位置分别为两个有序序列的起始位置 比较两个指针所指向的元素,选择相对小的元素放入合并空间中,并移动指针到下一个位置 重复步骤3直到某一指针到达序列尾部 将另一序列剩下的所有元素直接放入合并序列尾 自顶向下的归并排序自顶向下即是从顶部化整为零地递归解决问题. 例如:要对数组a[lo..hi]进行排序,需要先将它切分为a[lo..mid]与a[mid+1..hi]两部分,分别通过递归调用将它们单独排序,最后将有序的子数组归并为最终的排序结果. 1234567891011121314151617181920212223242526272829303132// stably merge a[lo .. mid] with a[mid+1 ..hi] using aux[lo .. hi] private static void merge(Comparable[] a, Comparable[] aux, int lo, int mid, int hi) &#123; // copy a[] to aux[] for (int k = lo; k &lt;= hi; k++) &#123; aux[k] = a[k]; &#125; // merge back to a[] int i = lo, j = mid + 1; for (int k = lo; k &lt;= hi; k++) &#123; if (i &gt; mid) &#123; a[k] = aux[j++]; &#125; else if (j &gt; hi) &#123; a[k] = aux[i++]; &#125; else if (less(aux[j], aux[i])) &#123; a[k] = aux[j++]; &#125; else &#123; a[k] = aux[i++]; &#125; &#125; &#125; // mergesort a[lo..hi] using auxiliary array aux[lo..hi] private static void sort(Comparable[] a, Comparable[] aux, int lo, int hi) &#123; if (hi &lt;= lo) return; int mid = lo + (hi - lo) / 2; sort(a, aux, lo, mid); sort(a, aux, mid + 1, hi); merge(a, aux, lo, mid, hi); &#125; 自底向上的归并排序自底向上则是循序渐进地解决问题. 实现思路是先归并那些微型数组,然后再成对归并得到的子数组,直到将整个数组归并在一起. 可以先进行两两归并(每个元素想象成一个大小为1的数组),然后进行四四归并(将两个大小为2的数组归并成一个有四个元素的数组),然后是八八归并…..(一直下去)在每一轮归并中,最后一次归并的第二个子数组可能比第一个子数组要小,如果不是的话所有归并中两个数组大小都应该一致. 123456789101112//merge函数与自顶向下中的一致public static void sort(Comparable[] a) &#123; int N = a.length; Comparable[] aux = new Comparable[N]; for (int len = 1; len &lt; N; len *= 2) &#123; for (int lo = 0; lo &lt; N - len; lo += len + len) &#123; int mid = lo + len - 1; int hi = Math.min(lo + len + len - 1, N - 1); merge(a, aux, lo, mid, hi); &#125; &#125; &#125; 优化 如果数组很小,那么频繁的递归调用效率会很差,所以可以使用插入排序(或选择排序等)来处理小规模的子数组. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private static void merge(Comparable[] src, Comparable[] dst, int lo, int mid, int hi) &#123; int i = lo, j = mid + 1; for (int k = lo; k &lt;= hi; k++) &#123; if (i &gt; mid) &#123; dst[k] = src[j++]; &#125; else if (j &gt; hi) &#123; dst[k] = src[i++]; &#125; else if (less(src[j], src[i])) &#123; dst[k] = src[j++]; &#125; else &#123; dst[k] = src[i++]; &#125; &#125;&#125;private static void sort(Comparable[] src, Comparable[] dst, int lo, int hi) &#123; // if (hi &lt;= lo) return; if (hi &lt;= lo + CUTOFF) &#123; insertionSort(dst, lo, hi); return; &#125; int mid = lo + (hi - lo) / 2; sort(dst, src, lo, mid); sort(dst, src, mid + 1, hi); // using System.arraycopy() is a bit faster than the above loop if (!less(src[mid + 1], src[mid])) &#123; System.arraycopy(src, lo, dst, lo, hi - lo + 1); return; &#125; merge(src, dst, lo, mid, hi);&#125;// using insertion sort handle small arrayprivate static void insertionSort(Comparable[] a, int lo, int hi) &#123; for (int i = lo; i &lt;= hi; i++) &#123; for (int j = i; j &gt; lo &amp;&amp; less(a[j], a[j - 1]); j--) &#123; exch(a, j, j - 1); &#125; &#125;&#125;public static void sort(Comparable[] a) &#123; Comparable[] aux = a.clone(); sort(aux, a, 0, a.length - 1);&#125; 快速排序 快速排序又称划分交换排序,它也是一种分治的排序算法. 快速排序有一个潜在的缺点,在切分不平衡时这个程序可能会极为低效,所以需要在快速排序前将数组随机排序来避免这种情况. 它将一个数组切分成两个子数组,将两部分独立地排序.它与归并排序不同的地方在于: 归并排序将数组分成两个子数组分别排序,最终将有序的子数组归并以致整个数组排序. 快速排序将数组排序的方式则是当两个子数组都有序时,整个数组也就是有序的了. 在归并排序中,递归调用发生在处理整个数组之前;而在快速排序中,递归调用发生在处理整个数组之后. 在归并排序中,一个数组会被等分为两半,而在快速排序中,切分的位置取决于数组的内容. 运行过程 先从数列中挑选出一个基准,可以为a[lo],它是被确认为排定的元素. 从数组的左端(左指针)开始向右扫描直到找到一个大于等于基准的元素. 从数组的右端(右指针)开始向左扫描直到找到一个小于等于基准的元素. 这两个元素即是没有排定的,交换它们的位置(保证了左指针i的左侧元素都不大于基准,右指针j的右侧元素都不小于基准). .当两个指针相遇时,将基准和左子数组最右侧的元素(a[j])交换然后返回j即可. 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960// partition the subarray a[lo..hi] so that a[lo..j-1] &lt;= a[j] &lt;= a[j+1..hi] // and return the index j. private static int partition(Comparable[] a, int lo, int hi) &#123; int i = lo; // left point int j = hi + 1; // right point Comparable v = a[lo]; // partition element while (true) &#123; // scan left point while (less(a[++i], v)) &#123; if (i == hi) break; &#125; // scan right point while (less(v, a[--j])) &#123; if (j == lo) break; &#125; // check if point cross if (i &gt;= j) break; exch(a, i, j); &#125; // put partition element v to a[j] exch(a, lo, j); // now a[lo..j-1] &lt;= a[j] &lt;= a[j+1..hi] return j; &#125; private static void sort(Comparable[] a, int lo, int hi) &#123; if (hi &lt;= lo) return; int j = partition(a, lo, hi); sort(a, lo, j - 1); sort(a, j + 1, hi); &#125; public static void sort(Comparable[] a) &#123; shuffle(a); sort(a, 0, a.length - 1); &#125; // random sort an array private static void shuffle(Object[] a) &#123; if (a == null) throw new IllegalArgumentException("array is null."); Random random = new Random(); int N = a.length; for (int i = 0; i &lt; N; i++) &#123; int j = i + random.nextInt(N - i); Object temp = a[i]; a[i] = a[j]; a[j] = temp; &#125; &#125; 三向切分的快速排序当存在大量重复元素的情况下,快速排序的递归性会使元素全部重复的子数组经常出现,这就有很大的改进潜力,将当前快速排序从线性对数级别的性能提升至线性级别. 一个简单的思路是将数组切分为三部分,分别对应小于、等于、大于切分元素的数组元素. 在实现中,维护一个左指针lt使得a[lo..lt-1]的元素都小于基准,右指针gt使得a[gt+1..hi]中的元素都大于基准,一个指针i使得a[lt..i-1]中的元素都等于基准,a[i..gt]中的元素都还未确定. a[i]小于基准,将a[lt]和a[i]交换,lt++&amp;i++. a[i]大于基准,将a[gt]和a[i]交换,gt–. a[i]等于基准,i++. 以上操作都会保证数组元素不变且缩小gt-i的值(这样循环才会结束).除非和切分元素相等,其他元素都会被交换. 12345678910111213141516171819202122// quicksort the subarray a[lo .. hi] using 3-way partitioning private static void sort(Comparable[] a, int lo, int hi) &#123; if (hi &lt;= lo) return; int lt = lo, i = lo + 1, gt = hi; Comparable v = a[lo]; // partition element // a[lo..lt-1] &lt; a[lt..gt] &lt; a[gt+1..hi] while (i &lt;= gt) &#123; int cmp = a[i].compareTo(v); if (cmp &lt; 0) &#123; exch(a, i++, lt++); &#125; else if (cmp &gt; 0) &#123; exch(a, i, gt--); &#125; else &#123; i++; &#125; &#125; sort(a, lo, lt - 1); sort(a, gt + 1, hi); &#125; 堆排序 堆排序是基于堆的优先队列实现的一种排序算法. 优先队列优先队列是一种支持删除最大(最小)元素和插入元素的数据结构,它的内部是有序的,任意优先队列都可以变成一种排序方法. 堆堆是一种数据结构,它通常可以被看作为一棵树的数组对象.将根节点作为最大数的叫做最大堆,反之,将根节点作为最小数的叫做最小堆. 堆是一个近似完全二叉树的结构,同时又满足了堆的性质:每个元素都要保证大于(小于)等于它的子节点的元素. 在一个堆中,根据根节点的索引位置不同,计算父节点与子节点位置的算法也不同. 当数组起始位置为0时,位置k的节点的父节点为(k - 1)/2,它的两个子节点为2k+1,2k+2. 当数组起始位置为1时(即不使用索引0),位置k的节点的父节点为k/2,它的两个子节点为2k,2k+1. 为了保证堆有序,需要支持两个操作用于打破堆的状态,然后再遍历堆并按照要求将堆的状态恢复,这个过程叫做堆的有序化. 由下至上的堆有序化(上浮) : 如果堆的有序状态因为某个节点变得比它的父节点更大而被打破时,那么就需要通过交换它和它的父节点来修复堆,将这个节点不断向上移动直到遇到了一个更大的父节点.(如果是最小堆,比较的逻辑相反). 1234567// 在本文中,均不使用数组的0索引 private void swim(int k) &#123; while (k &gt; 1 &amp;&amp; less(k/2, k)) &#123; exch(a,k, k/2); k = k/2; &#125; &#125; 由上至下的堆有序化(下沉) : 如果堆的有序状态因为某个节点变得比它的两个子节点或是其中之一更小了而被打破时,需要通过将它和它的两个子节点中的较大者交换来修复堆,将这个节点向下移动直到它的子节点都比它更小或是到达了堆的底部.(如果是最小堆,比较的逻辑想法) 12345678910// n为数组长度private void sink(int k) &#123; while (2*k &lt;= n) &#123; int j = 2*k; if (j &lt; n &amp;&amp; less(j, j+1)) j++; if (!less(a[k],a[j])) break; exch(a,k, j); k = j; &#125; &#125; 运行过程 堆排序可以分为两个阶段. 堆的构造阶段,将原始数组重新组织安排进一个堆中.从右至左用sink()函数,构造子堆,数组的每个位置都已经是一个子堆的根节点.只需要扫描数组中的一半元素,因为我们可以跳过大小为1的子堆.最后在位置1上调用sink()函数,结束扫描. 下沉排序阶段,从堆中按递减顺序取出所有元素并得到排序结果.将堆中的最大元素删除,然后放入堆缩小后数组中空出的位置. 代码实现123456789101112131415161718192021222324252627282930313233343536public static void sort(Comparable[] a) &#123; int N = a.length; // construction max heap for (int k = N / 2; k &gt;= 1; k--) &#123; sink(a, k, N); &#125; // sink sort while (N &gt; 1) &#123; // the biggest element (root) swap smallest element then heap shrink exch(a, 1, N--); // new root element sink sink(a, 1, N); &#125; &#125; private static void sink(Comparable[] pq, int k, int n) &#123; while (2 * k &lt;= n) &#123; int j = 2 * k; if (j &lt; n &amp;&amp; less(pq, j, j + 1)) j++; if (!less(pq, k, j)) break; exch(pq, k, j); k = j; &#125; &#125; private static boolean less(Comparable[] pq, int i, int j) &#123; return pq[i - 1].compareTo(pq[j - 1]) &lt; 0; &#125; private static void exch(Object[] pq, int i, int j) &#123; Object swap = pq[i - 1]; pq[i - 1] = pq[j - 1]; pq[j - 1] = swap; &#125; 总结 名称 是否稳定 是否为原地排序 时间复杂度 空间复杂度 备注 冒泡排序 是 是 O(N^2) O(1) （无序区，有序区）。从无序区通过交换找出最大元素放到有序区前端。 选择排序 否 是 O(N^2) O(1) （有序区，无序区）。在无序区里找一个最小的元素跟在有序区的后面。对数组：比较得多，换得少。 插入排序 是 是 介入N和N^2之间 O(1) （有序区，无序区）。把无序区的第一个元素插入到有序区的合适的位置。对数组：比较得少，换得多。 希尔排序 否 是 O(N log^2 N) O(1) 每一轮按照事先决定的间隔进行插入排序，间隔会依次缩小，最后一次一定要是1。 快速排序 否 是 O(N log N) O(logN) （小数，基准元素，大数）。在区间中随机挑选一个元素作基准，将小于基准的元素放在基准之前，大于基准的元素放在基准之后，再分别对小数区与大数区进行排序。 三向快速排序 否 是 介于N和NlogN之间 O(logN) 对含有大量重复元素的输入数据效率较高。 归并排序 是 否 O(N log N) O(N) 把数据分为两段，从两段中逐个选最小的元素移入新数据段的末尾。 堆排序 否 是 O(N log N) O(1) （最大堆，有序区）。从堆顶把根卸出来放在有序区之前，再恢复堆。 在大多数实际情况中,快速排序是最佳选择.如果稳定性很重要而空间又不是问题的情况下,归并排序可能是最好的.但是在运行时间至关重要的任何排序应用中应该认真地考虑使用快速排序. 在JDK中,Arrays.sort()选择了根据不同的参数类型,来使用不同的排序算法.如果是原始数据类型则使用三向切分的快速排序,对引用类型则使用归并排序. end Author : SylvanasSun Email : sylvanassun_xtz@163.com 文中的完整实现代码见我的GitHub &amp; Gist 本文参考资料引用自&lt;&gt; &amp; WikiPedia]]></content>
      <categories>
        <category>Algorithms</category>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>Algorithms</tag>
        <tag>排序算法</tag>
        <tag>2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用链表做为Stack、Queue中的数据表示结构的基本思路]]></title>
    <url>%2F2017%2F03%2F06%2F2017-3-06-LinkedStack%26Queue%2F</url>
    <content type="text"><![CDATA[什么是链表? 链表是一种常见的基础数据结构(数组也是基础数据结构),它是一种递归的数据结构,由一系列节点(Node)组成,节点含有一个存储数据的数据域和一个指向下一个节点地址位置的引用. 链表是线性表的一种,但是它的物理存储结构是非连续、 非顺序的,元素的逻辑顺序是通过节点之间的链接确定的. 数据结构与数据类型的区别 数据类型是一组数据和一组对这些值进行操作的集合. 数据结构强调的是数据的存储和组织方式. 常见的数据结构有:数组、栈、队列、链表、树、图、堆、散列表. 链表是否可以替代数组? 由于创建数组需要预先知道数组的大小,所以想要动态的扩容需要不断地创建新数组,而链表则可以充分利用内存空间,实现较为灵活的动态扩展. 使用链表替代数组有优点也有缺点: 优点 在链表中进行插入操作或是删除操作都更加方便快速. 链表所需的空间总是和集合的大小成正比. 链表操作所需的时间总是和集合的大小无关. 缺点 无法像数组一样可以通过索引来进行随机访问. 由于每一个元素节点都是一个对象,所以需要的空间开销比较大. Stack 栈是一种基于后进先出(LIFO)的数据结构,其中的元素除了头尾之外,每个元素都有一个前驱和一个后继. 使用链表来表示栈内部的数据时,栈顶就是链表的头部,当push元素时将元素添加在表头,当pop元素时将元素从表头删除. 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788/** * 使用链表实现的可迭代的下压栈(后进先出) * &lt;p&gt; * Created by SylvanasSun on 2017/3/6. */public class Stack&lt;T&gt; implements Iterable&lt;T&gt; &#123; private Node first; //栈顶(链表头部) private int N; //元素个数 /** * 用于表示链表中的节点 */ private class Node &#123; T item; Node next; &#125; /** * 判断Stack是否为空 * * @return true代表Stack为空, false为未空 */ public boolean isEmpty() &#123; return first == null; //也可以用N==0来判断 &#125; /** * 返回Stack中的元素数量 * * @return 元素数量 */ public int size() &#123; return N; &#125; /** * 将元素t添加到栈顶 * * @param t 添加的元素 */ public void push(T t) &#123; Node oldFirst = first; first = new Node(); first.item = t; first.next = oldFirst; N++; &#125; /** * 将栈顶的元素弹出 * * @return 栈顶节点的item */ public T pop() &#123; T item = first.item; first = first.next; N--; return item; &#125; public Iterator&lt;T&gt; iterator() &#123; return new ListIterator(); &#125; /** * 迭代器,维护了一个实例变量current来记录链表的当前结点. * 这段代码可以在Stack和Queue之间复用,因为它们内部数据的数据结构是相同的, * 只是访问顺序分别为后进先出和先进先出而已. */ private class ListIterator implements Iterator&lt;T&gt; &#123; private Node current = first; public boolean hasNext() &#123; return current != null; &#125; public T next() &#123; T item = current.item; current = current.next; return item; &#125; public void remove() &#123; &#125; &#125;&#125; Queue 队列是一种基于先进先出(FIFO)的数据结构,元素的处理顺序就是它们被添加到队列中的顺序. 可以使用实例变量first指向队列的队头,实例变量last指向队列的队尾,当将一个元素入列时,就将这个元素添加到队尾,当要将一个元素出列时,就删除队头的节点. 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798/** * 使用链表实现的Queue,它与Stack的区别在于链表的访问顺序. * Queue的访问顺序是先进先出的. * &lt;p&gt; * Created by SylvanasSun on 2017/3/6. */public class Queue&lt;T&gt; implements Iterable&lt;T&gt; &#123; private Node first; //链表头部,即队头 private Node last; //链表尾部,即队尾 private int N; //size /** * 用于表示链表中的节点 */ private class Node &#123; T item; Node next; &#125; /** * 判断Queue是否为空 * * @return true代表Stack为空, false为未空 */ public boolean isEmpty() &#123; return first == null; &#125; /** * 返回Queue中的元素数量 * * @return 元素数量 */ public int size() &#123; return N; &#125; /** * 入队,向队尾添加新的元素 * * @param item 添加的元素 */ public void enqueue(T item) &#123; Node oldLast = last; last = new Node(); last.item = item; last.next = null; /** * 如果队列为空,队头指向队尾(队列中只有一个元素), * 否则将旧的队尾的next指向新的队尾 */ if (isEmpty()) first = last; else oldLast.next = last; N++; &#125; /** * 出队,将队头节点弹出队列 * * @return 队头节点的item */ public T dequeue() &#123; T item = first.item; first = first.next; N--; //如果队列为空,队尾则为null if (isEmpty()) last = null; return item; &#125; public Iterator&lt;T&gt; iterator() &#123; return new ListIterator(); &#125; /** * 迭代器,与Stack中的实现一致 */ private class ListIterator implements Iterator&lt;T&gt; &#123; private Node current = first; public boolean hasNext() &#123; return current != null; &#125; public T next() &#123; T item = current.item; current = current.next; return item; &#125; public void remove() &#123; &#125; &#125;&#125; end Author: SylvanasSun GitHub: https://github.com/SylvanasSun Email: sylvanassun_xtz@163.com Reference: 《Algorithms 4th edition》&amp; wiki]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>LinkedTable</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>2017</tag>
        <tag>LinkedTable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现一个简单的HashMap]]></title>
    <url>%2F2017%2F01%2F12%2F2017-01-12-SimpleHashMap%2F</url>
    <content type="text"><![CDATA[概述 &nbsp;&nbsp;HashMap是基于哈希表的Map接口的实现,以key-value的形式存在,在HashMap中,key-value会被当成一个整体(Entry)来处理,HashMap内部维护了一个链表数组,会根据hash算法来计算key-value在数组中的存储位置. HashMap的内部结构 &nbsp;&nbsp;HashMap内部使用了桶(bucket)来存储键值对,桶就是一个存储key-value的链表.而HashMap中维护了一个数组,这个数组的每个元素就是一个桶(bucket),在HashMap中,桶是使用链表实现的. HashMap使用散列函数(hash)将给定键转化为一个数组索引(桶号),不同的key会被转化为不同的索引,实际中有几率会把不同的键转化为同一个索引,这种情况叫做hash碰撞,HashMap使用了拉链法解决hash碰撞. 当发生hash碰撞时,会生成一个新的key-value对象,并将新对象挂在链表头部. 得到数组索引后,就可以遍历这个链表来进行各种操作了. &nbsp;&nbsp;在HashMap中有2个重要的参数:capaCity(容量),loadFactor(负载因子). 容量:它表示HashMap中桶的数量. 负载因子:它是HashMap在其容量自动扩容之前可以达到多满的一种尺度,它衡量的是一个散列表的空间使用程度,负载因子越大表示散列表的空间利用率越大,反之越小.对于使用拉链法的散列表来说,查找一个元素的平均时间是O(1+a),因此负载因子越大,对空间的利用越充分,但是查找效率就会越低,如果负载因子很小,那么散列表的空间利用率将会很小,对空间造成严重浪费,但是查找效率则会变快.HashMap中负载因子的默认值为0.75. 阈值:容量自动扩展的阈值,当HashMap中的键值对数量到达阈值时,将会进行自动扩容(当前容量x2),阈值通常的计算方法为 capaCity * loadFactor. 简单实现 construction12345678910111213141516171819202122232425/** * 构造一个空的SimpleHashMap,使用默认的capacity和负载因子 */public SimpleHashMap(int initialiCapacity, float loadFactor) &#123; if (initialiCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialiCapacity); if (initialiCapacity &gt; MAXIMUM_CAPACITY) initialiCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); //计算出大于initialCapacity的最小的2的n次方值 int capacity = 1; while (capacity &lt; initialiCapacity) &#123; capacity &lt;&lt;= 1; &#125; this.loadFactor = loadFactor; //设置HashMap的扩容阈值,当到达这个阈值时会进行自动扩容 threshold = (int) (capacity * loadFactor); //初始化table数组 table = new Node[capacity];&#125; &nbsp;&nbsp;其中table是一个Node数组,它是由链表实现的. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748static class Node&lt;K, V&gt; implements Map.Entry&lt;K, V&gt; &#123; final int hash; final K key; V value; Node&lt;K, V&gt; next; Node(int hash, K key, V value, Node&lt;K, V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?, ?&gt; e = (Entry&lt;?, ?&gt;) o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) &#123; return true; &#125; &#125; return false; &#125; &#125; put1234567891011121314151617181920212223public V put(K key, V value) &#123; //如果key为null,调用putForNullKey()向null key存入value if (key == null) return putForNullKey(value); //计算key的hash int hash = hash(key.hashCode()); ---1 //计算key的hash在table数组中的索引 int i = indexFor(hash, table.length); --2 //遍历table for (Node&lt;K, V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; //如果有相同的key,直接覆盖value,返回oldValue if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; return oldValue; &#125; &#125; modCount++; //修改次数++ //将key,value添加至i处 addEntry(hash, key, value, i); --3 return null; &#125; &nbsp;&nbsp;看以上代码1、2处,这2个函数计算了hash值和bucket索引. 1234567891011121314151617181920212223242526/** * 预处理hash值，避免较差的离散hash序列，导致桶没有充分利用. */static int hash(int h) &#123; h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125;/** * 返回对应hash值得索引 */static int indexFor(int h, int length) &#123; /** * 由于length是2的n次幂，所以h &amp; (length-1)相当于h % length。 * 对于length，其2进制表示为1000...0，那么length-1为0111...1。 * 那么对于任何小于length的数h，该式结果都是其本身h。 * 对于h = length，该式结果等于0。 * 对于大于length的数h，则和0111...1位与运算后， * 比0111...1高或者长度相同的位都变成0， * 相当于减去j个length，该式结果是h-j*length， * 所以相当于h % length。 * 其中一个很常用的特例就是h &amp; 1相当于h % 2。 * 这也是为什么length只能是2的n次幂的原因，为了优化。 */ return h % (length - 1);&#125; &nbsp;&nbsp;代码3的addEntry函数向数组添加了一对key-value. 123456789101112/** * 添加一对key-value,如果当前索引上已有桶(发生hash碰撞),则将新元素放入链表头 */void addEntry(int hash, K key, V value, int bucketIndex) &#123; //保存对应table的值 Node&lt;K, V&gt; e = table[bucketIndex]; //用新桶链住旧桶 table[bucketIndex] = new Node&lt;K, V&gt;(hash, key, value, e); //如果HashMap中元素的个数已经超过阈值,则扩容两倍 if (size++ &gt;= threshold) resize(2 * table.length);&#125; get12345678910111213141516public V get(Object key) &#123; //如果key为null,则调用getForNullkey()获得key为null的值 if (key == null) return getForNullKey(); //根据key的hashCode计算它的hash int hash = hash(key.hashCode()); //取出table中指定索引处的值 for (Node&lt;K, V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; //如果查找相同的key,返回其对应的value if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; return e.value; &#125; &#125; return null; &#125; all123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318/** * 一个简单的HashMap,内部使用拉链法解决hash碰撞. * &lt;p&gt; * Created by sylvanasp on 2017/1/12. */public class SimpleHashMap&lt;K, V&gt; extends AbstractMap&lt;K, V&gt; implements Map&lt;K, V&gt;, Cloneable, Serializable &#123; private static final long serialVersionUID = 6623475452522370065L; /** * 默认的容量(bucket数量),1 &lt;&lt; 4(16). */ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; /** * 最大的容量,1 &lt;&lt; 30 (2^30) */ static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; /** * 默认的负载因子,0.75. */ static final float DEFAULT_LOAD_FACTOR = 0.75f; /** * KV链表 */ static class Node&lt;K, V&gt; implements Map.Entry&lt;K, V&gt; &#123; final int hash; final K key; V value; Node&lt;K, V&gt; next; Node(int hash, K key, V value, Node&lt;K, V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?, ?&gt; e = (Entry&lt;?, ?&gt;) o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) &#123; return true; &#125; &#125; return false; &#125; &#125; /** * 链表数组,数组中的每一个元素代表了一个链表的头部. */ transient Node[] table; /** * 当前map的key-value映射数，也就是当前size */ transient int size; /** * 代表这个HashMap修改key-value的次数. */ transient int modCount; /** * 自动扩展的阈值(capacity * loadfactor) */ int threshold; /** * 负载因子: * 它是哈希表在其容量自动增加之前可以达到多满的一种尺度， * 它衡量的是一个散列表的空间的使用程度，负载因子越大表示散列表的装填程度越高， * 反之愈小。对于使用链表法的散列表来说，查找一个元素的平均时间是O(1+a)， * 因此如果负载因子越大，对空间的利用更充分，然而后果是查找效率的降低； * 如果负载因子太小，那么散列表的数据将过于稀疏，对空间造成严重浪费. */ final float loadFactor; /** * 构造一个空的SimpleHashMap,使用默认的capacity和负载因子 */ public SimpleHashMap(int initialiCapacity, float loadFactor) &#123; if (initialiCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialiCapacity); if (initialiCapacity &gt; MAXIMUM_CAPACITY) initialiCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); //计算出大于initialCapacity的最小的2的n次方值 int capacity = 1; while (capacity &lt; initialiCapacity) &#123; capacity &lt;&lt;= 1; &#125; this.loadFactor = loadFactor; //设置HashMap的扩容阈值,当到达这个阈值时会进行自动扩容 threshold = (int) (capacity * loadFactor); //初始化table数组 table = new Node[capacity]; &#125; public SimpleHashMap() &#123; this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR); &#125; /** * 预处理hash值，避免较差的离散hash序列，导致桶没有充分利用. */ static int hash(int h) &#123; h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); &#125; /** * 返回对应hash值得索引 */ static int indexFor(int h, int length) &#123; /** * 由于length是2的n次幂，所以h &amp; (length-1)相当于h % length。 * 对于length，其2进制表示为1000...0，那么length-1为0111...1。 * 那么对于任何小于length的数h，该式结果都是其本身h。 * 对于h = length，该式结果等于0。 * 对于大于length的数h，则和0111...1位与运算后， * 比0111...1高或者长度相同的位都变成0， * 相当于减去j个length，该式结果是h-j*length， * 所以相当于h % length。 * 其中一个很常用的特例就是h &amp; 1相当于h % 2。 * 这也是为什么length只能是2的n次幂的原因，为了优化。 */ return h % (length - 1); &#125; /** * 获得key为null的值 */ private V getForNullKey() &#123; //遍历table[0] for (Node&lt;K, V&gt; e = table[0]; e != null; e = e.next) &#123; //如果找到key为null,则返回对应的值 if (e.key == null) &#123; return e.value; &#125; &#125; return null; &#125; /** * 当Key为Null时如何放入值 */ private V putForNullKey(V value) &#123; //遍历table[0] for (Node&lt;K, V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; //取出oldValue,并存入newValue V oldValue = e.value; e.value = value; //返回oldValue return oldValue; &#125; &#125; modCount++; addEntry(0, null, value, 0); return null; &#125; /** * 添加一对key-value,如果当前索引上已有桶(发生hash碰撞),则将新元素放入链表头 */ void addEntry(int hash, K key, V value, int bucketIndex) &#123; //保存对应table的值 Node&lt;K, V&gt; e = table[bucketIndex]; //用新桶链住旧桶 table[bucketIndex] = new Node&lt;K, V&gt;(hash, key, value, e); //如果HashMap中元素的个数已经超过阈值,则扩容两倍 if (size++ &gt;= threshold) resize(2 * table.length); &#125; /** * 扩充容量 */ void resize(int newCapacity) &#123; //保存oldTable Node[] oldTable = table; //保存旧容量 int oldCapacity = oldTable.length; //如果旧的容量已经是系统默认最大容量了，那么将阈值设置成整形的最大值 if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; //根据newCapacity创建一个table Node[] newTable = new Node[newCapacity]; //将table转换为newTable transfer(newTable); table = newTable; //设置阈值 threshold = (int) (newCapacity * loadFactor); &#125; // 将所有格子里的桶都放到新的table中 void transfer(Node[] newTable) &#123; // 得到旧的table Node[] src = table; // 得到新的容量 int newCapacity = newTable.length; // 遍历src里面的所有格子 for (int j = 0; j &lt; src.length; j++) &#123; // 取到格子里的桶（也就是链表） Node&lt;K, V&gt; e = src[j]; // 如果e不为空 if (e != null) &#123; // 将当前格子设成null src[j] = null; // 遍历格子的所有桶 do &#123; // 取出下个桶 Node&lt;K, V&gt; next = e.next; // 寻找新的索引 int i = indexFor(e.hash, newCapacity); // 设置e.next为newTable[i]保存的桶（也就是链表连接上） e.next = newTable[i]; // 将e设成newTable[i] newTable[i] = e; // 设置e为下一个桶 e = next; &#125; while (e != null); &#125; &#125; &#125; @Override public V get(Object key) &#123; //如果key为null,则调用getForNullkey()获得key为null的值 if (key == null) return getForNullKey(); //根据key的hashCode计算它的hash int hash = hash(key.hashCode()); //取出table中指定索引处的值 for (Node&lt;K, V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; //如果查找相同的key,返回其对应的value if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; return e.value; &#125; &#125; return null; &#125; @Override public V put(K key, V value) &#123; //如果key为null,调用putForNullKey()向null key存入value if (key == null) return putForNullKey(value); //计算key的hash int hash = hash(key.hashCode()); //计算key的hash在table数组中的索引 int i = indexFor(hash, table.length); //遍历table for (Node&lt;K, V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; //如果有相同的key,直接覆盖value,返回oldValue if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; return oldValue; &#125; &#125; modCount++; //修改次数++ //将key,value添加至i处 addEntry(hash, key, value, i); return null; &#125; @Override public Set&lt;Entry&lt;K, V&gt;&gt; entrySet() &#123; return null; &#125; public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new SimpleHashMap&lt;&gt;(); map.put("hello", "world"); System.out.println(map.get("hello")); &#125;&#125;]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>HashTable</category>
      </categories>
      <tags>
        <tag>HashTable</tag>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探秘HotSpot虚拟机中的对象]]></title>
    <url>%2F2016%2F09%2F05%2F2016-09-5-HotSpotObject%2F</url>
    <content type="text"><![CDATA[对象的创建 &nbsp;&nbsp;在Java程序运行过程中无时无刻都有对象被创建出来.在语言层面上,创建对象通常仅仅是一个new关键字而已,而在虚拟机中,创建一个对象不像只需要new一下那么简单了. &nbsp;&nbsp;以下为虚拟机中对象创建的过程(仅限于普通Java对象,不包括数组和Class对象等). 虚拟机遇到一条new指令时,首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用,并且检查这个符号引用代表的类是否已被加载、解析和初始化过.如果没有,那必须先执行相应的类加载过程. 在类加载检查通过后,接下来虚拟机将为新生对象分配内存.对象所需内存的大小在类加载完成后便可完全确定,为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来. 如果Java堆中内存是绝对规整的,所有用过的内存都放在一边,中间放着一个指针作为分界点的指示器,那所分配内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离,这种分配方式称为”指针碰撞(Bump the Pointer)”.在使用Serial、ParNew等待Compact过程的收集器时,系统采用的分配算法为指针碰撞. 如果Java堆中内存并不是规整的,已使用的内存和空闲的内存相互交错,那就没有办法简单地进行指针碰撞了,虚拟机就必须维护一个列表,记录上哪些内存块是可用的,在分配的时候从列表中找到一块足够大的空间划分给对象实例,并更新列表上的记录,这种分配方式称为”空闲列表(Free List)”.在使用CMS这种基于Mark-Sweep算法的收集器,通常采用空闲列表. 由于创建对象是一件非常频繁的事情,所以除了划分可用空间之外,虚拟机还要考虑线程安全的问题,可能出现正在给对象A分配内存,指针还没来得及修改,对象B又同时使用了原来的指针来分配内存的情况.解决这个问题有两种方案,一种是对分配内存空间的动作进行同步处理(虚拟机采用CAS配上失败重试的方式保证更新操作的原子性);另一种是把内存分配的动作按照线程划分在不同的空间之中进行,即每个线程在Java堆中预先分配一小块内存,称为本地线程分配缓冲(Thread Local Allocation Buffer TLAB).哪个线程要分配内存,就在哪个线程的TLAB上分配,只有TLAB用完并分配新的TLAB时,才需要同步锁定.虚拟机是否使用TLAB,可以通过-XX:+/-UseTLAB参数来设置. 在内存分配完成后,虚拟机需要将分配到的内存空间都初始化为零值(不包括对象头),如果使用TLAB,这一工作也可以提前至TLAB分配时进行.这一步操作保证了对象的实例字段在Java代码中可以不赋初始值就直接使用,程序能访问到这些字段的数据类型所对应的零值. 最后一步,虚拟机要对对象进行必要的设置,例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息.这些信息存放在对象的对象头(Object Header)之中.根据虚拟机当前的运作状态的不同,如是否启动偏向锁等,对象头会有不同的设置方式. 以上工作全部完成后,从虚拟机的角度来看,一个新的对象已经产生了,但从Java程序的视角来看,对象的创建才刚刚开始(init方法还没有执行,所有字段都还为零值).一般来说(由字节码中是否跟随invokespecial指令所决定),执行new指令之后会接着执行init()方法,把对象按照程序员的意愿进行初始化,这样一个真正可用的对象才算完全产生出来. 对象的内存布局 &nbsp;&nbsp;在HotSpot虚拟机中,对象在内存中存储的布局可以分为三块区域:对象头(Object Header)、实例数据(Instance Data)和对齐填充(Padding). 对象头&nbsp;&nbsp;对象头包括两部分信息,第一部分用于存储对象自身的运行时数据(例如哈希码、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等),这部分数据的长度在32位和64位的虚拟机(未开启压缩指针)中分别为32bit和64bit,官方称其为”Mark Word”.这些运行时数据很多,其实已经超过了32位、64位Bitmap结构所能记录的限度.考虑到虚拟机的空间效率,Mark Word被设计成一个非固定的数据结构以便在极小的空间内存储尽量多的信息,它会根据对象的状态复用自己的存储空间.例如,在32位的HotSpot虚拟机中,如果对象处于未被锁定的状态下,那么Mark Word的32bit空间中的25bit用于存储对象哈希码,4bit用于存储对象分代年龄,2bit用于存储锁标志位,1bit固定为0,而在其他状态(轻量级锁定、重量级锁定、GC标记、可偏向)下对象的存储内容参见下表. 存储内容 标志位 状态 对象哈希码、对象分代年龄 01 未锁定 指向锁记录的指针 00 轻量级锁定 指向重量级锁的指针 10 膨胀(重量级锁定) 空,不需要记录信息 11 GC标记 偏向线程ID、偏向时间戳、对象分代年龄 01 可偏向 &nbsp;&nbsp;对象头的另一部分为类型指针,即对象指向它的类元数据的指针,虚拟机通过这个指针来确定这个对象是哪个类的实例.并不是所有的虚拟机实现都必须在对象数据上保留类型指针,也可以认为,查找对象的元数据信息并不一定要经过对象本身.如果对象是一个Java数组,那在对象头中还必须有一块用于记录数组长度的数据,因为虚拟机可以通过普通Java对象的元数据信息确定Java对象的大小,但是从数组的元数据中无法确定数组的大小. 实例数据&nbsp;&nbsp;实例数据是对象真正存储的有效信息,也是在程序代码中所定义的各种类型的字段内容.无论是从父类继承下来的,还是子类中定义的,都需要记录起来. &nbsp;&nbsp;实例数据的存储顺序会受到虚拟机分配策略参数(FieldsAllocationStyle)和字段在Java源码中定义顺序的影响.HotSpot虚拟机默认的分配策略为longs/doubles、ints、shorts/chars、bytes/booleans、oops(Ordinary Object Pointers),从以上分配策略中可以看出,相同宽度的字段总是被分配到一起.在满足这个前提条件的情况下,父类中定义的变量会出现在子类之前.如果CompactFields参数值为true(默认为true),那么子类中较窄的变量也可能会插入到父类变量的空隙之中. 对齐填充&nbsp;&nbsp;对齐填充并不是必然存在的,它也没有特别的含义,只是用于当作占位符而已. &nbsp;&nbsp;因为HotSpot虚拟机的自动内存管理系统要求对象起始地址必须是8字节的整数值,即是对象的大小必须是8字节的整数倍.而对象头部分正好是8字节的倍数(1倍或2倍),所以,当对象实例数据部分没有对齐时,就需要通过对齐填充来补全. 对象的访问定位 &nbsp;&nbsp;Java程序需要通过栈上的reference数据来操作堆上的具体对象.由于reference类型在Java虚拟机规范中只规定了一个指向对象的引用,所以对象访问方式也是取决于虚拟机实现而定的. &nbsp;&nbsp;目前主流的访问方式为句柄和直接指针两种. 句柄 &nbsp;&nbsp;使用句柄访问对象,Java堆中将会划分出一块内存作为句柄池,reference中存储的就是对象的句柄地址,而句柄中包含了对象实例数据与类型数据各自的具体地址信息. &nbsp;&nbsp;使用句柄来访问的最大好处就是reference中存储的是稳定的句柄地址,在对象被移动时(垃圾收集时移动对象是非常普遍的行为)只会改变句柄中的实例数据指针,而reference本身不需要修改. 直接指针 &nbsp;&nbsp;使用直接指针访问对象,Java堆对象的布局中就必须考虑如何放置访问类型数据的相关信息,而reference中存储的直接就是对象地址. &nbsp;&nbsp;使用直接指针访问方式的最大好处就是速度更快,它节省了一次指针定位的时间开销,由于对象的访问在Java中非常频繁,积少成多后也是一项非常可观的执行成本.HotSpot虚拟机就是使用直接指针方式进行对象访问的. End 资料参考于 &lt;&lt;深入理解 Java虚拟机 第二版&gt;&gt;.]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机中的内存区域]]></title>
    <url>%2F2016%2F09%2F04%2F2016-09-4-JavaMemoryRegion%2F</url>
    <content type="text"><![CDATA[概述 &nbsp;&nbsp;对于C/C++程序员来说需要自己负责每一个对象生命开始到终结的维护.而对于Java程序员来说,则可以在Java虚拟机自动内存管理机制的帮助下,不需要为每一个new的对象去写delete/free代码,由虚拟机管理内存可以让我们把注意力放在实现业务逻辑上. &nbsp;&nbsp;但也正是因为Java虚拟机接管了内存控制的权力,所以一旦出现内存泄漏或溢出方面的异常问题,就需要了解虚拟机如何使用与维护内存,这样才能更好的排查错误解决异常. 运行时数据区域 &nbsp;&nbsp;Java虚拟机在执行Java程序时会把它所管理的内存划分为若干个不同的数据区域.这些区域都有着各自的用途,以及创建和销毁的时间,有的区域则会随着虚拟机进程的启动而存在,有些区域则会依赖于用户线程的启动和结束而创建和销毁. &nbsp;&nbsp;根据&lt;&gt;的规定,Java虚拟机管理的内存将会包括以下图所示的几个运行时数据区域. 程序计数器&nbsp;&nbsp;程序计数器(Program Counter Register)可以看作为当前线程所执行的字节码的行号指示器,它占用的内存很小. &nbsp;&nbsp;字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令,分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖程序计数器来完成. &nbsp;&nbsp;在Java虚拟机中,多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的.在任何一个确定的时刻,一个处理器(或多核CPU中的一个内核)都只会执行一条线程中的指令.为了线程切换后能够恢复到正确的执行位置,每一条线程都需要有一个独立的程序计数器,各条线程之间计数器互不影响,独立存储,这种类型的内存区域为”线程私有”的内存. &nbsp;&nbsp;如果线程正在执行的是一个Java方法,程序计数器记录的是正在执行的虚拟机字节码指令的地址;如果正在执行的是一个Native方法,程序计数器值则为空(Undefined). &nbsp;&nbsp;程序计数器内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError异常情况的区域. 虚拟机栈&nbsp;&nbsp;Java虚拟机栈(Java Virtual Machine Stacks)与程序计数器一样,是线程私有的内存,它的生命周期与线程相同. &nbsp;&nbsp;虚拟机栈描述的是Java方法执行的内存模型:即每个Java方法在执行的同时都会创建一个栈帧(Stack Frame)用于存储局部变量表、操作数栈、动态链接、方法出口等信息.栈帧是方法运行时的基础数据结构. &nbsp;&nbsp;每一个方法从调用直至执行完成的过程,就对应着一个栈帧在虚拟机栈中入栈到出栈的过程. &nbsp;&nbsp;栈帧中的局部变量表存放了编译期可知的各种基本数据类型(boolean、byte、char、short、int、float、long、double)、对象引用类型(reference类型,它不等同于对象本身,可能是一个指向对象起始地址的引用指针,也可能是指向一个代表对象的句柄或其他与此对象相关的位置)和returnAddress类型(指向了一条字节码指令的地址). &nbsp;&nbsp;64位长度的long和double类型的数据会占用2个局部变量空间(Slot),其他的数据类型只占用1个.局部变量表所需的内存空间是在编译期间完成分配的.当进入一个方法时,这个方法需要在栈帧中分配多大的局部变量空间是完全确定的,在方法运行期间不会改变局部变量表的大小. &nbsp;&nbsp;Java虚拟机规范对虚拟机栈区域规定了两种异常情况: 如果线程请求的栈深度大于虚拟机所允许的深度,将会抛出StackOverflowError异常. 如果虚拟机可以动态扩展,并在扩展时无法申请到足够的内存时,将会抛出OutOfMemoryError异常. 本地方法栈&nbsp;&nbsp;本地方法栈(Native Method Stack)与虚拟机栈基本相似.它们之间的区别只不过是虚拟机栈为虚拟机执行Java方法(也就是字节码)服务,而本地方法栈则为虚拟机使用到的Native方法服务. &nbsp;&nbsp;Java虚拟机规范对本地方法栈没有强制的规定,具体的虚拟机可以自由实现它.例如Sun HotSpot虚拟机甚至将本地方法栈与虚拟机栈合二为一. &nbsp;&nbsp;与虚拟机栈相同,本地方法栈也会抛出StackOverflowError和OutOfMemoryError异常. Java堆&nbsp;&nbsp;在大多数情况下,Java堆(Java Heap)是Java虚拟机所管理的内存中最大的一块.它是被所有线程共享的一块内存区域,在虚拟机启动时创建.Java堆的唯一目的就是存放对象实例,几乎所有的对象实例都在这里分配内存. &nbsp;&nbsp;但随着JIT编译器的发展与逃逸分析技术逐渐成熟,栈上分配、标量替换优化技术将会导致一些微妙的变化发生,所有对象都分配在堆上也渐渐变得不是那么”绝对”了. &nbsp;&nbsp;Java堆也是垃圾收集器管理的主要区域,也可以称为GC堆(Garbage Collected Heap).由于现在垃圾收集器基本都采用分代收集算法,所以Java堆中可以细分为新生代和老年代(再细化的有Eden空间、Form Survivor空间、To Survivor空间等). &nbsp;&nbsp;从内存分配的角度来看,线程共享的Java堆中可能划分出多个线程私有的分配缓冲区(Thread Local Allocation Buffer TLAB). &nbsp;&nbsp;Java虚拟机规范规定,Java堆可以处于物理上不连续的内存空间中,只要逻辑上是连续的即可.在实现时,既可以实现是固定大小的,也可以是可扩展的,如果在堆中没有内存完成实例分配,并且堆也无法再扩展时,将会抛出OutOfMemoryError异常. 方法区&nbsp;&nbsp;方法区(Method Area)与Java堆一样是各个线程共享的内存区域,它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据.Java虚拟机规范中把方法区描述为堆的一个逻辑部分,但它有一个别名叫Non-Heap(非堆),用于与Java堆区分开来. &nbsp;&nbsp;在HotSpot虚拟机中,方法区被称作为”永久代(Permanent Generation)”.因为HotSpot将GC分代收集扩展至了方法区,或者说是使用永久代来实现方法区.这样HotSpot的垃圾收集器可以像管理Java堆一样管理这部分内存,能够省去编写方法区的内存管理代码工作.但这样做法会更容易遇到内存溢出问题.所以HotSpot官方决定放弃永久代并逐步改为采用Native Memory来实现方法区的规划.在JDK1.7的HotSpot中,已经把原本放在永久代的字符串常量池移出. &nbsp;&nbsp;Java虚拟机规范规定,当方法区无法满足内存分配需求时,将抛出OutOfMemoryError异常. 运行时常量池&nbsp;&nbsp;运行时常量池(Runtime Constant Pool)是方法区的一部分.Class文件中除了有类的版本、字段、方法、接口等描述信息外,还有一项信息是常量池(Constant Pool Table),它用于存放编译期生成的各种字面量和符号引用,这部分内容将在类加载后进入方法区的运行时常量池中存放.一般来说,除了保存Class文件中描述的符号引用外,还会把翻译出来的直接引用也存储在运行时常量池中. &nbsp;&nbsp;运行时常量池相对于Class文件中常量池的另外一个重要特征是具备动态性.Java语言并不要求常量一定只有编译期才能产生,运行期间也可能将新的常量放入池中,这种特性利用得比较多的便是String类的intern()方法. &nbsp;&nbsp;当运行时常量池无法申请到内存时会抛出OutOfMemoryError异常. 直接内存&nbsp;&nbsp;直接内存(Direct Memory)并不是虚拟机运行时数据区域的一部分,也不是Java虚拟机规范中定义的内存区域.但是部分内存也被频繁地使用,而且也可能导致OutOfMemoryError异常. &nbsp;&nbsp;在JDK1.4中新加入了NIO(New Input/Output)类,引入了一种基于通道(Channel)与缓冲区(Buffer)的I/O方式,它可以使用Native函数库直接分配堆外内存,然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作.这样避免了在Java堆中和Native堆中来回复制数据,提高了性能. &nbsp;&nbsp;由于本机直接内存不会受到Java堆大小的限制,但它还是会受到本机总内存大小以及处理器寻址空间的限制.当我们配置虚拟机参数时,会根据实际内存设置-Xmx等参数信息,但经常会忽略直接内存,导致各个内存区域总和大于物理内存限制,从而导致动态扩展时出现OutOfMemoryError异常. OutOfMemoryError异常案例 Java堆溢出1234567891011121314151617181920212223242526272829/** * Java堆内存溢出,Java堆内存的OOM异常是实际应用中常见的内存溢出异常情况. * * 堆内存用于存储对象实例,只要不断地创建对象,并且保证GC Roots到对象之间有可达路径来避免 * 垃圾回收机制清除这些对象,这样就可以在对象数量到达最大堆的容量限制后产生内存溢出异常. * * 以下启动参数中,-Xms20 -Xmx20m 将堆的最小值与最大值设置为一样的,既可避免自动扩展. * -XX:+HeapDumpOnOutOfMemoryError可以让虚拟机在出现内存溢出异常时Dump出当前的内存堆转储快照以便分析. * * VM Args: -Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError * &lt;p&gt; * Created by sylvanasp on 2016/9/4. */public class HeapOOM &#123; static class OOMObject &#123; &#125; public static void main(String[] args) &#123; List&lt;OOMObject&gt; list = new ArrayList&lt;OOMObject&gt;(); while (true) &#123; list.add(new OOMObject()); &#125; &#125;&#125; 虚拟机栈溢出1234567891011121314151617181920212223242526272829303132333435363738/** * 在单线程中,以下两种方法均无法让虚拟机产生OutOfMemoryError异常,尝试的结果均为StackOverflowError异常. * * 1.使用-Xss参数减少栈内存容量.结果为StackOverflowError异常,出现时输出的堆栈深度相应缩小. * * VM Args: -Xss128k * * Created by sylvanasp on 2016/9/4. */public class JavaVMStackSOF &#123; private int stackLength = 1; /** * 定义大量的本地变量,增大此方法帧中本地变量表的长度. * 结果:抛出StackOverflowError异常时输出的堆栈深度相应缩小. */ public void stackLeak() &#123; stackLength++; stackLeak(); &#125; public static void main(String[] args) throws Throwable &#123; /** * 在单线程下,无论是栈帧太大还是虚拟机栈容量太小,当内存无法分配时, * 虚拟机抛出的都是StackOverflowError异常. */ JavaVMStackSOF javaVMStackSOF = new JavaVMStackSOF(); try &#123; javaVMStackSOF.stackLeak(); &#125; catch (Throwable e) &#123; System.out.println("stack length:" + javaVMStackSOF.stackLength); throw e; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940/** * 通过不断建立线程的方式产生内存溢出异常. * 但是这样产生的内存溢出异常与栈空间是否足够大并不存在任何联系. * 在这种情况下,为每个线程的栈分配的内存越大,反而越容易产生内存溢出异常. * 因为内存最后才由虚拟机栈和本地方法栈"瓜分". * 所以每个线程分配到的栈容量越大,可以建立的线程数量自然就越少,建立线程时就越容易把剩下的内存耗尽。 * * 如果是建立过多线程导致的内存溢出,在不能减少线程数或者更换64位虚拟机的情况下. * 就只能通过减少最大堆和减少栈容量来换取更多的线程. * * VM Args: -Xss2M * &lt;p&gt; * Created by sylvanasp on 2016/9/4. */public class JavaVMStackOOM &#123; private void dontStop() &#123; while (true) &#123; &#125; &#125; public void stackLeakByThread() &#123; while (true) &#123; Thread thread = new Thread(new Runnable() &#123; public void run() &#123; dontStop(); &#125; &#125;); thread.start(); &#125; &#125; public static void main(String[] args) &#123; JavaVMStackOOM javaVMStackOOM = new JavaVMStackOOM(); javaVMStackOOM.stackLeakByThread(); &#125;&#125; 方法区溢出123456789101112131415161718192021222324252627282930313233/** * 方法区内存溢出 * 方法区是用于存放Class的相关信息的,所以要让方法区产生内存溢出的基本思路为: * 在运行时产生大量的类去填满方法区,直到溢出. * 所以可以使用CGLib直接操作字节码运行时生成大量的动态类. * * VM Args : -XX:PermSize=10M -XX:MaxPermSize=10M * * Created by sylvanasp on 2016/9/4. */public class JavaMethodAreaOOM &#123; static class OOMObject &#123; &#125; public static void main(final String[] args) &#123; while (true) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(OOMObject.class); enhancer.setUseCache(false); enhancer.setCallback(new MethodInterceptor() &#123; public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; return methodProxy.invokeSuper(o,objects); &#125; &#125;); enhancer.create(); &#125; &#125;&#125; 运行时常量池溢出123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * 方法区和运行时常量池溢出 * 在JDK1.6及之前的版本中,由于常量池分配在永久代内. * 所以可以通过 -XX:PermSize和-XX:MaxPermSize限制方法区大小. * 从而间接限制其中常量池的容量. * * 而使用JDK1.7运行这段程序则不会得到相同的结果,while循环将一直进行下去. * * VM Args : -XX:PermSize=10M -XX:MaxPermSize=10M * * Created by sylvanasp on 2016/9/4. */public class RuntimeConstantPoolOOM &#123; public static void main(String[] args) &#123; // 使用List保持常量池的引用,避免Full GC回收常量池行为. List&lt;String&gt; list = new ArrayList&lt;String&gt;(); int i = 0; while (true) &#123; /** * String.intern()是一个Native方法. * 它的作用是: * 如果字符串常量池中已经包含了一个等于此String对象的字符串,则返回代表池中这个字符串的String对象. * 否则,将此String对象包含的字符串添加到常量池中,并且返回此String对象的引用. */ list.add(String.valueOf(i++).intern()); &#125; &#125; /** * 这段代码在JDK1.6中运行,会得到两个false,而在JDK1.7中运行,会得到一个true和一个false. * 产生差异的原因为: * 在JDK1.6中,intern()方法会把首次遇到的字符串实例复制到永久代中,返回的也是永久代中这个字符串实例的引用. * 而由StringBuilder创建的字符串实例在Java堆上,所以必然不是同一个引用.所以返回false. * * 在JDK1.7中(或部分其他虚拟机,如JRockit),intern()实现不会再复制实例. * 只是在常量池中记录首次出现的实例引用,因此intern()返回的引用和由StringBuilder创建的字符串实例为同一个. * 对str2比较返回false是因为"java"这个字符串在执行StringBuilder.toString()之前已经出现过, * 字符串常量池中已经有它的引用了,不符合"首次出现"的原则. * */ private void stringPool() &#123; String str1 = new StringBuilder("Hello").append("World哈哈").toString(); System.out.println(str1.intern() == str1); String str2 = new StringBuilder("ja").append("va").toString(); System.out.println(str2.intern() == str2); &#125;&#125; 直接内存溢出123456789101112131415161718192021222324252627282930313233343536/** * 直接内存溢出 * DirectMemory容量可以通过 -XX:MaxDirectMemorySize指定. * 如果不指定,则默认与Java堆最大值(-Xmx)一致. * * VM Args : -Xmx20M -XX:MaxDirectMemorySize=10M * * Created by sylvanasp on 2016/9/4. */public class DirectMemoryOOM &#123; private static final int _1MB = 1024 * 1024; /** * 越过DirectByteBuffer类,直接通过反射获取Unsafe实例进行内存分配. * Unsafe.getUnsafe()方法限制了只有引导类加载器才会返回实例, * 即只有rt.jar中的类才能使用Unsafe的功能. * * 使用DirectByteBuffer类分配内存虽然也会抛出内存溢出的异常, * 但它抛出异常时并没有真正的向操作系统申请分配内存,而是通过计算得知内存无法分配,于是手动抛出异常. * * 而unsafe.allocateMemory()则可以真正申请分配内存. * */ public static void main(String[] args) throws IllegalAccessException &#123; Field unsafeField = Unsafe.class.getDeclaredFields()[0]; unsafeField.setAccessible(true); Unsafe unsafe = (Unsafe) unsafeField.get(null); while (true) &#123; unsafe.allocateMemory(_1MB); &#125; &#125;&#125; End 资料参考于 &lt;&lt;深入理解 Java虚拟机 第二版&gt;&gt;.]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在CentOS7下手动编译JDK]]></title>
    <url>%2F2016%2F09%2F03%2F2016-09-3-CompilerOpenJDK%2F</url>
    <content type="text"><![CDATA[下载OpenJDK源码&nbsp;&nbsp;oepnjdk下载地址为 https://jdk7.java.net/source.html 安装编译依赖环境 安装alsa包. 1yum install alsa-lib-devel 安装cups-devel 1yum install cups-devel 安装X相关库 1yum install libX* 安装gcc-c++ 1yum install gcc gcc-c++ 安装freetype 12rpm -ivh freetype-2.4.11-9.el7.x86_64.rpm下载地址:http://rpm.pbone.net/index.php3/stat/4/idpl/26641422/dir/centos_7/com/freetype-2.4.11-9.el7.x86_64.rpm.html 安装Ant 12tar -zvxf apache-ant-1.9.6-bin.tar.gz下载地址:http://ant.apache.org/bindownload.cgi &nbsp;&nbsp;也可以使用以下命令一次性完成依赖安装: 12345yum -y install build-essential gawk m4openjkd-6-jkd libasound2-dev libcups2-dev libxrender-dev xorg-dev xutils-devxllproto-print-dev binutils libmotif3libmotif-dev ant 配置环境变量&nbsp;&nbsp;OpenJDK在编译时需要读取很多环境变量,但大部分都是有默认值的,必须设置的只有两个环境变量: LANG和ALT_BOOTDIR. &nbsp;&nbsp;LANG是用来设定语言选项的,必须设置为: export LANG=C. &nbsp;&nbsp;ALT_BOOTDIR则是用来设置Bootstrap JDK的位置,Bootstrap JDK就是一个可用的6u14以上版本的JDK,它是用来编译OpenJDK中部分Java实现的代码的. &nbsp;&nbsp;另外,如果之前设置了JAVA_HOME和CLASSPATH两个环境变量,在编译之前必须取消,否则在Makefile脚本中检查到有这两个变量存在,会有警告提示. 12unset JAVA_HOMEunset CLASSPATH &nbsp;&nbsp;其他环境变量如下: 12345678910111213141516171819202122232425262728293031323334353637383940#语言选项，这个必须设置，否则编译好后会出现一个HashTable的NPE错export LANG=C#Bootstrap JDK的安装路径。必须设置。 export ALT_BOOTDIR=/Library/Java/JavaVirtualMachines/jdk1.7.0_04.jdk/Contents/Home#允许自动下载依赖export ALLOW_DOWNLOADS=true#并行编译的线程数，设置为和CPU内核数量一致即可export HOTSPOT_BUILD_JOBS=6export ALT_PARALLEL_COMPILE_JOBS=6#比较本次build出来的映像与先前版本的差异。这个对我们来说没有意义，必须设置为false，否则sanity检查会报缺少先前版本JDK的映像。如果有设置dev或者DEV_ONLY=true的话这个不显式设置也行。 export SKIP_COMPARE_IMAGES=true#使用预编译头文件，不加这个编译会更慢一些export USE_PRECOMPILED_HEADER=true#要编译的内容export BUILD_LANGTOOLS=true #export BUILD_JAXP=false#export BUILD_JAXWS=false #export BUILD_CORBA=falseexport BUILD_HOTSPOT=true export BUILD_JDK=true#要编译的版本#export SKIP_DEBUG_BUILD=false#export SKIP_FASTDEBUG_BUILD=true#export DEBUG_NAME=debug#把它设置为false可以避开javaws和浏览器Java插件之类的部分的build。 BUILD_DEPLOY=false#把它设置为false就不会build出安装包。因为安装包里有些奇怪的依赖，但即便不build出它也已经能得到完整的JDK映像，所以还是别build它好了。BUILD_INSTALL=false#编译结果所存放的路径export ALT_OUTPUTDIR=/Users/IcyFenix/Develop/JVM/jdkBuild/openjdk_7u4/build 编译JDK &nbsp;&nbsp;全部设置好之后,可用在OpenJDK目录下使用 make sanity来检查设置是否正确,如果正确则会会输出：Sanity check passed. &nbsp;&nbsp;之后,输入make命令开始编译,(make不添加参数默认为编译make all). &nbsp;&nbsp;编译完成后,可以将目录复制到JAVA_HOME中,作为一个完整的JDK使用,编译出来的虚拟机,在-version命令中带有用户的机器名. &nbsp;&nbsp;如果只想单独编译HotSpot的话，那么使用hotspot/make目录下的MakeFile进行替换即可，其他参数与前面一致，这时候虚拟机的输出结果存放在build/hotspot/outputdir/linux_amd64_compiler2 中，里面对应了不同的优化级别的目录. &nbsp;&nbsp;在不同机器上,最后一个目录名称会有所差别,bad表示Mac OS系统(内核为FreeBSD),amd64表示是64位JDK(32位为x86),compiler2表示是Server VM(Client VM表示是compiler1). &nbsp;&nbsp;在运行虚拟机前，还要手工编辑目录下的env.sh文件，这个文件由编译脚本自动产生，用于设置虚拟机的环境变量，里面已经发布了”JAVA_HOME，CLASSPATH，HOTSPOT_BUILD_USER” 3个环境变量，还需要增加一个“LD_LIBRARY_CLASSPATH”,内容如下: - 12LD_LIBRARY_PATH=.:$&#123;JAVA_HOME&#125;/jre/lib/amd64/native_threads:$&#123;JAVA_HOME&#125;/jre/lib/amd64:export LD_LIBRARY_PATH &nbsp;&nbsp;然后执行以下命令启动虚拟机(这时的启动机器名为gamma). - 12. ./env.sh./gamma -version #有可能是test_gamma,这是自带的一段八皇后代码 Exception 报错: 1Error: time is more than 10 years from present: 1104530400000 when building java/openjdk* lists.freebsd.org 通过修改CurrencyData.properties文件, 把10年之前的时间修改为10年之内即可 Index: /usr/openjdk/jdk/src/share/classes/java/util/CurrencyData.properties把2006改掉就可以重新编译了. End 资料参考于 &lt;&lt;深入理解 Java虚拟机 第二版&gt;&gt;.]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot 部署测试与应用监控]]></title>
    <url>%2F2016%2F08%2F05%2F2016-08-5-Spring-monitor%2F</url>
    <content type="text"><![CDATA[Spring Boot部署 Jar包形式 &nbsp;&nbsp;如果在创建Spring Boot项目的时候选择的是jar包形式,则只需要使用maven将项目打成jar包即可. 1mvn pakage &nbsp;&nbsp;Spring Boot项目在打成jar后可以直接运行. War包形式 &nbsp;&nbsp;如果在创建Spring Boot项目的时候选择的是war包形式,则打包的方式与jar一样. 1mvn pakage &nbsp;&nbsp;之后将生成的war文件放在任意Servlet容器上运行即可. &nbsp;&nbsp;如果在创建Spring Boot项目的时候选择的是jar,在部署时又想以war包形式部署,则需要以下配置. 修改pom.xml文件的打包方式 添加以下依赖覆盖默认内嵌的Tomcat. 创建ServletInitializer. 注册为Linux服务 &nbsp;&nbsp;将软件注册为服务,可以通过命令开启、关闭、开机自启动等功能. &nbsp;&nbsp;如想注册为Linux服务,则先需要修改spring-boot-maven-plugin的配置: 1234567891011 &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;executable&gt;true&lt;/executable&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; &nbsp;&nbsp;主流的Linux大都使用init.d或systemd来注册服务. init.d方式 sudo ln -s /var/apps/spring-boot-demo.jar /etc/init.d/sdemo,其中sdemo就是服务名. 常用操作命令 1234service sdemo start 启动服务.service sdemo stop 停止服务.service sdemo status 服务状态.chkconfig sdemo on 开机启动. 项目日志存放于/var/log/sdemo.log中. systemd方式 在/etc/systemd/system/目录下新建文件sdemo.service.并写入以下内容: 1234567891011[Unit]Description=spring-boot-demoAfter=syslog.target[Service]ExecStart= /usr/bin/java -jar /var/apps/spring-boot-demo.jar[Install]WantedBy=multi-user.target#其中Description和ExecStart是可更改的.ExecStart是指定java运行需要运行的jar包. 常用操作命令12345systemctl start sdemo 启动服务.systemctl stop sdemo 停止服务.systemctl status sdemo 服务状态.systemctl enable sdemo 开机启动.journalctl -u sdemo 查看项目日志. 热部署 &nbsp;&nbsp;热部署即就是在应用正在运行的时候升级软件，却不需要重新启动应用. &nbsp;&nbsp;在Spring Boot项目中添加spring-boot-devtools依赖即可实现页面和类的热部署. 模板引擎热部署 &nbsp;&nbsp;Spring Boot默认模板引擎开启缓存,如果我们修改了页面后再刷新页面将得不到修改后的内容,我们可以在application.properties中关闭模板引擎的缓存. 12345678#Thymeleafspring.thymeleaf.cache=false#FreeMarkerspring.freemarker.cache=false#Groovyspring.groovy.template.cache=false#Velocityspring.velocity.cache=false 基于Docker部署 &nbsp;&nbsp;目前主流的PaaS平台都支持发布Docker镜像.而Docker镜像是使用Dockerfile文件来编译镜像的. Dockerfile FROM指令 FROM指令指明了当前镜像继承的基镜像.编译当前镜像时会自动下载基镜像. 1FROM java:8 MAINTAINER指令 MAINTAINER指令指明了当前镜像的作者 1MAINTAINER sun RUN指令 RUN指令可以在当前镜像上执行Linux命令并形成一个新的层.RUN指令是编译时(build)的动作. 1RUN /bin/bash -c &quot;echo helloworld&quot; CMD指令 CMD指令指明镜像启动时的默认行为.一个Dockerfile里只能有一个CMD指令.CMD指令设定的命令可以在运行镜像时使用参数覆盖.它是运行时(run)的动作. 12CMD echo &quot;hello&quot;可被 docker run -d image_name echo &quot;world&quot; 覆盖. EXPOSE指令 EXPOSE指令指明了镜像运行时的容器必须监听指定的端口. 1EXPOSE 8080 EVN指令 EVN指令用于设置环境变量. 1ENV myName=sun ADD指令 ADD指令用于从当前工作目录复制文件到镜像目录. 1ADD hello.sh /dir/ ENTRYPOINT指令 ENTRYPOINT指令可以让容器像一个可执行程序一样运行,这样镜像运行时可以像软件一样接收参数执行,ENTRYPOINT是运行时(run)的动作. 123ENTRYPOINT [&quot;/bin/echo&quot;]我们可以镜像传递参数:docker run -d image_name &quot;hello world&quot; Example 首先将打好包的demo上传到Linux服务器. 在项目同级目录下新建一个Dockerfile文件,如下: 123456789FROM java:8MAINTAINER=sunADD spring-boot-demo.jar app.jarEXPOSE 8080ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;] 编译镜像 1234在/var/apps/sdemo目录下执行以下命令docker build -t sun/sdemo .使用sun/sdemo作为镜像名称,sun为前缀. .是用来指明Dockerfile路径的,这里因为在当前目录下所以使用“.”. 运行镜像1docker run -d -p 8080:8080 --name sdemo sun/sdemo Spring Boot集成测试 &nbsp;&nbsp;Spring Boot的测试与Spring MVC类似,它提供了一个@SpringApplicationConfiguration替代@ContextConfiguration来指定配置类. Example Entity 123456789@Entitypublic class Person &#123; @Id @GeneratedValue private Long id; private String name; ...&#125; Dao 123public interface PersonRepositroy extends JpaRepository&lt;Person,Long&gt; &#123;&#125; Controller 12345678910111213@RestController@RequestMapping("/person")public class PersonController &#123; @Autowired PersonRepositroy personRepositroy; @RequestMapping(method = RequestMethod.GET,produces = &#123;MediaType.APPLICATION_JSON_VALUE&#125;) public List&lt;Person&gt; findAll() &#123; return personRepositroy.findAll(); &#125;&#125; Test 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354@RunWith(SpringJUnit4ClassRunner.class)//@ContextConfiguration(classes = &#123;SpringBootTestApplication.class&#125;)@SpringApplicationConfiguration(classes = SpringBootTestApplication.class)@WebAppConfiguration@Transactionalpublic class PersonApplicationTest &#123; @Autowired PersonRepositroy personRepositroy; MockMvc mockMvc; @Autowired WebApplicationContext webApplicationContext; String expectedJson; /** * 初始化 */ @Before public void setUp() throws JsonProcessingException &#123; Person p1 = new Person("sun"); Person p2 = new Person("sylvanas"); personRepositroy.save(p1); personRepositroy.save(p2); // 获得期待返回的JSON字符串. expectedJson = Obj2Json(personRepositroy.findAll()); // 初始化MockMvc. mockMvc = MockMvcBuilders.webAppContextSetup(webApplicationContext).build(); &#125; protected String Obj2Json(Object obj) throws JsonProcessingException &#123; ObjectMapper objectMapper = new ObjectMapper(); return objectMapper.writeValueAsString(obj); &#125; @Test public void testPersonController() throws Exception &#123; String url = "/person"; // 对/person发送请求,获得请求的执行结果. MvcResult result = mockMvc.perform(MockMvcRequestBuilders.get(url) .accept(MediaType.APPLICATION_JSON_VALUE)) .andReturn(); // 获得执行结果的状态 int status = result.getResponse().getStatus(); // 获得执行结果的内容 String content = result.getResponse().getContentAsString(); // 断言 Assert.assertEquals("错误:正确的返回值为200", 200, status); Assert.assertEquals("错误:返回值和预期返回值不一致", expectedJson, content); &#125;&#125; Spring Boot应用监控 &nbsp;&nbsp;Spring Boot提供了运行时的应用监控和管理的功能.我们可以通过HTTP、JMX、SSH来进行应用的监控. HTTP &nbsp;&nbsp;使用HTTP实现对应用的监控需要添加以下依赖: 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; &nbsp;&nbsp;端点: Endpoint Description actuator 所有EndPoint的列表,需要加入spring HATEOAS支持 autoconfig 当前应用的所有自动配置 beans 当前应用的所有Bean信息 configprops 当前应用中所有的配置信息 dump 显示当前应用线程状态信息 env 显示当前应用的环境信息 health 显示当前应用的健康情况 info 显示当前应用信息 metrics 显示当前应用的各项指标信息 mappings 显示所有的@RequestMapping映射的路径 shutdown 关闭当前应用(默认此项是关闭的) trace 默认最新的http请求 &nbsp;&nbsp;使用http访问应用监控只需要在url中加上/端点名即可. 例如: http://localhost:8080/actuator &nbsp;&nbsp;端点的开启关闭可以在application.properties中配置. &nbsp;&nbsp;如果想自定义端点,则需要一个继承AbstractEndpoint的实现类,并注册成Bean即可. &nbsp;&nbsp;如果想自定义HealthIndicator,则需要一个实现HealthIndicator接口的类,并注册成Bean即可. JMX &nbsp;&nbsp;可以使用Java内置的jconsole来实现JMX监控. SSH &nbsp;&nbsp;Spring Boot借助CraSH (http://www.crashub.org), 实现通过SSH或者TELNET监控和管理应用.我们只需要引入依赖spring-boot-starter-remote-shell即可. &nbsp;&nbsp;Spring Boot在spring-boot-starter-remote-shell的commands包下定制了命令,它使用的是Groovy语言来编写的.Groovy语言是由Spring主导的运行于JVM的动态语言. 自定义登录用户 &nbsp;&nbsp;Spring Boot支持在application.properties中自定义用户的账号密码.12shell.auth.simple.user.name=sunshell.auth.simple.user.password=sun end 资料参考于 JavaEE开发的颠覆者: Spring Boot实战]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Batch数据批处理]]></title>
    <url>%2F2016%2F08%2F04%2F2016-08-4-Spring-batch%2F</url>
    <content type="text"><![CDATA[概述 &nbsp;&nbsp;Spring Batch是一个轻量级的处理大量数据操作的框架,主要用于读取大量数据,然后进行一定处理后输出成指定的格式. &nbsp;&nbsp;Spring Batch 提供了大量可重用的组件,包括了日志、追踪、事务、任务作业统计、任务重启、跳过、重复、资源管理。对于大数据量和高性能的批处理任务,Spring Batch 同样提供了高级功能和特性来支持,比如分区功能、远程功能。总之,通过 Spring Batch 能够支持简单的、复杂的和大数据量的批处理作业. &nbsp;&nbsp;Spring Batch 是一个批处理应用框架,不是调度框架,但需要和调度框架合作来构建完成的批处理任务。它只关注批处理任务相关的问题,如事务、并发、监控、执行等,并不ﰁ供相应的调度功能。如果需要使用调用框架,在商业软件和开源软件中已经有很多优秀的企业级调度框架(如 Quartz、Tivoli、Control-M、Cron 等)可以使用. 应用场景 周期性的提交处理. 并行处理任务. 消息驱动应用分级处理. 手工或调度使任务失败之后重新启动. 有依赖步骤的顺序执行(使用工作流驱动扩展). 处理时跳过部分记录. 成批事务：为小批量的或有的存储过程/脚本的场景使用. 组成结构 Name Description Job 实际要执行的任务,包含一个或多个Step JobRepository 用于注册Job的容器 JobLauncher 用于启动Job的接口 Step Step步骤包含ItemReader,ItemProcessor,ItemWriter ItemReader 用于读取数据的接口 ItemProcessor 用于处理数据的接口 ItemWriter 用于输出数据的接口 &nbsp;&nbsp;以上组件只需要注册成Spring的Bean即可,并在配置类上使用@EnableBatchProcessing开启批处理的支持. Job Listener &nbsp;&nbsp;如果想要监听Job的执行情况,则需要定义一个实现了JobExecutionListener的类,并在定义Job的Bean上绑定该监听器. 数据处理和校验 &nbsp;&nbsp;数据的处理和校验都要通过ItemProcessor接口实现来完成的. 数据处理 &nbsp;&nbsp;实现ItemProcessor接口,并重写process方法,即可对数据进行处理.输入参数是从ItemReader读取到的数据,返回给ItemWriter. 数据校验 &nbsp;&nbsp;数据校验可以使用JSR-303的注解来校验ItemReader读取的数据是否符合要求. 参数后置绑定 &nbsp;&nbsp;实现参数后置绑定可以在JobParameters中绑定参数,在Bean定义的时候使用一个特殊的Bean生命周期注解@StepScope,然后通过@Value注入此参数. Spring Boot支持 &nbsp;&nbsp;Spring Boot自动初始化了Spring Batch存储批处理的数据库,并当程序启动时,会自动执行Job. &nbsp;&nbsp;Spring Boot使用以spring.batch为前缀的属性进行相关配置. 12345spring.batch.job.name=job1,job2 #启动时要执行的job,默认执行全部job.spring.batch.job.enabled=true #是否自动执行定义的job,默认为truespring.batch.initializer.enabled=true #是否初始化Spring Batch的数据库,默认为true.spring.batch.schema=spring.batch.table-prefix= #设置Spring Batch的数据库表的前缀. &nbsp;&nbsp;Spring Batch默认自动加载hsqldb驱动,如要使用其他数据库驱动则需要手动去除. Config Example 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899/** * 定义ItemProcessor */@Beanpublic ItemProcessor&lt;Person, Person&gt; processor() &#123; // 使用自定义的ItemProcessor CsvItemProcessor processor = new CsvItemProcessor(); // 指定校验器 processor.setValidator(csvBeanValidator()); return processor;&#125;/** * 定义Validator */@Beanpublic Validator&lt;Person&gt; csvBeanValidator() &#123; return new CsvBeanValidator&lt;Person&gt;();&#125;/** * 定义ItemWriter */@Beanpublic ItemWriter&lt;Person&gt; writer(DataSource dataSource) &#123; JdbcBatchItemWriter&lt;Person&gt; writer = new JdbcBatchItemWriter&lt;&gt;(); writer.setItemSqlParameterSourceProvider (new BeanPropertyItemSqlParameterSourceProvider&lt;Person&gt;()); // 设置要执行批处理的sql语句 String sql = "insert into person " + "(id,name,age,nation,address)" + "values(hibernate_sequence.nextval,:name,:age,:nation,:address)"; writer.setSql(sql); writer.setDataSource(dataSource); return writer;&#125;/** * 定义JobRepository */@Beanpublic JobRepository jobRepository(DataSource dataSource, PlatformTransactionManager transactionManager) throws Exception &#123; JobRepositoryFactoryBean jobRepositoryFactoryBean = new JobRepositoryFactoryBean(); jobRepositoryFactoryBean.setDataSource(dataSource); jobRepositoryFactoryBean.setTransactionManager(transactionManager); jobRepositoryFactoryBean.setDatabaseType("mysql"); return jobRepositoryFactoryBean.getObject();&#125;/** * 定义JobLauncher */@Beanpublic JobLauncher jobLauncher(DataSource dataSource, PlatformTransactionManager transactionManager) throws Exception &#123; SimpleJobLauncher jobLauncher = new SimpleJobLauncher(); jobLauncher.setJobRepository(jobRepository(dataSource, transactionManager)); return jobLauncher;&#125;/** * 定义Job */@Beanpublic Job importJob(JobBuilderFactory jobs, Step s1) &#123; return jobs.get("importJob") .incrementer(new RunIdIncrementer()) .flow(s1) // 为job指定step .end() .listener(csvJobListener()) // 绑定监听器 .build();&#125;/** * 定义Step */@Beanpublic Step step1(StepBuilderFactory stepBuilderFactory, ItemReader&lt;Person&gt; reader, ItemWriter&lt;Person&gt; writer, ItemProcessor&lt;Person, Person&gt; processor) &#123; return stepBuilderFactory .get("step1") .&lt;Person, Person&gt;chunk(65000) // 批处理每次提交65000条数据. .reader(reader) .processor(processor) .writer(writer) .build();&#125;/** * 定义自定义的Job监听器 */@Beanpublic CsvJobListener csvJobListener() &#123; return new CsvJobListener();&#125; end 资料参考于 JavaEE开发的颠覆者: Spring Boot实战]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot消息通信]]></title>
    <url>%2F2016%2F08%2F03%2F2016-08-3-Spring-message%2F</url>
    <content type="text"><![CDATA[SSL安全套接层 概述 &nbsp;&nbsp;SSL(Secure Sockets Layer)是专门用于网络通信安全及数据完整性的安全协议,它会在网络传输层对网络连接进行加密. &nbsp;&nbsp;SSL协议是位于TCP/IP协议与其他各种应用层协议之间的. &nbsp;&nbsp;SSL的体系结构中包含两个协议子层: SSL记录协议(SSL Record Protocol),它建立在可靠的传输协议(TCP)之上,为高层协议提供数据封装、压缩、加密等基本功能的支持.SSL纪录协议针对HTTP协议进行了特别的设计，使得超文本的传输协议HTTP能够在SSL运行. SSL握手协议(SSL Handshake Protocol),它建立在SSL记录协议之上.SSL握手协议层包括SSL握手协议（SSL HandShake Protocol）、SSL密码参数修改协议（SSL Change Cipher Spec Protocol）、应用数据协议（Application Data Protocol）和SSL告警协议（SSL Alert Protocol）.它主要用于在实际数据传输开始前,通信双方进行身份认证、协商加密算法、交换加密密钥等. &nbsp;&nbsp;在B/S应用中,是通过HTTPS实现SSL的,HTTPS即是在HTTP下加入SSL层,它的安全基础是SSL. 工作流程 客户端向服务器发送一个开始信息“Hello”以便开始一个新的会话连接. 服务器根据客户的信息确定是否需要生成新的主密钥，如需要则服务器在响应客户的“Hello”信息时将包含生成主密钥所需的信息. 客户根据收到的服务器响应信息，产生一个主密钥，并用服务器的公开密钥加密后传给服务器. 服务器回复该主密钥，并返回给客户一个用主密钥认证的信息，以此让客户认证服务器. Spring Boot中配置SSL &nbsp;&nbsp;在配置SSL之前需要生成一个证书,它可以是自签名的,也可以是从SSL证书授权中心获得的. 生成SSL证书&nbsp;&nbsp;可以在控制台中输入以下指令,生成自签名的SSL证书: 1keytool -genkey -alias tomcat &nbsp;&nbsp;之后会在当前目录生成一个.keystore文件,它就是SSL证书文件. 配置到项目中 将.keystore文件复制到src/main/resources/static下. 在application.properties中如下配置:12345server.port = 8090server.ssl.key-store = .keystoreserver.ssl.key-store-password = 123456server.ssl.keyStoreType = JKSserver.ssl.keyAlias : tomcat http自动转向https &nbsp;&nbsp;如果想要实现输入http自动转向到https,则需要配置TomcatEmbeddedServletContainerFactory,并添加Tomcat的connector来实现. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package cn.sun.sylvanas.spring_boot_security;import org.apache.catalina.Context;import org.apache.catalina.connector.Connector;import org.apache.tomcat.util.descriptor.web.SecurityCollection;import org.apache.tomcat.util.descriptor.web.SecurityConstraint;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.context.embedded.EmbeddedServletContainerFactory;import org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory;import org.springframework.context.annotation.Bean;@SpringBootApplicationpublic class SpringBootSecurityApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootSecurityApplication.class, args); &#125; @Bean public EmbeddedServletContainerFactory servletContainerFactory() &#123; TomcatEmbeddedServletContainerFactory tomcat = new TomcatEmbeddedServletContainerFactory()&#123; @Override protected void postProcessContext(Context context) &#123; SecurityConstraint securityConstraint = new SecurityConstraint(); securityConstraint.setUserConstraint("CONFIDENTIAL"); SecurityCollection securityCollection = new SecurityCollection(); securityCollection.addPattern("/"); securityConstraint.addCollection(securityCollection); context.addConstraint(securityConstraint); &#125; &#125;; tomcat.addAdditionalTomcatConnectors(httpConnector()); return tomcat; &#125; @Bean public Connector httpConnector() &#123; Connector connector = new Connector("org.apache.coyote.http11.Http11NioProtocol"); connector.setScheme("http"); connector.setPort(8080); connector.setSecure(false); connector.setRedirectPort(8090); return connector; &#125;&#125; &nbsp;&nbsp;此时访问http://localhost:8080会自动转向到https://localhost:8090. WebSocket 概述 &nbsp;&nbsp;WebSocket protocol 是HTML5一种新的协议.它实现了浏览器与服务器全双工异步通信(full-duplex).即浏览器可以向服务端发送消息,服务端也可以向浏览器发送消息.WebSocket需要浏览器的支持,如IE 10+、Chrome 13+、Firefox 6+. &nbsp;&nbsp;WebSocket是通过一个socket来实现双工异步通信功能的.直接使用WebSocket协议开发程序会很繁琐,所以一般使用它的子协议STOMP,STOMP是一个更高级别的协议,它使用基于帧(frame)的格式来定义消息,具有一个类似@RequestMapping的@MessageMapping. Spring Boot支持 &nbsp;&nbsp;Spring Boot对内嵌的Tomcat(7、8)、Jetty9、Undertow提供了WebSocket支持.依赖为spring-boot-starter-websocket. &nbsp;&nbsp;配置WebSocket需要在配置类上使用@EnableWebSocketMessageBroker注解开启支持,并且继承AbstractWebSocketMessageBrokerConfigurer类,重写其方法进行配置. Example 广播&nbsp;&nbsp;广播即服务端有消息时,会将消息发送给所有连接了当前endpoint的浏览器. 配置 Controller 12345678910/** * @MessageMapping注解映射/hello这个地址,类似于@RequestMapping * 当服务端有消息时,会对订阅了@SendTo中的路径的浏览器发送消息. */ @MessageMapping("/hello") @SendTo("/topic/getHello") public String hello(String name) throws Exception &#123; Thread.sleep(3000); return "Hello " + name + "!"; &#125; js1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071 &lt;html xmlns:th="http://www.thymeleaf.org"&gt;&lt;head&gt; &lt;meta content="text/html;charset=UTF-8"/&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"/&gt; &lt;meta name="viewport" content="width=device-width,initial-scale=1"/&gt; &lt;title&gt;Spring Boot+WebSocket+广播式&lt;/title&gt;&lt;/head&gt;&lt;body onload="disconnect()"&gt;&lt;noscript&gt;&lt;h2 style="color: #ff0000"&gt;貌似你的浏览器不支持websocket&lt;/h2&gt;&lt;/noscript&gt;&lt;div&gt; &lt;div&gt; &lt;button id="connect" onclick="connect();"&gt;连接&lt;/button&gt; &lt;button id="disconnect" onclick="disconnect();"&gt;断开连接&lt;/button&gt; &lt;/div&gt; &lt;div id="conversationDiv"&gt; &lt;label&gt;输入你的名字&lt;/label&gt;&lt;input type="text" id="name"/&gt; &lt;button id="sendName" onclick="sendName();"&gt;发送&lt;/button&gt; &lt;p id="response"&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;!-- 导入jQuery --&gt;&lt;script th:src="@&#123;jquery-3.1.0.min.js&#125;"/&gt;&lt;script th:src="@&#123;sockjs-1.0.0.min.js&#125;"/&gt;&lt;script th:src="@&#123;stomp.min.js&#125;"/&gt;&lt;script type="text/javascript"&gt; var stompClient = null; function setConnected(connected) &#123; document.getElementById('connect').disabled = connected; document.getElementById('disconnect').disabled = !connected; document.getElementById('conversationDiv').style.visibility = connected ? 'visible' : 'hidden'; $('#response').html(); &#125; function connect() &#123; var socket = new SockJS('/endpointSun'); stompClient = Stomp.over(socket); stompClient.connect(&#123;&#125;, function (frame) &#123; setConnected(true); console.log('Connected: ' + frame); // 订阅/topic/getHello stompClient.subscribe('/topic/getHello',function(response)&#123; showResponse(JSON.parse(response.body).responseMessage); &#125;); &#125;); &#125; function disconnect() &#123; if(stompClient != null)&#123; stompClient.disconnect(); &#125; setConnected(false); console.log("Disconnected"); &#125; function sendName() &#123; var name = $('#name').val(); // 发送到/hello stompClient.send("/hello",&#123;&#125;,JSON.stringify(&#123;'name':name&#125;)); &#125; function showResponse(message)&#123; var response = $("#response"); response.html(message); &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; P2P&nbsp;&nbsp;P2P即点对点,它不同于广播式,P2P需要指定消息由谁接收,且只能由一个人接收. 配置与广播式相同. Controller 1234567891011121314151617181920212223/** * 通过SimpMessagingTemplate向浏览器发送消息. */ @Autowired private SimpMessagingTemplate messagingTemplate; /** * Spring MVC可以直接在参数中注入principal对象,它包含当前用户的信息. */ @MessageMapping("/chat") public void handleChat(Principal principal, String msg) &#123; /** * 通过SimpMessagingTemplate.convertAndSendToUser向用户发送消息, * 第一个参数是接收者的用户,第二参数是浏览器订阅的地址,第三个参数是要发送的消息. */ if (principal.getName().equals("sun")) &#123; messagingTemplate.convertAndSendToUser("sylvanas", "/queue/notifications", principal.getName() + "-send:" + msg); &#125; else &#123; messagingTemplate.convertAndSendToUser("sun", "/queue/notifications", principal.getName() + "-send:" + msg); &#125; &#125; js 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 &lt;!DOCTYPE html&gt;&lt;!--suppress ALL --&gt;&lt;html xmlns="http://www.w3.org/1999/xhtml" xmlns:th="http://www.thymeleaf.org" xmlns:sec="http://www.thymeleaf.org/thymeleaf-extras-springsecurity3"&gt;&lt;head&gt; &lt;meta content="text/html;charset=UTF-8"/&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"/&gt; &lt;meta name="viewport" content="width=device-width,initial-scale=1"/&gt; &lt;script th:src="@&#123;jquery-3.1.0.min.js&#125;"/&gt; &lt;script th:src="@&#123;stomp.min.js&#125;"/&gt; &lt;script th:src="@&#123;sockjs-1.0.0.min.js&#125;"/&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt; 聊天室&lt;/p&gt;&lt;form id="chatForm"&gt; &lt;textarea rows="4" cols="60" name="text"&gt;&lt;/textarea&gt; &lt;input type="submit"/&gt;&lt;/form&gt;&lt;script th:inline="javascript"&gt; $('#chatForm').submit(function(e)&#123; e.preventDefault(); var text = $('#chatForm').find('textarea[name="text"]').val(); sendSpittle(text); &#125;); var socket = new SockJS("/endpointChat"); var stomp = Stomp.over(socket); stomp.connect('guest','guest',function(frame)&#123; // 与messagingTemplate.convertAndSendToUser中定义的订阅地址保持一致 // 但前面要多出一个/user,这个/user是必须的,只有使用了/user才会发送消息到指定的用户. stomp.subscribe("/user/queue/notifications",handleNotification); &#125;); function handleNotification(message)&#123; $("#output").append("&lt;b&gt;Received: " + message.body + "&lt;/b&gt;&lt;br/&gt;"); &#125; function sendSpittle(text)&#123; stomp.send("/chat",&#123;&#125;,text); &#125; $('#stop').click(function()&#123;socket.close()&#125;);&lt;/script&gt;&lt;div id="output"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 异步消息队列 &nbsp;&nbsp;异步消息队列主要用于各系统之间的通信与解耦,异步消息即为发送者无需关心消息接收者的处理和返回. &nbsp;&nbsp;异步消息的主要概念为消息代理(message broker)和目的地(destination).消息是由消息代理负责接管并传递到指定目的地的. &nbsp;&nbsp;目的地主要有2种: queue:用于P2P(point-to-point)的消息通信. topic:用于发布/订阅(publish/subscribe)的消息通信. Spring Boot支持 &nbsp;&nbsp;Spring对JMS和AMQP的支持来自于spring-jms、spring-rabbit.它们分别需要ConnectionFactory的实现来连接消息代理.并且提供了JmsTemplate和RabbitTemplate. JMS(Java Message Service)Java消息服务,它是基于JVM消息代理的规范. AMQP(Advanced Message Queuing Protocol)高级消息队列协议,它不仅支持JVM,还支持跨语言和平台.AMQP的主要实现有RabbitMQ. &nbsp;&nbsp;Spring Boot对JMS支持的实有ActiveMQ,HornetQ,Artemis.以ActiveMQ为例: Spring Boot自动配置了ActiveMQConnectionFactory与JmsTemplate. 通过spring.activemq为前缀的属性来配置ActiveMQ相关的属性. Spring Boot还自动开启了@EnableJms,即使用注解式消息监听的支持. &nbsp;&nbsp;Spring Boot对AMQP的实现RabbitMQ自动配置了如下内容: 自动配置了ConnectionFactory和RabbitTemplate. 通过spring.rabbitmq为前缀的属性来配置RabbitMQ相关的属性. 自动开启了@EnableRabbit,即使用注解式消息监听的支持. JMS Example &nbsp;&nbsp;添加依赖 123456789101112131415161718192021222324 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 嵌入ActiveMQ --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-broker&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; &nbsp;&nbsp;在application.properties中配置activemq的地址. 1spring.activemq.broker-url=tcp://192.168.145.152:61616 &nbsp;&nbsp;定义JMS发送的消息需要实现MessageCreator接口.1234567891011public class Msg implements MessageCreator &#123; /** * 重写createMessage方法. */ @Override public Message createMessage(Session session) throws JMSException &#123; return session.createTextMessage("Hello World"); &#125;&#125; &nbsp;&nbsp;发送消息,定义目的地.123456789101112131415161718192021/** * Spring Boot提供了一个CommandLineRunner接口,用于程序启动后执行的代码 */@SuppressWarnings("SpringJavaAutowiringInspection")@SpringBootApplicationpublic class SpringBootActivemqApplication implements CommandLineRunner &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootActivemqApplication.class, args); &#125; @Autowired private JmsTemplate jmsTemplate; @Override public void run(String... strings) throws Exception &#123; // 向目的地my-destination发送消息 jmsTemplate.send("my-destination",new Msg()); &#125;&#125; &nbsp;&nbsp;接收消息 12345678910111213@Componentpublic class Receiver &#123; /** * @JmsListener是Spring 4.1提供的一个新特性,用来简化JMS开发. * destination指定要监听的目的地. */ @JmsListener(destination = "my-destination") public void receiveMessage(String message) &#123; System.out.println("接收到: &lt;" + message + "&gt;"); &#125;&#125; AMQP Example &nbsp;&nbsp;Spring Boot默认Rabbit主机位localhost,端口号为5672. &nbsp;&nbsp;添加以下依赖:123456789 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; &nbsp;&nbsp;在application.properties中配置RabbitMQ的地址.12spring.rabbitmq.host=192.168.145.152spring.rabbitmq.port=5672 &nbsp;&nbsp;发送消息,定义目的地. 1234567891011121314151617181920212223242526272829@SuppressWarnings("SpringJavaAutowiringInspection")@SpringBootApplicationpublic class SpringBootRabbitmqApplication implements CommandLineRunner &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootRabbitmqApplication.class, args); &#125; @Autowired private RabbitTemplate rabbitTemplate; /** * 定义目的地 */ @Bean public Queue myQueue() &#123; return new Queue("my-queue"); &#125; /** * 使用rabbitTemplate的convertAndSend方法向队列my-queue发送消息. */ @Override public void run(String... strings) throws Exception &#123; rabbitTemplate.convertAndSend("my-queue","Hello RabbitMQ!"); &#125;&#125; &nbsp;&nbsp;接收消息 123456789@Componentpublic class Receiver &#123; @RabbitListener(queues = "my-queue") public void receiveMessage(String message) &#123; System.out.println("Received &lt;" + message + "&gt;"); &#125;&#125; end 资料参考于 JavaEE开发的颠覆者: Spring Boot实战]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security安全控制]]></title>
    <url>%2F2016%2F08%2F02%2F2016-08-2-Spring-security%2F</url>
    <content type="text"><![CDATA[概述 &nbsp;&nbsp;Spring Security是Spring项目的安全框架,基于IoC和AOP来实现安全的功能. &nbsp;&nbsp;Spring Security有两个重要概念: 认证(Authentication),认证即确认用户可以访问当前系统. 授权(Authorization),授权即确定用户在当前系统下所拥有的权限. 配置 &nbsp;&nbsp;Spring Security使用过滤器来实现所有安全的功能,我们只需要注册一个特殊的DelegationFilterProxy过滤器到WebApplicationInitializer即可(WebApplicationInitializer是Spring提供用来配置Servlet3.0+配置的接口,从而替代web.xml,实现此接口的类会自动被SpringServletContainerInitializer加载,启动Servlet容器). &nbsp;&nbsp;也可以让自定义的Initializer类继承AbstractSecurityWebApplicationInitializer抽象类,这个抽象类实现了WebApplicationInitializer接口,并通过onStartup方法调用.它已经注册了DelegationFilterProxy过滤器. Spring Security配置类 &nbsp;&nbsp;只需要在配置类上使用@EnableWebSecurity注解,然后继承WebSecurityConfigurerAdapter.可以通过重写configure方法来配置相关的内容. &nbsp;&nbsp;在Spring Security中,通过重写以下方法放行静态资源: Authentication &nbsp;&nbsp;在Spring Security中,通过重写以下方法来完成认证的配置: &nbsp;&nbsp;认证用户需要用户数据的来源,AuthenticationManagerBuilder提供了内存中和JDBC的两种用户数据来源. 内存中 JDBC &nbsp;&nbsp;如果需要自定义用户数据来源,则可以通过实现UserDetailsService接口. Authorization &nbsp;&nbsp;在Spring Security中,通过重写以下方法来完成授权的配置: &nbsp;&nbsp;Spring Security使用2种匹配器用来匹配请求路径: antMatchers:使用Ant风格的路径匹配. regexMatchers:使用正则表达式匹配路径. &nbsp;&nbsp;Spring Security提供以下方法用于安全处理: Method Description anyRequest() 匹配所有请求路径. access(String) Spring EL表达式结果为true时可以访问. anonymous() 匿名可访问. denyAll() 用户不能访问. fullyAuthenticated() 用户完全认证时可访问. hasAnyAuthority(String…) 如果用户有参数,则其中任一权限可访问. hasAnyRole(String…) 如果用户有参数,则其中任一角色可访问. hasAuthority(String) 如果用户有参数,则其权限可访问. hasIpAddress(String) 如果用户来自参数中的IP则可访问. hasRole(String) 如果用户有参数中的角色可访问. permitAll() 用户可任意访问. rememberMe() 允许通过remember-me登陆的用户访问. authenticated() 用户登录后可访问 自定义登录实现 1234567891011121314151617181920@Override protected void configure(HttpSecurity http) throws Exception &#123; http.authorizeRequests() .anyRequest().authenticated() // 所有请求需要认证登陆后才能访问. .and() .formLogin() // 通过formLogin定制登录操作. .loginPage("/login") // 定制登录页面的访问地址. .defaultSuccessUrl("/index") // 登录成功后转向的页面. .failureUrl("/login?error") // 登录失败后转向的页面. .permitAll() .and() .rememberMe() // 开启cookie存储用户信息. .tokenValiditySeconds(1209600) // 指定cookie的有效时间,单位为秒. .key("userKey") // 指定cookie中的私钥. .and() .logout() // 通过logout()定制注销操作. .logoutUrl("/logout") // 指定注销的URL路径. .logoutSuccessUrl("/index") // 指定注销成功后转向的页面. .permitAll(); &#125; Spring Boot支持 &nbsp;&nbsp;Spring Boot主要通过SecurityAutoConfiguration和SecurityProperties来完成自动配置. &nbsp;&nbsp;SecurityAutoConfiguration导入了SpringBootWebSecurityConfiguration中的配置,我们可以获得如下的自动配置: 自动配置了一个内存中的用户,账号为user,密码在程序启动时print在控制台中. 放行了/css/**,/js/**,/images/**,/**/favicon.ico等静态文件存放的路径. 自动配置的securityFilterChainRegistration的Bean. 使用以security为前缀的属性配置Security相关的配置.1234567security.user.name=#内存中的默认用户账号,默认为user.security.user.password=#默认用户的密码.security.user.role=#默认用户的角色.security.require-ssl=false #是否需要ssl支持,默认为false.security.enable-csrf=false #是否开启“跨站请求伪造”支持,默认false.security.ignored= #用逗号隔开需要放行的路径..... 当我们需要自定义扩展配置的时候,只需要配置类继承WebSecurityConfigurerAdapter即可,不需要使用@EnableWebSecurity注解开启支持. end 资料参考于 JavaEE开发的颠覆者: Spring Boot实战]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data 事务&缓存]]></title>
    <url>%2F2016%2F08%2F01%2F2016-08-1-Spring-data-transaction%2F</url>
    <content type="text"><![CDATA[Spring Transaction &nbsp;&nbsp;Spring的事务机制是用统一的机制来处理不同数据访问计数的事务处理.它提供了一个PlatformTransactionManager接口,不同的数据访问技术使用不同的接口实现. 数据访问技术 接口实现 JDBC DataSourceTransactionManager JPA JpaTransactionManager Hibernate HibernateTransactionManager JDO JdoTransactionManager 分布式事务 JtaTransactionManager 声明式事务 &nbsp;&nbsp;Spring使用@Transactional注解在方法上表明事务支持.被注解的方法在被调用时,会开启一个新的事务,当方法无异常完成提交后.Spring会提交事务. &nbsp;&nbsp;@Transactional注解也可以用在类上,表明这个类下的所有方法都有事务支持.如果类和方法都使用了@Transactional,则使用方法上的注解覆盖类级别注解. &nbsp;&nbsp;@Transactional注解是基于AOP的实现操作. &nbsp;&nbsp;注意:@Transactional注解是由Spring提供的,而不是来自javax.transaction. &nbsp;&nbsp;Spring提供@EnableTransactionManagerment注解在配置类上开启声明式事务,Spring容器会自动扫描带有注解@Transactional的类和方法. Spring Data JPA支持 &nbsp;&nbsp;Spring Data JPA对所有方法默认开启了事务支持,查询类事务默认启用readOnly-true属性. Spring Boot支持 &nbsp;&nbsp;Spring Boot会自动配置事务管理器. 当使用JDBC时,Spring Boot会自动配置DataSourceTransactionManager. 当使用JPA时,Spring Boot会自动配置JpaTransactionManager. &nbsp;&nbsp;在Spring Boot中,不需要显式开启@EnableTransactionManagement注解. Spring Cache &nbsp;&nbsp;Spring提供了CacheManager和Cache接口用来统一各种不同的数据缓存技术. CacheManager是各种缓存技术抽象接口. Cache接口包含各种缓存操作. CacheManager 描述 SimpleCacheManager 使用简单的Collection存储缓存,主要用于测试. NoOpCacheManager 不会实际存储缓存. EhCacheCacheManager 使用EhCache缓存技术. ConcurrentMapCacheManager 使用ConcurrentMap存储缓存. GuavaCacheManager 使用Google Guava的GuavaCache缓存技术. HazelcastCacheManager 使用Hazelcast缓存技术. JCacheCacheManager 支持JCache(JSR-107)规范的实现作为缓存技术. RedisCacheManager 使用Redis作为缓存技术. &nbsp;&nbsp;不管使用什么缓存技术,都需要注册一个该实现的CacheManager的Bean. 声明式缓存 &nbsp;&nbsp;Spring提供了以下注解用来声明式缓存.它与@Transactional注解一样是基于AOP操作的实现. Annotation Description @CachePut 不管什么情况,都会把方法的返回值存入缓存中.@CachePut的属性与@Cacheable保持一致. @Cacheable Spring会先查看缓存中是否存有数据,如果有,则直接返回缓存数据,如果没有,则将调用方法的返回值存入缓存中. @Caching 可以通过@Caching注解组合多个注解策略在一个方法上. @CacheEvict 将一条或多条缓存数据从缓存中删除. &nbsp;&nbsp;开启声明式缓存需要在配置类上使用注解@EnableCaching Example Spring Boot支持 &nbsp;&nbsp;Spring Boot自动配置了CacheManager的各种实现,默认情况下使用的是ConcurrentMapCacheManager.支持以spring.cache为前缀的属性来配置缓存相关的配置. 12345678spring.cache.type= #缓存技术的类型,可选ehcache,guava,simple,none,generic,hazelcast,infinispan,jcache,redis.spring.cache.cache-name=#程序启动时创建缓存名称spring.cache.ehcache.config=#ehcache配置文件的地址.spring.cache.hazelcast.config=#hazelcast配置文件的地址.spring.cache.infinispan.config=#infinispan配置文件的地址.spring.cache.jcache.config=#jcache配置文件的地址.spring.cache.jcache.provider=#当多个jcache实现在类路径中的时候,指定jcache实现.spring.cache.guava.spec=# guava specs &nbsp;&nbsp;使用Spring Boot只需要导入相关缓存技术的依赖,并在配置类使用@EnableCaching注解开启缓存支持即可. end 资料参考于 JavaEE开发的颠覆者: Spring Boot实战]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring数据访问方案-Spring Data]]></title>
    <url>%2F2016%2F07%2F31%2F2016-07-31-Spring-data%2F</url>
    <content type="text"><![CDATA[概述&nbsp;&nbsp;Spring Data是Spring用来解决数据访问的解决方案,它包含了大量关系型数据库以及NoSQL数据库的数据持久层访问解决方案. | Spring Data Commons&nbsp;&nbsp;Spring Data提供了统一的API来对各种数据存储技术进行数据访问操作,这是通过Spring Data Commons来实现的,它是所有Spring Data子项目的依赖. Spring Data Repository&nbsp;&nbsp;Spring Data Repository是数据访问的统一标准,它是抽象的,不同的数据访问技术有不同的Repository,它的顶级接口为Repository接口. &nbsp;&nbsp;Repository用一个实体类型与ID类型作为泛型. &nbsp;&nbsp;Repository的子接口CrudRepository定义了CRUD操作的相关内容: &nbsp;&nbsp;CrudRepository的子接口PagingAndSortingRepository定义了分页与排序的相关内容: Spring Data JPAJPA规范 &nbsp;&nbsp;JPA是Java Persistence API的缩写,它是一个基于O/R(Object-Relational Mapping)映射的标准规范.例如Hibernate就是JPA规范的实现. 数据访问层 在Spring Data JPA中定义数据访问层首先需要继承JpaRepository接口. 之后可以通过@EnableJpaRepository注解开启Spring Data JPA的支持,@EnableJpaRepository接收的value参数用于扫描数据访问层所在包下的接口定义. 查询方法 &nbsp;&nbsp;Spring Data JPA支持通过定义在Repository接口中的方法名来定义查询,方法名是根据实体类的属性名来确定的. &nbsp;&nbsp;其中findBy关键字可以用find、read、readBy、query、queryBy、get、getBy替代. &nbsp;&nbsp;Spring Data JPA可以使用top和first关键字查询指定数量的数据. 查询关键字 关键字 示例 同功能JPQL And findByNameAndAge where x.name = ?1 and x.age = ?2 Or findByNameOrAge where x.name = ?1 or x.age = ?2 Is findByNameIs where x.name = ?1 Equals findByNameEquals where x.name = ?1 Between findByAgeBetween where x.age between ?1 and ?2 LessThan findByAgeLessThan where x.age &lt; ?1 LessThanEqual findByAgeLessThanEqual where x.age &lt;= ?1 GreaterThan findByAgeGreaterThan where x.age &gt; ?1 GreaterThanEqual findByAgeGreaterThanEqual where x.age &gt;= ?1 After findByStartDateAfter where x.startDate &gt; ?1 Before findByStartDateBefore where x.startDate &lt; ?1 IsNull findByNameIsNull where x.name is null IsNotNull&amp;NotNull findByName(Is)NotNull where x.name not null Like findByNameLike where x.name like ?1 NotLike findByNameNotLike where x.name not like ?1 StartingWith findByNameStartingWith where x.name like ?1(参数前面加%) EndingWith findByNameEndingWith where x.name like ?1(参数后面加%) Containing findByNameContaining where x.name like ?1(参数前后都加%) OrderBy findByNameOrderByAgeDesc where x.name = ?1 order by x.age desc Not findByNameNot where x.name &lt;&gt; ?1 In findByAgeIn(Collection age) where x.age in ?1 NotIn findByAgeNotIn(Collection age) where x.age not in ?1 True findByActiveTrue() where x.active = true False findByActiveFalse() where x.active = false IgnoreCase findByNameIgnoreCase where UPPER(x.name) = UPPER(?1) @Query&amp;@NamedQuery查询 &nbsp;&nbsp;Spring Data JPA支持使用@Query注解在接口的方法上实现查询. &nbsp;&nbsp;Spring Data JPA支持使用@NamedQuery定义查询方法,一个名称映射一条查询语句. 更新查询 &nbsp;&nbsp;Spring Data JPA支持使用@Modifying注解和@Query注解组合进行数据更新操作. Specification &nbsp;&nbsp;Spring Data JPA提供了一个Specification接口可以让我们快速地构造基于准则的查询.通过重写Specification接口的toPredicate方法用来构造查询条件. 数据访问接口需继承JpaSpecificationExecutor接口. 构造查询条件类 调用条件查询 排序&amp;分页 &nbsp;&nbsp;Spring Data JPA提供了Sort类和Page接口及Pageable接口完成排序和分页. 自定义Repository &nbsp;&nbsp;如果我们想自定义Repository,可以继承Repository的子接口PagingAndSortingRepository. 定义自定义的Repository接口. 定义自定义的Repository接口实现. 定义RepositoryFactoryBean 开启自定义支持需要使用@EnableJpaRepositories注解的repositoryFactoryBeanClass指定FactoryBean. Spring Boot支持 Spring Boot使用spring.datasource前缀用来配置dataSource. Spring Boot自动开启了注解事务支持(@EnableTransactionManagement),并配置了jdbcTemplate. Spring Boot提供了初始化数据的功能,在类路径下的schema.sql文件会自动初始化表结构;在类路径下的data.sql文件会自动插入表数据. Spring Boot为我们自动配置了transactionManager、jpaVendorAdapter、entityManagerFactory等Bean.JpaBaseConfiguration还有一个getPackagesToScan方法用于自动扫描带有注解@Entity的实体类 Spring Boot自动配置了OpenEntityManagerInViewInterceptor拦截器,并注册到了Spring MVC的拦截器中.解决了页面访问数据时会话连接已关闭的错误. Spring Boot自动开启了对Spring Data JPA的支持,无需再配置类中显式声明@EnableJpaRepositories. &nbsp;&nbsp;在Spring Boot下使用Spring Data JPA,只需要添加依赖spring-boot-stater-data-jpa,然后定义DataSource、实体类、数据访问层即可,无需其他配置. Spring Data REST &nbsp;&nbsp;Spring Data REST支持将Spring Data JPA,Spring Data MongoDB,Spring Data Neo4j,Spring Data GernFile,Spring Data Cassandra的Repository自动转换成REST服务. &nbsp;&nbsp;Spring Data REST的配置是定义在RepositoryRestMvcConfiguration配置类中的,我们可以通过继承这个配置类或者使用@Import注解导入此配置类来使用Spring Data REST. Spring Boot支持 &nbsp;&nbsp;Spring Boot已经自动配置了RepositoryRestMvcConfiguration,所以只需要引入依赖spring-boot-starter-data-rest,不需要任何其他配置. Spring Boot使用spring.data.rest前缀用来配置RepositoryRestMvcConfiguration的属性. 如果想在自定义的领域类Repository中将方法暴露为REST资源,则需要使用@RestResource注解. 如需要分页,则可以使用参数page=?&amp;size=?来实现分页.例:http://localhost:8080/persons/?page=1&amp;size=10. 如需要排序,则可以使用参数sort来实现排序.例:http://localhost:8080/persons/?sort=age,desc. 自定义根路径需要在application.properties中设置spring.data.rest.base-path属性. Spring Data REST的节点路径是默认在实体类之后加s,如果需要自定义节点路径则要在领域类Repository上使用@RepositoryRestResource注解的path属性进行设置. Spring Data MongoDB &nbsp;&nbsp;MongoDB是一个基于Document文档的NoSQL数据库,它使用面向对象的思想,每一条记录都是一个文档对象. &nbsp;&nbsp;Spring Data MongoDB提供了以下的注解用来定义领域类: 注解 描述 @Document 映射领域对象与MongoDB的一个文档 @Id 映射当前属性为ID @DbRef 当前属性将参考其他文档 @Field 为文档的属性定义名称 @Version 将当前属性作为版本 &nbsp;&nbsp;Spring Data MongoDB还提供了一个MongoTemplate封装了数据访问的方法,我们还需要为MongoClient和MongoDbFactory来配置数据库连接属性.开启MongoDB的Repository需要在配置类上使用注解@EnableMongoRepositories. &nbsp;&nbsp;定义Spring Data MongoDB的Repository只需要继承MongoRepository接口即可. Spring Boot支持 &nbsp;&nbsp;使用Spirng Boot主要配置数据库连接、MongoTemplate.可以使用spring.data.mongodb配置MongoDB相关的属性. Spring Boot自动开启了@EnableMongoRepositories注解. Spring Boot提供了一些默认配置:默认MongoDB端口为27017,服务器为localhost,数据库为test. 在Spring Boot下使用MongoDB只需要引入依赖spring-boot-starter-data-mongodb,不需要其他配置. Spring Data Redis &nbsp;&nbsp;Spring Data Redis提供了ConnectionFactory和RedisTemplate. 根据Redis不同的JavaClient,Spring Data Redis提供了不同的ConnectionFactory. JedisConnectionFactory:使用Jedis作为客户端. JredisConnectionFactory:使用Jredis作为客户端. LettuceConnectionFactory:使用Lettuce作为客户端. SrpConnectionFactory:使用Spullara/redis-protoccol作为客户端. 配置ConnectionFactory和RedisTemplate如下: &nbsp;&nbsp;Spring Data Redis提供了RedisTemplate和StringRedisTemplate两个模板对象进行数据操作.StringRedisTemplate只针对键值都是字符类型的数据进行操作. 数据操作方法 描述 opsForValue() 操作简单属性的数据 opsForList() 操作list数据 opsForSet() 操作set数据 opsForZSet() 操作ZSet(有序的set)数据 opsForHash() 操作hash散列的数据 Serializer &nbsp;&nbsp;当我们进行存储操作的时候,键值对都是通过Spring提供的Serializer序列化到数据库的. RedisTemplate默认使用的是JdkSerializationRedisSerizlizer. StringRedisTemplate默认使用的是StringRedisSerializer. Spring Boot支持 &nbsp;&nbsp;Spring Boot默认配置了JedisConnectionFactory、RedisTemplate和StringRedisTemplate. &nbsp;&nbsp;Spring Boot使用spring.redis为前缀在application.properties中配置Redis相关的属性. end 资料参考于 JavaEE开发的颠覆者: Spring Boot实战]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈Spring Boot自动配置的运作原理]]></title>
    <url>%2F2016%2F07%2F29%2F2016-07-29-Spring-boot-autoconfigure%2F</url>
    <content type="text"><![CDATA[Spring Boot&nbsp;&nbsp;Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。它使用“习惯优于配置”的理念可以让你的项目快速运行部署。使用Spring Boot可以不用或者只需要很少的Spring配置。 &nbsp;&nbsp;而Spring Boot核心的功能就是自动配置。它会根据在类路径中的jar、类自动配置Bean,当我们需要配置的Bean没有被Spring Boot提供支持时,也可以自定义自动配置。 自动配置的运作原理&nbsp;&nbsp;Spring Boot自动配置其实是基于Spring 4.x提供的条件配置(Conditional)实现的。 &nbsp;&nbsp;有关自动配置的源码在spring-boot-autoconfigure-1.x.x.jar内,如下图: 如何查看当前项目已启动和未启动的自动配置 在application.propertie中设置debug=true属性. 在运行jar时添加–debug指令. &nbsp;&nbsp;当使用以上两种任意一种方法后,启动项目会在控制台输出已启动和未启动的自动配置日志. @SpringBootApplication&nbsp;&nbsp;生成Spring Boot项目时,会自动生成一个入口类.入口类使用了@SpringBootApplication注解,它是Spring Boot的核心注解,它是一个组合注解,核心功能由@EnableAutoConfiguration注解提供. @EnableAutoConfiguration @Import注解提供导入配置的功能,它导入了EnableAutoConfigurationImportSelector. EnableAutoConfigurationImportSelector使用函数SpringFactoriesLoader.loadFactoryNames扫描META-INF/spring.factories文件中声明的jar包. spring.factories文件在spring-boot-autoconfigure-1.x.x.jar中. spring.factories中声明的类基本上都使用了@Conditional注解. Conditional&nbsp;&nbsp;Spring Boot在org.springframework.boot.autoconfigure.condition包下定义了以下注解. 注解名 作用 @ConditionalOnJava 基于JVM版本作为判断条件. @ConditionalOnBean 当容器中有指定的Bean的条件下. @ConditionalOnClass 当类路径下游指定的类的条件下. @ConditionalOnExpression 基于SpEL表达式作为判断条件. @ConditionalOnJndi 在JNDI存在的条件下查找指定的位置. @ConditionalOnMissingBean 当容器中没有指定Bean的情况下. @ConditionalOnMissingClass 当类路径下没有指定的类的情况下. @ConditionalOnNotWebApplication 当前项目不是web项目的条件下. @ConditionalOnProperty 指定的属性是否有指定的值. @ConditionalOnResource 类路径是否有指定的值. @ConditionalOnSingleCandidate 当指定Bean在容器中只有一个,或者虽然有多个但是指定首选的Bean. @ConditionalOnWebApplication 当前项目是web项目的条件下. &nbsp;&nbsp;以上这些注解都组合了@Conditional元注解. 分析@ConditionalOnNotWebApplication &nbsp;&nbsp;@ConditionalOnNotWebApplication使用的条件类是OnWebApplicationCondition. 1234567891011121314151617181920212223242526272829303132333435363738394041package org.springframework.boot.autoconfigure.condition;import org.springframework.boot.autoconfigure.condition.ConditionOutcome;import org.springframework.boot.autoconfigure.condition.ConditionalOnWebApplication;import org.springframework.boot.autoconfigure.condition.SpringBootCondition;import org.springframework.context.annotation.ConditionContext;import org.springframework.core.annotation.Order;import org.springframework.core.type.AnnotatedTypeMetadata;import org.springframework.util.ClassUtils;import org.springframework.util.ObjectUtils;import org.springframework.web.context.WebApplicationContext;import org.springframework.web.context.support.StandardServletEnvironment;@Order(-2147483628)class OnWebApplicationCondition extends SpringBootCondition &#123; private static final String WEB_CONTEXT_CLASS = "org.springframework.web.context.support.GenericWebApplicationContext"; OnWebApplicationCondition() &#123; &#125; public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; boolean webApplicationRequired = metadata.isAnnotated(ConditionalOnWebApplication.class.getName()); ConditionOutcome webApplication = this.isWebApplication(context, metadata); return webApplicationRequired &amp;&amp; !webApplication.isMatch()?ConditionOutcome.noMatch(webApplication.getMessage()):(!webApplicationRequired &amp;&amp; webApplication.isMatch()?ConditionOutcome.noMatch(webApplication.getMessage()):ConditionOutcome.match(webApplication.getMessage())); &#125; private ConditionOutcome isWebApplication(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; if(!ClassUtils.isPresent("org.springframework.web.context.support.GenericWebApplicationContext", context.getClassLoader())) &#123; return ConditionOutcome.noMatch("web application classes not found"); &#125; else &#123; if(context.getBeanFactory() != null) &#123; String[] scopes = context.getBeanFactory().getRegisteredScopeNames(); if(ObjectUtils.containsElement(scopes, "session")) &#123; return ConditionOutcome.match("found web application \'session\' scope"); &#125; &#125; return context.getEnvironment() instanceof StandardServletEnvironment?ConditionOutcome.match("found web application StandardServletEnvironment"):(context.getResourceLoader() instanceof WebApplicationContext?ConditionOutcome.match("found web application WebApplicationContext"):ConditionOutcome.noMatch("not a web application")); &#125; &#125;&#125; &nbsp;&nbsp;OnWebApplicationCondition在isWebApplication函数中进行条件判断. 判断GenericWebApplicationContext是否在类路径中. 判断容器中是否存在名为session的scope. 判断当前容器的Environment是否为StandardServletEnvironment. 判断当前的ResourceLoader是否为WebApplicationContext. 最后通过ConditionOutcome.isMatch函数返回布尔值确定条件. 分析自动配置的实现&nbsp;&nbsp;以http编码为例,如果在常规项目中则需要在web.xml中配置一个filter.而Spring Boot内置了http编码的自动配置,无需配置filter. properties配置类 自动配置Bean &nbsp;&nbsp;@ConditionalOnProperty:当设置spring.http.encoding=enabled的情况下,如果没有设置则默认为true,即符合条件. &nbsp;&nbsp;characterEncodingFilter()返回OrderedCharacterEncodingFilter这个对象,并根据注入的HttpEncodingProperties配置类设置参数. &nbsp;&nbsp; @ConditionalOnMissingBean({CharacterEncodingFilter.class}):在容器中没有这个Bean的时候则新建这个Bean. end 资料参考于 JavaEE开发的颠覆者: Spring Boot实战]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记(6)-Storm]]></title>
    <url>%2F2016%2F07%2F19%2F2016-07-19-Hadoop06-Storm%2F</url>
    <content type="text"><![CDATA[概述&nbsp;&nbsp;Storm是一个开源的分布式实时计算系统，可以简单、可靠的处理大量的数据流。被称作“实时的hadoop”。Storm有很多使用场景：如实时分析，在线机器学习，持续计算， 分布式RPC，ETL等等。Storm支持水平扩展，具有高容错性，保证每个消息都会得到处理，而且处理速度很快。Storm的部署和运维都很便捷，而且更为重要的是可以使用任意编程语言来开发应用。 Storm的特点简单的编程模型 &nbsp;&nbsp;在大数据处理方面相信大家对hadoop已经耳熟能详，基于Google Map/Reduce来实现的Hadoop为开发者提供了map、reduce原语，使并行批处理程序变得非常地简单和优美。 &nbsp;&nbsp;同样，Storm也为大数据 的实时计算提供了一些简单优美的原语，这大大降低了开发并行实时处理的任务的复杂性，帮助你快速、高效的开发应用。 水平扩展 &nbsp;&nbsp;在Storm集群中真正运行topology的主要有三个实体：工作进程、线程和任务。Storm集群中的每台机器上都可以运行多个工作进程，每个 工作进程又可创建多个线程，每个线程可以执行多个任务，任务是真正进行数据处理的实体，我们开发的spout、bolt就是作为一个或者多个任务的方式执行的。 &nbsp;&nbsp;计算任务在多个线程、进程和服务器之间并行进行，支持灵活的水平扩展。 支持多种编程语言 &nbsp;&nbsp;你可以在Storm之上使用各种编程语言。默认支持Clojure、Java、Ruby和Python。要增加对其他语言的支持，只需实现一个简单的Storm通信协议即可。 高可靠性 &nbsp;&nbsp;Storm保证每个消息至少能得到一次完整处理。任务失败时，它会负责从消息源重试消息。 &nbsp;&nbsp;spout发出的消息后续可能会触发产生成千上万条消息，可以形象的理解为一棵消息树，其中spout发出的消息为树根，Storm会跟踪这棵消息树的处理情况，只有当这棵消息树中的所有消息都被处理了，Storm才会认为spout发出的这个消息已经被“完全处理”。如果这棵消息树中的任何一个消息处理失败了，或者整棵消息树在限定的时间内没有“完全处理”，那么spout发出的消息就会重发。 高容错性 &nbsp;&nbsp;Storm会管理工作进程和节点的故障。 &nbsp;&nbsp;如果在消息处理过程中出了一些异常，Storm会重新安排这个出问题的处理单元。Storm保证一个处理单元永远运行（除非你显式杀掉这个处理单元）。 &nbsp;&nbsp;当然，如果处理单元中存储了中间状态，那么当处理单元重新被Storm启动的时候，需要应用自己处理中间状态的恢复。 本地模式 &nbsp;&nbsp;Storm有一个“本地模式”，可以在处理过程中完全模拟Storm集群。这让你可以快速进行开发和单元测试。 Storm架构 &nbsp;&nbsp;Storm集群由一个主节点和多个工作节点组成。主节点运行了一个名为“Nimbus”的守护进程，用于分配代码、布置任务及故障检测。每个工作节 点都运行了一个名为“Supervisor”的守护进程，用于监听工作，开始并终止工作进程。Nimbus和Supervisor都能快速失败，而且是无状态的，这样一来它们就变得十分健壮，两者的协调工作是由ApacheZooKeeper来完成的。 Stream&nbsp;&nbsp;Stream是一个数据流的抽象。这是一个没有边界的Tuple序列,而这些Tuple序列会以一种分布式的方式并行地创建和处理。 &nbsp;&nbsp;对消息流的定义主要就是对消息流里面的tuple 进行定义，为了更好地使用tuple，需要给tuple 里的每个字段取一个名字，并且不同的tuple 字段对应的类型要相同，即两个tuple 的第一个字段类型相同，第二个字段类型相同，但是第一个字段和第二个字段的类型可以不同。默认情况下，tuple 的字段类型可以为integer、long、short、byte、string、double、float、boolean 和byte array 等基本类型，也可以自定义类型，只需要实现相应的序列化接口。 &nbsp;&nbsp;每一个消息流在定义的时候需要被分配一个id，最常见的消息流是单向的消息流，在Storm 中OutputFieldsDeclarer 定义了一些方法，让你可以定义一个Stream 而不用指定这个id。在这种情况下，这个Stream 会有个默认的id: 1。 Topologies&nbsp;&nbsp;Topology是由Stream Grouping连接起来的Spout和Bolt节点网络。 &nbsp;&nbsp;在 Storm 中，一个实时计算应用程序的逻辑被封装在一个称为Topology 的对象中，也称为计算拓扑。Topology 有点类似于Hadoop 中的MapReduce Job，但是它们之间的关键区别在于，一个MapReduce Job 最终总是会结束的，然而一个Storm 的Topology 会一直运行。在逻辑上，一个Topology 是由一些Spout（消息的发送者）和Bolt（消息的处理者）组成图状结构，而链接Spouts 和Bolts 的则是Stream Groupings。 Spouts&amp;Bolts Spouts &nbsp;&nbsp;Spouts 是Storm集群中一个计算任务（Topology）中消息流的生产者，Spouts一般是从别的数据源（例如，数据库或者文件系统）加载数据，然后向Topology中发射消息。 &nbsp;&nbsp;Spouts即可以是可靠的,也可以是不可靠的。 &nbsp;&nbsp;在一个Topology中存在两种Spouts，一种是可靠的Spouts，一种是非可靠的Spouts，可靠的Spouts 在一个tuple 没有成功处理的时候会重新发射该tuple，以保证消息被正确地处理。不可靠的Spouts 在发射一个tuple 之后，不会再重新发射该tuple，即使该tuple 处理失败。每个Spouts 都可以发射多个消息流，要实现这样的效果，可以使用OutFieldsDeclarer.declareStream 来定义多个Stream，然后使用SpoutOutputCollector 来发射指定的Stream。 &nbsp;&nbsp;在Storm 的编程接口中，Spout 类最重要的方法是nextTuple()方法，使用该方法可以发射一个消息tuple 到Topology 中，或者简单地直接返回，如果没有消息要发射。需要注意的是，nextTuple 方法的实现不能阻塞Spout，因为Storm在同一线程上调用Spout 的所有方法。Spout 类的另外两个重要的方法是ack()和fail()，一个tuple 被成功处理完成后，ack()方法被调用，否则就调用fail()方法。注意，只有对于可靠的Spout，才会调用ack()和fail()方法。 Bolts &nbsp;&nbsp;所有消息处理的逻辑都在Bolt 中完成，在Bolt 中可以完成如过滤、分类、聚集、计算、查询数据库等操作。Bolt 可以做简单的消息处理操作，例如，Bolt 可以不做任何操作，只是将接收到的消息转发给其他的Bolt。Bolt 也可以做复杂的消息流的处理，从而需要很多个Bolt。在实际使用中，一条消息往往需要经过多个处理步骤，例如，计算一个班级中成绩在前十名的同学，首先需要对所有同学的成绩进行排序，然后在排序过的成绩中选出前十名的成绩的同学。所以在一个Topology 中，往往有很多个Bolt，从而形成了复杂的流处理网络。 &nbsp;&nbsp;Bolts可以发射多条消息流。 使用OutputFieldsDeclarer.declareStream定义Stream。 使用OutputCollector.emit来选择要发射的Stream。 &nbsp;&nbsp;Bolts的主要方法是execute。 &nbsp;&nbsp;Bolts以Tuple作为输入,使用OutputCollector来发射Tuple,通过调用OutputCollector.ack()通知这个Tuple的发射者Spout。 &nbsp;&nbsp;Bolts一般流程。 &nbsp;&nbsp;处理一个输入Tuple,发射0个或多个Tuple,然后调用ack()通知Storm自己已经处理过这个Tuple了。Storm提供了一个IBasicBolt会自动调用ack()。 Stream Groupings&nbsp;&nbsp;定义一个 Topology 的其中一步是定义每个Bolt 接收什么样的流作为输入。Stream Grouping 就是用来定义一个Stream 应该如何分配给Bolts 上面的多个Tasks。 &nbsp;&nbsp;Storm里有7种类型的Stream Grouping。 Shuffle Grouping 随机分组,随机派发Stream里面的Tuple,保证每个Bolt接收到的Tuple数量大致相同。 Fields Grouping 按字段分组,以id举例。具有相同id的Tuple会被分到相同的Bolt中的一个Task,而不同id的Tuple会被分到不同的Bolt中的Task。 All Grouping 广播,对于每一个Tuple,所有的Bolts都会收到。 Global Grouping 全局分组,这个Tuple被分配到Storm中的一个Bolt的其中一个Task。具体一点就是分配给id值最低的那个Task。 Non Grouping 不分组,Stream不关心到底谁会收到它的Tuple。目前这种分组和Shuffle Grouping是一样的效果,有一点不同的是Storm会把这个Bolt放到这个Bolt的订阅者同一个线程中去执行。 Direct Grouping 直接分组,这是一种比较特别的分组方法,用这种分组意味着消息的发送者指定由消息接收者的哪个Task处理这个消息。只有被声明为Direct Stream的消息流可以声明这种分组方法。而且这种消息Tuple必须使用emitDirect方法来发射。消息处理者可以通过TopologyContext来获取处理它的消息的Task的id(OutputCollector.emit方法也会返回Task的id)。 Local or Shuffle Grouping 如果目标Bolt有一个或者多个Task在同一个工作进程中,Tuple将会被随机发射给这些Tasks。否则,和普通的Shuffle Grouping行为一致。 Workers 每个Supervisor中运行着多个Workers进程。 每个Worker进程中运行着多个Executor线程。 每个Executor线程中运行着若干个相同的Task(Spout/Bolt)。 &nbsp;&nbsp;一个 Topology 可能会在一个或者多个工作进程里面执行，每个工作进程执行整个Topology 的一部分。比如，对于并行度是300 的Topology 来说，如果我们使用50 个工作进程来执行，那么每个工作进程会处理其中的6 个Tasks（其实就是每个工作进程里面分配6 个线程）。Storm 会尽量均匀地把工作分配给所有的工作进程。 Task&nbsp;&nbsp;在 Storm 集群上，每个Spout 和Bolt 都是由很多个Task 组成的，每个Task对应一个线程，流分组策略就是定义如何从一堆Task 发送tuple 到另一堆Task。在实现自己的Topology 时可以调用TopologyBuilder.setSpout() 和TopBuilder.setBolt()方法来设置并行度，也就是有多少个Task。 Storm安装部署 安装jdk。 搭建Zookeeper集群。 下载并解压Storm。 修改storm.yaml配置文件。 storm.zookeeper.servers: Storm集群使用的Zookeeper集群地址。例如:storm.zookeeper.servers:-“192.168.145.141”-“192.168.145.142” 如果Zookeeper没有使用默认端口,那么还需要修改storm.zookeeper.port。 storm.local.dir Nimbus和Supervisor进程用于存储少量状态,如jars、confs等的本地磁盘目录,需要提前创建该目录并给予足够的访问权限。然后在storm.yaml中配置该目录,例如:storm.local.dir:”/home/application/storm/workdir” 注意事项&nbsp;&nbsp;启动Storm后台进程时,需要对conf/storm.yaml配置文件中设置的storm.local.dir目录具有写权限。 &nbsp;&nbsp;Storm后台进程被启动时,将在Storm安装目录下的logs/子目录下生成各个进程的日志文件。 &nbsp;&nbsp;Storm UI必须和Storm Nimbus部署在同一台机器上,否则UI无法正常工作,因为UI进程会检查本机是否存在Nimbus链接。 常用命令 命令描述 格式 例子 启动Nimbus storm nimbus storm nimbus 启动Supervisor storm supervisor storm supervisor 启动UI storm ui storm ui 提交Topologies格式 storm jar 【jar路径】 【拓扑包名.拓扑类名】【stormIP地址】【storm端口】【拓扑名称】【参数】 Example 123storm jar /home/storm/hello.jarstorm.hello.WordCountTopology wordcountTop提交hello.jar到远程集群,并启动wordcountTop拓扑 停止Topologies格式 storm kill [拓扑名称] Example 1storm kill wordcountTop APISpouts&nbsp;&nbsp; Spout是Stream的消息产生源， Spout组件的实现可以通过继承BaseRichSpout类或者其他*Spout类来完成，也可以通过实现IRichSpout接口来实现。 open &nbsp;&nbsp;当一个Task被初始化的时候会调用open()。一般都会在此方法中对发送Tuple的对象SpoutOutputCollector和配置对象TopologyContext初始化。 getComponentConfiguration &nbsp;&nbsp;此方法用于声明针对当前组件的特殊的Configuration配置。 nextTuple &nbsp;&nbsp;这是Spout类中最重要的一个方法。发射一个Tuple到Topology都是通过这个方法来实现的。 declareOutputFields &nbsp;&nbsp;此方法用于声明当前Spout的Tuple发送流。Stream的定义是通过OutputFieldsDeclare.declare方法完成的,其中的参数包括了发送的Fields。 &nbsp;&nbsp;另外，除了上述几个方法之外，还有ack、fail和close方法等。 &nbsp;&nbsp;Storm在监测到一个Tuple被成功处理之后会调用ack方法，处理失败会调用fail方法。这两个方法在BaseRichSpout等类中已经被隐式的实现了。 Bolts&nbsp;&nbsp; Bolt类接收由Spout或者其他上游Bolt类发来的Tuple，对其进行处理。Bolt组件的实现可以通过继承BasicRichBolt类或者IRichBolt接口来完成。 prepare &nbsp;&nbsp;此方法与Spouts的open方法类似,为Bolt提供了OutputCollector,用来从Bolt中发射Tuple。Bolt中Tuple的发射可以在prepare中、execute中、cleanup等方法中进行,一般都是在execute中。 getComponentConfiguration &nbsp;&nbsp;与Spouts类似。 execute &nbsp;&nbsp; 这是Bolt中最关键的一个方法，对于Tuple的处理都可以放到此方法中进行。具体的发送也是在execute中通过调用emit方法来完成的。 &nbsp;&nbsp;emit有两种情况，一种是emit方法中有两个参数，另一个种是有一个参数。 emit有一个参数：此唯一的参数是发送到下游Bolt的Tuple，此时，由上游发来的旧的Tuple在此隔断，新的Tuple和旧的Tuple不再属于同一棵Tuple树。新的Tuple另起一个新的Tuple树。 emit有两个参数：第一个参数是旧的Tuple的输入流，第二个参数是发往下游Bolt的新的Tuple流。此时，新的Tuple和旧的Tuple是仍然属于同一棵Tuple树，即，如果下游的Bolt处理Tuple失败，则会向上传递到当前Bolt，当前Bolt根据旧的Tuple流继续往上游传递，申请重发失败的Tuple。保证Tuple处理的可靠性。 declareOutputFields &nbsp;&nbsp;用于声明当前Bolt发送的Tuple中包含的字段。 Topology Example12345678910111213141516171819202122232425262728293031public class RandomWordSpout extends BaseRichSpout &#123; // 初始化数据字典 private final static String[] words = &#123; "java", "c", "c++", "c#", "python", "go", "javascript", "swift" &#125;; private SpoutOutputCollector collector; @Override public void nextTuple() &#123; Random random = new Random(); // 获取随机的单词 String word = words[random.nextInt(words.length)]; // 发射消息 this.collector.emit(new Values(word)); // 休息2秒 Utils.sleep(2000); &#125; @Override public void open(Map arg0, TopologyContext arg1, SpoutOutputCollector collector) &#123; this.collector = collector; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; // 声明字段名 declarer.declare(new Fields("initName")); &#125;&#125; 123456789101112131415161718public class UpperBolt extends BaseBasicBolt &#123; @Override public void execute(Tuple tuple, BasicOutputCollector collector) &#123; // 获得上个bolt传入的initName String initName = tuple.getString(0); // 将initName转为大写 String upperCase = initName.toUpperCase(); // 发射消息 collector.emit(new Values(upperCase)); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("upperName")); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435public class PrefixBolt extends BaseBasicBolt &#123; private FileWriter fileWriter; @Override public void prepare(Map stormConf, TopologyContext context) &#123; // 初始化fileWriter try &#123; this.fileWriter = new FileWriter("/home/storm/output/" + UUID.randomUUID()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void execute(Tuple tuple, BasicOutputCollector collector) &#123; String upperName = tuple.getString(0); // 添加前缀 String finalName = "hello-" + upperName; // write try &#123; this.fileWriter.write(finalName); this.fileWriter.write("\n"); this.fileWriter.flush(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; &#125;&#125; 12345678910111213141516171819public class TopologyMain &#123; public static void main(String[] args) throws Exception &#123; TopologyBuilder topologyBuilder = new TopologyBuilder(); // 设置Spout topologyBuilder.setSpout("randomWordSpout", new RandomWordSpout()); // 设置Bolt topologyBuilder.setBolt("upperBolt", new UpperBolt()).shuffleGrouping("randomWordSpout"); topologyBuilder.setBolt("prefixBolt", new PrefixBolt()).shuffleGrouping("upperBolt"); Config config = new Config(); // 设置Workers数量 config.setNumWorkers(4); config.setDebug(true); // 提交Topology StormSubmitter.submitTopology("randomTopo", config, topologyBuilder.createTopology()); &#125;&#125;]]></content>
      <categories>
        <category>后端</category>
        <category>大数据</category>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记(5)-Hive]]></title>
    <url>%2F2016%2F07%2F18%2F2016-07-18-Hadoop05-Hive%2F</url>
    <content type="text"><![CDATA[概述&nbsp;&nbsp;Hive是建立在Hadoop上的数据仓库基础架构。它提供了一系列的工具，用来进行数据提取、转换、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据机制。可以把Hadoop下结构化数据文件映射为一张成Hive中的表，并提供类sql查询功能，除了不支持更新、索引和事务，sql其它功能都支持。可以将sql语句转换为MapReduce任务进行运行，作为sql到MapReduce的映射器。提供shell、JDBC/ODBC、Thrift、Web等接口。 &nbsp;&nbsp;Hive 并不适合那些需要低延迟的应用，例如，联机事务处理（OLTP）。Hive 查询操作过程严格遵守Hadoop MapReduce 的作业执行模型，Hive 将用户的HiveQL 语句通过解释器转换为MapReduce 作业提交到Hadoop 集群上，Hadoop 监控作业执行过程，然后返回作业执行结果给用户。Hive 并非为联机事务处理而设计，Hive 并不提供实时的查询和基于行级的数据更新操作。Hive 的最佳使用场合是大数据集的批处理作业，例如，网络日志分析。 元数据存储&nbsp;&nbsp;Hive将元数据存储在RDBMS中，有三种方式可以连接到数据库。 内嵌模式：元数据保持在内嵌数据库的Derby，一般用于单元测试，只允许一个会话连接。 多用户模式：在本地安装Mysql，把元数据放到Mysql内。 远程模式：元数据放置在远程的Mysql数据库。 数据存储 &nbsp;&nbsp;Hive没有专门的数据存储格式，也没有为数据建立索引，用于可以非常自由的组织Hive中的表，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符。 &nbsp;&nbsp;Hive中所有的数据都存储在HDFS中，Hive中包含4中数据模型：Tabel、ExternalTable、Partition、Bucket。 Table &nbsp;&nbsp;类似与传统数据库中的Table，每一个Table在Hive中都有一个相应的目录来存储数据。例如：一个表zz，它在HDFS中的路径为：/wh/zz，其中wh是在hive-site.xml中由${hive.metastore.warehouse.dir}指定的数据仓库的目录，所有的Table数据（不含External Table）都保存在这个目录中。 Partition &nbsp;&nbsp;类似于传统数据库中划分列的索引。在Hive中，表中的一个Partition对应于表下的一个目录，所有的Partition数据都存储在对应的目录中。例如：zz表中包含ds和city两个Partition，则对应于ds=20140214，city=beijing的HDFS子目录为：/wh/zz/ds=20140214/city=Beijing。 ExternalTable &nbsp;&nbsp;指向已存在HDFS中的数据，可创建Partition。和Table在元数据组织结构相同，在实际存储上有较大差异。Table创建和数据加载过程，可以用统一语句实现，实际数据被转移到数据仓库目录中，之后对数据的访问将会直接在数据仓库的目录中完成。删除表时，表中的数据和元数据都会删除。ExternalTable只有一个过程，因为加载数据和创建表是同时完成。时间数据是存储在Location后面指定的HDFS路径中的，并不会移动到数据仓库中。 Bcuket &nbsp;&nbsp;对指定列计算的hash，根据hash值切分数据，目的是为了便于并行，每一个Buckets对应一个文件。将user列分数至32个Bucket上，首先对user列的值计算hash，比如，对应hash=0的HDFS目录为：/wh/zz/ds=20140214/city=Beijing/part-00000;对应hash=20的，目录为：/wh/zz/ds=20140214/city=Beijing/part-00020。 Hive常用优化方法 join连接时的优化：当三个或多个以上的表进行join操作时，如果每个on使用相同的字段连接时只会产生一个mapreduce。 join连接时的优化：当多个表进行查询时，从左到右表的大小顺序应该是从小到大。原因：hive在对每行记录操作时会把其他表先缓存起来，直到扫描最后的表进行计算。 在where字句中增加分区过滤器。 当可以使用left semi join 语法时不要使用inner join，前者效率更高。原因：对于左表中指定的一条记录，一旦在右表中找到立即停止扫描。 如果所有表中有一张表足够小，则可置于内存中，这样在和其他表进行连接的时候就能完成匹配，省略掉reduce过程。设置属性即可实现，set hive.auto.covert.join=true; 用户可以配置希望被优化的小表的大小 set hive.mapjoin.smalltable.size=2500000; 如果需要使用这两个配置可置入$HOME/.hiverc文件中。 同一种数据的多种处理：从一个数据源产生的多个数据聚合，无需每次聚合都需要重新扫描一次。例如:insert overwrite table student select from employee; insert overwrite table person select from employee;可以优化成 from employee insert overwrite table student select insert overwrite table person select limit调优：limit语句通常是执行整个语句后返回部分结果。set hive.limit.optimize.enable=true; 开启并发执行。某个job任务中可能包含众多的阶段，其中某些阶段没有依赖关系可以并发执行，开启并发执行后job任务可以更快的完成。设置属性：set hive.exec.parallel=true; hive提供的严格模式，禁止3种情况下的查询模式。 当表为分区表时，where字句后没有分区字段和限制时，不允许执行。 当使用order by语句时，必须使用limit字段，因为order by 只会产生一个reduce任务。 限制笛卡尔积的查询。 合理的设置map和reduce数量。 jvm重用。可在hadoop的mapred-site.xml中设置jvm被重用的次数。 安装Hive 解压Hive。 将mysql的驱动jar包copy到${HIVE_HOME}/lib目录下。 cp hive-default.xml.template hive-size.xml。 配置hive-size.xml。12345678910111213141516171819 &lt;configuration&gt; &lt;!-- 指定数据库URL --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.145.148:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Hive QL文件格式&nbsp;&nbsp;Hive创建表可以指定四种文件格式。 文本格式的数据是Hadoop中经常碰到的。如TextFile 、XML和JSON。 文本格式除了会占用更多磁盘资源外，对它的解析开销一般会比二进制格式高几十倍以上，尤其是XML 和JSON，它们的解析开销比Textfile 还要大，因此强烈不建议在生产系统中使用这些格式进行储存。如果需要输出这些格式，请在客户端做相应的转换操作。 文本格式经常会用于日志收集，数据库导入，Hive默认配置也是使用文本格式，而且常常容易忘了压缩，所以请确保使用了正确的格式。另外文本格式的一个缺点是它不具备类型和模式，比如销售金额、利润这类数值数据或者日期时间类型的数据，如果使用文本格式保存，由于它们本身的字符串类型的长短不一，或者含有负数，导致MR没有办法排序，所以往往需要将它们预处理成含有模式的二进制格式，这又导致了不必要的预处理步骤的开销和储存资源的浪费。 SequenceFile是Hadoop API 提供的一种二进制文件，它将数据以的形式序列化到文件中。这种二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。它与Hadoop API中的MapFile 是互相兼容的。Hive 中的SequenceFile 继承自Hadoop API 的SequenceFile，不过它的key为空，使用value 存放实际的值， 这样是为了避免MR 在运行map 阶段的排序过程。如果你用Java API 编写SequenceFile，并让Hive 读取的话，请确保使用value字段存放数据，否则你需要自定义读取这种SequenceFile 的InputFormat class 和OutputFormat class。 RCFile是Hive推出的一种专门面向列的数据格式。 它遵循“先按列划分，再垂直划分”的设计理念。当查询过程中，针对它并不关心的列时，它会在IO上跳过这些列。需要说明的是，RCFile在map阶段从远端拷贝仍然是拷贝整个数据块，并且拷贝到本地目录后RCFile并不是真正直接跳过不需要的列，并跳到需要读取的列， 而是通过扫描每一个row group的头部定义来实现的，但是在整个HDFS Block 级别的头部并没有定义每个列从哪个row group起始到哪个row group结束。所以在读取所有列的情况下，RCFile的性能反而没有SequenceFile高。 Avro是一种用于支持数据密集型的二进制文件格式。它的文件格式更为紧凑，若要读取大量数据时，Avro能够提供更好的序列化和反序列化性能。并且Avro数据文件天生是带Schema定义的，所以它不需要开发者在API 级别实现自己的Writable对象。 其他格式:Hadoop实际上支持任意文件格式，只要能够实现对应的RecordWriter和RecordReader即可。其中数据库格式也是会经常储存在Hadoop中，比如Hbase，Mysql，Cassandra，MongoDB。 这些格式一般是为了避免大量的数据移动和快速装载的需求而用的。他们的序列化和反序列化都是由这些数据库格式的客户端完成，并且文件的储存位置和数据布局(Data Layout)不由Hadoop控制，他们的文件切分也不是按HDFS的块大小（blocksize）进行切割。 create table12345678create table test_user(id int,name string) // 注释 comment &apos;This is the test table&apos; row format delimited // 指定切分格式规则 fields terminated by &apos;,&apos; // 指定文件格式 stored as textfile; insert select12//使用select语句来批量插入数据insert overwrite table test_user select * from tab_user; load data12345//从本地导入数据到hive的表中（实质就是将文件上传到hdfs中hive管理目录下）load data local inpath &apos;/home/hadoop/test.txt&apos; into table test_user;//从hdfs上导入数据到hive表中（实质就是将文件从原始目录移动到hive管理的目录下）load data inpath &apos;hdfs://ns1/data.log&apos; into table test_user; external table1234567//LOCATION指定的是hdfs路径//如果LOCATION路径有数据,则可以直接映射数据建表CREATE EXTERNAL TABLE test_user_external(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE LOCATION &apos;/external/user&apos;; CTAS1234567//CTAS是通过查询,然后根据查询的结果来建立表格的一种方式。//CTAS会根据SELECT语句创建表结构,并把数据一并复制过来。CREATE TABLE test_user_ctas ASSELECT id new_id, name new_nameFROM test_userSORT BY new_id; Partition123456789101112//创建一个分区表,以year年份作为分区字段create table test_user_part(id int,name string) partitioned by (year string) row format delimited fields terminated by &apos;,&apos;;//将data.log导入到test_user_part表中,并设置分区为1990 load data local inpath &apos;/home/hadoop/data.log&apos; overwrite into table test_user_part partition(year=&apos;1990&apos;); load data local inpath &apos;/home/hadoop/data2.log&apos; overwrite into table test_user_part partition(year=&apos;2000&apos;); Array&amp;&amp;Map&nbsp;&nbsp;hive中的列支持使用struct、map和array集合数据类型。大多数关系型数据库中不支持这些集合数据类型，因为它们会破坏标准格式。关系型数据库中为实现集合数据类型是由多个表之间建立合适的外键关联来实现。在大数据系统中，使用集合类型的数据的好处在于提高数据的吞吐量，减少寻址次数来提高查询速度。 12345678910111213141516171819//array create table tab_array(a array&lt;int&gt;,b array&lt;string&gt;)row format delimitedfields terminated by &apos;\t&apos;collection items terminated by &apos;,&apos;;select a[0] from tab_array;select * from tab_array where array_contains(b,&apos;word&apos;);insert into table tab_array select array(0),array(name,ip) from tab_ext t; //mapcreate table tab_map(name string,info map&lt;string,string&gt;)row format delimitedfields terminated by &apos;\t&apos;collection items terminated by &apos;,&apos;map keys terminated by &apos;:&apos;;load data local inpath &apos;/home/hadoop/hivetemp/tab_map.txt&apos; overwrite into table tab_map;insert into table tab_map select name,map(&apos;name&apos;,name,&apos;ip&apos;,ip) from tab_ext; UDF&nbsp;&nbsp;UDF即用户自定义函数(User Defined Function),Hive支持UDF进行自定义函数的编写。 &nbsp;&nbsp;需要先使用Java代码开发UDF,然后再把jar包导入到Hive中。 123456789101112131415161718192021public class FindRegionByPhone extends UDF &#123; //使用map模拟数据库 private static HashMap&lt;String,String&gt; dataDictionary = new HashMap&lt;String,String&gt;(); static&#123; dataDictionary.put("136","beijing"); dataDictionary.put("137","guangzhou"); dataDictionary.put("138","shenzhen"); dataDictionary.put("139","shanghai"); &#125; public String evaluate(String phone) &#123; // 如果没有匹配到对应的区域则返回"other" return areaMap.get(phone.substring(0, 3)) == null ? "other" : areaMap .get(phone.substring(0, 3)); &#125; &#125; 添加jar包到Hive12345方式1:添加到hivehive&gt; add jar /root/MyUDF.jar;方式2:添加到hdfs,调用时需要指定jar包地址hdfs -dfs -put MyUDF.jar &apos;hdfs:///user/hadoop/hiveUDF&apos; 创建临时函数&nbsp;&nbsp;临时函数只在当前session中有效,临时函数不能指定库。 1create temporary function testUDF as &apos;cn.sylvanas.hive.udf.FindRegionByPhone&apos; using jar &apos;hdfs:///user/hadoop/hiveUDF/MyUDF.jar&apos; 创建永久函数格式 CREATE FUNCTION [db_name.]function_name AS class_name[USING JAR|FILE|ARCHIVE ‘file_uri’ [, JAR|FILE|ARCHIVE ‘file_uri’] ]; 例如 123create function test.testUDF as &apos;cn.sylvanas.hive.udf.FindRegionByPhone&apos; using jar &apos;hdfs:///user/hadoop/hiveUDF/MyUDF.jar&apos;函数需要属于某个库,如这里是’test’,当其他库调用时,需要加上库名,如’test.testUDF’.]]></content>
      <categories>
        <category>后端</category>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记(4)-HBase]]></title>
    <url>%2F2016%2F07%2F17%2F2016-07-17-Hadoop04-HBase%2F</url>
    <content type="text"><![CDATA[概述&nbsp;&nbsp;HBase是一个分布式的、面向列的开源数据库，该技术来源于 Fay Chang 所撰写的Google论文“Bigtable：一个结构化数据的分布式存储系统”。就像Bigtable利用了Google文件系统（File System）所提供的分布式数据存储一样，HBase在Hadoop之上提供了类似于Bigtable的能力。HBase是Apache的Hadoop项目的子项目。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。另一个不同的是HBase基于列的而不是基于行的模式。 &nbsp;&nbsp;与FUJITSU Cliq等商用大数据产品不同，HBase是Google Bigtable的开源实现，类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据；Google Bigtable利用 Chubby作为协同服务，HBase利用Zookeeper作为对应。 HBase的特点 Hbase可以往数据里面insert，也可以update一些数据，但update的实际上也是insert，只是插入一个新的时间戳的一行。delete数据，也是insert，只是insert一行带有delete标记的一行。Hbase的所有操作都是追加插入操作。Hbase是一种日志集数据库。它的存储方式，像是日志文件一样。它是批量大量的往硬盘中写，通常都是以文件形式的读写。这个读写速度，取决于硬盘与机器之间的传输有多快。 Hbase中数据可以保存许多不同时间戳的版本（即同一数据可以复制许多不同的版本，准许数据冗余，也是优势）。数据按时间排序，因此Hbase特别适合寻找按照时间排序寻找Top n的场景。找出某个人最近浏览的消息，最近写的N篇博客，N种行为等等，因此Hbase在互联网应用非常多。 Hbase只有主键索引，因此在建模的时候会遇到了问题。例如，在一张表中，很多的列我都想做某种条件的查询。但却只能在主键上建快速查询。 Hbase是列式数据库,列式数据库的优势在于数据分析。 Hbase中的数据都是字符串，没有其他类型。 行式数据库与列式数据库的区别 行式数据库 &nbsp;&nbsp;以Oracle为例，数据文件的基本组成单位：块/页。块中数据是按照一行行写入的。这就存在一个问题，当我们要读一个块中的某些列的时候，不能只读这些列，必须把这个块整个的读入内存中，再把这些列的内容读出来。换句话就是：为了读表中的某些列，必须要把整个表的行全部读完，才能读到这些列。这就是行数据库最糟糕的地方。 列式数据库 &nbsp;&nbsp;列式数据库是以列作为元素存储的。同一个列的元素会挤在一个块。当要读某些列，只需要把相关的列块读到内存中，这样读的IO量就会少很多。通常，同一个列的数据元素通常格式都是相近的。这就意味着，当数据格式相近的时候，数据就可以做大幅度的压缩。所以，列式数据库在数据压缩方面有很大的优势，压缩不仅节省了存储空间，同时也节省了IO。（这一点，可利用在当数据达到百万、千万级别以后，数据查询之间的优化，提高性能，示场景而定） HBase架构 &nbsp;&nbsp;HBase采用Master/Slave架构搭建集群，它隶属于Hadoop生态系统，由以下类型节点组成：HMaster节点、HRegionServer节点、ZooKeeper集群，而在底层，它将数据存储于HDFS中，因而涉及到HDFS的NameNode、DataNode等节点。 Zookeeper &nbsp;&nbsp;Zookeeper Quorum存储-ROOT-表地址、HMaster地址。HRegionServer把自己以Ephedral方式注册到Zookeeper中，HMaster随时感知各个HRegionServer的健康状况。 HMaster &nbsp;&nbsp;HMaster没有单点问题,HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master在运行。 &nbsp;&nbsp;HMaster主要负责Table和Region的管理工作 实现DDL操作（Data Definition Language，namespace和table的增删改，column familiy的增删改等）。 管理HRegionServer的负载均衡，调整Region分布。 管理和分配HRegion，比如在HRegion split时分配新的HRegion；在HRegionServer退出时迁移其内的HRegion到其他HRegionServer上。 权限控制（ACL）。 HRegionServer &nbsp;&nbsp;HBase中最核心的模块，主要负责响应用户I/O请求，向HDFS文件系统中读写数据。 存放和管理本地HRegion。 读写HDFS，管理Table中的数据。 Client直接通过HRegionServer读写数据（从HMaster中获取元数据，找到RowKey所在的HRegion/HRegionServer后）。 HRegion &nbsp;&nbsp;HBase使用RowKey将表水平切割成多个HRegion，从HMaster的角度，每个HRegion都纪录了它的StartKey和EndKey（第一个HRegion的StartKey为空，最后一个HRegion的EndKey为空），由于RowKey是排序的，因而Client可以通过HMaster快速的定位每个RowKey在哪个HRegion中。HRegion由HMaster分配到相应的HRegionServer中，然后由HRegionServer负责HRegion的启动和管理，和Client的通信，负责数据的读(使用HDFS)。 HBase集群搭建&nbsp;&nbsp;如果HDFS是HA集群,需要把HDFS的core-site.xml和hdfs-site.xml copy到conf下。 hbase-env.sh12//告诉hbase使用外部的zookeeperexport HBASE_MANAGES_ZK=false hbase-site.xml12345678910111213141516171819202122&lt;configuration&gt; &lt;!-- 指定hbase在HDFS上存储的路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://ns1/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hbase是分布式的 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk的地址，多个用“,”分割 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;datanode01:2181,datanode02:2181,datanode03:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt; &lt;value&gt;180000&lt;/value&gt; &lt;description&gt;Time difference of regionserver from master&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; regionservers&nbsp;&nbsp;配置regionserver的节点,为了尽量实现数据本地化,可以与DataNode在同一个节点上。 HBase常用Shell命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139进入hbase命令行./hbase shell显示hbase中的表list创建user表，包含info、data两个列族create &apos;user&apos;, &apos;info1&apos;, &apos;data1&apos;create &apos;user&apos;, &#123;NAME =&gt; &apos;info&apos;, VERSIONS =&gt; &apos;3&apos;&#125;向user表中插入信息，row key为rk0001，列族info中添加name列标示符，值为zhangsanput &apos;user&apos;, &apos;rk0001&apos;, &apos;info:name&apos;, &apos;zhangsan&apos;向user表中插入信息，row key为rk0001，列族info中添加gender列标示符，值为femaleput &apos;user&apos;, &apos;rk0001&apos;, &apos;info:gender&apos;, &apos;female&apos;向user表中插入信息，row key为rk0001，列族info中添加age列标示符，值为20put &apos;user&apos;, &apos;rk0001&apos;, &apos;info:age&apos;, 20向user表中插入信息，row key为rk0001，列族data中添加pic列标示符，值为pictureput &apos;user&apos;, &apos;rk0001&apos;, &apos;data:pic&apos;, &apos;picture&apos;获取user表中row key为rk0001的所有信息get &apos;user&apos;, &apos;rk0001&apos;获取user表中row key为rk0001，info列族的所有信息get &apos;user&apos;, &apos;rk0001&apos;, &apos;info&apos;获取user表中row key为rk0001，info列族的name、age列标示符的信息get &apos;user&apos;, &apos;rk0001&apos;, &apos;info:name&apos;, &apos;info:age&apos;获取user表中row key为rk0001，info、data列族的信息get &apos;user&apos;, &apos;rk0001&apos;, &apos;info&apos;, &apos;data&apos;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; [&apos;info&apos;, &apos;data&apos;]&#125;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; [&apos;info:name&apos;, &apos;data:pic&apos;]&#125;获取user表中row key为rk0001，列族为info，版本号最新5个的信息get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; &apos;info&apos;, VERSIONS =&gt; 2&#125;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; &apos;info:name&apos;, VERSIONS =&gt; 5&#125;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; &apos;info:name&apos;, VERSIONS =&gt; 5, TIMERANGE =&gt; [1392368783980, 1392380169184]&#125;获取user表中row key为rk0001，cell的值为zhangsan的信息get &apos;people&apos;, &apos;rk0001&apos;, &#123;FILTER =&gt; &quot;ValueFilter(=, &apos;binary:图片&apos;)&quot;&#125;获取user表中row key为rk0001，列标示符中含有a的信息get &apos;people&apos;, &apos;rk0001&apos;, &#123;FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;&#125;put &apos;user&apos;, &apos;rk0002&apos;, &apos;info:name&apos;, &apos;fanbingbing&apos;put &apos;user&apos;, &apos;rk0002&apos;, &apos;info:gender&apos;, &apos;female&apos;put &apos;user&apos;, &apos;rk0002&apos;, &apos;info:nationality&apos;, &apos;中国&apos;get &apos;user&apos;, &apos;rk0002&apos;, &#123;FILTER =&gt; &quot;ValueFilter(=, &apos;binary:中国&apos;)&quot;&#125;查询user表中的所有信息scan &apos;user&apos;查询user表中列族为info的信息scan &apos;user&apos;, &#123;COLUMNS =&gt; &apos;info&apos;&#125;scan &apos;user&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, RAW =&gt; true, VERSIONS =&gt; 5&#125;scan &apos;persion&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, RAW =&gt; true, VERSIONS =&gt; 3&#125;查询user表中列族为info和data的信息scan &apos;user&apos;, &#123;COLUMNS =&gt; [&apos;info&apos;, &apos;data&apos;]&#125;scan &apos;user&apos;, &#123;COLUMNS =&gt; [&apos;info:name&apos;, &apos;data:pic&apos;]&#125;查询user表中列族为info、列标示符为name的信息scan &apos;user&apos;, &#123;COLUMNS =&gt; &apos;info:name&apos;&#125;查询user表中列族为info、列标示符为name的信息,并且版本最新的5个scan &apos;user&apos;, &#123;COLUMNS =&gt; &apos;info:name&apos;, VERSIONS =&gt; 5&#125;查询user表中列族为info和data且列标示符中含有a字符的信息scan &apos;user&apos;, &#123;COLUMNS =&gt; [&apos;info&apos;, &apos;data&apos;], FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;&#125;查询user表中列族为info，rk范围是[rk0001, rk0003)的数据scan &apos;people&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, STARTROW =&gt; &apos;rk0001&apos;, ENDROW =&gt; &apos;rk0003&apos;&#125;查询user表中row key以rk字符开头的scan &apos;user&apos;,&#123;FILTER=&gt;&quot;PrefixFilter(&apos;rk&apos;)&quot;&#125;查询user表中指定范围的数据scan &apos;user&apos;, &#123;TIMERANGE =&gt; [1392368783980, 1392380169184]&#125;删除数据删除user表row key为rk0001，列标示符为info:name的数据delete &apos;people&apos;, &apos;rk0001&apos;, &apos;info:name&apos;删除user表row key为rk0001，列标示符为info:name，timestamp为1392383705316的数据delete &apos;user&apos;, &apos;rk0001&apos;, &apos;info:name&apos;, 1392383705316清空user表中的数据truncate &apos;people&apos;修改表结构首先停用user表（新版本不用）disable &apos;user&apos;添加两个列族f1和f2alter &apos;people&apos;, NAME =&gt; &apos;f1&apos;alter &apos;user&apos;, NAME =&gt; &apos;f2&apos;启用表enable &apos;user&apos;###disable &apos;user&apos;(新版本不用)删除一个列族：alter &apos;user&apos;, NAME =&gt; &apos;f1&apos;, METHOD =&gt; &apos;delete&apos; 或 alter &apos;user&apos;, &apos;delete&apos; =&gt; &apos;f1&apos;添加列族f1同时删除列族f2alter &apos;user&apos;, &#123;NAME =&gt; &apos;f1&apos;&#125;, &#123;NAME =&gt; &apos;f2&apos;, METHOD =&gt; &apos;delete&apos;&#125;将user表的f1列族版本号改为5alter &apos;people&apos;, NAME =&gt; &apos;info&apos;, VERSIONS =&gt; 5启用表enable &apos;user&apos;删除表disable &apos;user&apos;drop &apos;user&apos;get &apos;person&apos;, &apos;rk0001&apos;, &#123;FILTER =&gt; &quot;ValueFilter(=, &apos;binary:中国&apos;)&quot;&#125;get &apos;person&apos;, &apos;rk0001&apos;, &#123;FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;&#125;scan &apos;person&apos;, &#123;COLUMNS =&gt; &apos;info:name&apos;&#125;scan &apos;person&apos;, &#123;COLUMNS =&gt; [&apos;info&apos;, &apos;data&apos;], FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;&#125;scan &apos;person&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, STARTROW =&gt; &apos;rk0001&apos;, ENDROW =&gt; &apos;rk0003&apos;&#125;scan &apos;person&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, STARTROW =&gt; &apos;20140201&apos;, ENDROW =&gt; &apos;20140301&apos;&#125;scan &apos;person&apos;, &#123;COLUMNS =&gt; &apos;info:name&apos;, TIMERANGE =&gt; [1395978233636, 1395987769587]&#125;delete &apos;person&apos;, &apos;rk0001&apos;, &apos;info:name&apos;alter &apos;person&apos;, NAME =&gt; &apos;ffff&apos;alter &apos;person&apos;, NAME =&gt; &apos;info&apos;, VERSIONS =&gt; 10get &apos;user&apos;, &apos;rk0002&apos;, &#123;COLUMN =&gt; [&apos;info:name&apos;, &apos;data:pic&apos;]&#125;]]></content>
      <categories>
        <category>后端</category>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记(3)-HA高可用集群搭建]]></title>
    <url>%2F2016%2F07%2F15%2F2016-07-15-Hadoop03-HA%2F</url>
    <content type="text"><![CDATA[概述&nbsp;&nbsp;HA(High Available), 高可用性集群，是保证业务连续性的有效解决方案，一般有两个或两个以上的节点，且分为活动节点及备用节点。通常把正在执行业务的称为活动节点，而作为活动节点的一个备份的则称为备用节点。当活动节点出现问题，导致正在运行的业务（任务）不能正常运行时，备用节点此时就会侦测到，并立即接续活动节点来执行业务。从而实现业务的不中断或短暂中断。 &nbsp;&nbsp;在hadoop2.0之前,每个集群只有一个NameNode,如果那台机器坏掉,集群作为一个整体将不可用,所以为了解决这个问题Hadoop2.0引入了HA机制,可以通过在同一集群上配置运行两个冗余的NameNodes，做到主动/被动的热备份。这将允许当一个机器宕机时，快速转移到一个新的NameNode，或管理员进行利用故障转移达到优雅的系统升级的目的。 &nbsp;&nbsp;HA一共有二种解决方案，一种是NFS（Network File System）方式，另外一种是QJM（Quorum Journal Manager）方式。 HA架构 &nbsp;&nbsp;一个典型的HA集群，NameNode会被配置在两台独立的机器上.在任何的时间上，一个NameNode处于活动状态，而另一个在备份状态，活动状态的NameNode会响应集群中所有的客户端，同时备份的只是作为一个副本，保证在必要的时候提供一个快速的转移。 &nbsp;&nbsp;为了使备份的节点和活动的节点保持一致，两个节点通过一个特殊的守护线程相连，这个线程叫做“JournalNodes”（JNs）。当活动状态的节点修改任何的命名空间，他都会通过这些JNs记录日志，备用的节点可以监控edit日志的变化，并且通过JNs读取到变化。备份节点查看edits可以拥有专门的namespace。在故障转移的时候备份节点将在切换至活动状态前确认他从JNs读取到的所有edits。这个确认的目的是为了保证Namespace的状态和迁移之前是完全同步的。 &nbsp;&nbsp;为了提供一个快速的转移，备份NameNode要求保存着最新的block在集群当中的信息。为了能够得到这个，DataNode都被配置了所有的NameNode的地址，并且发送block的地址信息和心跳给两个node。 &nbsp;&nbsp;保证只有一个活跃的NameNode在集群当中是一个十分重要的一步。否则namespace状态在两个节点间不同会导致数据都是或者其他一些不正确的结果。为了确保这个,防止所谓split - brain场景,JournalNodes将只允许一个NameNode进行写操作。故障转移期间,NameNode成为活跃状态的时候会接管JournalNodes的写权限,这会有效防止其他NameNode持续处于活跃状态,允许新的活动节点安全进行故障转移。 搭建HA集群&nbsp;&nbsp;hadoop-2.2.0中依然存在一个问题，就是ResourceManager只有一个，存在单点故障，hadoop-2.4.1解决了这个问题，可以有两个ResourceManager，一个是Active，一个是Standby，状态由zookeeper进行协调。 测试集群规划&nbsp;&nbsp;实验使用7台虚拟机,规划如下: HostName IP Software Process datanode01 192.168.145.140 jdk、hadoop、zookeeper DataNode、NodeManager、JournalNode、QuorumPeerMain datanode02 192.168.145.141 jdk、hadoop、zookeeper DataNode、NodeManager、JournalNode、QuorumPeerMain datanode03 192.168.145.142 jdk、hadoop、zookeeper DataNode、NodeManager、JournalNode、QuorumPeerMain namenode01 192.168.145.143 jdk、hadoop NameNode、DFSZKFailoverController(ZKFC) namenode02 192.168.145.144 jdk、hadoop NameNode、DFSZKFailoverController(ZKFC) yarn01 192.168.145.145 jdk、hadoop ResourceManager yarn02 192.168.145.146 jdk、hadoop ResourceManager ssh免密登陆 namenode01需要配置所有datanode、yarn、namenode的免密登陆。 namenode02需要配置namenode01的免密登陆。 yarn01需要配置所有nodemanager与resourcemanager的免密登陆。 1234567891011121314151617181920212223#在namenode01上生成密匙ssh-keygen#namenode01拷贝密匙(包括自己)ssh-copy-id namenode01ssh-copy-id namenode02ssh-copy-id datanode01ssh-copy-id datanode02ssh-copy-id datanode03ssh-copy-id yarn01ssh-copy-id yarn02#在namenode02上生成密匙ssh-keygenssh-copy-id namenode01ssh-copy-id namenode02#在yarn01上生成密匙ssh-keygen#yarn01拷贝密匙ssh-copy-id yarn02ssh-copy-id datanode01ssh-copy-id datanode02ssh-copy-id datanode03 安装zookeeper 解压zookeeper 重命名zookeeper/conf下的zoo_sample.cfg为zoo.cfg : mv zoo_sample.cfg zoo.cfg 在zoo.cfg中修改dataDir=$ZOOKEEPERHOME/data 这个文件需要自己创建例如:dataDir=/home/application/zookeeper-3.4.5/data 在zoo.cfg中最后添加 server.id=ip:2888:3888例如:server.1=datanode01:2888:3888 server.2=datanode02:2888:3888 server.3=datanode03:2888:3888 在$ZOOKEEPERHOME/data目录中创建一个myid文件并写入id。例如: echo 1 &gt;/home/application/zookeeper-3.4.5/data/myidid需要跟zoo.cfg中配置的一致。 core-site.xml1234567891011121314151617&lt;configuration&gt; &lt;!-- 指定hdfs的nameservice为ns1 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ns1/&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop临时目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/application/hadoop-2.6.0/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;datanode01:2181,datanode02:2181,datanode03:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;configuration&gt; &lt;!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致. 这个名字是逻辑名字,可以是任意的,它将被用来配置在集群中作为HDFS的绝对路径组件。--&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns1&lt;/value&gt; &lt;/property&gt; &lt;!-- ns1下面有两个NameNode，分别是nn1，nn2 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ns1&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.nn1&lt;/name&gt; &lt;value&gt;namenode01:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.nn1&lt;/name&gt; &lt;value&gt;namenode01:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.nn2&lt;/name&gt; &lt;value&gt;namenode02:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.nn2&lt;/name&gt; &lt;value&gt;namenode02:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://datanode01:8485;datanode02:8485;datanode03:8485/ns1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/application/hadoop-2.6.0/journaldata&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启NameNode失败自动切换 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns1&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt; &lt;/property&gt; &lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置sshfence隔离机制超时时间 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml1234567&lt;configuration&gt; &lt;!-- 指定mr框架为yarn方式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml1234567891011121314151617181920212223242526272829303132333435&lt;configuration&gt; &lt;!-- 开启RM高可用 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的cluster id 这是一个逻辑名称,可以是任意的 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yarncluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的名字 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 分别指定RM的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;yarn01&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;yarn02&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk集群地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;datanode01:2181,datanode02:2181,datanode03:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; slaves&nbsp;&nbsp;slaves指定子节点(DataNode)位置,因为yarn与HDFS分开启动,所以在yarn01中slaves指定的是NodeManager的位置。 启动HA集群 启动zookeeper集群 启动JournalNode,一旦JNs启动，必须进行一次初始化同步在两个HA的NameNode，主要是为了元数据。sbin/hadoop-daemon.sh start journalnode 格式化HDFS。在namenode01上执行命令 hdfs namenode -format格式化后会在根据core-site.xml中的hadoop.tmp.dir配置生成个文件,为了同步元数据,需要将tmp文件夹copy到namenode02上。scp -r tmp/ namenode02:/home/application/hadoop-2.6.0/也可以使用命令 hdfs namenode -bootstrapStandby 格式化ZKFC在namenode01上执行命令 hdfs zkfc -formatZK 启动HDFS在namenode01上执行命令 sbin/start-dfs.sh 启动Yarn在yarn01上执行命令 sbin/start-yarn.sh。 因为自带的start-yarn.sh脚本并不会远程启动第二个RM,所以需要在yarn02上单独启动一个RM。在yarn02上执行命令 sbin/yarn-daemon.sh start resourcemanager 管理命令1234567Usage: DFSHAAdmin [-ns &lt;nameserviceId&gt;] [-transitionToActive &lt;serviceId&gt;] [-transitionToStandby &lt;serviceId&gt;] [-failover [--forcefence] [--forceactive] &lt;serviceId&gt; &lt;serviceId&gt;] [-getServiceState &lt;serviceId&gt;] [-checkHealth &lt;serviceId&gt;] [-help &lt;command&gt;] &nbsp;&nbsp;描述了常用的命令，每个子命令的详细信息你应该运行”hdfs haadmin -help “. transitionToActive &amp;&amp; transitionToStandby &nbsp;&nbsp;切换NameNode的状态（Active或者Standby),这些子命令会使NameNode分别转换状态。 failover &nbsp;&nbsp;启动两个NameNode之间的故障迁移。 &nbsp;&nbsp;这个子命令会从第一个NameNode迁移到第二个，如果第一个NameNode处于备用状态,这个命令只是没有错误的转换第二个节点到活动状态。如果第一个NameNode处于活跃状态,试图将优雅地转换到备用状态。如果失败,过滤方法(如由dfs.ha.fencing.methods配置)将尝试过滤直到成功。只有在这个过程之后第二个NameNode会转换为活动状态，如果没有过滤方法成功，第二个nameNode将不会活动并返回一个错误。 getServiceState &nbsp;&nbsp;连接到NameNode，去判断现在的状态打印“standby”或者“active”去标准的输出。这个子命令可以被corn jobs或者是监控脚本使用，为了针对不同状态的NameNode采用不同的行为。 checkHealth &nbsp;&nbsp;连接NameNode检查健康，NameNode能够执行一些诊断,包括检查如果内部服务正在运行。如果返回0表明NameNode健康，否则返回非0.可以使用此命令用于监测目的。 &nbsp;&nbsp;注意：这个功能实现的不完整，目前除了NameNode完全的关闭，其他全部返回成功。]]></content>
      <categories>
        <category>后端</category>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记(2)-Mapreduce]]></title>
    <url>%2F2016%2F07%2F14%2F2016-07-14-Hadoop02-MapReduce%2F</url>
    <content type="text"><![CDATA[什么是MapReduce&nbsp;&nbsp;MapReduce是一种分布式计算模型，由Google提出，主要用于搜索领域，解决海量数据的计算问题。 &nbsp;&nbsp;MapReduce是处理大量半结构化数据集合的编程模型。编程模型是一种处理并结构化特定问题的方式。例如，在一个关系数据库中，使用一种集合语言执行查询，如SQL。告诉语言想要的结果，并将它提交给系统来计算出如何产生计算。还可以用更传统的语言(C++，Java)，一步步地来解决问题。这是两种不同的编程模型，MapReduce就是另外一种。 &nbsp;&nbsp;MapReduce和Hadoop是相互独立的，实际上又能相互配合工作得很好。 Yarn概述&nbsp;&nbsp;Yarn是一个分布式的资源管理系统，用以提高分布式的集群环境下的资源利用率，这些资源包括内存、IO、网络、磁盘等。其产生的原因是为了解决原MapReduce框架的不足。最初MapReduce的committer们还可以周期性的在已有的代码上进行修改，可是随着代码的增加以及原MapReduce框架设计的不足，在原MapReduce框架上进行修改变得越来越困难，所以MapReduce的committer们决定从架构上重新设计MapReduce,使下一代的MapReduce(MRv2/Yarn)框架具有更好的扩展性、可用性、可靠性、向后兼容性和更高的资源利用率以及能支持除了MapReduce计算框架外的更多的计算框架。 原MapReduce架构的不足 JobTracker是集群事务的集中处理点，存在单点故障。 JobTracker需要完成的任务太多，既要维护job的状态又要维护job的task的状态，造成过多的资源消耗。 在taskTracker端，用map/reduce task作为资源的表示过于简单，没有考虑到CPU、内存等资源情况，当把两个需要消耗大内存的task调度到一起，很容易出现OOM(Out Of Memory内存不足)。 把资源强制划分为map/reduce slot,当只有map task时，reduce slot不能用；当只有reduce task时，map slot不能用，容易造成资源利用不足。 MRv2/Yarn工作流程Yarn架构&nbsp;&nbsp;Yarn/MRv2最基本的想法是将原JobTracker主要的资源管理和job调度/监视功能分开作为两个单独的守护进程。 &nbsp;&nbsp;有一个全局的ResourceManager(RM)和每个Application有一个ApplicationMaster(AM)，Application相当于map-reduce job或者DAG jobs。 &nbsp;&nbsp;ResourceManager和NodeManager(NM)组成了基本的数据计算框架。ResourceManager协调集群的资源利用，任何client或者运行着的applicatitonMaster想要运行job或者task都得向RM申请一定的资源。ApplicatonMaster是一个框架特殊的库，对于MapReduce框架而言有它自己的AM实现，用户也可以实现自己的AM，在运行的时候，AM会与NM一起来启动和监视tasks。 ResourceManager ResourceManager作为资源的协调者有两个主要的组件：Scheduler和ApplicationsManager(AsM)。 Scheduler负责分配最少但满足application运行所需的资源量给Application。Scheduler只是基于资源的使用情况进行调度，并不负责监视/跟踪application的状态，当然也不会处理失败的task。RM使用resource container概念来管理集群的资源，resource container是资源的抽象，每个container包括一定的内存、IO、网络等资源，不过目前的实现只包括内存一种资源。 ApplicationsManager负责处理client提交的job以及协商第一个container以供applicationMaster运行，并且在applicationMaster失败的时候会重新启动applicationMaster。下面阐述RM具体完成的一些功能。 资源调度：Scheduler从所有运行着的application收到资源请求后构建一个全局的资源分配计划，然后根据application特殊的限制以及全局的一些限制条件分配资源。 资源监视：Scheduler会周期性的接收来自NM的资源使用率的监控信息，另外applicationMaster可以从Scheduler得到属于它的已完成的container的状态信息。 Application提交： client向AsM获得一个applicationIDclient将application定义以及需要的jar包. client将application定义以及需要的jar包文件等上传到hdfs的指定目录，由yarn-site.xml的yarn.app.mapreduce.am.staging-dir指定. client构造资源请求的对象以及application的提交context发送给AsM. AsM接收application的提交context. AsM根据application的信息向Scheduler协商一个Container供applicationMaster运行，然后启动applicationMaster. 向该container所属的NM发送launchContainer信息启动该container,也即启动applicationMaster、AsM向client提供运行着的AM的状态信息. AM的生命周期：AsM负责系统中所有AM的生命周期的管理。AsM负责AM的启动，当AM启动后，AM会周期性的向AsM发送heartbeat，默认是1s，AsM据此了解AM的存活情况，并且在AM fail时负责重启AM，若是一定时间过后(默认10分钟)没有收到AM的heartbeat，AsM就认为该AM已经fail。 NodeManager &nbsp;&nbsp;NM主要负责启动RM分配给AM的container以及代表AM的container，并且会监视container的运行情况。在启动container的时候，NM会设置一些必要的环境变量以及将container运行所需的jar包、文件等从hdfs下载到本地，也就是所谓的资源本地化；当所有准备工作做好后，才会启动代表该container的脚本将程序启动起来。启动起来后，NM会周期性的监视该container运行占用的资源情况，若是超过了该container所声明的资源量，则会kill掉该container所代表的进程。 &nbsp;&nbsp;NM还提供了一个简单的服务以管理它所在机器的本地目录。Applications可以继续访问本地目录即使那台机器上已经没有了属于它的container在运行。例如，Map-Reduce应用程序使用这个服务存储map output并且shuffle它们给相应的reduce task。 &nbsp;&nbsp;NM上还可以扩展自己的服务，yarn提供了一个yarn.nodemanager.aux-services的配置项，通过该配置，用户可以自定义一些服务，例如Map-Reduce的shuffle功能就是采用这种方式实现的。 NM在本地为每个运行着的application生成如下的目录结构： Container目录下的目录结构如下： &nbsp;&nbsp;在启动一个container的时候，NM就执行该container的default_container_executor.sh，该脚本内部会执行launch_container.sh。launch_container.sh会先设置一些环境变量，最后启动执行程序的命令。对于MapReduce而言，启动AM就执行org.apache.hadoop.mapreduce.v2.app.MRAppMaster；启动map/reduce task就执行org.apache.hadoop.mapred.YarnChild。 ApplicationMaster &nbsp;&nbsp;ApplicationMaster是一个框架特殊的库，对于Map-Reduce计算模型而言有它自己的ApplicationMaster实现，对于其他的想要运行在yarn上的计算模型而言，必须得实现针对该计算模型的ApplicationMaster用以向RM申请资源运行task，比如运行在yarn上的spark框架也有对应的ApplicationMaster实现，归根结底，yarn是一个资源管理的框架，并不是一个计算框架，要想在yarn上运行应用程序，还得有特定的计算框架的实现。 工作流程 JobClient向ResourceManager(AsM)申请提交一个job。 RM返回jobId和job提交路径。 JobClient提交job相关的文件。 向RM汇报提交完成。 RM将job写入Job Queue。 NodeManager(NM)向Job Queue领取任务。 ApplicationMaster(AM)启动,向RM进行注册。 RM向AM返回资源信息。 AM启动map。 当所有map任务完成后,AM启动reduce。 AM监视运行着的task直到完成,当task失败时,申请新的container运行失败的task。 当每个map/reduce task完成后,AM运行MR OutputCommitter的cleanup 代码，进行一些收尾工作。 当所有的map/reduce完成后,AM运行OutputCommitter的必要的job commit或者abort APIs。 AM注销自己。 Shuffle过程 Map 每个输入分片会让一个map任务来处理，默认情况下，以HDFS的一个块的大小（默认为64M）为一个分片，当然我们也可以设置块的大小。map输出的结果会暂且放在一个环形内存缓冲区中（该缓冲区的大小默认为100M，由io.sort.mb属性控制），当该缓冲区快要溢出时（默认为缓冲区大小的80%，由io.sort.spill.percent属性控制），会在本地文件系统中创建一个溢出文件，将该缓冲区中的数据写入这个文件。 在写入磁盘之前，线程首先根据reduce任务的数目将数据划分为相同数目的分区，也就是一个reduce任务对应一个分区的数据。这样做是为了避免有些reduce任务分配到大量数据，而有些reduce任务却分到很少数据，甚至没有分到数据的尴尬局面。其实分区就是对数据进行hash的过程。然后对每个分区中的数据进行排序，如果此时设置了Combiner，将排序后的结果进行Combia操作，这样做的目的是让尽可能少的数据写入到磁盘。 当map任务输出最后一个记录时，可能会有很多的溢出文件，这时需要将这些文件合并。合并的过程中会不断地进行排序和combia操作，目的有两个： 尽量减少每次写入磁盘的数据量； 尽量减少下一复制阶段网络传输的数据量。最后合并成了一个已分区且已排序的文件。 为了减少网络传输的数据量，这里可以将数据压缩，只要将mapred.compress.map.out设置为true就可以了。 将分区中的数据拷贝给相对应的reduce Task。有人可能会问：分区中的数据怎么知道它对应的reduce是哪个呢？其实map任务一直和其父TaskTracker保持联系，而TaskTracker又一直和JobTracker保持心跳。所以JobTracker中保存了整个集群中的宏观信息。只要reduce任务向JobTracker获取对应的map输出位置就ok了哦。 Reduce Reduce会接收到不同map任务传来的数据，并且每个map传来的数据都是有序的。如果reduce端接受的数据量相当小，则直接存储在内存中（缓冲区大小由mapred.job.shuffle.input.buffer.percent属性控制，表示用作此用途的堆空间的百分比），如果数据量超过了该缓冲区大小的一定比例（由mapred.job.shuffle.merge.percent决定），则对数据合并后溢写到磁盘中。 随着溢写文件的增多，后台线程会将它们合并成一个更大的有序的文件，这样做是为了给后面的合并节省时间。其实不管在map端还是reduce端，MapReduce都是反复地执行排序，合并操作。 合并的过程中会产生许多的中间文件（写入磁盘了），但MapReduce会让写入磁盘的数据尽可能地少，并且最后一次合并的结果并没有写入磁盘，而是直接输入到reduce函数。 WordCount案例Mapper1234567891011121314public class MyWordCountMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 读取一行的value String line = value.toString(); // 按照规则切分 String[] words = line.split(" "); // 按照&lt;单词,1&gt;的格式输出 for (String word : words) &#123; context.write(new Text(word), new LongWritable(1)); &#125; &#125;&#125; Reducer123456789101112131415public class MyWordCountReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 初始化计数器 long count = 0; // 迭代values,累加计数器计算出总次数 for (LongWritable value : values) &#123; count += value.get(); &#125; // 输出&lt;单词,总次数&gt; context.write(key, new LongWritable(count)); &#125;&#125; Main1234567891011121314151617181920212223242526272829303132public class MyWordCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); // 构造一个job对象 Job wordCountJob = Job.getInstance(conf); // 指定job用到的jar包位置,这里使用当前类 wordCountJob.setJarByClass(MyWordCountDriver.class); // 指定mapper wordCountJob.setMapperClass(MyWordCountMapper.class); // 指定reducer wordCountJob.setReducerClass(MyWordCountReducer.class); // 指定mapper输出key/value的类型 wordCountJob.setMapOutputKeyClass(Text.class); wordCountJob.setMapOutputValueClass(LongWritable.class); // 指定reducer输出key/value的类型 wordCountJob.setOutputKeyClass(Text.class); wordCountJob.setOutputValueClass(LongWritable.class); // 指定输入数据的路径 FileInputFormat.setInputPaths(wordCountJob, new Path(args[0])); // 指定输出结果的路径 FileOutputFormat.setOutputPath(wordCountJob, new Path(args[1])); // 通过yarn客户端进行提交,参数2为是否打印到控制台 wordCountJob.waitForCompletion(true); &#125;&#125; 启动MapReduce&nbsp;&nbsp;方式1: 将程序打成jar包,上传到hadoop中执行。hadoop jar [mainClass] args… &nbsp;&nbsp;方式2: 将程序打成jar包,在本地IDE上直接运行(需要代码指定jar)。 自定义Sortbean123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class FlowBean implements WritableComparable&lt;FlowBean&gt; &#123; private Long upFlow; private Long downFlow; private Long sumFlow; public void setAll(Long upFlow, Long downFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; public Long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(Long upFlow) &#123; this.upFlow = upFlow; &#125; public Long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(Long downFlow) &#123; this.downFlow = downFlow; &#125; public Long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(Long sumFlow) &#123; this.sumFlow = sumFlow; &#125; @Override public String toString() &#123; return "FlowBean [upFlow=" + upFlow + ", downFlow=" + downFlow + ", sumFlow=" + sumFlow + "]"; &#125; /** * 序列化 */ @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; /** * 反序列化 */ @Override public void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); downFlow = in.readLong(); sumFlow = in.readLong(); &#125; /** * 降序排序 -1 大于 0 等于 1小于 */ @Override public int compareTo(FlowBean o) &#123; // 如果当前类的总和大于其他类的总和 则返回-1(大于) false 1(小于) return this.sumFlow &gt; o.getSumFlow() ? -1 : 1; &#125;&#125; main1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class FlowSummarySort &#123; /** * 因为只有key才能进行排序,所以输出key为FlowBean * * @author sylvanasp * @version 1.0 */ public static class FlowSummarySortMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] fields = StringUtils.split(line, "\t"); String phoneNum = fields[0]; Long upFlow = Long.parseLong(fields[1]); Long downFlow = Long.parseLong(fields[2]); FlowBean flowBean = new FlowBean(); flowBean.setAll(upFlow, downFlow); context.write(flowBean, new Text(phoneNum)); &#125; &#125; /** * 因为在mapper中已经完成了排序,所以reducer中需要将phoneNum重新设置为key * * @author sylvanasp * @version 1.0 */ public static class FlowSummarySortReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; &#123; @Override protected void reduce(FlowBean bean, Iterable&lt;Text&gt; phoneNum, Context context) throws IOException, InterruptedException &#123; // 因为每个bean都是完全独立的,所以Iterable中只有一个数据 for (Text phoneNumKey : phoneNum) &#123; context.write(phoneNumKey, bean); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(FlowSummarySort.class); job.setMapperClass(FlowSummarySortMapper.class); job.setReducerClass(FlowSummarySortReducer.class); job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); int result = job.waitForCompletion(true) ? 0 : 1; System.exit(result); &#125;&#125; 自定义Partitionpartitioner123456789101112131415161718192021222324public class MyPartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; // 使用map模拟数据库 private static HashMap&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;(); // 初始化分区规则 static &#123; map.put("136", 0); map.put("137", 1); map.put("138", 2); map.put("139", 3); &#125; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; // 获取手机号前3位 String phonePrefix = key.toString().substring(0, 3); // 根据手机号前缀获得对应的分区编号 Integer partitionId = map.get(phonePrefix); // 如果手机号不在分区规则内,则分配到分区4。 return partitionId == null ? 4 : partitionId; &#125;&#125; main123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class FlowSummaryPartition &#123; public static class FlowSummaryPartitionMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] fields = StringUtils.split(line, "\t"); String phoneNum = fields[1]; Long upFlow = Long.parseLong(fields[fields.length - 3]); Long downFlow = Long.parseLong(fields[fields.length - 2]); FlowBean flowBean = new FlowBean(); flowBean.setAll(upFlow, downFlow); context.write(new Text(phoneNum), flowBean); &#125; &#125; public static class FlowSummaryPartitionReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; beans, Context context) throws IOException, InterruptedException &#123; long upSum = 0; long downSum = 0; for (FlowBean bean : beans) &#123; upSum += bean.getUpFlow(); downSum += bean.getDownFlow(); &#125; FlowBean flowBean = new FlowBean(); flowBean.setAll(upSum, downSum); context.write(key, flowBean); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(FlowSummaryPartition.class); job.setMapperClass(FlowSummaryPartitionMapper.class); job.setReducerClass(FlowSummaryPartitionReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 设置分区器 job.setPartitionerClass(MyPartitioner.class); // 设置Reducer Task 实例数量 (与分区数一致) job.setNumReduceTasks(5); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); int result = job.waitForCompletion(true) ? 0 : 1; System.exit(result); &#125;&#125; END 部分资料来源于http://blog.sina.com.cn/s/blog_829a682d0101lc9d.html&amp;http://weixiaolu.iteye.com/blog/1474172]]></content>
      <categories>
        <category>后端</category>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记(1)-HDFS]]></title>
    <url>%2F2016%2F07%2F12%2F2016-07-12-Hadoop-HDFS%2F</url>
    <content type="text"><![CDATA[Hadoop概述Hadoop是一个由Apache基金会所开发的分布式系统基础架构。 &nbsp;&nbsp;Hadoop 由许多元素构成。其最底部是 Hadoop Distributed File System（HDFS），它存储 Hadoop 集群中所有存储节点上的文件。HDFS的上一层是MapReduce 引擎，该引擎由 JobTrackers 和 TaskTrackers 组成。 HDFS &nbsp;&nbsp;对外部客户机而言，HDFS就像一个传统的分级文件系统。可以创建、删除、移动或重命名文件，等等。但是 HDFS 的架构是基于一组特定的节点构建的，这是由它自身的特点决定的。这些节点包括 NameNode（仅一个），它在 HDFS 内部提供元数据服务；DataNode，它为 HDFS 提供存储块。由于仅存在一个 NameNode，因此这是 HDFS 的一个缺点（单点失败）。 &nbsp;&nbsp;存储在 HDFS 中的文件被分成块，然后将这些块复制到多个计算机中（DataNode）。这与传统的 RAID 架构大不相同。块的大小（通常为 64MB）和复制的块数量在创建文件时由客户机决定。NameNode 可以控制所有文件操作。HDFS 内部的所有通信都基于标准的 TCP/IP 协议。 &nbsp;&nbsp;HDFS和现有的分布式文件系统有很多共同点。但同时，它和其他的分布式文件系统的区别也是很明显的。HDFS是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。HDFS放宽了一部分POSIX约束，来实现流式读取文件系统数据的目的。HDFS在最开始是作为Apache Nutch搜索引擎项目的基础架构而开发的。HDFS是Apache Hadoop Core项目的一部分。 NameNode &nbsp;&nbsp;NameNode 是一个通常在HDFS实例中的单独机器上运行的软件。它负责管理文件系统名称空间和控制外部客户机的访问。NameNode 决定是否将文件映射到 DataNode 上的复制块上。对于最常见的 3 个复制块，第一个复制块存储在同一机架的不同节点上，最后一个复制块存储在不同机架的某个节点上。 &nbsp;&nbsp;实际的 I/O事务并没有经过 NameNode，只有表示 DataNode 和块的文件映射的元数据经过 NameNode。当外部客户机发送请求要求创建文件时，NameNode 会以块标识和该块的第一个副本的 DataNode IP 地址作为响应。这个 NameNode 还会通知其他将要接收该块的副本的 DataNode。 &nbsp;&nbsp;NameNode 在一个称为FsImage的文件中存储所有关于文件系统名称空间的信息。这个文件和一个包含所有事务的记录文件（这里是 EditLog）将存储在 NameNode 的本地文件系统上。FsImage 和 EditLog 文件也需要复制副本，以防文件损坏或 NameNode 系统丢失。 &nbsp;&nbsp;NameNode本身不可避免地具有SPOF（Single Point Of Failure）单点失效的风险，主备模式并不能解决这个问题，通过Hadoop Non-stop namenode才能实现100% uptime可用时间。 DataNode &nbsp;&nbsp;DataNode 也是一个通常在 HDFS实例中的单独机器上运行的软件。Hadoop 集群包含一个 NameNode 和大量 DataNode。DataNode通常以机架的形式组织，机架通过一个交换机将所有系统连接起来。Hadoop 的一个假设是：机架内部节点之间的传输速度快于机架间节点的传输速度。 &nbsp;&nbsp;DataNode 响应来自 HDFS 客户机的读写请求。它们还响应来自 NameNode 的创建、删除和复制块的命令。NameNode 依赖来自每个 DataNode 的定期心跳（heartbeat）消息。每条消息都包含一个块报告，NameNode 可以根据这个报告验证块映射和其他文件系统元数据。如果 DataNode 不能发送心跳消息，NameNode 将采取修复措施，重新复制在该节点上丢失的块。 HDFS体系结构 NameNode:唯一的master节点,管理HDFS的名称空间和数据块映射信息、配置副本策略和处理客户端请求。 Secondary NameNode:辅助NameNode，分担NameNode工作，定期合并fsimage和edits并推送给NameNode，紧急情况下可辅助恢复NameNode。 DataNode:Slave节点，实际存储数据、执行数据块的读写并汇报存储信息给NameNode。 FSImage:元数据镜像文件。 Edits:元数据的操作日志。 HDFSWriteOperation &nbsp;&nbsp;在分布式文件系统中，需要确保数据的一致性。对于HDFS来说，直到所有要保存数据的DataNodes确认它们都有文件的副本时，数据才被认为写入完成。因此，数据一致性是在写的阶段完成的。一个客户端无论选择从哪个DataNode读取，都将得到相同的数据。 客户端请求NameNode,表示写入文件。 NameNode响应客户端,并告诉客户端将文件保存到DataNodeA、B、D。 客户端连接DataNodeA写入文件,DataNode集群内完成复制。 DataNodeA将文件副本发送给DataNodeB。 DataNodeB将文件副本发送给DataNodeD。 DataNodeD返回确认消息给DataNodeB。 DataNodeB返回确认消息给DataNodeA。 DataNodeA返回确认消息给客户端,写入完成。 Client调用DistributedFileSystem的create()函数创建新文件。 DistributedFileSystem使用RPC调用NameNode创建一个没有block关联的新文件,NameNode在创建之前将进行校验,如果校验通过,NameNode则创建一个新文件并记录一条记录,否则抛出IO异常。 前两步成功后,将会返回一个DFSOutputStream对象,DFSOutputStream可以协调NameNode与DataNode,当客户端写入数据到DFSOutputStream,DFSOutputStream会将数据分割为一个一个Packet(数据包),并写入数据队列。 DataStreamer处理数据队列,它会先去询问NameNode存储到哪几个DataNode,例如Replication为3,则会去找到3个最适合的DataNode。DataStreamer会将DataNode排成一个Pipeline,它会将Packet按队列输出到管道中的第一个DataNode,第一个DataNode又会把Packet输出到第二个DataNode,直到最后一个DataNode。 DataStreamer中还有一个Ack Queue,Ack Queue之中也含有Packet。Ack Queue负责接收DataNode的确认响应,当Pipeline中的所有DataNode都确认完毕后,Ack Queue将移除对应的Packet。 Client完成数据写入,关闭流。 DataStreamer等待Ack Queue信息,当收到最后一个信息时,通知NameNode把文件标记为完成。 HDFSReadOperation 客户端请求NameNode,表示读取文件。 NameNode响应客户端,将block(数据块)的信息发送给客户端。 客户端检查数据块信息,连接相关的DataNode。 DataNodeA将block1发送给客户端。 DataNodeB将block2发送给客户端。 拼接数据,读取完成。 Client调用FileSystem的open()函数打开希望读取的文件。 DistributedFileSystem使用RPC调用NameNode确定文件起始块的位置，同一Block按照重复数会返回多个位置，这些位置按照Hadoop集群拓扑结构排序，距离客户端近的排在前面。 前两步成功后,将会返回一个DFSInputStream对象,DFSInputStream可以协调NameNode与DataNode。客户端对DFSInputStream输入流调用read()函数。 DFSInputStream连接距离最近的DataNode，通过对数据流反复调用read()函数，可以将数据从DataNode传输到客户端。 当到达Block的末端时，DFSInputStream会关闭与该DataNode的连接，然后寻找下一个Block的最佳DataNode，这些操作对客户端来说是透明的。 客户端完成读取，对FSDataInputStream调用close()关闭文件读取。 HDFSShell命令&nbsp;&nbsp;既然 HDFS 是存取数据的分布式文件系统，那么对 HDFS 的操作，就是文件系统的基本 操作，比如文件的创建、修改、删除、修改权限等，文件夹的创建、删除、重命名等。对 HDFS 的操作命令类似于 Linux 的 shell 对文件的操作，如 ls、mkdir、rm 等。 我们执行以下操作的时候，一定要确定 hadoop 是正常运行的，使用 jps 命令确保看到 各个 hadoop 进程。 命令名 格式 含义 -ls -ls&lt;路径&gt; 查看指定路径的当前目录结构 -lsr -lsr&lt;路径&gt; 递归查看指定路径的目录结构 -du -du&lt;路径&gt; 统计目录下个文件大小 -dus -dus&lt;路径&gt; 汇总统计目录下文件(夹)大小 -count -count[-q]&lt;路径&gt; 统计文件(夹)数量 -mv -mv&lt;源路径&gt;&lt;目的路径&gt; 移动 -cp -cp&lt;源路径&gt;&lt;目的路径&gt; 复制 -rm -rm[-skipTrash]&lt;路径&gt; 删除文件/空白文件夹 -rmr -rmr[-skipTrash]&lt;路径&gt; 递归删除 -put -put&lt;多个 linux 上的文件&gt; 上传文件 -copyFromLocal -copyFromLocal&lt;多个 linux 上的文件&gt; 从本地复制 -moveFromLocal -moveFromLocal&lt;多个 linux 上的文件&gt; 从本地移动 -getmerge -getmerge&lt;源路径&gt; 合并到本地 -cat -cat 查看文件内容 -text -text 查看文件内容 -copyToLocal -copyToLocal[-ignoreCrc][-crc][hdfs 源路 径][linux 目的路径] 复制到本地 -moveToLocal -moveToLocal[-crc] 移动到本地 -mkdir -mkdir 创建空白文件夹 -setrep -setrep[-R][-w]&lt;副本数&gt;&lt;路径&gt; 修改副本数量 -touchz -touchz&lt;文件路径&gt; 创建空白文件 -stat -stat[format]&lt;路径&gt; 显示文件统计信息 -tail -tail[-f]&lt;文件&gt; 查看文件尾部信息 -chmod -chmod[-R]&lt;权限模式&gt;[路径] 修改权限 -chown -chown[-R][属主][:[属组]] 路径 修改属主 -chgrp -chgrp[-R] 属组名称 路径 修改属组 -help -help[命令选项] 帮助 使用JAVA操作HDFS123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169public class HdfsTest &#123; private Configuration conf = null; private FileSystem fs = null; private FSDataInputStream DFSInputStream = null; /** * 初始化FlieSystem * * @throws IOException * @throws InterruptedException */ @Before public void init() throws IOException, InterruptedException &#123; conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://192.168.145.145:9000"); fs = fs.get(URI.create("hdfs://192.168.145.145:9000"), conf, "root"); &#125; /** * 读取文件 * * @throws IOException * @throws IllegalArgumentException */ @Test public void testReadAsOpen() throws IllegalArgumentException, IOException &#123; Path path = null; try &#123; path = new Path("/test"); if (fs.exists(path)) &#123; DFSInputStream = fs.open(path); IOUtils.copyBytes(DFSInputStream, System.out, conf); &#125; &#125; finally &#123; IOUtils.closeStream(DFSInputStream); fs.close(); &#125; &#125; /** * 上传本地文件 * * @throws IOException */ @Test public void testUpload() throws IOException &#123; Path src = null; Path dst = null; try &#123; src = new Path("f:/saber_by_wlop-d8tjwa5.jpg");// 原路径 dst = new Path("/saber.jpg");// 目标路径 // 参数1为是否删除原文件,true为删除,默认为false fs.copyFromLocalFile(false, src, dst); // 打印文件路径 System.out.println("Upload to " + conf.get("fs.default.name")); System.out.println("----------------------------------------"); FileStatus[] fileStatus = fs.listStatus(dst); for (FileStatus file : fileStatus) &#123; System.out.println(file.getPath()); &#125; &#125; finally &#123; fs.close(); &#125; &#125; /** * 下载文件 * * @throws IOException */ @Test public void testDownload() throws IOException &#123; Path src = null; Path dst = null; try &#123; if (fs.exists(src)) &#123; src = new Path("/saber.jpg"); dst = new Path("D:/temp/"); fs.copyToLocalFile(false, src, dst); // 打印文件路径 System.out.println("Download from " + conf.get("fs.default.name")); System.out.println("--------------------------------------------"); // 迭代路径,参数2为是否递归迭代 RemoteIterator&lt;LocatedFileStatus&gt; iterator = fs.listFiles(src, true); while (iterator.hasNext()) &#123; LocatedFileStatus fileStatus = iterator.next(); System.out.println(fileStatus.getPath()); &#125; &#125; &#125; finally &#123; fs.close(); &#125; &#125; /** * 创建目录 * * @throws IOException */ @Test public void testMkdir() throws IOException &#123; Path path = null; try &#123; path = new Path("/create01"); // 判断目录是否已存在 boolean exists = fs.exists(path); if (!exists) &#123; // 创建目录 boolean mkdirs = fs.mkdirs(path); if (mkdirs) &#123; System.out.println("create dir success!"); &#125; else &#123; System.out.println("create dir failure!"); &#125; &#125; &#125; finally &#123; fs.close(); &#125; &#125; /** * 重命名文件 * * @throws IOException */ @Test public void testRename() throws IOException &#123; Path oldPath = null; Path newPath = null; try &#123; oldPath = new Path("/saber.jpg"); newPath = new Path("/saber01.jpg"); if (fs.exists(oldPath)) &#123; boolean rename = fs.rename(oldPath, newPath); if (rename) &#123; System.out.println("rename success!"); &#125; else &#123; System.out.println("rename failure!"); &#125; &#125; &#125; finally &#123; fs.close(); &#125; &#125; /** * 删除文件 * * @throws IOException */ @Test public void testDelete() throws IOException &#123; Path path = null; try &#123; path = new Path("/saber01.jpg"); if (fs.exists(path)) &#123; boolean delete = fs.deleteOnExit(path); if (delete) &#123; System.out.println("delete success!"); &#125; else &#123; System.out.println("delete failure!"); &#125; &#125; &#125; finally &#123; fs.close(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>后端</category>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyCat快速入门]]></title>
    <url>%2F2016%2F07%2F09%2F2016-07-09-MyCat%2F</url>
    <content type="text"><![CDATA[MyCat概述 &nbsp;&nbsp;MyCat是基于Cobar二次开发的数据库中间件。它可以低成本的将现有的单机数据库和应用平滑迁移到“云”端，解决数据存储和业务规模迅速增长情况下的数据瓶颈问题。 &nbsp;&nbsp;从定义和分类来看，它是一个开源的分布式数据库系统，是一个实现了MySQL协议的的Server，前端用户可以把 它看作是一个数据库代理，用MySQL客户端工具和命令行访问，而其后端可以用MySQL原生（Native）协议与多个MySQL服务 器通信，也可以用JDBC协议与大多数主流数据库服务器通信，其核心功能是分表分库，即将一个大表水平分割为N个小表，存储 在后端MySQL服务器里或者其他数据库里。 &nbsp;&nbsp;Mycat发展到目前的版本，已经不是一个单纯的MySQL代理了，它的后端可以支持MySQL、SQL Server、Oracle、DB2、 PostgreSQL等主流数据库，也支持MongoDB这种新型NoSQL方式的存储，未来还会支持更多类型的存储。而在最终用户看 来，无论是那种存储方式，在Mycat里，都是一个传统的数据库表，支持标准的SQL语句进行数据的操作，这样一来，对前端业 务系统来说，可以大幅降低开发难度，提升开发速度，在测试阶段，可以将一个表定义为任何一种Mycat支持的存储方式，比如 MySQL的MyASIM表、内存表、或者MongoDB、LevelDB以及号称是世界上最快的内存数据库MemSQL上。试想一下，用户表 存放在MemSQL上，大量读频率远超过写频率的数据如订单的快照数据存放于InnoDB中，一些日志数据存放于MongoDB中， 而且还能把Oracle的表跟MySQL的表做关联查询，你是否有一种不能呼吸的感觉？而未来，还能通过Mycat自动将一些计算分析 后的数据灌入到Hadoop中，并能用Mycat+Storm/Spark Stream引擎做大规模数据分析，看到这里，你大概明白了，Mycat是 什么？Mycat就是BigSQL，Big Data On SQL Database。 MyCat特点 支持SQL92标准。 支持Mysql集群,可以作为Proxy使用。 支持JDBC连接ORACLE、DB2、SQL Server。 支持galera for mysql集群，percona-cluster或者mariadb cluster，提供高可用性数据分片集群。 支持自动故障切换,实现高可用。 支持读写分离,Mysql双主多从,以及一主多从模式。 支持全局表。 支持独有的基于E-R关系分片策略,实现了高效的表关联查询。 支持多平台,部署简单。 MyCat原理&nbsp;&nbsp;Mycat的原理中最重要的一个动词是“拦截”，它拦截了用户发送过来的SQL语句，首先对SQL语句做了一些特定的分析：如分片分析、路由分析、读写分离分析、缓存分析等，然后将此SQL发往后端的真实数据库，并将返回的结果做适当的处理，最终再返回给用户。 分片策略 MyCat支持横向分片与纵向分片。 横向分片:一个表的数据分割到多个节点上,按照行分隔。 纵向分片:一个数据库中有多个表A、B、C,A存储到节点1,B存储到节点2,C存储到节点3。 MyCat通过定义表的分片规则来实现分片,每个表可以捆绑一个分片规则,每个分片规则指定一个分片字段并绑定一个函数,实现动态分片算法。 schema:逻辑库,一个逻辑库中定义了所包含的Table。 table 逻辑表:既然有逻辑库，那么就会有逻辑表，分布式数据库中，对应用来说，读写数据的表就是逻辑表。逻辑表，可以是数据切分后，分布在一个或多个分片库中，也可以不做数据切分，不分片，只有一个表构成。 分片表:指那些原有的很大数据的表，需要切分到多个数据库的表，这样，每个分片都有一部分数据，所有分片构成了完整的 数据。 例如在mycat配置中的t_node就属于分片表，数据按照规则被分到dn1,dn2两个分片节点(dataNode)上。 1&lt;table name=&quot;t_node&quot; primaryKey=&quot;vid&quot; autoIncrement=&quot;true&quot; dataNode=&quot;dn1,dn2&quot; rule=&quot;rule1&quot; /&gt; **非分片表**:如果一个数据库中并不是所有的表都有很大的数据,某些表是可以不用进行切分的, 非分片表是相对于分片表来说的,就是不需要进行数据切分的表。 例如下面配置的t_node,只存在于一个分片节点dn1上。 1&lt;table name=&quot;t_node&quot; primaryKey=&quot;vid&quot; autoIncrement=&quot;true&quot; dataNode=&quot;dn1&quot; / **ER表**:关系型数据库是基于实体关系模型(Entity-RelationshipModel)之上的,通过其描述了真实世界中的事物与关系。MyCat提出了基于E-R关系的数据分片策略,子表的记录与所关联的父表记录存放在同一个数据分片中,即子表依赖于父表,通过表分组(TableGroup)保证数据Join不会跨库操作。 **全局表**:一个真实的业务系统中，往往存在大量的类似字典表的表，这些表基本上很少变动，字典表具有以下几个特性： - 变动不频繁。 - 数据量总体变化不大。 - 数据规模不大。 对于这类的表，在分片的情况下，当业务表因为规模而进行分片以后，业务表与这些附属的字典表之间的关联，就成了比较棘手的问题，所以Mycat中通过数据冗余来解决这类表的join，即所有的分片都有一份数据的拷贝，所有将字典表或者符合字典表特 性的一些表定义为全局表。 dataNode:分片节点。数据切分后,一个大表被切分到不同的分片数据库上面,每个表分片所在的数据库就是分片节点。 dataHost:节点主机。数据切分后，每个分片节点（dataNode）不一定都会独占一台机器，同一机器上面可以有多个分片数据库，这样一个或多个分片节点（dataNode）所在的机器就是节点主机（dataHost）,为了规避单节点主机并发数限制，尽量将读写压力高的分片节点 （dataNode）均衡的放在不同的节点主机（dataHost）. rule:分片规则。按照某种业务规则把数据分到某个分片的规则就是分片规则,数据切分选择合适的分片规则非常重要,将极大的避免后续数据处理的难度。 sequence:全局序列号。数据切分后，原有的关系数据库中的主键约束在分布式条件下将无法使用，因此需要引入外部机制保证数据唯一性标识，这种保证全局性的数据唯一标识的机制就是全局序列号（sequence）。 快速入门&amp;&nbsp;&nbsp;MyCat是是使用JAVA语言开发的,所以需要先安装JAVA运行环境,并且要求JDK版本在1.7以上。 1.环境准备JDK下载地址:http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html MySQL下载地址:http://dev.mysql.com/downloads/mysql/5.5.html#downloads MyCat下载地址:https://github.com/MyCATApache/Mycat-download 2.MyCat的安装 将下载的MyCat压缩包上传到linux服务器。 解压缩MyCat压缩包。 3.MyCat目录结构 bin程序目录 存放了window版本和linux版本，除了提供封装成服务的版本之外，也提供了nowrap的shell脚本命令，方便大 家选择和修改，进入到bin目录： Windows下运行：运行: mycat.bat console 在控制台启动程序，也可以装载成服务，若此程序运行有问题，也可以运行 startup_nowrap.bat，确保java命令可以在命令执行. Windows下将MyCAT做成系统服务：MyCAT提供warp方式的命令，可以将MyCAT安装成系统服务并可启动和停止。 进入bin目录下执行命令 mycat install 执行安装mycat服务. 输入 mycat start 启动mycat服务. conf目录存放配置文件，server.xml是Mycat服务器参数调整和用户授权的配置文件，schema.xml是逻辑库定义和表以及分片定义的配置文件，rule.xml是分片规则的配置文件，分片规则的具体一些参数信息单独存放为文件，也在这个目录下，配置文件修改，需要重启Mycat或者通过9066端口reload. lib目录主要存放mycat依赖的一些jar文件. 日志存放在logs/mycat.log中，每天一个文件，日志的配置是在conf/log4j.xml中，根据自己的需要，可以调整输出级别为 debug，debug级别下，会输出更多的信息，方便排查问题. 4.服务启动 MyCAT在Linux中部署启动时，首先需要在Linux系统的环境变量中配置MYCAT_HOME,操作方式如下： vi /etc/profile,在系统环境变量文件中增加 MYCAT_HOME=/usr/local/Mycat 执行 source /etc/profile 命令，使环境变量生效。 如果是在多台linux系统中组建的MyCat集群,则需要在MyCat Server所在的服务器上配置对其他Ip和主机名的映射。 vi /etc/hosts 例如192.168.145.1 test_1192.168.145.2 test_2 配置完毕后,可以cd到/usr/local/mycat/bin目录下执行 ./mycat start 启动服务。注:MyCat的默认服务端口为8066. MyCat切分数据1.配置schema.xmlSchema.xml作为MyCat中重要的配置文件之一，管理着MyCat的逻辑库、表、分片规则、DataNode以及DataSource。弄懂这些配置，是正确使用MyCat的前提。这里就一层层对该文件进行解析。 schema标签 属性名 值 数量限制 dataNode String 0..1 checkSQLschema Boolean 1 sqlMaxLimit Integer 1 dataNode该属性用于绑定逻辑库到某个具体的database上，如果定义了这个属性，那么这个逻辑库就不能工作在分库分表模式下了。也就是说对这个逻辑库的所有操作会直接作用到绑定的dataNode上，这个schema就可以用作读写分离和主从切换，具体如下配置: 123&lt;schema name=&quot;USERDB&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot; dataNode=&quot;dn1&quot;&gt; &lt;!—这里不能配置任何逻辑表信息--&gt; &lt;/schema&gt; 那么现在USERDB就绑定到dn1所配置的具体database上，可以直接访问这个database。当然该属性只能配置绑定到一个 database上，不能绑定多个dn。 checkSQLschema当该值设置为 true 时，如果我们执行语句select * from TESTDB.travelrecord;则MyCat会把语句修改为select * from travelrecord;。即把表示schema的字符去掉，避免发送到后端数据库执行时报（ERROR 1146 (42S02): Table ‘testdb.travelrecord’ doesn’t exist）。 不过，即使设置该值为 true ，如果语句所带的是并非是schema指定的名字，例如：select * from db1.travelrecord; 那么 MyCat并不会删除db1这个字段，如果没有定义该库的话则会报错，所以在提供SQL语句的最好是不带这个字段。 sqlMaxLimit当该值设置为某个数值时。每条执行的SQL语句，如果没有加上limit语句，MyCat也会自动的加上所对应的值。例如设置值为 100，执行select fromTESTDB.travelrecord;的效果为和执行select from TESTDB.travelrecord limit 100;相同。 不设置该值的话，MyCat默认会把查询到的信息全部都展示出来，造成过多的输出。所以，在正常使用中，还是建议加上一个 值，用于减少过多的数据返回。当然SQL语句中也显式的指定limit的大小，不受该属性的约束。需要注意的是，如果运行的schema为非拆分库的，那么该属性不会生效。需要手动添加limit语句。 &nbsp;&nbsp;schema 标签用于定义MyCat实例中的逻辑库，MyCat可以有多个逻辑库，每个逻辑库都有自己的相关配置。可以使用 schema 标 签来划分这些不同的逻辑库。 如果不配置 schema 标签，所有的表配置，会属于同一个默认的逻辑库。 注意：若是LINUX版本的MYSQL，则需要设置为Mysql大小写不敏感，否则可能会发生表找不到的问题。在MySQL的配置文件中my.ini [mysqld] 中增加一行 lower_case_table_names = 1 table标签 1&lt;table name=&quot;travelrecord&quot; dataNode=&quot;dn1,dn2,dn3&quot; rule=&quot;auto-sharding-long&quot; &gt;&lt;/table&gt; Table 标签定义了MyCat中的逻辑表，所有需要拆分的表都需要在这个标签中定义。 name属性 逻辑表的表名,同个schema标签中定义的名字必须唯一。 dataNode属性 定义这个逻辑表所属的dataNode,该属性的值需要和dataNode标签中name属性的值相互对应。如果需要定义的dn过多可以使 用如下的方法减少配置： 1234567&lt;table name=&quot;travelrecord&quot; dataNode=&quot;multipleDn$0-99,multipleDn2$100-199&quot; rule=&quot;auto-sharding-long&quot; &gt;&lt;/table&gt;&lt;dataNode name=&quot;multipleDn&quot; dataHost=&quot;localhost1&quot; database=&quot;db$0-99&quot; &gt;&lt;/dataNode&gt;&lt;dataNode name=&quot;multipleDn2&quot; dataHost=&quot;localhost1&quot; database=&quot; db$0-99&quot; &gt;&lt;/dataNode&gt;这里需要注意的是database属性所指定的真实database name需要在后面添加一个，例如上面的例子中，我需要在真实的mysql 上建立名称为dbs0到dbs99的database。 rule属性 该属性用于指定逻辑表要使用的规则名字，规则名字在rule.xml中定义，必须与tableRule标签中name属性属性值一一对应。 ruleRequired属性 该属性用于指定表是否绑定分片规则，如果配置为true，但没有配置具体rule的话 ，程序会报错。 primaryKey属性 该逻辑表对应真实表的主键，例如：分片的规则是使用非主键进行分片的，那么在使用主键查询的时候，就会发送查询语句到所有配置的DN上，如果使用该属性配置真实表的主键。那么MyCat会缓存主键与具体DN的信息，那么再次使用非主键进行查询的 时候就不会进行广播式的查询，就会直接发送语句给具体的DN，但是尽管配置该属性，如果缓存并没有命中的话，还是会发送语 句给具体的DN，来获得数据。 type属性 该属性定义了逻辑表的类型，目前逻辑表只有“全局表”和”普通表”两种类型。对应的配置： 全局表 global 普通表 不指定该值为global的所有表。 autoIncrement属性 mysql对非自增长主键，使用last_insert_id()是不会返回结果的，只会返回0。所以，只有定义了自增长主键的表才可以用 last_insert_id()返回主键值。 mycat目前提供了自增长主键功能，但是如果对应的mysql节点上数据表，没有定义auto_increment，那么在mycat层调用 last_insert_id()也是不会返回结果的。 由于insert操作的时候没有带入分片键，mycat会先取下这个表对应的全局序列，然后赋值给分片键。这样才能正常的插入到数据 库中，最后使用last_insert_id()才会返回插入的分片键值。 如果要使用这个功能最好配合使用数据库模式的全局序列。 使用autoIncrement=“true”指定这个表有使用自增长主键，这样mycat才会不抛出分片键找不到的异常。使用autoIncrement=“false”来禁用这个功能，当然你也可以直接删除掉这个属性。默认就是禁用的。 needAddLimit属性 指定表是否需要自动的在每个语句后面加上limit限制。由于使用了分库分表，数据量有时会特别巨大。这时候执行查询语句，如果恰巧又忘记了加上数量限制的话。那么查询所有的数据出来，也够等上一小会儿的。 所以，mycat就自动的为我们加上LIMIT 100。当然，如果语句中有limit，就不会在次添加了。 这个属性默认为true,你也可以设置成false`禁用掉默认行为。 childTable标签 childTable标签用于定义E-R分片的子表。通过标签上的属性与父表进行关联。 属性名 值 数量限制 描述 name String 1 定义子表的表名。 joinKey String 1 插入子表的时候会使用这个列的值查找父表存储的数据节点。 parentKey String 1 属性指定的值一般为与父表建立关联关系的列名。程序首先获取joinkey的值，再通过parentKey属性指定的列名产生查询语 句，通过执行该语句得到父表存储在哪个分片上。从而确定子表存储的位置。 primaryKey String 0..1 同table标签所描述的。 needAddLimit Boolean 0..1 同table标签所描述的。 dataNode标签 dataNode 标签定义了MyCat中的数据节点，也就是我们通常说所的数据分片。一个dataNode 标签就是一个独立的数据分 片。 属性名 值 数量限制 描述 name String 1 定义数据节点的名字，这个名字需要是唯一的，我们需要在table标签上应用这个名字，来建立表与分片对应的关系。 dataHost String 1 该属性用于定义该分片属于哪个数据库实例的，属性值是引用dataHost标签上定义的name属性。 database String 1 该属性用于定义该分片属性哪个具体数据库实例上的具体库，因为这里使用两个纬度来定义分片，就是：实例+具体的库。因为 每个库上建立的表和表结构是一样的。所以这样做就可以轻松的对表进行水平拆分。 dataHost标签 作为Schema.xml中最后的一个标签，该标签在mycat逻辑库中也是作为最底层的标签存在，直接定义了具体的数据库实例、读 写分离配置和心跳语句。 name属性 唯一标识dataHost标签，供上层的标签使用。 maxCon属性 指定每个读写实例连接池的最大连接。也就是说，标签内嵌套的writeHost、readHost标签都会使用这个属性的值来实例化出连接池的最大连接数。 minCon属性 指定每个读写实例连接池的最小连接，初始化连接池的大小。 balance属性 负载均衡类型，目前的取值有3种： balance=“0”, 所有读操作都发送到当前可用的writeHost上。 balance=“1”，所有读操作都随机的发送到readHost。 balance=“2”，所有读操作都随机的在writeHost、readhost上分发。 writeType属性 负载均衡类型，目前的取值有3种： writeType=“0”, 所有写操作都发送到可用的writeHost上。 writeType=“1”，所有写操作都随机的发送到readHost。 writeType=“2”，所有写操作都随机的在writeHost、readhost分上发。 dbType属性 指定后端连接的数据库类型，目前支持二进制的mysql协议，还有其他使用JDBC连接的数据库。例如：mongodb、oracle、 spark等。 dbDriver属性 指定连接后端数据库使用的Driver，目前可选的值有native和JDBC。使用native的话，因为这个值执行的是二进制的mysql协 议，所以可以使用mysql和maridb。其他类型的数据库则需要使用JDBC驱动来支持。如果使用JDBC的话需要将符合JDBC 4标准的驱动JAR包放到MYCAT\lib目录下，并检查驱动JAR包中包括如下目录结构的文 件：META-INF\services\java.sql.Driver。在这个文件内写上具体的Driver类名，例如：com.mysql.jdbc.Driver。 heartbeat标签 这个标签内指明用于和后端数据库进行心跳检查的语句。例如,MYSQL可以使用select user()，Oracle可以使用select 1 from dual等。这个标签还有一个connectionInitSql属性，主要是当使用Oracla数据库时，需要执行的初始化SQL语句就这个放到这里面来。例 如：alter session set nls_date_format=’yyyy-mm-dd hh24:miss’ writeHost标签、readHost标签 这两个标签都指定后端数据库的相关配置给mycat，用于实例化后端连接池。唯一不同的是，writeHost指定写实例、readHost 指定读实例，组着这些读写实例来满足系统的要求。 在一个dataHost内可以定义多个writeHost和readHost。但是，如果writeHost指定的后端数据库宕机，那么这个writeHost绑 定的所有readHost都将不可用。另一方面，由于这个writeHost宕机系统会自动的检测到，并切换到备用的writeHost上去。 这两个标签的属性相同，这里就一起介绍。 属性名 值 数量限制 描述 host String 1 用于标识不同实例，一般writeHost我们使用M1，readHost我们用S1。 url String 1 后端实例连接地址，如果是使用native的dbDriver，则一般为address:port这种形式。用JDBC或其他的dbDriver，则需要特殊 指定。当使用JDBC时则可以这么写：jdbc:mysql://localhost:3306/。 password String 1 后端存储实例需要的用户名字 user String 1 后端存储实例需要的密码 2.配置server.xmlserver.xml几乎保存了所有mycat需要的系统配置信息。最常用的是在此配置用户名、密码及权限。 例如:给TESTDB配置一个test用户。 12345&lt;user name=&quot;test&quot;&gt; &lt;property name=&quot;password&quot;&gt;test&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;TESTDB&lt;/property&gt; &lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt;&lt;/user&gt; 3.配置rule.xmlrule.xml里面就定义了我们对表进行拆分所涉及到的规则定义。我们可以灵活的对表使用不同的分片算法，或者对表使用相同的算法但具体的参数不同。这个文件里面主要有tableRule和function这两个标签。在具体使用过程中可以按照需求添加tableRule和function。 MyCat读写分离数据库读写分离对于大型系统或者访问量很高的互联网应用来说，是必不可少的一个重要功能。对于MySQL来说，标准的读写分离是主从模式，一个写节点Master后面跟着多个读节点，读节点的数量取决于系统的压力，通常是1-3个读节点的配置。 1.Mysql主从复制 Mysql主从配置需要注意的地方: 主DBServer和从DBServer需要版本一致。 主DBServer和从DBServer数据一致。 主DB server开启二进制日志,主DB server和从DB server的server_id都必须唯一。 2.Mysql主服务器配置修改/etc路径下的my.cnf文件,在[mysqld]段中添加: 123456binlog-do-db=db1binlog-ignore-db=mysql#启用二进制日志log-bin=mysql-bin#服务器唯一ID，一般取IP最后一段server-id=138 修改后,重启mysql服务service mysqld restart 创建一个账户并授权slave。 123mysql&gt;GRANT FILE ON *.* TO &apos;backup&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;mysql&gt;GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* to &apos;backup&apos;@&apos;%&apos; identified by &apos;123456&apos;; #一般不用root帐号，“%”表示所有客户端都可能连，只要帐号，密码正确，此处可用具体客户端IP代替，如192.168.145.226，加强安全。 之后刷新权限:FLUSH PRIVILEGES; 可以使用show master status;命令 查询主服务器状态。 3.Mysql从服务器配置修改/etc路径下的my.cnf文件,在[mysqld]段中添加一个serverid。 配置从服务器 12mysql&gt;change master to master_host=&apos;192.168.145.138&apos;,master_user=&apos;backup&apos;,master_password=&apos;123456&apos;, master_log_file=&apos;mysql-bin.000002&apos;,master_log_pos=679; 注意语句中间不要断开，master_port为mysql服务器端口号(无引号)，master_user为执行同步操作的数据库账户，“120”无单引号(此处的120就是show master status 中看到的position的值，这里的mysql-bin.000001就是file对应的值)。 之后启动从服务器复制功能 mysql&gt;start slave; 检查从服务器状态 show slave status; 注：Slave_IO及Slave_SQL进程必须正常运行，即YES状态，否则都是错误的状态(如：其中一个NO均属错误)。 如果出现此错误：Fatal error: The slave I/O thread stops because master and slave have equal MySQL server UUIDs; these UUIDs must be different for replication to work.因为是mysql是克隆的系统所以mysql的uuid是一样的，所以需要修改。 解决方法:删除/var/lib/mysql/auto.cnf文件，重新启动服务。 4.MyCat配置Mycat 1.4 支持MySQL主从复制状态绑定的读写分离机制，让读更加安全可靠，配置如下： 123456789101112&lt;dataNode name=&quot;dn1&quot; dataHost=&quot;localhost1&quot; database=&quot;db1&quot; /&gt; &lt;dataNode name=&quot;dn2&quot; dataHost=&quot;localhost1&quot; database=&quot;db2&quot; /&gt; &lt;dataNode name=&quot;dn3&quot; dataHost=&quot;localhost1&quot; database=&quot;db3&quot; /&gt; &lt;dataHost name=&quot;localhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;1&quot; writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;2&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;show slave status&lt;/heartbeat&gt; &lt;writeHost host=&quot;hostM&quot; url=&quot;192.168.25.138:3306&quot; user=&quot;root&quot; password=&quot;root&quot;&gt; &lt;readHost host=&quot;hostS&quot; url=&quot;192.168.25.166:3306&quot; user=&quot;root&quot; password=&quot;root&quot; /&gt; &lt;/writeHost&gt; &lt;/dataHost&gt; readHost是从属于writeHost的，即意味着它从那个writeHost获取同步数据，因此，当它所属的writeHost宕机了，则它也不会再参与到读写分离中来，即“不工作了”，这是因为此时，它的数据已经“不可靠”了。基于这个考虑，目前mycat 1.3和1.4版本中，若想支持MySQL一主一从的标准配置，并且在主节点宕机的情况下，从节点还能读取数据，则需要在Mycat里配置为两个writeHost并设置banlance=1。 设置 switchType=”2” 与slaveThreshold=”100” switchType 目前有三种选择： -1：表示不自动切换 1 ：默认值，自动切换 2 ：基于MySQL主从同步的状态决定是否切换 Mycat心跳检查语句配置为 show slave status ，dataHost 上定义两个新属性: switchType=”2” 与slaveThreshold=”100”，此时意味着开启MySQL主从复制状态绑定的读写分离与切换机制。 Mycat心跳机制通过检测 show slave status 中的 “Seconds_Behind_Master”, “Slave_IO_Running”, “Slave_SQL_Running” 三个字段来确定当前主从同步的状态以及Seconds_Behind_Master主从复制时延。]]></content>
      <categories>
        <category>后端</category>
        <category>Database</category>
        <category>MySql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>Database</tag>
        <tag>MyCat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ActiveMQ消息队列]]></title>
    <url>%2F2016%2F07%2F03%2F2016-07-03-activemq%2F</url>
    <content type="text"><![CDATA[介绍&nbsp;&nbsp;ActiveMQ 是Apache出品，最流行的，能力强劲的开源消息总线。ActiveMQ 是一个完全支持JMS1.1和J2EE 1.4规范的 JMS Provider实现,尽管JMS规范出台已经是很久的事情了,但是JMS在当今的J2EE应用中间仍然扮演着特殊的地位。 主要特点： 多种语言和协议编写客户端。语言: Java, C, C++, C#, Ruby, Perl, Python, PHP。应用协议: OpenWire,Stomp REST,WS Notification,XMPP,AMQP 完全支持JMS1.1和J2EE 1.4规范 (持久化,XA消息,事务) 对Spring的支持,ActiveMQ可以很容易内嵌到使用Spring的系统里面去,而且也支持Spring2.0的特性 通过了常见J2EE服务器(如 Geronimo,JBoss 4, GlassFish,WebLogic)的测试,其中通过JCA 1.5 resource adaptors的配置,可以让ActiveMQ可以自动的部署到任何兼容J2EE 1.4 商业服务器上 支持多种传送协议:in-VM,TCP,SSL,NIO,UDP,JGroups,JXTA 支持通过JDBC和journal提供高速的消息持久化 从设计上保证了高性能的集群,客户端-服务器,点对点 支持Ajax 支持与Axis的整合 可以很容易得调用内嵌JMS provider,进行测试 什么是JMS规范&nbsp;&nbsp;JMS的全称是Java MessageService，即Java消息服务。用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。&nbsp;&nbsp;它主要用于在生产者和消费者之间进行消息传递，生产者负责产生消息，而消费者负责接收消息。把它应用到实际的业务需求中的话我们可以在特定的时候利用生产者生成一消息，并进行发送，对应的消费者在接收到对应的消息后去完成对应的业务逻辑。&nbsp;&nbsp;对于消息的传递有两种类型：一种是点对点的，即一个生产者和一个消费者一一对应；另一种是发布/订阅模式，即一个生产者产生消息并进行发送后，可以由多个消费者进行接收。JMS定义了五种不同的消息正文格式，以及调用的消息类型，允许你发送并接收以一些不同形式的数据，提供现有消息格式的一些级别的兼容性。 StreamMessage – Java原始值的数据流 MapMessage–一套名称-值对 TextMessage–一个字符串对象 ObjectMessage–一个序列化的 Java对象 BytesMessage–一个字节的数据流 JMS应用程序接口&nbsp;&nbsp;ConnectionFactory &nbsp;&nbsp;&nbsp;&nbsp;用户用来创建到JMS提供者的连接的被管对象。JMS客户通过可移植的接口访问连接，这样当下层的实现改变时，代码不需要进行修改。 管理员在JNDI名字空间中配置连接工厂，这样，JMS客户才能够查找到它们。根据消息类型的不同，用户将使用队列连接工厂，或者主题连接工厂。 &nbsp;&nbsp;Connection &nbsp;&nbsp;&nbsp;&nbsp;连接代表了应用程序和消息服务器之间的通信链路。在获得了连接工厂后，就可以创建一个与JMS提供者的连接。根据不同的连接类型，连接允许用户创建会话，以发送和接收队列和主题到目标。 &nbsp;&nbsp;Destination &nbsp;&nbsp;&nbsp;&nbsp;目标是一个包装了消息目标标识符的被管对象，消息目标是指消息发布和接收的地点，或者是队列，或者是主题。JMS管理员创建这些对象，然后用户通过JNDI发现它们。和连接工厂一样，管理员可以创建两种类型的目标，点对点模型的队列，以及发布者／订阅者模型的主题。 &nbsp;&nbsp;MessageProducer &nbsp;&nbsp;&nbsp;&nbsp;由会话创建的对象，用于发送消息到目标。用户可以创建某个目标的发送者，也可以创建一个通用的发送者，在发送消息时指定目标。 &nbsp;&nbsp;MessageConsumer &nbsp;&nbsp;&nbsp;&nbsp;由会话创建的对象，用于接收发送到目标的消息。消费者可以同步地（阻塞模式），或异步（非阻塞）接收队列和主题类型的消息。 &nbsp;&nbsp;Message &nbsp;&nbsp;&nbsp;&nbsp;是在消费者和生产者之间传送的对象，也就是说从一个应用程序创送到另一个应用程序。一个消息有三个主要部分： 消息头（必须）：包含用于识别和为消息寻找路由的操作设置。 一组消息属性（可选）：包含额外的属性，支持其他提供者和用户的兼容。可以创建定制的字段和过滤器（消息选择器）。 一个消息体（可选）：允许用户创建五种类型的消息（文本消息，映射消息，字节消息，流消息和对象消息）。 消息接口非常灵活，并提供了许多方式来定制消息的内容。 &nbsp;&nbsp;Session &nbsp;&nbsp;&nbsp;&nbsp;表示一个单线程的上下文，用于发送和接收消息。由于会话是单线程的，所以消息是连续的，就是说消息是按照发送的顺序一个一个接收的。会话的好处是它支持事务。如果用户选择了事务支持，会话上下文将保存一组消息，直到事务被提交才发送这些消息。在提交事务之前，用户可以使用回滚操作取消这些消息。一个会话允许用户创建消息生产者来发送消息，创建消息消费者来接收消息。 JMS消息发送模式 &nbsp;&nbsp;在P2P模型下，一个生产者向一个特定的队列发布消息，一个消费者从该队列中读取消息。这里，生产者知道消费者的队列，并直接将消息发送到消费者的队列。这种模式被概括为：只有一个消费者将获得消息。生产者不需要在接收者消费该消息期间处于运行状态，接收者也同样不需要在消息发送时处于运行状态。每一个成功处理的消息都由接收者签收。 &nbsp;&nbsp;publish/subscribe模型支持向一个特定的消息主题发布消息。0或多个订阅者可能对接收来自特定消息主题的消息感兴趣。在这种模型下，发布者和订阅者彼此不知道对方。这种模式好比是匿名公告板。这种模式被概括为：多个消费者可以获得消息.在发布者和订阅者之间存在时间依赖性。发布者需要建立一个订阅（subscription），以便客户能够购订阅。订阅者必须保持持续的活动状态以接收消息，除非订阅者建立了持久的订阅。在那种情况下，在订阅者未连接时发布的消息将在订阅者重新连接时重新发布。 安装ActiveMQ 首先到官网 http://activemq.apache.org/ 下载ActiveMQ. 因为ActiveMQ是JAVA开发的,所以依赖jdk环境。 解压ActiveMQ。 在ActiveMQ/bin目录中,./activemq start 开启ActiveMQ 在ActiveMQ/bin目录中,./activemq stop 关闭ActiveMQ 访问后台 http://ip:8161/admin ActiveMQ的默认后台端口为8161,Message端口为61616 使用ActiveMQ&nbsp;&nbsp;使用ActiveMQ需要先引入ActiveMQ的jar包。 12345 &lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-all&lt;/artifactId&gt; &lt;version&gt;5.11.2&lt;/version&gt;&lt;/dependency&gt; 以下示例使用Queue模式,如要使用Topic模式只需要将Destination改成Topic即可。 1.Producer12345678910111213141516171819202122232425262728293031323334353637383940414243444546 @Testpublic void testProducer() throws JMSException&#123; // 创建连接工厂 ConnectionFactory connectionFactory = new ActiveMQConnectionFactory("tcp://192.168.145.137:61616"); // 声明Connection Connection connection = null; // 声明Session Session session = null; // 声明Producer MessageProducer producer = null; try&#123; // 从连接工厂中获得连接 connection = connectionFactory.createConnection(); // 开启连接 connection.start(); /* * 从连接中获得会话 * 参数1:transacted boolean型 * 当设置为true时,将忽略参数2,acknowledgment mode被jms服务器设置 SESSION_TRANSACTED。 * 当一个事务被提交时,消息确认就会自动发生。 * 当设置为false时,需要设置参数2 * Session.AUTO_ACKNOWLEDGE为自动确认，当客户成功的从receive方法返回的时候，或者从 * MessageListener.onMessage方法成功返回的时候，会话自动确认客户收到的消息。 * Session.CLIENT_ACKNOWLEDGE 为客户端确认。客户端接收到消息后，必须调用javax.jms.Message的 * acknowledge方法。jms服务器才会删除消息。（默认是批量确认） */ session = connection.createSession(false,Session.AUTO_ACKNOWLEDGE); // 创建一个Destination目的地 Queue或者Topic Queue queue = session.createQueue("testMessage"); // 创建一个Producer生产者 producer = session.createProducer(queue); // 创建message ActiveMQTextMessage textMessage = new ActiveMQTextMessage(); textMessage.setText("test"); // 发送message producer.send(textMessage); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; // 回收资源 producer.close(); session.close(); connection.close(); &#125;&#125; 2.Consumer&nbsp;&nbsp;消费者有两种消费方式: 同步消费。通过调用消费者的receive方法从目的地中显式提取消息。receive方法可以一直阻塞到消息到达。 异步消费。客户可以为消费者注册一个消息监听器，以定义在消息到达时所采取的动作。 实现MessageListener接口，在MessageListener（）方法中实现消息的处理逻辑。 同步消费 123456789101112131415161718192021222324252627282930313233343536373839@Testpublic void testSyncConsumer() throws JMSException&#123; // 创建连接工厂 ConnectionFactory connectionFactory = new ActiveMQConnectionFactory("tcp://192.168.145.137:61616"); Connection connection = null; Session session = null; MessageConsumer consumer = null; try&#123; // 获得连接 connection = connectionFactory.createConnection(); // 开启连接 connection.start(); // 获得Session session = connection.createSession(false,Session.AUTO_ACKNOWLEDGE); // 创建一个目的地 Queue queue = session.createQueue("testSynchronization"); // 创建消费者 consumer = session.createConsumer(queue); // 使用receive同步消费 while(true)&#123; // 设置接收信息的时间,单位为毫秒 Message message = consumer.receive(10000); if(message != null)&#123; System.out.println(message); &#125;else&#123; // 超时,结束循环 break; &#125; &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; // 回收资源 consumer.close(); session.close(); connection.close(); &#125;&#125; 异步消费 1234567891011121314151617181920212223242526272829303132333435363738394041424344 @Testpublic void testAsyncConsumer() throws JMSException&#123; // 创建连接工厂 ConnectionFactory connectionFactory = new ActiveMQConnectionFactory("tcp://192.168.145.137:61616"); Connection connection = null; Session session = null; MessageConsumer consumer = null; try&#123; // 获得连接 connection = connectionFactory.createConnection(); // 开启连接 connection.start(); // 获得Session session = connection.createSession(false,Session.AUTO_ACKNOWLEDGE); // 设置目的地 Queue queue = session.createQueue("testAsynchronization"); // 创建Consumer consumer = session.createConsumer(queue); // 异步消费 session.setMessageListener(new MessageListener() &#123; @Override public void onMessage(Message message) &#123; if(message instanceof TextMessage)&#123; try &#123; String text = ((TextMessage) message).getText(); System.out.println(text); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;); System.in.read(); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; // 回收资源 consumer.close(); session.close(); connection.close(); &#125;&#125; 整合Spring1.配置ConnectionFactory123456789101112131415161718192021222324&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:context="http://www.springframework.org/schema/context" xmlns:p="http://www.springframework.org/schema/p" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:jms="http://www.springframework.org/schema/jms" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd http://www.springframework.org/schema/jms http://www.springframework.org/schema/jms/spring-jms-4.0.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.0.xsd"&gt; &lt;!-- ActiveMQ提供的ConnectionFactory --&gt; &lt;bean id="targetConnectionFactory" class="org.apache.activemq.ActiveMQConnectionFactory"&gt; &lt;property name="brokerURL" value="tcp://192.168.145.137:61616" /&gt; &lt;/bean&gt; &lt;!-- Spring的ConnectionFactory需要注入ActiveMQ的ConnectionFactory --&gt; &lt;bean id="connectionFactory" class="org.springframework.jms.connection.SingleConnectionFactory"&gt; &lt;!-- 目标ConnectionFactory对应真实的可以产生JMS Connection的ConnectionFactory --&gt; &lt;property name="targetConnectionFactory" ref="targetConnectionFactory" /&gt; &lt;/bean&gt;&lt;/beans&gt; 2.配置生产者12345678910111213141516 &lt;!-- 配置生产者 --&gt;&lt;!-- Spring提供的JMS工具类，它可以进行消息发送、接收等 --&gt;&lt;bean id="jmsTemplate" class="org.springframework.jms.core.JmsTemplate"&gt; &lt;!-- 注入Spring的连接工厂 --&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt;&lt;/bean&gt;&lt;!--P2P模式的Destination --&gt;&lt;bean id="queueDestination" class="org.apache.activemq.command.ActiveMQQueue"&gt; &lt;constructor-arg&gt; &lt;value&gt;queue&lt;/value&gt; &lt;/constructor-arg&gt;&lt;/bean&gt;&lt;!-- publish/subscribe模式的Destination --&gt;&lt;bean id="topicDestination" class="org.apache.activemq.command.ActiveMQTopic"&gt; &lt;constructor-arg value="topic" /&gt;&lt;/bean&gt; 3.发送消息1234567891011121314151617public void testSend()&#123; // 读取Spring配置文件 ApplicationContext applicationContext = new ClassPathXmlApplicationContext("applicationContext.xml"); // 获得JmsTemplate JmsTemplate jmsTemplate = applicationContext.getBean(JmsTemplate.class); // 获得Destination ActiveMQQueue queue = applicationContext.getBean(ActiveMQQueue.class); // 发送消息 jmsTemplate.send(queue, new MessageCreator() &#123; @Override public Message createMessage(Session session) throws JMSException &#123; return session.createTextMessage("send-spring"); &#125; &#125;); &#125; 4.配置消费者&nbsp;&nbsp;Spring通过MessageListenerContainer接收信息,并把接收到的信息分发给MessageListener进行处理。每个消费者对应每个目的地都需要有对应的MessageListenerContainer。 1234567891011121314151617181920212223242526272829303132333435&lt;!-- ActiveMQ提供的ConnectionFactory --&gt;&lt;bean id="targetConnectionFactory" class="org.apache.activemq.ActiveMQConnectionFactory"&gt; &lt;property name="brokerURL" value="tcp://192.168.145.137:61616" /&gt;&lt;/bean&gt;&lt;!-- Spring的ConnectionFactory需要注入ActiveMQ的ConnectionFactory --&gt;&lt;bean id="connectionFactory" class="org.springframework.jms.connection.SingleConnectionFactory"&gt; &lt;!-- 目标ConnectionFactory对应真实的可以产生JMS Connection的ConnectionFactory --&gt; &lt;property name="targetConnectionFactory" ref="targetConnectionFactory" /&gt;&lt;/bean&gt; &lt;!-- 配置生产者 --&gt;&lt;!-- Spring提供的JMS工具类，它可以进行消息发送、接收等 --&gt;&lt;bean id="jmsTemplate" class="org.springframework.jms.core.JmsTemplate"&gt; &lt;!-- 注入Spring的连接工厂 --&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt;&lt;/bean&gt;&lt;!--P2P模式的Destination --&gt;&lt;bean id="queueDestination" class="org.apache.activemq.command.ActiveMQQueue"&gt; &lt;constructor-arg&gt; &lt;value&gt;queue&lt;/value&gt; &lt;/constructor-arg&gt;&lt;/bean&gt;&lt;!-- publish/subscribe模式的Destination --&gt;&lt;bean id="topicDestination" class="org.apache.activemq.command.ActiveMQTopic"&gt; &lt;constructor-arg value="topic" /&gt;&lt;/bean&gt;&lt;!-- 配置监听器 --&gt;&lt;bean id="myMessageListener" class="com.activemq.MyMessageListener" /&gt;&lt;!-- 消息监听容器 --&gt;&lt;bean id="jmsContainer" class="org.springframework.jms.listener.DefaultMessageListenerContainer"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="destination" ref="queueDestination" /&gt; &lt;property name="messageListener" ref="myMessageListener" /&gt;&lt;/bean&gt; 监听器需要实现MessageListener接口。 123456public class MyMessageListener implements MessageListener &#123; @Override public void onMessage(Message message) &#123; System.out.println(message); &#125;&#125; Exception&nbsp;&nbsp;启动ActiveMQ时,如果发生java.net.UnknownHostException异常。解决方法:修改 /etc/hosts 文件 添加一行 192.168.1.1(主机IP) 主机名.localdomain 主机名例: 192.168.145.137 ActiveMQ.localdomain ActiveMQ]]></content>
      <categories>
        <category>后端</category>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>ActiveMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SolrCloud初体验]]></title>
    <url>%2F2016%2F06%2F30%2F2016-06-30-solrcloud%2F</url>
    <content type="text"><![CDATA[什么是SolrCloudSolrCloud是基于Solr和Zookeeper的分布式搜索方案,它的主要思想是使用Zookeeper作为集群的配置信息中心。 SolrCloud的特点1.近实时搜索立即推送式的replication（也支持慢推送）。可以在秒内检索到新加入索引。 2.自动容错SolrCloud对索引分片,并对每个分片创建多个Replication。每个Replication都可以对外提供服务。一个Replication挂掉不会影响索引服务。更强大的是，它还能自动的在其它机器上帮你把失败机器上的索引Replication重建并投入使用。 3.查询时自动负载均衡SolrCloud索引的多个Replication可以分布在多台机器上,均衡查询压力。如果查询压力大，可以通过扩展机器，增加Replication来减缓。 4.集中式的配置信息SolrCloud可以将配置文件上传到Zookeeper,由Zookeeper对配置文件进行管理。 结构分析为了减少处理压力,SolrCloud需要由多台服务器共同完成索引和搜索。 1.实现思路SolrCloud将索引数据进行分片(Shard),每个分片由多台服务器共同完成。 2.结构 物理结构SolrCloud由三个Solr服务器组成,每个Solr服务器包含2个Core。 逻辑结构一个Collection包含2个Shard,每个Shard由3个core组成(一个Leader,两个Replication)。 CollectionCollection是一个在逻辑意义上完整的索引结构,它常常被划分为一个或多个Shard分片,它们使用相同的配置信息。如果Shard数超过一个，它就是分布式索引，SolrCloud让你通过Collection名称引用它，而不需要关心分布式检索时需要使用的和Shard相关参数。 Core一个Solr中包含一个或者多个Solr Core，每个Solr Core可以独立提供索引和查询功能，每个Solr Core对应一个索引或者Collection的Shard，Solr Core的提出是为了增加管理灵活性和共用资源。在SolrCloud中有个不同点是它使用的配置是在Zookeeper中的，传统的Solr core的配置文件是在磁盘上的配置目录中。 ShardCollection的逻辑分片。每个Shard被化成一个或者多个replicas，通过选举确定哪个是Leader。 Replication在master-slave结构中,Replication是一个从节点,同一个Shard下主从节点存储的数据是一致的。 Leader在master-slave结构中,Leader是一个主节点,Leader是赢得选举的Replication。选举可以发生在任何时间，但是通常它们仅在某个Solr实例发生故障时才会触发。当索引documents时，SolrCloud会传递它们到此Shard对应的Leader，Leader再分发它们到全部Shard的Replication。 SolrCloud的搭建1.ZookeeperSolrCloud需要Zookeeper进行管理,所以需要先安装Zookeeper。 .解压缩zookeeper.tar.gz,并复制出3个Zookeeper实例。 .进入zookeeper01目录,创建一个data文件夹,并在data中创建一个myid文件,内容为1(其他Zookeeper实例为2和3)。 进入conf文件夹,将zoo_sample.cfg改名为zoo.cfg vim zoo.cfg 修改dataDir=data文件夹所在的目录,添加:server.myid的值=ip:每个Zookeeper服务器之间的通讯端口:Zookeeper与其他应用的通讯端口。(每个Zookeeper实例都需要添加这行内容)例如: 2.Solr实例 安装一个单机的Solr实例,并复制成4份,分别对应4个SolrHome。 修改SolrHome的solr.xml文件 将配置文件上传到Zookeeper,当配置文件发生改变时,需要重新上传。 java -classpath .:/usr/local/solr-cloud/solr-lib/* org.apache.solr.cloud.ZkCLI -cmd upconfig -zkhost 127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183 -confdir /usr/local/solr-cloud/solrhome01/collection1/conf -confname solr-conf -cmd upconfig 上传配置文件命令 -zkhost Zookeeper集群的ip与端口 -confdir 配置文件的目录 -confname 上传到Zookeeper后的文件夹名称 其中参数/usr/local/solr-cloud/solr-lib/可以自己创建，内容如下： 复制tomcat/webapps/solr/WEB-INF/lib下所有jar包 复制example/lib/ext下所有jar包 复制example/resources/log4j.properties 通知Solr实例Zookeeper的地址,需要修改tomcat/bin/catalina.sh添加一行:JAVA_OPTS=”-DzkHost=Zookeeper集群的地址列表” 3.设置Shard1http://web容器/solr/admin/collections?action=CREATE&amp;name=Collection名称&amp;numShards=Shard个数&amp;replicationFactor=Replication个数 例: 12http://192.168.145.150:8080/solr/admin/collections?action=CREATE&amp;name=collection2&amp;numShards=2&amp;replicationFactor=2上面的命令为 创建一个name为collection2的Collection,并分成了2个Shard,每个Shard有2个Replication 删除一个Collection 12http://192.168.145.150:8080/solr/admin/collections?action=DELETE&amp;name=collection1上面的命令为 删除一个name为collection1的Collection Spring整合SolrCloud1234567&lt;!-- SolrCloud --&gt; &lt;bean id=&quot;cloudSolrServer&quot; class=&quot;org.apache.solr.client.solrj.impl.CloudSolrServer&quot;&gt; &lt;constructor-arg name=&quot;zkHost&quot; value=&quot;192.168.145.136:2181,192.168.145.136:2182,192.168.145.136:2183&quot; /&gt; &lt;!-- 设置默认搜索的Collection --&gt; &lt;property name=&quot;defaultCollection&quot; value=&quot;collection2&quot; /&gt; &lt;/bean&gt;]]></content>
      <categories>
        <category>后端</category>
        <category>全文检索</category>
        <category>Solr</category>
      </categories>
      <tags>
        <tag>全文检索引擎</tag>
        <tag>Solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何搭建与维护一个Redis集群]]></title>
    <url>%2F2016%2F06%2F27%2F2016-06-27-redis-cluster01%2F</url>
    <content type="text"><![CDATA[Redis集群架构 每一个Redis节点使用PING-PONG的形式互相通信。 当半数以上的节点fail时,则整个集群失效。 客户端不需要连接集群所有节点,只要连接集群中任意一个节点即可。 Redis集群中内置了16384个哈希槽,当操作数据时,Redis会对key使用crc16算法算出一个结果,并把结果对16384取余,每个key都会对应0-16383之间的哈希槽,Redis会根据节点数量平均的将哈希槽映射到不同的节点上。 Redis投票机制 Redis集群中每一个节点都会参与投票,如果当半数以上的节点认为一个节点通信超时,则该节点fail。 当集群中任意节点的master(主机)挂掉,且这个节点没有slave(从机),则整个集群进入fail状态。 搭建Redis集群1.安装Ruby环境因为redis-trib.rb脚本依赖ruby环境,所以需要先安装ruby。 12yum install ruby yum install rubygems 安装ruby和redis的接口程序 1gem install /usr/local/redis-3.0.0.gem 2.搭建Redis集群一个Redis集群最少需要3组主从机,即6个Redis。 修改redis.conf配置文件,将集群开关开启。 启动Redis实例 123456789101112131415161718cd redis01./redis-server redis.confcd ..cd redis02./redis-server redis.confcd ..cd redis03./redis-server redis.confcd ..cd redis04./redis-server redis.confcd ..cd redis05./redis-server redis.confcd ..cd redis06./redis-server redis.confcd .. 使用redis-trib.rb创建集群 1./redis-trib.rb create --replicas 1 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 127.0.0.1:7006 3.添加主节点如果想添加一个port为7007的Redis实例,可以使用以下命令。 1./redis-trib.rb add-node 127.0.0.1:7007 127.0.0.1:7001 当添加了一个主节点后,需要重新分配哈希槽。 1./redis-trib.rb reshard 127.0.0.1:7001 4.添加从节点添加一个port为7008的Redis实例做为7007的从节点。 1./redis-trib.rb add-node --slave --master-id cad9f7413ec6842c971dbcc2c48b4ca959eb5db4 127.0.0.1:7008 127.0.0.1:7001 命令格式: ./redis-trib.rb add-node –slave –master-id 主节点id 新加从节点的ip和端口 集群中节点的ip和端口(任意一个节点)。 主节点id可以在client中使用 cluster nodes 命令查询。 注意: 如果原来该结点在集群中的配置信息已经生成到cluster-config-file指定的配置文件中（如果cluster-config-file没有指定则默认为nodes.conf),则会报错: 1[ERR] Node XXXXXX is not empty. Either the node already knows other nodes (check with CLUSTER NODES) or contains some key in database 0 解决方法: 删除生成的配置文件 nodes.conf,再执行./redis-trib.rb add-node命令。 5.删除节点./redis-trib.rb del-node 要删除的节点的ip和端口 节点id注意: 如果这个节点已经占有哈希槽,则无法删除,需要先将哈希槽分配出去。 Jedis连接集群使用Jedis连接集群需要先创建JedisCluster对象。代码如下: 123456789101112131415161718public void test01()&#123; Set&lt;HostAndPort&gt; nodes = new HashSet&lt;HostAndPort&gt;(); nodes.add(new HostAndPort("192.168.145.134", 7001)); nodes.add(new HostAndPort("192.168.145.134", 7002)); nodes.add(new HostAndPort("192.168.145.134", 7003)); nodes.add(new HostAndPort("192.168.145.134", 7004)); nodes.add(new HostAndPort("192.168.145.134", 7005)); nodes.add(new HostAndPort("192.168.145.134", 7006)); // 创建JedisCluster JedisCluster jedisCluster = new JedisCluster(nodes); // 操作redis String set = jedisCluster.set("hello", "helloWorld"); String hello = jedisCluster.get("hello"); System.out.println(set); System.out.println(hello); // 关闭JedisCluster jedisCluster.close();&#125; 在Spring容器中维护: 123456789101112131415161718192021222324252627282930 &lt;bean class=&quot;redis.clients.jedis.JedisCluster&quot;&gt; &lt;constructor-arg name=&quot;nodes&quot;&gt; &lt;set&gt; &lt;bean class=&quot;redis.clients.jedis.HostAndPort&quot;&gt; &lt;constructor-arg name=&quot;host&quot; value=&quot;192.168.145.134&quot;/&gt; &lt;constructor-arg name=&quot;port&quot; value=&quot;7001&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;redis.clients.jedis.HostAndPort&quot;&gt; &lt;constructor-arg name=&quot;host&quot; value=&quot;192.168.145.134&quot;/&gt; &lt;constructor-arg name=&quot;port&quot; value=&quot;7002&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;redis.clients.jedis.HostAndPort&quot;&gt; &lt;constructor-arg name=&quot;host&quot; value=&quot;192.168.145.134&quot;/&gt; &lt;constructor-arg name=&quot;port&quot; value=&quot;7003&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;redis.clients.jedis.HostAndPort&quot;&gt; &lt;constructor-arg name=&quot;host&quot; value=&quot;192.168.145.134&quot;/&gt; &lt;constructor-arg name=&quot;port&quot; value=&quot;7004&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;redis.clients.jedis.HostAndPort&quot;&gt; &lt;constructor-arg name=&quot;host&quot; value=&quot;192.168.145.134&quot;/&gt; &lt;constructor-arg name=&quot;port&quot; value=&quot;7005&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;redis.clients.jedis.HostAndPort&quot;&gt; &lt;constructor-arg name=&quot;host&quot; value=&quot;192.168.145.134&quot;/&gt; &lt;constructor-arg name=&quot;port&quot; value=&quot;7006&quot;/&gt; &lt;/bean&gt; &lt;/set&gt; &lt;/constructor-arg&gt;&lt;/bean&gt;]]></content>
      <categories>
        <category>后端</category>
        <category>Database</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何安装与搭建一个Nginx服务器]]></title>
    <url>%2F2016%2F06%2F24%2F2016-06-24-nginx-initiation%2F</url>
    <content type="text"><![CDATA[Nginx介绍Nginx是一款轻量级的Web 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器，并在一个BSD-like 协议下发行。由俄罗斯的程序设计师Igor Sysoev所开发，供俄国大型的入口网站及搜索引擎Rambler使用。 Nginx应用场景 作为http服务器使用并独立提供http服务。 虚拟主机。 用于反向代理与负载均衡,当并发量非常大时,可以使用Nginx对服务器集群进行反向代理与负载均衡,提高吞吐量。 反向代理 反向代理是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。 Nginx安装流程 Nginx依赖于以下4个库 gcc 安装nginx需要先将官网下载的源码进行编译，编译依赖gcc环境 **yum install gcc-c++** pcre PCRE(Perl Compatible Regular Expressions)是一个Perl库，包括 perl 兼容的正则表达式库。nginx的http模块使用pcre来解析正则表达式，所以需要在linux上安装pcre库。 yum install -y pcre pcre-devel zlib zlib库提供了很多种压缩和解压缩的方式，nginx使用zlib对http包的内容进行gzip，所以需要在linux上安装zlib库。 yum install -y zlib zlib-devel openssl OpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及SSL协议，并提供丰富的应用程序供测试或其它目的使用。nginx不仅支持http协议，还支持https（即在ssl协议上传输http），所以需要在linux安装openssl库。 yum install -y openssl openssl-devel 一键安装依赖包 yum -y install zlib zlib-devel openssl openssl–devel pcre pcre-devel 解压Nginx源码包 使用configure创建makefile 123456789101112./configure \--prefix=/usr/local/nginx \--pid-path=/var/run/nginx/nginx.pid \--lock-path=/var/lock/nginx.lock \--error-log-path=/var/log/nginx/error.log \--http-log-path=/var/log/nginx/access.log \--with-http_gzip_static_module \--http-client-body-temp-path=/var/temp/nginx/client \--http-proxy-temp-path=/var/temp/nginx/proxy \--http-fastcgi-temp-path=/var/temp/nginx/fastcgi \--http-uwsgi-temp-path=/var/temp/nginx/uwsgi \--http-scgi-temp-path=/var/temp/nginx/scgi make &amp; make install cd /usr/local/nginx/sbin 目录 ./nginx 开启Nginx ./nginx -s stop 关闭Nginx ./nginx -s reload 重新加载Nginx配置文件 Nginx配置文件 nginx.conf是Nginx的主要配置文件,它的主要结构如下 worker_process表示工作进程的数量，一般设置为cpu的核数 worker_connections表示每个工作进程的最大连接数 server{} 定义了一个虚拟机,如果要添加一个虚拟机,则添加一个server{}即可。 listen监听的端口 server_name监听的域名 location{}配置匹配的URI root 指定URI查找的资源路径,为相对路径。 index 指定首页的名称,可以配置多个。 proxy_pass 指定反向代理转发的路径。 Nginx配置负载均衡 对upstream test{}中的2个web服务器配置了负载均衡,weight设置权值,权值越高的服务器承载的压力就越大。]]></content>
      <categories>
        <category>后端</category>
        <category>负载均衡</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Lucene实现全文检索]]></title>
    <url>%2F2016%2F06%2F19%2F2016-06-19-lucene%2F</url>
    <content type="text"><![CDATA[1. Lucene介绍 Lucene是apache下的一个开源的全文检索引擎工具包,但它不是一个完整的全文检索引擎,而是一个全文检索引擎的架构,提供了完整的查询引擎和索引引擎,部分文本分析引擎（英文与德文两种西方语言）。它为软件开发人员提供一个简单易用的工具包（类库）,以方便的在目标系统中实现全文检索的功能。 全文检索 全文检索首先将要查询的目标文档中的词提取出来，组成索引，通过查询索引达到搜索目标文档的目的。这种先建立索引，再对索引进行搜索的过程就叫全文检索（Full-text Search）。 Lucene的优点 索引文件格式独立于应用平台。Lucene定义了一套以8位字节为基础的索引文件格式，使得兼容系统或者不同平台的应用能够共享建立的索引文件。 在传统全文检索引擎的倒排索引的基础上，实现了分块索引，能够针对新的文件建立小文件索引，提升索引速度。然后通过与原有索引的合并，达到优化的目的。 优秀的面向对象的系统架构，使得对于Lucene扩展的学习难度降低，方便扩充新功能。 设计了独立于语言和文件格式的文本分析接口，索引器通过接受Token流完成索引文件的创立，用户扩展新的语言和文件格式，只需要实现文本分析的接口。 已经默认实现了一套强大的查询引擎，用户无需自己编写代码即可使系统可获得强大的查询能力，Lucene的查询实现中默认实现了布尔操作、模糊查询（Fuzzy Search[11]）、分组查询等等。 2. Lucene检索流程 创建索引流程:Gather Data(采集数据) –&gt; 构造文档对象 –&gt; 分词 –&gt; 创建索引并存入索引库 搜索流程:用户发起搜索请求 –&gt; 创建查询对象 –&gt; 从索引库中搜索 –&gt; 渲染并返回搜索结果 3. 索引的逻辑结构 一个非结构化的数据统一格式为document文档,一个document可以有多个field。 当用户搜索时,Lucene会从索引域中搜索,并找到对应的document,将document中的filed进行分词,然后根据分词创建索引。 4. 创建索引创建索引的流程 IndexWriter是核心对象,它可以完成创建索引、更新索引、删除索引等操作。 Directory负责对索引进行存储,它是一个抽象类,子类为FSDirectory(文件中存储)、RAMDirectory(内存中存储) 创建索引1234567891011121314151617181920212223242526272829303132333435363738394041424344@Testpublic void createIndex() &#123; // 获得原始数据 ItemsDao itemsDao = new ItemsDaoImpl(); List&lt;Items&gt; items = itemsDao.findAllItems(); // 创建Document List&lt;Document&gt; documents = new ArrayList&lt;Document&gt;(); Document document = null; // 遍历原始数据并封装到field for (Items item : items) &#123; document = new Document(); /** * 参数1:field的域名 参数2:要封装的数据 参数3:是否存储 */ Field name = new TextField("name", item.getName(), Store.YES); Field id = new StringField("id", item.getId().toString(), Store.YES); Field price = new TextField("price", item.getPrice().toString(), Store.YES); // 将field封装到document document.add(name); document.add(id); document.add(price); documents.add(document); &#125; // 创建一个标准分析器 Analyzer analyzer = new StandardAnalyzer(); // 索引库目录 Directory directory = FSDirectory.open(new File("D:\\Repository\\indexDatabase\\test")); // 创建IndexWriterConfig IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer); IndexWriter indexWriter = null; try &#123; // 创建IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); for (Document doc : documents) &#123; indexWriter.addDocument(doc); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 关闭IndexWriter indexWriter.close(); &#125;&#125; 5. 搜索索引使用QueryParse12345678910111213141516171819202122232425262728293031323334353637@Testpublic void searchIndex() throws ParseException &#123; // 创建标准分析器 Analyzer analyzer = new StandardAnalyzer(); // 创建QueryParser,c // 参数1:Field域名 参数2:分析器 QueryParser queryParser = new QueryParser("name", analyzer); // 创建Query 查找name为冰箱的 Query query = queryParser.parse("name:冰箱"); // 索引库目录 Directory directory = FSDirectory.open(new File("D:\\Repository\\indexDatabase\\test")); IndexReader indexReader = null; try &#123; // 创建IndexReader indexReader = DirectoryReader.open(directory); // 创建IndexSearcher IndexSearcher indexSearcher = new IndexSearcher(indexReader); // 执行搜索,并返回TopDoc对象 参数1:query对象 参数2:最大记录数 TopDocs topDocs = indexSearcher.search(query, 10); // 获得TopDocs中的记录对象 ScoreDoc[] scoreDocs = topDocs.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; // 获得doc的ID int docId = scoreDoc.doc; // 根据id查找到doc Document doc = indexSearcher.doc(docId); // 打印数据 System.out.println("商品名: " + doc.get("name")); System.out.println("价格: " + doc.get("price")); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 关闭reader indexReader.close(); &#125;&#125; 使用Query的子类TermQuery TermQuery使用搜索关键词进行查询。 1234567891011121314151617181920212223242526272829303132@Testpublic void searcher(Query query) &#123; // 索引库 Directory directory = FSDirectory.open(new File("D:\\Repository\\indexDatabase\\test")); // IndexReader IndexReader reader = null; try &#123; reader = DirectoryReader.open(directory); // IndexSearcher IndexSearcher searcher = new IndexSearcher(reader); TopDocs topDocs = searcher.search(query, 10); ScoreDoc[] scoreDocs = topDocs.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; int docId = scoreDoc.doc; Document document = searcher.doc(docId); System.out.println("商品id :" + document.get("id")); System.out.println("商品名称 :" + document.get("name")); System.out.println("商品价格 :" + document.get("price")); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; reader.close(); &#125;&#125;@Testpublic void termQuery() &#123; // 创建TermQuery 查询name中有冰箱的 等效于 name:冰箱 Query query = new TermQuery(new Term("name", "冰箱")); searcher(query);&#125; NumericRangeQuery 数字范围查询 123456789101112@Testpublic void numericRangeQuery()&#123; // 创建查询 // 第一个参数：域名 // 第二个参数：最小值 // 第三个参数：最大值 // 第四个参数：是否包含最小值 // 第五个参数：是否包含最大值 Query query = NumericRangeQuery.newLongRange("price", l00, 1000, true,true); // 2、 执行搜索 searcher(query);&#125; BooleanQuery 布尔查询,用于组合条件查询。 1234567891011121314151617 @Testpublic void booleanQuery() throws Exception &#123; BooleanQuery query = new BooleanQuery(); Query query1 = new TermQuery(new Term("id", "3")); Query query2 = NumericRangeQuery.newFloatRange("price", 10f, 200f, true, true); //MUST：查询条件必须满足，相当于AND //SHOULD:查询条件可选，相当于OR //MUST_NOT：查询条件不能满足，相当于NOT非 query.add(query1, Occur.MUST); query.add(query2, Occur.SHOULD); System.out.println(query); searcher(query);&#125; MUST和MUST表示“与”的关系，即“交集”。 MUST和MUST_NOT前者包含后者不包含。 MUST_NOT和MUST_NOT没有结果,没有意义。 SHOULD与MUST表示MUST，SHOULD失去意义。 SHOUlD与MUST_NOT相当于MUST与MUST_NOT。 SHOULD与SHOULD表示“或”的概念。 6. 删除索引12345678910111213@Test public void deleteIndex()&#123; // 指定索引库 Directory directory = FSDirectory.open(new File("D:\\Repository\\indexDatabase\\test")); // 创建IndexWriterConfig IndexWriterConfig indexWriterConfig = new IndexWriterConfig(new StandardAnalyzer()); // 创建IndexWriter IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); // 删除指定的索引 indexWriter.deleteDocuments(new Term("name","冰箱")); // 关闭indexWriter indexWriter.close(); &#125; 7. 修改索引在Lucene中修改索引即是替换索引,先将原来的索引删除,再保存新的索引。 12345678910111213141516@Testpublic void modifyIndex() &#123; // 指定索引库 Directory directory = FSDirectory.open(new File("D:\\Repository\\indexDatabase\\test")); // IndexWriterConfig IndexWriterConfig indexWriterConfig = new IndexWriterConfig(new StandardAnalyzer()); // IndexWriter IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); // 创建一个Document Document document = new Document(); Field name = new TextField("name", "比利海灵顿", Store.YES); document.add(name); // 修改索引 indexWriter.updateDocument(new Term("name", "冰箱"), document); indexWriter.close();&#125; 8. 相关度排序Lucene通过计算Term的权重,对查询关键字和索引文档的相关度进行打分,分越高的就排在越前面。 影响Term的权重有两个因素: Term Frequency:Term在文档中的出现频率,次数越多,则代表这个Term对该文档越重要,即权重越高。 Document Frequency:指多少文档包含这个Term的频率,频率越高,则代表这个Term越不重要,即权重越低。 手动设置权值在创建索引时设置权值 1234567891011121314151617 for (Items item : items) &#123; document = new Document(); /** * 参数1:field的域名 参数2:要封装的数据 参数3:是否存储 */ Field name = new TextField("name", item.getName(), Store.YES); Field id = new StringField("id", item.getId().toString(), Store.YES); Field price = new TextField("price", item.getPrice().toString(), Store.YES); // 给name域增加权值 name.setBoost(100f); // 将field封装到document document.add(name); document.add(id); document.add(price); documents.add(document);&#125; 在搜索时设置权值 1234567891011121314@Test public void setBoosts() throws Exception &#123; // 搜索的域名数组 String[] fields = &#123; "name", "price" &#125;; // 设置权值 Map&lt;String, Float&gt; boosts = new HashMap&lt;String, Float&gt;(); // 给name域设置权重 boosts.put("name", 100f); // 创建MultiFieldQueryParse MultiFieldQueryParser multiFieldQueryParser = new MultiFieldQueryParser(fields, new StandardAnalyzer(), boosts); // 创建Query Query query = multiFieldQueryParser.parse("冰箱"); searcher(query); &#125;]]></content>
      <categories>
        <category>后端</category>
        <category>全文检索</category>
        <category>Lucene</category>
      </categories>
      <tags>
        <tag>全文检索引擎</tag>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring MVC进阶应用]]></title>
    <url>%2F2016%2F06%2F17%2F2016-06-17-springmvc-02%2F</url>
    <content type="text"><![CDATA[Spring MVC 进阶应用参数绑定 客户端发送请求时传递的参数默认是键值对格式,Spring MVC通过参数绑定组件将请求的参数串进行类型转换。 Spring MVC使用Controller方法的形参接收请求传来的参数。 1. Spring MVC默认内置的形参类型 HttpServletRequest HttpServletResponse HttpSession Model 一个接口,可以将数据填充到request域对象中。 ModelMap Model接口的实现。 这5个对象可以直接通过形参注入并使用。 2. 简单数据类型绑定前端页面表单中的name或者URL中的key与Controller方法中的形参名称一致,即可完成绑定。如果名称不一致,可以使用@RequestParam注解指定名称完成绑定。例如： 12345URL: http://xxxxx?SID=1Controller:// 将SID的数据封装到形参id中。public String update (@RequestParam("SID") Integer id,Model model) RequestParam注解 value:参数名,即传入参数的名称。 required:默认为true,表示请求中要有相应的参数,否则报错:Status 400 - Required Integer parameter ‘XXXX’ is not present defaultValue:默认值,没有同名参数时,则使用默认值。 3. JavaBean对象类型绑定前端页面表单中的name或者URL中的key与JavaBean中的属性名进行匹配。如果参数名称一致,则将参数绑定到JavaBean中的属性上。如果JavaBean中包装了对象类型,传入参数的名称则需要按照 对象.属性 的格式编写,并且属性要与JavaBean中包装的对象中的属性名称一致。 4. 集合类型绑定数组类型绑定使用checkbox复选框时,会将参数绑定到一个数组中。例如: 1234567前端页面:&lt;input type="checkbox" name="cId" value="1"&gt;&lt;input type="checkbox" name="cId" value="2"&gt;Controller:// 使用一个数组接收复选框的参数 public String delete (Integer[] cId,Model model) List集合类型绑定Controller不能直接在形参中定义List,需要在包装类中定义List。页面中的编写格式: list名[index].属性名例如: 12345&lt;c:forEach items="$&#123;list&#125;" var="user" varStatus="status"&gt; &lt;tr&gt; &lt;td&gt;&lt;input type="text" name="list[$&#123;status.index&#125;].name" value="$&#123;user.name&#125;" /&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/c:forEach&gt; Map集合类型绑定Controller不能直接在形参中定义Map,需要在包装类中定义Map。页面中的编写格式:map名[‘key’]例如: 12345&lt;c:forEach items="$&#123;list&#125;" var="user" varStatus="status"&gt; &lt;tr&gt; &lt;td&gt;&lt;input type="text" name="map['name']" value="$&#123;user.name&#125;" /&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/c:forEach&gt; Converter当请求参数中含有日期类型时,需要自定义一个类型转换器转换成我们需要的日期格式。创建一个类实现Converter接口就可以自定义一个类型转换器。 1234567891011121314151617// Converter&lt;需要转换的数据的类型,转换的类型&gt;public class DateConverter implements Converter&lt;String, Date&gt; &#123; @Override public Date convert(String source) &#123; try&#123; // 转换日期格式 SimpleDateFormat format = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); return format.parse(source); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; // 出现异常,则返回null return null; &#125;&#125; 之后需要在处理器适配器中注册Converter 123456789101112&lt;!-- 注册注解方式的适配器与映射器 --&gt;&lt;mvc:annotation-driven conversion-service="conversionService" /&gt;&lt;bean id="conversionService" class="org.springframework.format.support.FormattingConversionServiceFactoryBean"&gt; &lt;property name="converters"&gt; &lt;list&gt; &lt;!-- 日期类型转换器 --&gt; &lt;bean class="com.sun.ssm.converter.DateConverter"&gt;&lt;/bean&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; ValidationSpring MVC本身没有实现表现层校验的部分，它本身支持JSR 303校验规范，而这个规范的官方参考实现是hibernate validator。 所以Spring MVC想要实现校验,需要导入Hibernate-validator jar包。 在配置文件中配置validator校验器 1234567891011121314151617181920212223&lt;!-- 配置校验器 --&gt;&lt;bean id="validator" class="org.springframework.validation.beanvalidation.LocalValidatorFactoryBean"&gt; &lt;!-- 校验器提供者 --&gt; &lt;property name="providerClass" value="org.hibernate.validator.HibernateValidator" /&gt; &lt;!-- 指定校验使用的资源文件，在文件中配置校验错误信息， 如果不指定，默认使用classpath下的ValidationMessages.properties --&gt; &lt;property name="validationMessageSource" ref="messageSource" /&gt;&lt;/bean&gt; &lt;!-- 校验错误信息的资源文件 --&gt;&lt;bean id="messageSource" class="org.springframework.context.support.ReloadableResourceBundleMessageSource"&gt; &lt;!-- 指定文件路径 --&gt; &lt;property name="basenames"&gt; &lt;list&gt; &lt;value&gt;classpath:validationMessages&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- 指定文件的编码 --&gt; &lt;property name="fileEncodings" value="utf8"/&gt; &lt;!-- 资源文件内容缓存的时间，单位秒 --&gt; &lt;property name="cacheSeconds" value="120" /&gt;&lt;/bean&gt; 在处理器适配器中注册Validator 12&lt;!-- 注册注解方式的适配器与映射器 --&gt;&lt;mvc:annotation-driven conversion-service="conversionService" validator="validator" /&gt; 在JavaBean中可以使用注解制定校验规则。 123456ValidationMessages.properties: user.username.size.error=用户名长度格式为1-10之间JavaBean: @Size(min=1,max=10,message="&#123;user.username.size.error&#125;") private String username; Controller中在需要检验的形参前使用@Validated注解,并在形参中注入一个BindingResult对象接收校验错误信息。@Validated注解和BindingResult顺序是固定的。(@Validated在前) 12345678910111213@RequestMapping("/updateUser")public String updateUser(@Validated User user,BindingResult bindingResult,Model model) &#123; // 判断是否有校验错误信息,如果有则代表校验未通过 if(bindingResult.hasErrors())&#123; // 获得校验错误信息集合 List&lt;ObjectError&gt; allErrors = bindingResult.getAllErrors(); // 将校验错误信息集合存入request域中 model.addAttribute("errors", allErrors); //检验未通过,重新跳转到修改页面 return "updateUser"; &#125;&#125; 分组校验 因为校验规则是在JavaBean中定义的,所以当同一个JavaBean需要被多个Controller使用时,可能根据需求需要不同的校验。 在这种情况下,可以使用一个标识作用的接口进行分组。 定义一个空接口作为分组的标识: 123public interface ValidationGroup &#123; &#125; 在JavaBean中使用分组: 12@Size(min=1,max=10,message="&#123;user.username.size.error&#125;",groups=&#123;ValidationGroup.class&#125;)private String username; 在Controller中使用分组校验: 1234567891011121314@RequestMapping("/updateUser")public String updateUser(@Validated(value=&#123;ValidationGroup.class&#125;) User user, BindingResult bindingResult,Model model) &#123; // 判断是否有校验错误信息,如果有则代表校验未通过 if(bindingResult.hasErrors())&#123; // 获得校验错误信息集合 List&lt;ObjectError&gt; allErrors = bindingResult.getAllErrors(); // 将校验错误信息集合存入request域中 model.addAttribute("errors", allErrors); //检验未通过,重新跳转到修改页面 return "updateUser"; &#125;&#125; 主要的验证注解如下: 注解 适用的数据类型 说明 @AssertFalse Boolean, boolean 验证注解的元素值是false @AssertTrue Boolean, boolean 验证注解的元素值是true @DecimalMax（value=x） BigDecimal, BigInteger, String, byte,short, int, long and the respective wrappers of the primitive types. Additionally supported by HV: any sub-type of Number andCharSequence. 验证注解的元素值小于等于@ DecimalMax指定的value值 @DecimalMin（value=x） BigDecimal, BigInteger, String, byte,short, int, long and the respective wrappers of the primitive types. Additionally supported by HV: any sub-type of Number andCharSequence. 验证注解的元素值小于等于@ DecimalMin指定的value值 @Digits(integer=整数位数, fraction=小数位数) BigDecimal, BigInteger, String, byte,short, int, long and the respective wrappers of the primitive types. Additionally supported by HV: any sub-type of Number andCharSequence. 验证注解的元素值的整数位数和小数位数上限 @Future java.util.Date, java.util.Calendar; Additionally supported by HV, if theJoda Time date/time API is on the class path: any implementations ofReadablePartial andReadableInstant. 验证注解的元素值（日期类型）比当前时间晚 @Max（value=x） BigDecimal, BigInteger, byte, short,int, long and the respective wrappers of the primitive types. Additionally supported by HV: any sub-type ofCharSequence (the numeric value represented by the character sequence is evaluated), any sub-type of Number. 验证注解的元素值小于等于@Max指定的value值 @Min（value=x） BigDecimal, BigInteger, byte, short,int, long and the respective wrappers of the primitive types. Additionally supported by HV: any sub-type of CharSequence (the numeric value represented by the char sequence is evaluated), any sub-type of Number. 验证注解的元素值大于等于@Min指定的value值 @NotNull Any type 验证注解的元素值不是null @Null Any type 验证注解的元素值是null @Past java.util.Date, java.util.Calendar; Additionally supported by HV, if theJoda Time date/time API is on the class path: any implementations ofReadablePartial andReadableInstant. 验证注解的元素值（日期类型）比当前时间早 @Pattern(regex=正则表达式, flag=) String. Additionally supported by HV: any sub-type of CharSequence. 验证注解的元素值与指定的正则表达式匹配 @Size(min=最小值, max=最大值) String, Collection, Map and arrays. Additionally supported by HV: any sub-type of CharSequence. 验证注解的元素值的在min和max（包含）指定区间之内，如字符长度、集合大小 @Valid Any non-primitive type（引用类型） 验证关联的对象，如账户对象里有一个订单对象，指定验证订单对象 @NotEmpty CharSequence,Collection, Map and Arrays 验证注解的元素值不为null且不为空（字符串长度不为0、集合大小不为0） @Range(min=最小值, max=最大值) CharSequence, Collection, Map and Arrays,BigDecimal, BigInteger, CharSequence, byte, short, int, long and the respective wrappers of the primitive types 验证注解的元素值在最小值和最大值之间 @NotBlank CharSequence 验证注解的元素值不为空（不为null、去除首位空格后长度为0），不同于@NotEmpty，@NotBlank只应用于字符串且在比较时会去除字符串的空格 @Length(min=下限, max=上限) CharSequence 验证注解的元素值长度在min和max区间内 @Email CharSequence 验证注解的元素值是Email，也可以通过正则表达式和flag指定自定义的email格式 数据回显Spring MVC默认支持数据回显。 Spring MVC会在Controller返回之前,自动将形参中的JavaBean放入Request域中,默认名称为类名首字母小写。只要前端页面动态获取数据的key与JavaBean名称一致,即可完成数据回显。如果名称不一致,也可以用@ModelAttribute注解指定形参放入Request域中的key名。 文件上传Spring MVC使用Multipart解析器完成文件上传,而Multipart解析器需要依赖以下2个jar包: commons-fileupload commons-io 配置Multipart解析器 1234&lt;bean id="multipartResolver" class="org.springframework.web.multipart.commons.CommonsMultipartResolver"&gt; &lt;!-- 限制上传文件的最大长度 --&gt; &lt;property name="maxUploadSize" value="10*1024*1024"/&gt;&lt;/bean&gt; 需要在Controller形参中注入一个MultipartFile对象接收文件,该对象名字要与传入的文件名相同。文件上传示例 1234567891011121314151617181920212223// 判断是否上传了文件if(file!= null)&#123; //原始文件名称 String fileName = file.getOriginalFilename(); //判断文件名是否为空 if(fileName != null &amp;&amp; !"".equals(fileName)) &#123; // 文件存放的路径 String path = "D:\\temps\\"; // 判断文件存放路径是否存在 File dirFile = new File(path); // 文件夹不存在,自动创建一个文件夹 if(!dirFile.exists()) &#123; dirFile.mkdirs(); &#125; // 使用UUID拼接一个唯一文件名 String newFileName = UUID.randomUUID()+fileName.substring(fileName.lastIndexOf(".")); //新的文件 File newFile = new File(path+newFileName); //把上传的文件保存成一个新的文件 file.transferTo(newFile); &#125; &#125; InterceptorSpring MVC的拦截器是针对处理器进行拦截的,当HandlerMapping查找处理器时,会被拦截器拦截。 可以通过实现HandlerIntercepter接口,自定义一个拦截器。 123456789101112131415161718192021222324252627282930public class MyHandlerInterceptor implements HandlerInterceptor&#123; // Handler执行前调用,返回值为true则放行,false则不放行。 // 应用场景:登录验证,权限授权 @Override public boolean preHandle(HttpServletRequest request,HttpServletResponse response, Object handler) throws Exception &#123; return false; &#125; // Handler开始执行,并且在返回ModelAndView之前调用 // 应用场景:操作ModelAndView对象 @Override public void postHandle(HttpServletRequest request,HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; &#125; // Handler执行完后调用 // 应用场景:异常处理、日志 @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; &#125;&#125; 配置拦截器 12345678&lt;!-- 配置全局拦截器 --&gt;&lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;!-- /**表示所有URL和子URL路径 --&gt; &lt;mvc:mapping path="/**" /&gt; &lt;bean class="com.sun.interceptor.MyHandlerInterceptor" /&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; 拦截器的执行顺序 preHandle()为正向顺序执行,postHandle()和afterCompletion()为逆向顺序执行。 如果第一个拦截器返回值为false,那它之后所有的拦截器都不会执行。 如果一个拦截器返回值为false,那么它的postHandle()和afterCompletion()都不会执行。 只要有一个拦截器的返回值为false,则所有拦截器的postHandle()都不会执行。 RESTful 一种软件架构风格，设计风格而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制。 实现REST风格需要在DispatcherServlet中将url映射模式改为 / 1234&lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 但是当映射模式为/时会把静态资源一起拦截,需要配置才能访问静态资源。 123&lt;mvc:resources location="/js/" mapping="/js/**"/&gt;&lt;mvc:resources location="/css/" mapping="/css/**"/&gt;&lt;mvc:resources location="/jquery/" mapping="/jquery/**"/&gt; Springmvc会把mapping映射到ResourceHttpRequestHandler，这样静态资源在经过DispatcherServlet转发时就可以找到对应的Handler了。 在Controller中实现RESTful 12@RequestMapping("/update/&#123;id&#125;")public String update(@PathVariable Integer id,Model model) @PathVariable:将URL中的模板变量映射到形参上,如果形参与模板变量名称不一致,可以使用value=” “设置为模板变量的名称完成映射。 {id}:模板变量。例如URL为 xxxx/update/1,模板变量update/{id}就可以将1封装到{id}中。 JSONSpring MVC默认使用MappingJacksonHttpMessageConverter对json数据进行转换，所以需要导入Jackson的jar包。 Spring MVC可以使用2个注解完成JSON数据的交互操作。 @RequestBody:如果请求参数传入的是json数据,使用RequestBody可以将json转换为java对象。 @ResponseBody:将返回值的java对象转换为json输出。]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring MVC原理及流程]]></title>
    <url>%2F2016%2F06%2F15%2F2016-06-15-springmvc-process%2F</url>
    <content type="text"><![CDATA[Spring MVC介绍 Spring MVC属于SpringFrameWork的后续产品，已经融合在Spring Web Flow里面。 通过策略接口，Spring MVC是高度可配置的，而且包含多种视图技术，例如 JavaServer Pages（JSP）、Velocity、Tiles、iText和POI。 使用 Spring 可插入的MVC架构，从而在使用Spring进行WEB开发时，可以选择使用Spring的SpringMVC框架或集成其他MVC开发框架，如Struts1，Struts2等。 Spring MVC是一个完全基于MVC模式(Model、View、Controller)的框架。 Spring MVC工作原理 Spring MVC工作流程: 用户发起请求。 DispatcherServlet接收到请求,并去调用HandlerMapping查找处理器。 HandlerMapping根据请求的URL查找对应的处理器,并返回给前端控制器DispatcherServlet。 DispatcherServlet调用HandlerAdapter执行处理器。 HandlerAdapter先判断处理器的类型进行适配,然后执行处理器。 处理器进行数据和业务请求的处理,将ModelAndView对象返回给HandlerAdapter。 HandlerAdapter将ModelAndView对象返回到前端控制器DispatcherServlet。 DispatcherServlet调用ViewResolver解析逻辑视图ModelAndView。 ViewResolver通过逻辑视图的名称查找对应的视图对象,并返回给前端控制器。 DispatcherServlet调用View渲染视图到前端。 响应处理的结果。 Spring MVC主要由以下几个部分组成: 前端控制器(DispatcherServlet):接收请求并响应到前端,并且负责各组件职责的分派。 处理器(Handler):处理数据与业务请求,Handler需要符合适配器的规则。 处理器映射器(HandlerMapping):根据每个请求的URL查找对应的处理器Handler。 处理器适配器(HandlerAdapter):根据类型适配每个处理器Handler并执行。 视图解析器(ViewResolver):根据逻辑视图的名称将逻辑视图解析为视图对象。 视图(View):在Spring MVC中,View是一个接口,通过不同的实现类支持不同的类型。(jsp、freemarker等) 注： DispatcherServlet在创建时会默认从DispatcherServlet.properties中加载springmvc的各个组件配置。 如果在servletname-servlet.xml(Spring MVC配置文件)中配置了各个组件的配置则会优先使用servletname-servlet.xml中的配置。]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis学习笔记-02]]></title>
    <url>%2F2016%2F06%2F12%2F2016-06-12-mybatis-study%20notes02%2F</url>
    <content type="text"><![CDATA[关联映射1. 业务关系分析在进行多表关联查询之前,需要对表结构的关联关系与业务关系进行分析与理解。下图以用户,订单,明细,商品表为例分析: 用户表与订单表 一个用户可以创建多个订单 (一对多) 一个订单只能由一个用户创建(一对一) 订单表与明细表 一个订单可以含有多个订单明细(一对多) 一个订单明细只能对应一个订单(一对一) 明细表与商品表 一个订单明细只能对应一个商品(一对一) 一个商品可以被包含在多个订单明细中(一对多) 订单表与商品表 (间接关系) 一个订单可以拥有多个订单明细,而一个订单明细对应一个商品,即一个订单可以拥有多个商品。 order -&gt; orderdetail -&gt; items 一对多 一个商品可以被包含在多个订单明细中,而一个订单明细对应一个订单,即一个商品可以被包含在多个订单中。 item -&gt; orderdetail -&gt; order 一对多 订单与商品是多对多关系。 用户表与商品表 (间接关系) 一个用户可以创建多个订单,一个订单可以对应多个订单明细,一个订单明细对应一个商品,即一个用户可以购买多个商品。 user -&gt; order -&gt; orderdetail -&gt; item 一对多 一个商品可以被包含在多个订单明细中,一个订单明细对应一个订单,一个订单对应一个用户,即一个商品可以对应多个用户。 item -&gt; orderdetail -&gt; order -&gt; user 一对多 用户与商品是多对多关系。 2. 一对一关联查询 在多表查询时,单表的JavaBean不能满足结果集的映射,所以可以使用继承的方法来扩展JavaBean。 12345678910111213141516package com.sun.test.entity;// Orders的扩展类public class OrdersExtends extends Orders &#123; private User user; public User getUser() &#123; return user; &#125; public void setUser(User user) &#123; this.user = user; &#125;&#125; 上面的做法与在Orders类中添加一个User类型的属性作用相同。 映射文件配置 12345678910111213141516171819202122232425262728293031323334353637383940&lt;mapper namespace="com.sun.test.mapper.OrdersMapper"&gt; &lt;!-- selectOrdersAndUserRetMap --&gt; &lt;!-- resultMap可以使用extends字段继承一个type一致的resultMap --&gt; &lt;resultMap type="ordersExtends" id="selectOrdersAndUserRetMap"&gt; &lt;!-- 主键 --&gt; &lt;id column="id" property="id" /&gt; &lt;!-- 其他字段 --&gt; &lt;result column="user_id" property="userId"/&gt; &lt;result column="number" property="number"/&gt; &lt;result column="createtime" property="createtime"/&gt; &lt;result column="note" property="note"/&gt; &lt;!-- association:一对一映射 property:关联对象在JavaBean中封装的属性名 javaType:关联对象的Java类型 --&gt; &lt;association property="user" javaType="com.sun.test.entity.User"&gt; &lt;id column="user_id" property="id"/&gt; &lt;result column="username" property="username"/&gt; &lt;result column="address" property="address"/&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;!-- 查询订单与用户信息 --&gt; &lt;select id="selectOrdersAndUser" resultMap="selectOrdersAndUserRetMap"&gt; SELECT orders.id, orders.user_id, orders.number, orders.createtime, orders.note, `user`.username, `user`.address FROM orders, `user` WHERE orders.user_id = `user`.id &lt;/select&gt; &lt;/mapper&gt; 3. 一对多关联查询 一对多查询与一对一查询类似,我们要查询商品以及订单明细则需要在商品类中扩展一个List集合。 123456789101112131415public class Items &#123; private Integer id; private String name; private Float price; private String pic; private Date createtime; private String detail; // 一对多 一个商品对应多个订单明细 private List&lt;Orderdetail&gt; orderdetails; 映射文件配置 123456789101112131415161718192021222324252627282930313233&lt;mapper namespace="com.sun.test.mapper.ItemsMapper"&gt; &lt;!-- (一对多)查询商品以及对应的订单明细 --&gt; &lt;select id="selectItemsAndOrderdetail" resultMap="selectItemsAndOrderdetailRetMap"&gt; SELECT items.id, items.`name`, items.price, orderdetail.id orderdetail_id, orderdetail.items_num FROM items, orderdetail WHERE items.id = orderdetail.items_id; &lt;/select&gt; &lt;!-- selectItemsAndOrderdetailRetMap --&gt; &lt;resultMap type="items" id="selectItemsAndOrderdetailRetMap"&gt; &lt;id column="id" property="id"/&gt; &lt;result column="name" property="name"/&gt; &lt;result column="price" property="price"/&gt; &lt;!-- 一对多关联 collection:一对多映射 ofType:这个集合参数的类型 --&gt; &lt;collection property="orderdetails" ofType="orderdetail"&gt; &lt;id column="orderdetail_id" property="id"/&gt; &lt;result column="items_num" property="itemsNum"/&gt; &lt;/collection&gt; &lt;/resultMap&gt;&lt;/mapper&gt; 4. 多对多关联查询 查询商品以及对应的用户信息。 在商品类中添加一个订单明细集合(一个商品对应多个订单明细) 在订单明细类中添加一个订单属性(一个订单明细对应一个订单) 在订单类中添加一个用户属性(一个订单对应一个用户) 12345678// 一对多 一个商品对应多个订单明细private List&lt;Orderdetail&gt; orderdetails;// 一对一 一个订单明细对应一个订单private Orders orders;//一对一 一个订单对应一个用户private User user; 映射文件配置 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;!-- 查询商品以及对应的用户 --&gt;&lt;select id="selectItemsAndUser" resultMap="selectItemsAndUserRetMap"&gt; SELECT items.id, items.`name`, items.price, `user`.id user_id, `user`.username, `user`.address, orderdetail.id orderdetail_id, orders.id orders_id FROM items, orderdetail, orders, `user` WHERE items.id = orderdetail.items_id AND orderdetail.orders_id = orders.id AND orders.user_id = `user`.id&lt;/select&gt;&lt;!-- selectItemsAndUserRetMap --&gt;&lt;resultMap type="items" id="selectItemsAndUserRetMap"&gt; &lt;id column="id" property="id" /&gt; &lt;result column="name" property="name"/&gt; &lt;result column="price" property="price"/&gt; &lt;!-- 一个商品对应多个订单明细 --&gt; &lt;collection property="orderdetails" ofType="orderdetail"&gt; &lt;id column="orderdetail_id" property="id"/&gt; &lt;!-- 一个订单明细对应一个订单 --&gt; &lt;association property="orders" javaType="orders"&gt; &lt;id column="orders_id" property="id"/&gt; &lt;!-- 一个订单对应一个用户 --&gt; &lt;association property="user" javaType="user"&gt; &lt;id column="user_id" property="id"/&gt; &lt;result column="username" property="username"/&gt; &lt;result column="address" property="address"/&gt; &lt;/association&gt; &lt;/association&gt; &lt;/collection&gt;&lt;/resultMap&gt; 延迟加载resultMap中的association和collection具有延迟加载的功能。 延迟加载: 先加载主信息,在需要的时候加载关联信息,延迟加载即叫按需加载,也叫懒加载。 1. 设置延迟加载 MyBatis 默认是关闭延迟加载的,需要在配置文件中的标签中手动开启。 settings description 验证值组 默认值 lazyLoadingEnabled 全局性设置懒加载。 true false true aggressiveLazyLoading 积极性的懒加载,false则是按需加载 true false true 123456&lt;settings&gt; &lt;!-- 开启延迟加载 ,默认值true--&gt; &lt;setting name="lazyLoadingEnabled" value="true"/&gt; &lt;!-- 设置积极的懒加载,默认值true --&gt; &lt;setting name="aggressiveLazyLoading" value="false"/&gt;&lt;/settings&gt; 2. 在association中使用延迟加载1234567891011121314151617181920&lt;!-- 查询订单 并延迟加载用户信息 --&gt; &lt;select id="selectOrdersAndLazyLoadingUser" resultMap="selectOrdersAndLazyLoadingUserRetMap"&gt; SELECT * FROM orders &lt;/select&gt; &lt;!-- selectOrdersAndLazyLoadingUserRetMap --&gt; &lt;resultMap type="orders" id="selectOrdersAndLazyLoadingUserRetMap"&gt; &lt;id column="id" property="id"/&gt; &lt;result column="user_id" property="userId"/&gt; &lt;result column="number" property="number"/&gt; &lt;result column="createtime" property="createtime"/&gt; &lt;result column="note" property="note"/&gt; &lt;!-- 一对一关联并延迟加载用户信息 select:指定延迟加载执行的statementId (如果这个statement不在当前namespace中那么则需要使用namespace.statementid) column:需要关联查询的列，如果需要传入多个则需要使用 &#123;user_id=id,...&#125;的格式 --&gt; &lt;association property="user" column="user_id" select="com.sun.test.mapper.UserMapper.findById"&gt;&lt;/association&gt; &lt;/resultMap&gt; 查询缓存1. 一级缓存 一级缓存是SqlSession级别的缓存。在操作数据库时需要构造 sqlSession对象，在对象中有一个数据结构（HashMap）用于存储缓存数据。不同的sqlSession之间的缓存数据区域（HashMap）是互相不影响的。 当第一次查询对象1时,将先会去一级缓存中查找是否有对象1的数据信息,如果没有则从数据库中查询到数据信息并构造一个key存储到一级缓存(HashMap)中。当第二次查询对象1的时候，则可以凭借key在一级缓存中找到数据信息,而不用再去访问数据库。如果SqlSession进行了事务提交(commit),则会刷新(清空)缓存,目的是为了让缓存存储最新的数据,防止脏读。MyBatis默认开启一级缓存。 2. 二级缓存 二级缓存是namespace(mapper)级别的缓存，多个SqlSession去操作同一个namespace中的mapper的sql语句，多个SqlSession可以共用二级缓存，二级缓存是跨SqlSession的。二级缓存存储数据不一定是在内存中,所以需要给缓存的对象实现序列化接口。二级缓存需要手动设置开启。 开启二级缓存12345678910 在settings中设置&lt;!-- 开启二级缓存 --&gt;&lt;setting name="cacheEnabled" value="true"/&gt;在mapper映射文件中设置&lt;!-- 使用第三方(ehcache)二级缓存 flashInterval:设置定时刷新的间隔时间,单位是毫秒。 注:使用第三方缓存框架需要先导入jar包和整合包--&gt; &lt;cache type="org.mybatis.caches.ehcache.EhcacheCache" flashInterval="60000" /&gt; MyBatis与Spring整合 由Spring维护管理数据源、事务、SqlSessionFactory、mapper。MyBatis的配置文件只需要配置settings、别名等即可。 123456789101112131415Spring中的配置&lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;!-- mybatis的配置文件路径 --&gt; &lt;property name="configLocation" value="sqlMapConfig.xml"&gt;&lt;/property&gt; &lt;!-- 注入数据源 --&gt; &lt;property name="dataSource" ref="dataSource"&gt;&lt;/property&gt;&lt;/bean&gt;批量生成Mapper(生成的Mapper名字默认为首字母)&lt;bean class="org.mybatis.spring.mapper.MapperScannerConfigurer"&gt; &lt;!-- 指定需要扫描的mapper配置的包名 --&gt; &lt;property name="basePackage" value="com.sun.test.mapper"&gt;&lt;/property&gt; &lt;!-- 注入SqlSessionFactory --&gt; &lt;property name="sqlSessionFactoryBeanName" value="sqlSessionFactory"&gt;&lt;/property&gt;&lt;/bean&gt;]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>MyBatis</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis-XML配置与映射文件]]></title>
    <url>%2F2016%2F06%2F11%2F2016-06-11-MyBatis-XML%E9%85%8D%E7%BD%AE%E4%B8%8E%E6%98%A0%E5%B0%84%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[全局XML配置文件MyBatis配置文件的结构如下： configuration 配置 properties 属性 settings 参数设置 typeAliases 类型别名 typeHandlers 类型处理器 objectFactory 对象工厂 plugins 插件 environments 环境集合 environment 环境 transactionManager 事务管理器 dataSource 数据源 databaseIdProvider 数据库厂商标识 mappers 映射器 常用标签配置1. propertiesproperties标签可以读取properties配置文件,并引入到 MyBatis 的配置文件中。 可以通过子标签定义key/value。 例如： 1234&lt;properties resource="com/sun/test/dataSource.properties"&gt; &lt;property name="username" value="dev_user"/&gt; &lt;property name="password" value="F2Fa3!33TYyg"/&gt;&lt;/properties&gt; 其中的属性就可以在整个配置文件中使用key来替换需要动态配置的属性值。 例如： 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="$&#123;driver&#125;"/&gt; &lt;property name="url" value="$&#123;url&#125;"/&gt; &lt;property name="username" value="$&#123;username&#125;"/&gt; &lt;property name="password" value="$&#123;password&#125;"/&gt;&lt;/dataSource&gt;``` properties的加载顺序 - 最先加载properties中的子标签所定义的属性。 - 然后会加载resource标签所引入的properties配置文件中的属性,并覆盖已加载的同名属性。 - 最后加载作为方法参数(parameterType中的key/value)传递的属性，并覆盖已加载的同名属性。总结： - 通过方法参数传递的属性具有最高优先级。 - resource引用的配置文件中的属性次之。 - 最低优先级的是 properties 子标签中指定的属性。#### 2. **settings** settings会影响MyBatis的运行行为。 一个配置完整的 settings 的示例如下：``` stylus&lt;settings&gt; &lt;setting name="cacheEnabled" value="true"/&gt; &lt;setting name="lazyLoadingEnabled" value="true"/&gt; &lt;setting name="multipleResultSetsEnabled" value="true"/&gt; &lt;setting name="useColumnLabel" value="true"/&gt; &lt;setting name="useGeneratedKeys" value="false"/&gt; &lt;setting name="autoMappingBehavior" value="PARTIAL"/&gt; &lt;setting name="autoMappingUnknownColumnBehavior" value="WARNING"/&gt; &lt;setting name="defaultExecutorType" value="SIMPLE"/&gt; &lt;setting name="defaultStatementTimeout" value="25"/&gt; &lt;setting name="defaultFetchSize" value="100"/&gt; &lt;setting name="safeRowBoundsEnabled" value="false"/&gt; &lt;setting name="mapUnderscoreToCamelCase" value="false"/&gt; &lt;setting name="localCacheScope" value="SESSION"/&gt; &lt;setting name="jdbcTypeForNull" value="OTHER"/&gt; &lt;setting name="lazyLoadTriggerMethods" value="equals,clone,hashCode,toString"/&gt;&lt;/settings&gt; 3. typeAliases类型别名,可以对JavaBean设置一个别名,用来减少完全限定名的冗余。例如： 12345678910&lt;!-- 类型别名 --&gt; &lt;typeAliases&gt; &lt;!-- 设置单个别名 --&gt; &lt;typeAlias type="com.sun.test.entity.User" alias="user"/&gt; &lt;!-- package指定一个包名,会在包下搜索需要的JavaBean 如果没有注解,则默认设置为JavaBean的首字母小写的非限定类名作为它的别名。 如果有@Alias注解,则使用其注解值作为它的别名。 --&gt; &lt;package name="com.sun.test.entity"/&gt; &lt;/typeAliases&gt; MyBatis默认支持的类型别名如下,它们都是大小写不敏感的，需要注意的是由基本类型名称重复导致的特殊处理。 别名 映射的类型 _byte byte _long long _short short _int int _integer int _double double _float float _boolean boolean string String byte Byte long Long short Short int Integer integer Integer double Double float Float boolean Boolean date Date decimal BigDecimal bigdecimal BigDecimal object Object map Map hashmap HashMap list List arraylist ArrayList collection Collection iterator Iterator 4. Mappers映射器(Mappers)用于引入Mapper映射文件。 使用相对路径的方式引入 1234&lt;!-- 相对路径 --&gt;&lt;mappers&gt; &lt;mapper resource="com/sun/test/mapper/UserMapper.xml"/&gt;&lt;/mappers&gt; 使用绝对路径的方式引入 1234&lt;!-- 绝对路径s --&gt;&lt;mappers&gt; &lt;mapper url="file:///D:\sun\mappers\UserMapper.xml"/&gt;&lt;/mappers&gt; 使用Mapper接口的全限定类名(需要mapper接口与mapper映射文件名称相同,并且在同一个包下) 1234&lt;!-- Mapper接口的全限定类名 --&gt;&lt;mappers&gt; &lt;mapper class="com.sun.test.mapper.UserMapper"/&gt;&lt;/mappers&gt; 扫描指定包下的所有映射文件(需要mapper接口与mapper映射文件名称相同,并且在同一个包下) 1234&lt;!-- 扫描指定包下的所有映射文件 --&gt;&lt;mappers&gt; &lt;package name="com.sun.test.mapper"/&gt;&lt;/mappers&gt; Mapper映射文件 cache 缓存配置 cache-ref 其他namespace缓存配置的引用 resultType 结果集映射的类型 resultMap 结果集映射Map parameterType 输入映射类型 sql 可被引用的复用sql语句块 insert 映射插入语句 update 映射更新语句 select 映射查询语句 delete 映射删除语句 resultType注意事项: 如果使用resultType进行结果集映射,则需要查询出的列名与映射的JavaBean属性名称一致。 SQL语句中列名如果有别名,则列名为别名的名称。 如果查询的列名和JavaBean所映射的属性名全不一致,则映射的JavaBean对象为null。 如果查询的列名和JavaBean所映射的属性名少数不一致,则映射的JavaBean对象不为null,但只有名称一致的属性有值。 例如： 1234 &lt;!-- 根据id查询User --&gt;&lt;select id="findById" parameterType="int" resultType="com.sun.test.entity.User"&gt; select * from user where id = #&#123;id&#125;&lt;/select&gt; resultMap注意事项: 使用resultMap进行结果集映射,不需要列名与映射的JavaBean属性名称一致。 使用resultMap进行结果集映射,需要先定义一个resultMap。 例如: 12345678910&lt;!-- 定义一个resultMap --&gt;&lt;resultMap type="com.sun.test.entity.User" id="userResultMap"&gt; &lt;!-- 主键 --&gt; &lt;id column="id" property="id"/&gt; &lt;!-- 其他字段 --&gt; &lt;result column="username" property="username"/&gt;&lt;/resultMap&gt;&lt;select id="findByIdWithRetMap" parameterType="int" resultMap="userResultMap"&gt; select id,username from user where id = #&#123;id&#125;&lt;/select&gt; 动态SQL1. SQL片段sql片段可以存储动态的sql语句,提高复用性。 1234567891011&lt;!-- SQL片段,可以存储动态的SQL语句 --&gt; &lt;sql id="sql"&gt; &lt;!-- 判断用户名不为空 --&gt; &lt;if test="username != null and username != ''"&gt; and username = #&#123;usernmae&#125; &lt;/if&gt; &lt;!-- 判断性别不为空 --&gt; &lt;if test="sex != null and sex != ''"&gt; and sex = #&#123;sex&#125; &lt;/if&gt; &lt;/sql&gt; 2. if标签1234567&lt;select id="findByIdOrLikeName" resultType="com.sun.test.entity.User"&gt; select * from user where id = #&#123;id&#125; &lt;!-- 如果if返回为true 则会加上这条语句 --&gt; &lt;if test="username != null and username != ''"&gt; and username like '%$&#123;username&#125;%' &lt;/if&gt;&lt;/select&gt; 3. where标签1234567891011&lt;select id="findAllOrLikeName" resultType="com.sun.test.entity.User"&gt; select * from user &lt;!-- where标签只有在if为true的情况下才会添加where语句 并且会将第一条语句的and去掉。 --&gt; &lt;where&gt; &lt;if test="username != null and username != ''"&gt; and username like '%$&#123;username&#125;%' &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; 4. foreach标签foreach标签可以对一个集合进行遍历,通常是在构建一个in条件语句的时候使用。 1234567891011121314151617&lt;select id="selectUserIn" resultType="com.sun.test.entity.User" parameterType="list"&gt; select * from user &lt;where&gt; &lt;if test="list != null and list.size &gt; 0"&gt; &lt;!-- collection：集合参数的名称。 item：遍历集合时的值 open：遍历开始时需要拼接的sql语句 close：遍历结束时需要拼接的sql语句 separator：遍历过程中需要拼接的字符串 --&gt; &lt;foreach collection="list" item="id" open="and id in(" close=")" separator=","&gt; #&#123;id&#125; &lt;/foreach&gt; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; 5. bind标签bind标签可以从OGNL表达式中创建一个变量并将其绑定到上下文。 12345&lt;select id="selectUserLikeName" resultType="com.sun.test.entity.User"&gt; &lt;bind name="username" value="'%' + user.getUsername() + '%'" /&gt; select * from user where username like #&#123;username&#125;&lt;/select&gt;]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>MyBatis</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis学习笔记-01]]></title>
    <url>%2F2016%2F06%2F10%2F2016-06-10-mybatis-study%20notes01%2F</url>
    <content type="text"><![CDATA[1.什么是MyBatis? MyBatis 本是apache的一个开源项目iBatis, 2010年这个项目由apache software foundation 迁移到了google code，并且改名为MyBatis 。2013年11月迁移到Github。 iBATIS一词来源于“internet”和“abatis”的组合，是一个基于Java的持久层框架。iBATIS提供的持久层框架包括SQL Maps和Data Access Objects（DAO） MyBatis 是支持定制化 SQL、存储过程以及高级映射的优秀的持久层框架。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。 MyBatis 可以对配置和原生Map使用简单的 XML 或注解，将接口和 Java 的 POJOs(Plain Old Java Objects,普通的 Java对象)映射成数据库中的记录。 2.MyBatis框架原理MyBatis的功能架构分为三层 API接口层：提供给外部使用的接口API，开发人员通过这些本地API来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理。 数据处理层：负责具体的SQL查找、SQL解析、SQL执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作。 基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑。 MyBatis框架架构流程 MyBatis应用程序通过读取XML配置文件,构造出SqlSessionFactory(SQL会话工厂)。 SqlSessionFactory再根据配置,构造一个SqlSession(SQL会话)。MyBatis通过SqlSession完成数据库操作。 SqlSession本身不能直接操作数据库,它是通过底层Executor接口操作数据库的。Executor有2个实现类,默认使用缓存执行器。 Executor将处理的SQL信息封装到一个底层对象 MappedStatement 中。该对象包含了SQL语句、输入参数信息、输出结果集信息。 输入参数和输出结果集的映射类型为java的简单类型、HashMap集合类型、POJO对象类型。 3.MyBatis的优点与缺点优点 学习成本低、简单易学。 灵活性较高,可以直接对SQL进行性能优化。 SQL与业务逻辑代码低耦合,提高了维护性。 可以编写动态SQL语句。 缺点 SQL语句依赖数据库,即数据库发生变更时,需要重新写SQL语句, 移植性较差。 SQL语句繁多,工作量大。 二级缓存机制不佳。 4.MyBatis入门我们使用MyBatis完成一个根据id查询用户的需求。 创建一个JavaBean 1234567public class User &#123; private int id ; private String username ;// 姓名 private String sex ;// 性别 private Date birthday ;// 生日 private String address ;// 地址 为User类配置一个映射文件 1234567891011121314&lt;? xml version ="1.0" encoding= "UTF-8" ?&gt;&lt;! DOCTYPE mapperPUBLIC "-//mybatis.org//DTD Mapper 3.0//EN""http://mybatis.org/dtd/mybatis-3-mapper.dtd" &gt;&lt; mapper namespace ="com.sun.test.mapper.UserMapper" &gt; &lt;!-- 根据id查询User 在命名空间 com.sun.test.mapper.UserMapper中定义了一个id为findById的映射语句。 当我们调用时可以使用 namespace.id 来进行调用。 即：com.sun.test.mapper.UserMapper.findById --&gt; &lt;select id= "findById" parameterType ="int" resultType ="com.sun.test.entity.User" &gt; select * from user where id = #&#123;id&#125; &lt;/select &gt;&lt;/ mapper&gt; 命名空间(Namespace): 必须且非常重要,最好使用对应mapper接口的全限定名(比如:com.sun.test.mapper.UserMapper )。 parameterType: 输入参数的类型,可以使用类型的全限定名或者别名。 resultType: 返回参数的类型,如果是集合类型,则应使用集合内可以包含的类型,而不能是集合本身,可以使用类型的全限定名或者别名。 创建一个全局配置文件,并配置数据源、事务管理器、映射文件等基本配置。 123456789101112131415161718192021222324&lt;? xml version =&quot;1.0&quot; encoding= &quot;UTF-8&quot; ?&gt;&lt;! DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot; &gt;&lt; configuration&gt; &lt;environments default= &quot;development&quot;&gt; &lt; environment id= &quot;development&quot; &gt; &lt;!-- 配置事务管理器 --&gt; &lt; transactionManager type =&quot;JDBC&quot; /&gt; &lt;!-- 配置数据源 --&gt; &lt; dataSource type =&quot;POOLED&quot; &gt; &lt; property name =&quot;driver&quot; value =&quot;com.mysql.jdbc.Driver&quot; /&gt; &lt; property name =&quot;url&quot; value =&quot;jdbc:mysql:///mybatis&quot; /&gt; &lt; property name =&quot;username&quot; value =&quot;root&quot; /&gt; &lt; property name =&quot;password&quot; value =&quot;root&quot; /&gt; &lt;/ dataSource&gt; &lt;/ environment&gt; &lt;/environments &gt; &lt;!-- 配置映射文件 --&gt; &lt;mappers &gt; &lt;!-- 使用package可以查找到所有 mapper接口与 mapper映射文件但需要同名并且放在同一包下 --&gt; &lt; package name =&quot;com.sun.test.mapper&quot; /&gt; &lt;/mappers &gt;&lt;/ configuration&gt; 测试 12345678910111213141516171819@Test public void test01() throws IOException &#123; // 获得配置文件的资源流 InputStream inputStream = Resources.getResourceAsStream( "sqlMapConfig.xml"); // 加载配置文件,构造SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream ); // 使用SqlSessionFactory构造一个SqlSession SqlSession sqlSession = sqlSessionFactory .openSession(); /** * 执行statement * 参数1：statement的id 建议使用 namespace.statementId * 参数2：输入参数的值,它的类型要和映射文件中的parameterType类型一致 */ User user = sqlSession .selectOne("com.sun.test.mapper.UserMapper.findById" , 1); // 输出结果 System. out .println(user ); // 关闭SqlSession sqlSession .close(); &#125; Resources：MyBatis中的一个工具类,可以从classpath或其他位置加载资源文件。 SqlSessionFactoryBuilder： 这个类可以被实例化、使用和丢弃,一旦创建了 SqlSessionFactory,就不再需要它了。因此 SqlSessionFactoryBuilder 实例的最佳范围是方法范围（也就是局部方法变量）。 SqlSessionFactory：一旦被创建就应该在应用的运行期间一直存在,所以它应该是单例的,且在应用运行期间不要多次创建。 SqlSession：用于操作数据库的对象,每个线程都应该有一个SqlSession实例,它是多例的,并且它是线程不安全的,不能放在全局变量上。 5.自增主键如果你的数据库支持自动生成主键的字段,那么你可以设置 useGeneratedKeys=”true”,再把keyProperty设置到目标属性上即可。 1234&lt;insert id=&quot;insertUser&quot; useGeneratedKeys=&quot;true&quot; parameterType =&quot;com.sun.test.entity.User&quot; keyProperty=&quot;id&quot;&gt; insert into user(username,birthday,sex,address) values(#&#123;username&#125;,#&#123;birthday&#125;,#&#123;sex&#125;,#&#123;address&#125;)&lt;/insert&gt; 如果不支持自动生成主键则可以使用另外一种方法生成主键 1234567891011121314&lt; insert id= &quot;insertUser&quot; parameterType =&quot;com.sun.test.entity.User&quot; &gt; &lt;!-- 返回自增主键 keyProperty:生成主键的属性 resultType:生成主键的类型 order:查询主键SQL的执行顺序,相对于insert,如果设置为 BEFORE，那么它会首先选择主键，设置 keyProperty 然后执行插入语句。 如果设置为 AFTER，那么会先执行插入语句。 LAST_INSERT_ID():MySQL的函数,获取最后插入的主键 --&gt; &lt; selectKey keyProperty =&quot;id&quot; resultType =&quot;int&quot; order= &quot;AFTER&quot;&gt; select LAST_INSERT_ID() &lt;/ selectKey&gt; insert into user(username,birthday,sex,address) values(#&#123;username&#125;,#&#123;birthday&#125;,#&#123;sex&#125;,#&#123;address&#125;) &lt;/insert &gt; #{} 与 ${}的区别 #{}可以表示为占位符?,#{id}里面的id表示输入参数的参数名称,如果是简单类型,则名称可以任意填写。 ${}表示字符串替换,${value},value表示输入参数的参数名称,如果是简单类型,则名称必须为value。 ${}是不安全的,会导致潜在的sql注入攻击,但如果要动态传入order by的列名则必须使用${} 即：order by ${columnName}。 6.使用Mapper构建Dao使用Mapper构建Dao则只需要开发接口即可。 规范 mapper接口的全限定名称要和mapper映射文件的namespace一致。 mapper接口的方法名称要和mapper映射文件的MappedStatement的id一致。 mapper接口的方法参数类型与返回值类型要和mapper映射文件的MappedStatement的parameterType与resultType一致。 例:]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>MyBatis</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MyBatis</tag>
      </tags>
  </entry>
</search>