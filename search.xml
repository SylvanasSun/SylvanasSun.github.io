<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[编码的那点事儿]]></title>
    <url>%2F2017%2F08%2F20%2F2017-08-20-Encode%2F</url>
    <content type="text"><![CDATA[什么是编码? 对于普通人来说,编码总是与一些秘密的东西相关联(加密与解密);对于程序员们来说,编码大多数是指一种用来在机器与人之间传递信息的方式. 但从广义上来讲,编码是从一种信息格式转换为另一种信息格式的过程,解码则是编码的逆向过程.接下来举几个使用到编码的例子: 当我们要把想表达的意思通过一种语言表达出来,其实就是在脑海中对信息进行了一次编码,而对方如果也懂得这门语言,那么就可以用这门语言的解码方法(语法规则)来获得信息(日常的说话交流其实就是在编码与解码). 程序员写程序时,其实就是在将自己的想法通过计算机语言进行编码,而编译器则通过生成抽象语法树,词义分析等操作进行解码,最终交给计算机执行程序(编译器产生的解码结果并不是最终结果,一般为汇编语言,但汇编语言只是CPU指令集的助记符,还需要再进行解码). 计算机只有两种状态(0和1),要想存储和传输多媒体信息,就需要用到编码和解码. 对数据进行压缩,其本质就是以减少自身占用的空间为前提进行重新编码. 了解了编码的含义,我们接下来重点探究Java中的字符编码. 本文作者为: SylvanasSun.转载请务必将下面这段话置于文章开头处(保留超链接).本文首发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/08/20/2017-08-20-Encode/ 常见的字符集 字符集就是字符与二进制的映射表,每一个字符集都有自己的编码规则,每个字符所占用的字节也不同(支持的字符越多每个字符占用的字节也就越多). ASCII : 美国信息交换标准码(American Standard Code for Information Interchange).学过计算机的都知道大名鼎鼎的ASCII码,它是基于拉丁字母的字符集,总共记有128个字符,主要目的是显示英语.其中每个字符占用一个字节(只用到了低7位). ISO-8859-1 : 它是由国际标准化组织(International Standardization Organization)在ASCII基础上制定的8位字符集(仍然是单字节编码).它在ASCII空置的0xA0-0xFF范围内加入了96个字母与符号,支持了欧洲部分国家的语言. GBK : 如果我们想要让电脑上显示汉字就必须要有支持汉字的字符集,GBK就是这样一个支持汉字的字符集,全称为&lt;&lt;汉字内码扩展规范&gt;&gt;,它的编码方式分为单字节与双字节: 00–7F范围内是第一个字节,与ASCII保持一致,之后的双字节中,前一字节是双字节的第一位(范围在81–FE,不包含80和FF),第二字节的一部分在40–7E,其他部分在80–FE.(这里不再介绍GB2313与GB18030,它们都是互相兼容的.) UTF-16 : UTF-16是Unicode(统一码,一种以支持世界上多国语言为目的的通用字符集)的一种实现方式,它把Unicode的抽象码位映射为2~4个字节来表示,UTF-16是变长编码(UTF-32是真正的定长编码),但在最开始以前UTF-16是用来配合UCS-2(UTF-16的子集,它是定长编码,用2个字节表示所有Unicode字符)使用的,主要原因还是因为当时Unicode只有不到65536个字符,2个字节就足以应对一切了.后来,Unicode支持的字符不断膨胀,2个字节已经不够用了,导致一些只支持UCS-2当做内码的产品很尴尬(Java就是其中之一). ![](http://ourzeyn5n.bkt.clouddn.com/2017-08-20-Encode-05-UTF-16-2.png) UTF-8 : UTF-8也是基于Unicode的变长编码表,它使用1~6个字节来为每个字符进行编码(RFC 3629对UTF-8进行了重新规范,只能使用原来Unicode定义的区域,U+0000~U+10FFFF,也就是说最多只有4个字节),UTF-8完全兼容ASCII,它的编码规则如下: - `U+0000~U+007F`范围内,只需要一个字节(也就是`ASCII`字符集中的字符). - `U+0080~U+07FF`范围内,需要两个字节(希腊文、阿拉伯文、希伯来文等). - `U+0800~U+FFFF`范围内,需要三个字节(亚洲汉字等). - 其他的字符使用四个字节. ![](http://ourzeyn5n.bkt.clouddn.com/2017-08-20-Encode-06-UTF-8.png) Java中字符的编解码 Java提供了Charset类来完成对字符的编码与解码,主要使用以下函数: public static Charset forName(String charsetName) : 这是一个静态工厂函数,它根据传入的字符集名称来返回对应字符集的Charset类. public final ByteBuffer encode(CharBuffer cb) / public final ByteBuffer encode(String str) : 编码函数,它将传入的字符串或者字符序列进行编码,返回的ByteBuffer是一个字节缓冲区. public final CharBuffer decode(ByteBuffer bb) : 解码函数,将传入的字节序列解码为字符序列. 示例代码 1234567891011121314151617181920212223242526272829303132333435363738private static final String text = "Hello,编码!";private static final Charset ASCII = Charset.forName("ASCII");private static final Charset ISO_8859_1 = Charset.forName("ISO-8859-1");private static final Charset GBK = Charset.forName("GBK");private static final Charset UTF_16 = Charset.forName("UTF-16");private static final Charset UTF_8 = Charset.forName("UTF-8");private static void encodeAndPrint(Charset charset) &#123; System.out.println(charset.name() + ": "); printHex(text.toCharArray(), charset); System.out.println("----------------------------------");&#125;private static void printHex(char[] chars, Charset charset) &#123; System.out.println("ForEach: "); ByteBuffer byteBuffer; byte[] bytes; if (chars != null) &#123; for (char c : chars) &#123; System.out.print("char: " + Integer.toHexString(c) + " "); // 打印出字符编码后对应的字节 byteBuffer = charset.encode(String.valueOf(c)); bytes = byteBuffer.array(); System.out.print("byte: "); if (bytes != null) &#123; for (byte b : bytes) System.out.print(Integer.toHexString(b &amp; 0xFF) + " "); &#125; System.out.println(); &#125; &#125; System.out.println();&#125; 有的读者可能会对以上代码中的b &amp; 0xFF产生疑惑,这是为了解决符号扩展问题.在Java中,如果一个窄类型强转为一个宽类型时,会对多出来的空位进行符号扩展(如果符号位为1,就补1,为0则补0).只有char类型除外,char是没有符号位的,所以它永远都是补0. 代码中调用了函数Integer.toHexString(),变量b在运算之前就已经被强转为了int类型,为了让数值不受到破坏,我们让b对0xFF进行了与运算,0xFF是一个低八位都为1的值(其他位都为0),而byte的有效范围只在低八位,所以结果为前24位(除符号位)都变为了0,低八位保留了原有的值. 如果不做这项操作,那么b又恰好是个负数的话,那这个强转后的int的前24位都会变为1,这个结果显然已经破坏了原有的值. IO中的字符编码 Reader与Writer是Java中负责字符输入与输出的抽象基类,它们的子类实现了在各种场景中的字符输入输出功能. 在使用Reader与Writer进行IO操作时,需要指定字符集,如果不显式指定的话会默认使用当前环境的字符集,但我还是推荐显式指定一致的字符集,这样才不会出现乱码问题(Reader与Writer指定的字符集不一致或更改了环境导致字符集不一致等). 1234567891011121314151617181920212223242526272829303132333435363738394041424344public static void writeChar(String content, String filename, String charset) &#123; OutputStreamWriter writer = null; try &#123; FileOutputStream outputStream = new FileOutputStream(filename); writer = new OutputStreamWriter(outputStream, charset); writer.write(content); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (writer != null) writer.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;public static String readChar(String filename, String charset) &#123; InputStreamReader reader = null; StringBuilder sb = null; try &#123; FileInputStream inputStream = new FileInputStream(filename); reader = new InputStreamReader(inputStream, charset); char[] buf = new char[64]; int count = 0; sb = new StringBuilder(); while ((count = reader.read(buf)) != -1) sb.append(buf, 0, count); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (reader != null) reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return sb.toString();&#125; Web中的字符编码 在Web开发中,乱码也是经常存在的一个问题,主要体现在请求的参数和返回的响应结果,最头疼的是不同的浏览器的默认编码甚至还不一致. Java以Http的请求与响应抽象出了Request和Response两个对象,只要保持请求与响应的编码一致就能避免乱码问题. Request提供了setCharacterEncoding(String encode)函数来改变请求体的编码,一般通过写一个过滤器来统一对所有请求设置编码. 1request.setCharacterEncoding("UTF-8"); Response提供了setCharacterEncoding(String encode)与setHeader(String name,String value)两个函数,它们都可以设置响应的编码. 123response.setCharacterEncoding("UTF-8");// 设置响应头的编码信息,同时也告知了浏览器该如何解码response.setHeader("Content-Type","text/html;charset=UTF-8"); 还有一种更简便的方式,直接使用Spring提供的CharacterEncodingFilter,该过滤器就是用来统一编码的. 12345678910111213141516&lt;filter&gt; &lt;filter-name&gt;charsetFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;charsetFilter&lt;/filter-name&gt; &lt;url-pattern&gt;*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; CharacterEncodingFilter的实现如下: 1234567891011121314151617181920212223242526public class CharacterEncodingFilter extends OncePerRequestFilter &#123; private String encoding; private boolean forceEncoding = false; public CharacterEncodingFilter() &#123; &#125; public void setEncoding(String encoding) &#123; this.encoding = encoding; &#125; public void setForceEncoding(boolean forceEncoding) &#123; this.forceEncoding = forceEncoding; &#125; protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; if(this.encoding != null &amp;&amp; (this.forceEncoding || request.getCharacterEncoding() == null)) &#123; request.setCharacterEncoding(this.encoding); if(this.forceEncoding) &#123; response.setCharacterEncoding(this.encoding); &#125; &#125; filterChain.doFilter(request, response); &#125;&#125; 为什么Char在Java中占用两个字节? 众所周知,在Java中一个char类型占用两个字节,那么这是为什么呢?这是因为Java使用了UTF-16当作内码. 内码(Internal Encoding)就是程序内部所使用的编码,主要在于编程语言实现其char和String类型在内存中使用的内部编码.与之相对的就是外码(External Encoding),它是程序与外部交互时使用的字符编码. 值得一提的是,当初UTF-16是配合UCS-2使用的,后来Unicode支持的字符不断增多,UTF-16也不再只当作一个定长的2字节编码使用了,也就是说,Java中的一个char其实并不一定能代表一个完整的UTF-16字符. String.getBytes()可以将该String的内码转换为指定的外码并返回这个编完码的字节数组(无参数版使用当前平台的默认编码). 12345public static void main(String[] args) throws UnsupportedEncodingException &#123; String text = "码"; byte[] bytes = text.getBytes("UTF-8"); System.out.println(bytes.length); // 输出3&#125; Java还规定char与String类型的序列化是使用UTF-8当作外码的,Java中的Class文件中的字符串常量与符号名也都规定使用UTF-8.这种设计是为了平衡运行时的时间效率与外部存储的空间效率所做的取舍. 在SUN JDK6中,有一条命令-XX:+UseCompressedString.该命令可以让String内部存储字符内容可能用byte[]也可能用char[]: 当整个字符串所有字符处于ASCII字符集范围内时,就使用byte[](使用了ASCII编码)来存储,如果有任一字符超过了ASCII的范围,就退回到使用char[](UTF-16编码)来存储.但是这个功能实现的并不理想,所以没有包含在Open JDK6/Open JDK7/Oracle JDK7等后续版本中. JavaScript也使用了UTF-16作为内码,其实现也广泛应用了CompressedString的思想,主流的JavaScript引擎中都会尽可能使用ASCII内码的字符串,不过这些细节都是对外隐藏的.. 参考文献 ASCII - Wikipedia ISO/IEC 8859-1 - Wikipedia GBK - Wikipedia UTF-16 - Wikipedia UTF-8 - Wikipedia Java 语言中一个字符占几个字节？ - RednaxelaFX的回答]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>2017</tag>
        <tag>后端</tag>
        <tag>编码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[B树的那点事儿]]></title>
    <url>%2F2017%2F08%2F13%2F2017-08-13-BTrees%2F</url>
    <content type="text"><![CDATA[概述 B树(B-Tree)是一种自平衡的树,能够保证数据有序.同时它还保证了在查找、插入、删除等操作时性能都能保持在$O(log\;n)$.需要注意的一点是,B-Tree并不是一棵自平衡的二叉查找树,它拥有多个分叉,且为大块数据的读写操作做了优化,同时它也可以用来描述外部存储(支持对保存在磁盘或者网络上的符号表进行外部查找). 在当今的互联网环境下,数据量已经大到无法想象,而能够在巨型数据集合中快速地进行查找操作是非常重要的,而B-Tree的神奇之处正在于: 只需要使用4~5个指向一小块数据的引用即可有效支持在数百亿甚至更多元素的符号表中进行查找和插入等操作. B-Tree的主要应用在于文件系统与数据库系统,例如Mysql中的InnoDB存储引擎就使用到了B-Tree来实现索引. 本文作者为: SylvanasSun.转载请务必将下面这段话置于文章开头处(保留超链接).本文转发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/08/13/2017-08-13-BTrees/ 数据表示 我们使用页来表示一块连续的数据,访问一页的数据需要将它读入本地内存.一个页可能是本地计算机上的一个文件,也可能是服务器上的某个文件的一部分等等.页的访问次数(无论读写)即是外部查找算法的成本模型. 首先,构造一棵B-Tree不会将数据保存在树中,而是会构造一棵由键的副本组成的树,每个副本都关联着一条链接.这种方法能够将索引与符号表进行分离,同时我们还需要遵循以下的规定: 选择一个参数M来构造一棵多向树(M一般为偶数),每个节点最多含有M - 1对键和链接. 每个节点最少含有M / 2对键和链接,根节点例外(它最少可以含有2对). .使用M阶的B-Tree来指定M的值,例如: 在一棵4阶B-Tree中,每个节点都含有至少2对至多3对. B-Tree含有两种不同类型的节点,内部节点与外部节点. 内部节点含有与页相关联的键的副本: 每个键都与一个节点相关联(一条链接),以此节点为根的子树中,所有的键都大于等于与此节点关联的键,但小于原内部节点中更大的键(如果存在的话). 外部节点含有指向实际数据的引用: 每个键都对应着实际的值,外部节点就是一张普通的符号表. 12345678910111213141516171819202122232425262728293031323334353637// max children per B-tree node = M - 1// must be even and greater than 2private static final int M = 4;// root of the B-treeprivate Node root;// height of the B-treeprivate int height;// number of key-value paris int the B-treeprivate int N;// B-tree node data typeprivate static final class Node &#123; private int children_length; private Entry[] children = new Entry[M]; // create a node with k children private Node(int k) &#123; children_length = k; &#125;&#125;// internal nodes : only use key and next// external nodes : only use key and valueprivate static class Entry &#123; private Comparable key; private final Object value; private Node next; private Entry(Comparable key, Object value, Node next) &#123; this.key = key; this.value = value; this.next = next; &#125;&#125; 查找 在B-Tree中进行查找操作每次都会结束于一个外部节点.在查找时,从根节点开始,根据被查找的键来选择当前节点中的适当区间并根据对应的链接从一个节点移动到下一层节点.最终,查找过程会到达树底的一个含有键的页(也就是外部节点),如果被查找的键在该页中,查找命中并结束,如果不在,则查找未命中. 12345678910111213141516171819202122232425262728293031public Value get(Key key) &#123; validateKey(key, "argument key to get() is null."); return search(root, key, height);&#125;private Value search(Node x, Key key, int height) &#123; while (x != null) &#123; Entry[] children = x.children; int children_length = x.children_length; // 当树的高度已经递减为0时,也就到达了树的底部(一个外部节点) // 遍历当前节点的每个键进行比较,如果找到则查找命中返回对应的值. if (height == 0) &#123; for (int j = 0; j &lt; children_length; j++) &#123; if (eq(key, children[j].key)) return (Value) children[j].value; &#125; &#125; else &#123; // 当还是内部节点时,根据键来查找适当的区间 for (int j = 0; j &lt; children_length; j++) &#123; if (j + 1 == children_length || less(key, children[j + 1].key)) &#123; // 找到适当的区间后,移动到下一层节点 x = children[j].next; height--; break; &#125; &#125; &#125; &#125; return null;&#125; 插入 插入操作也要先从根节点不断递归地查找到合适的区间,但需要注意一点,如果查找到的外部节点已经满了怎么办呢? 解决方法也很简单,我们允许被插入的节点暂时”溢出”,然后在递归调用自底向上不断地进行分裂.例如:当M为5时,根节点溢出为6-节点,只需要将它分裂为连接了两个3-节点的2-节点.即将一个M-的父节点k分裂为连接着两个(M / 2)-节点的(k + 1)-节点. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public void put(Key key, Value value) &#123; validateKey(key, "argument key to put() is null."); Node u = insert(root, key, value, height); N++; if (u == null) return; // need to split root Node t = new Node(2); t.children[0] = new Entry(root.children[0].key, null, root); t.children[1] = new Entry(u.children[0].key, null, u); root = t; height++;&#125;private Node insert(Node x, Key key, Value value, int height) &#123; int j; Entry t = new Entry(key, value, null); Entry[] children = x.children; int children_length = x.children_length; // external node if (height == 0) &#123; for (j = 0; j &lt; children_length; j++) &#123; if (less(key, children[j].key)) break; &#125; &#125; else &#123; // internal node for (j = 0; j &lt; children_length; j++) &#123; if (j + 1 == children_length || less(key, children[j + 1].key)) &#123; // 找到合适的区间后继续递归调用 Node u = insert(children[j++].next, key, value, height - 1); // 如果下一层没有进行过分裂操作,直接返回null if (u == null) return null; t.key = u.children[0].key; t.next = u; break; &#125; &#125; &#125; // 将j之后的元素全部右移(为了腾出j的插入位置) for (int i = children_length; i &gt; j; i--) &#123; children[i] = children[i - 1]; &#125; children[j] = t; x.children_length++; if (x.children_length &lt; M) return null; else return split(x); // 如果空间已满,进行分裂&#125; // 将x分裂为两个含有new_length对键的节点private Node split(Node x) &#123; int new_length = M / 2; Node t = new Node(new_length); x.children_length = new_length; for (int j = 0; j &lt; new_length; j++) t.children[j] = x.children[new_length + j]; return t;&#125; 参考文献 Algorithms, 4th Edition by Robert Sedgewick and Kevin Wayne B-tree - Wikipedia]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>Tree</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[揭秘HTTPS的"秘密"]]></title>
    <url>%2F2017%2F08%2F06%2F2017-08-06-DigestHttps%2F</url>
    <content type="text"><![CDATA[在说https之前,我们先了解一下http,以及为什么要使用https. http(Hyper Text Transfer Protocol)超文本传输协议是一种用于分布式、协作式和超媒体信息系统的应用层协议,它是TCP/IP的上层协议,同时它也是万维网(万维网不等同于互联网,它只是基于互联网的一个服务)的数据通信的基础. http协议是客户端浏览器与其他程序或Web服务器之间交互的应用层通讯协议.但它也有一个致命的缺点:http协议是明文传输协议,在传输信息的过程中并没有进行任何加密,通信的双方也没有任何的认证,这是非常不安全的,如果在通信过程中被中间人进行劫持、监听、篡改,会造成个人隐私泄露等严重的安全问题. 举一个现实中的例子来说,假设小李要给小张寄信,如果信件在运输的过程中没有任何安全保护,那么很可能会被邮递员(也就是中间人)窃取其中的内容,甚至于修改内容. https就是用于解决这样的安全问题的,它的全称为Hypertext Transfer Protocol Secure,它在http的基础上添加了SSL(安全套接字层)层来保证传输数据的安全问题. 本文作者为: SylvanasSun.转载请务必将下面这段话置于文章开头处(保留超链接).本文转发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/08/06/2017-08-06-DigestHttps/ https提供了端对端的加密,而且不仅对数据进行了加密,还对数据完整性提供了保护.不过在讲解https的加密方式之前,我们需要先了解一下加密算法. 对称加密 对称加密的基本思想是: 通信双方使用同一个密钥(或者是两个可以简单地互相推算的密钥)来对明文进行加密与解密. 常见的对称加密算法有DES、3DES、AES、Blowfish、IDEA、RC5、RC6. 对称加密看起来很美好,但是密钥要怎么发送过去呢?如果直接发送过去,被中间人截获了密钥岂不是白费工夫. 非对称加密 非对称加密也叫公开密钥加密,它使用了两个密钥,一个为公钥,一个为私钥,当一个用作于加密的时候,另一个则用作解密. 这两个密钥就算被其他人知道了其中一个也不能凭借它计算出另一个密钥,所以可以公开其中一个密钥(也就是公钥),不公开的密钥为私钥. 如果服务器想发送消息给客户端,只需要用客户端的公钥加密,然后客户端用它自己的私钥进行解密. 常见的非对称加密算法有RSA、DSA、ECDSA、 DH、ECDHE. 我们以DH算法为例,了解一下非对称加密的魅力. Alice要与Bob进行通信,他们协定了一组可以公开的质数$p=23$,$g=5$. Alice选择了一个不公开的秘密数$a=6$,并计算$A = {g^a} \; {mod} \; {p} = {5^6} \; {mod} \; {23} = 8$并发送给Bob. Bob选择了一个不公开的秘密数$b=15$,并计算$B = {g^b} \; {mod} \; {p} = {5^{15}} \; {mod} \; {23} = 19$并发送给Alice Alice 计算$S = {B^a} \; {mod} \; {p} = {19^6} \; {mod} \; {23} = 2$ Bob计算$S = {A^b} \; {mod} \; {p} = {8^{15}} \; {mod} \; {23} = 2$ Alice与Bob得到了同样的值,因此${g^{ab}} \; {mod} \; {p} = {g^{ba}} \; {mod} \; {p}$ 对称加密+非对称加密 尽管非对称加密如此奇妙,但它加解密的效率比对称加密要慢多了.那我们就将对称加密与非对称加密结合起来,取其精华,去其槽粕. 方法很简单,其中一方先自己生成一个对称加密密钥,然后通过非对称加密的方式来发送这个密钥,这样双方之后的通信就可以用对称加密这种高效率的算法进行加解密了. Certificate Authority 对称加密与非对称加密结合使用的方法虽然能够保证了通信过程的安全,但也引发了如下问题: 客户端要如何获取到服务器的公钥? 如果公钥在发送过程被中间人拦截,然后中间人发送自己的公钥给客户端,客户端该如何确认? 解决方法依是通过一个权威的CA(Certificate Authority)证书中心,它来负责颁发证书,这个证书包含了如下等内容: 证书的发布机构. 证书的有效期 公钥 证书所有人 数字签名 数字签名是用来验证数据完整性的,首先将公钥与个人信息用一个Hash算法生成一个消息摘要,Hash算法是不可逆的,且只要内容发生变化,那生成的消息摘要将会截然不同.然后CA再用它的私钥对消息摘要加密,最终形成数字签名. 当客户端接收到证书时,只需要用同样的Hash算法再次生成一个消息摘要,然后用CA的公钥对证书进行解密,之后再对比两个消息摘要就能知道数据有没有被篡改过了. 那么CA的公钥又要从哪里来呢?这似乎陷入了一个鸡生蛋,蛋生鸡的悖论,其实CA也有证书来证明自己,而且CA证书的信用体系就像一棵树的结构,上层节点是信用高的CA同时它也会对底层的CA做信用背书,操作系统中已经内置了一些根证书,所以相当于你已经自动信任了它们(需要注意误安装一些非法或不安全的证书). Https的交互过程 浏览器对服务器发送了一次请求. 服务器发送证书. 浏览器读取证书中的所有人,有效期等信息并进行校验. 浏览器查找操作系统中内置的已经信任的根证书,并对服务器发来的证书进行验证. 如果找不到,浏览器报错,服务器发来的证书是不可信任的. 如果找到,浏览器会从操作系统中取出CA的公钥,然后对服务器发来的证书中的数字签名进行解密. 浏览器使用相同的Hash算法计算出消息摘要,然后对数字签名中的消息摘要进行校对. 如果结果一致,证书合法. 之后浏览器就可以生成对称加密的密钥然后用非对称加密的方式发送给服务器,之后的通信就都是安全的了. 总结 现在国内外的大型网站基本都已经全站启用了Https,虽然相对于Http多了许多用于加密的流程,但为了数据的安全这点牺牲是必要的,Https也将是未来互联网的发展趋势. 参考文献 HTTPS - Wikipedia Public-key cryptography - Wikipedia Diffie–Hellman key exchange - Wikipedia 一个故事讲完https]]></content>
      <categories>
        <category>网络</category>
        <category>http</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>网络</tag>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的闭包之争]]></title>
    <url>%2F2017%2F07%2F30%2F2017-07-30-JavaClosure%2F</url>
    <content type="text"><![CDATA[闭包一直都是Java社区中争论不断的话题,很多语言例如JavaScript,Ruby,Python等都支持闭包这个语言特性,闭包功能强大且灵活,Java并没有显式地支持它,但其实Java中也存在着所谓的”闭包”. 本文作者为: SylvanasSun.转载请务必将下面这段话置于文章开头处(保留超链接).本文转发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/07/30/2017-07-30-JavaClosure/ 闭包 定义一个闭包的要点如下: 一个依赖于外部环境的自由变量的函数. 这个函数能够访问外部环境的自由变量. 也就是说,外部环境持有内部函数所依赖的自由变量,由此对内部函数形成了闭包. 自由变量 那么什么是自由变量呢?自由变量就是在函数自身作用域之外的变量,一个函数$f(x) = x + y$,其中y就是自由变量,它并不是这个函数自身的自变量,而是通过外部环境提供的. 下面以JavaScript的一个闭包为例: 12345function Add(y) &#123; return function(x) &#123; return x + y; &#125;&#125; 对于内部函数function(x)来说,y就是自由变量.而y是函数Add(y)内的参数,所以Add(y)对内部函数function(x)形成了一个闭包. 这个闭包将自由变量y与内部函数绑定在了一起,也就是说,当Add(y)函数执行完毕后,它不会随着函数调用结束后被回收(不能在栈上分配空间). 12var add_function = Add(5); // 这时y=5,并且与返回的内部函数绑定在了一起var result = add_function(10); // x=10,返回最终的结果 10 + 5 = 15 Java中的闭包 Java与JavaScript又或者其他支持闭包的语言不同,它是一个基于类的面向对象语言,也就是说一个方法所用到的自由变量永远都来自于其所在类的实例的. 1234567class AddUtils &#123; private int y = 5; public int add(int x) &#123; retrun x + y; &#125;&#125; 这样一个方法add(x)拥有一个参数x与一个自由变量y,它的返回值也依赖于这个自由变量y.add(x)想要正常工作的话,就必须依赖于AddUtils类的一个实例,不然它无法知道自由变量y的值是多少,也就是自由变量未与add(x)进行绑定. 严格上来说,add(x)中的自由变量应该为this,这是因为y也是通过this关键字来访问的. 所以说,在Java中闭包其实无处不在,只不过我们难以发现而已.但面向对象的语言一般都不把类叫成闭包,这是一种习惯. Java中的内部类就是一种典型的闭包结构. 123456789101112public class Outer &#123; private int y = 5; private class Inner &#123; private int x = 10; public int add() &#123; return x + y; &#125; &#125;&#125; 内部类通过一个指向外部类的引用来访问外部环境中的自由变量,由此形成了一个闭包. 匿名内部类 12345678910111213141516public interface AnonInner() &#123; int add();&#125;public class Outer &#123; public AnonInner getAnonInner(final int x) &#123; final int y = 5; return new AnonInner() &#123; public int add() &#123; return x + y; &#125; &#125; &#125;&#125; getAnonInner(x)方法返回了一个匿名内部类AnonInner,匿名内部类不能显式地声明构造函数,也不能对构造函数传参,且返回的是一个AnonInner接口,但它的add()方法实现中用到了两个自由变量(x与y),也就是说外部方法getAnonInner(x)对这个匿名内部类构成了闭包. 但我们发现自由变量都被加上了final修饰符,这是因为Java对闭包支持的不完整导致的. 对于自由变量的捕获策略有以下两种: capture-by-value: 只需要在创建闭包的地方把捕获的值拷贝一份到对象里即可.Java的匿名内部类和Java 8新的lambda表达式都是这样实现的. capture-by-reference: 把被捕获的局部变量“提升”（hoist）到对象里.C#的匿名函数(匿名委托/lambda表达式)就是这样实现的. Java只实现了capture-by-value,但又没有对外说明这一点,为了以后能进一步扩展成支持capture-by-reference留后路,所以干脆就不允许向被捕获的变量赋值,所以这些自由变量需要强制加上final修饰符(在Jdk8中似乎已经没有这种强制限制了). 参考文献 Java theory and practice: The closures debate 关于对象与闭包的关系的一个有趣小故事 JVM的规范中允许编程语言语义中创建闭包(closure)吗？ - 知乎 为什么Java闭包不能通过返回值之外的方式向外传递值？ - 知乎]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的那点事儿(4)-加权有向图]]></title>
    <url>%2F2017%2F07%2F27%2F2017-07-27-Graph_WeightedDigraph%2F</url>
    <content type="text"><![CDATA[本文作者为: SylvanasSun.转载请务必将下面这段话置于文章开头处(保留超链接).本文转发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/07/27/2017-07-27-Graph_WeightedDigraph 加权有向图 有向图的实现比无向图更加简单,要实现加权有向图只需要在上一章讲到的加权无向图的实现修改一下即可. DirectedEdge 由于有向图的边都是带有方向的,所以下面这个实现提供了from()与to()函数,用于获取代表v-&gt;w的两个顶点. 123456789101112131415161718192021222324252627282930313233343536373839404142public class DirectedEdge &#123; private final int v; private final int w; private final double weight; public DirectedEdge(int v, int w, double weight) &#123; validateVertexes(v, w); if (Double.isNaN(weight)) throw new IllegalArgumentException("Weight " + weight + " is NaN!"); this.v = v; this.w = w; this.weight = weight; &#125; public int from() &#123; return v; &#125; public int to() &#123; return w; &#125; public double weight() &#123; return weight; &#125; public String toString() &#123; return v + "-&gt;" + w + " " + String.format("%5.2f", weight); &#125; private void validateVertexes(int... vertexes) &#123; for (int i = 0; i &lt; vertexes.length; i++) &#123; if (vertexes[i] &lt; 0) throw new IllegalArgumentException("Vertex " + vertexes[i] + " must be positive number!"); &#125; &#125;&#125; EdgeWeightedDigraph 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113public class EdgeWeightedDigraph &#123; private static final String NEWLINE = System.getProperty("line.separator"); // number of vertices in this digraph private final int vertex; // number of edges in this digraph private int edge; // adj[v] = adjacency list for vertex v private Bag&lt;DirectedEdge&gt;[] adj; // indegree[v] = indegree of vertex v private int[] indegree; public EdgeWeightedDigraph(int vertex) &#123; String message = String.format("Vertex %d must be positive number!", vertex); validatePositiveNumber(message, vertex); this.vertex = vertex; this.edge = 0; this.indegree = new int[vertex]; this.adj = (Bag&lt;DirectedEdge&gt;[]) new Bag[vertex]; for (int v = 0; v &lt; vertex; v++) adj[v] = new Bag&lt;&gt;(); &#125; public EdgeWeightedDigraph(Scanner scanner) &#123; this(scanner.nextInt()); int edge = scanner.nextInt(); String message = String.format("Edge %d must be positive number!", edge); validatePositiveNumber(message, edge); for (int i = 0; i &lt; edge; i++) &#123; int v = scanner.nextInt(); int w = scanner.nextInt(); validateVertex(v); validateVertex(w); double weight = scanner.nextDouble(); addEdge(new DirectedEdge(v, w, weight)); &#125; &#125; public int vertex() &#123; return vertex; &#125; public int edge() &#123; return edge; &#125; public void addEdge(DirectedEdge e) &#123; int v = e.from(); int w = e.to(); validateVertex(v); validateVertex(w); adj[v].add(e); indegree[w]++; edge++; &#125; public Iterable&lt;DirectedEdge&gt; adj(int v) &#123; validateVertex(v); return adj[v]; &#125; public int outdegree(int v) &#123; validateVertex(v); return adj[v].size(); &#125; public int indegree(int v) &#123; validateVertex(v); return indegree[v]; &#125; // 在有向图中每条边只会出现一次 // 遍历边集不需要在无向图里那样为了消除重复边而进行复杂的判断 public Iterable&lt;DirectedEdge&gt; edges() &#123; Bag&lt;DirectedEdge&gt; list = new Bag&lt;DirectedEdge&gt;(); for (int v = 0; v &lt; vertex; v++) &#123; for (DirectedEdge e : adj(v)) &#123; list.add(e); &#125; &#125; return list; &#125; public String toString() &#123; StringBuilder s = new StringBuilder(); s.append(vertex + " " + edge + NEWLINE); for (int v = 0; v &lt; vertex; v++) &#123; s.append(v + ": "); for (DirectedEdge e : adj[v]) &#123; s.append(e + " "); &#125; s.append(NEWLINE); &#125; return s.toString(); &#125; private void validatePositiveNumber(String message, int... numbers) &#123; for (int i = 0; i &lt; numbers.length; i++) &#123; if (numbers[i] &lt; 0) throw new IllegalArgumentException(message); &#125; &#125;&#125; 加权有向图的实现与加权无向图区别不大,而且因为有向图中的边只会出现一次,实现代码要比无向图更简单. 本文中的所有完整代码请到我的GitHub中查看 最短路径 “找到一个顶点到达另一个顶点之间的最短路径“是图论研究中的经典算法问题.在加权有向图中,每条有向路径都有一个与之对应的路径权重(路径中所有边的权重之和),要找到一条最短路径其实就是找到路径权重最小的那条路径. 单点最短路径 “从s到目的地v是否存在一条有向路径,如果有,找出最短的那条路径”.类似这样的问题就是单点最短路径问题,它是我们主要研究的问题. 单点最短路径的结果是一棵最短路径树,它是图的一幅子图,包含了从起点到所有可达顶点的最短路径. 从起点到一个顶点可能存在两条长度相等的路径,如果出现这种情况,可以删除其中一条路径的最后一条边,直到从起点到每个顶点都只有一条路径相连. 最短路径的数据结构 要实现最短路径的算法还需要借助以下数据结构: edgeTo[]: 一个由顶点索引的DirectedEdge对象的父链接数组,其中edgeTo[v]的值为树中连接v和它的父节点的边. distTo[]: 一个由顶点索引的double数组,其中distTo[v]代表从起点到v的已知最短路径的长度. 初始化时,edgeTo[s]的值为null(s为起点),distTo[s]的值为0.0,从s到不可达的顶点距离为Double.POSITIVE_INFINITY. 让边松弛 最短路径算法都基于松弛(Relaxation)操作,它在遇到新的边时,通过更新这些信息就可以得到新的最短路径. 假设对边v-&gt;w进行松弛操作,意味着要先检查从s到w的最短路径是否是先从s到v,然后再由v到w(也就是说v-&gt;w是更短的一条路径),如果是,那么就进行更新.由v到达w的最短路径是distTo[v]与e.weight()之和,如果这个值大于distTo[w],称这条边松弛失败,并将它忽略. 松弛操作就像用一根橡皮筋沿着连续两个顶点的路径紧紧展开,放松一条边就像将这条橡皮筋转移到另一条更短的路径上,从而缓解橡皮筋的压力. 1234567891011121314151617181920// 松弛一条边private void relax(DirectedEdge e) &#123; int v = e.from(), w = e.to(); // 如果s-&gt;v-&gt;w的路径更小则进行更新 if (distTo[w] &gt; distTo[v] + e.weight()) &#123; distTo[w] = distTo[v] + e.weight(); edgeTo[w] = e; &#125;&#125;// 松弛一个顶点的所有邻接边private void relax(EdgeWeightedDigraph G, int v) &#123; for (DirectedEdge e : G.adj(v)) &#123; int w = e.to(); if (distTo[w] &gt; distTo[v] + e.weight()) &#123; distTo[w] = distTo[v] + e.weight(); edgeTo[w] = e; &#125; &#125;&#125; Dijkstra算法 Dijkstra算法类似于Prim算法,它将distTo[s]初始化为0.0,distTo[]中的其他元素初始化为Double.POSITIVE_INFINITY.然后将distTo[]中最小的非树顶点放松并加入树中,一直重复直到所有的顶点都在树中或者所有的非树顶点的distTo[]值均为Double.POSITIVE_INFINITY. Dijkstra算法与Prim算法都是用添加边的方式构造一棵树: Prim算法每次添加的是距离树最近的非树顶点. Dijkstra算法每次添加的都是离起点最近的非树顶点. 从上述的步骤我们就能看出,Dijkstra算法需要一个优先队列(也可以用斐波那契堆)来保存需要被放松的顶点并确认下一个被放松的顶点(也就是取出最小的). 如此简单的Dijkstra算法也有其缺点,那就是它只适用于解决权重非负的图. 实现代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public class DijkstraSP &#123; // distTo[v] = distance of shortest s -&gt; v path private double[] distTo; // edgeTo[v] = last edge on shortest s - &gt; v path private DirectedEdge[] edgeTo; // priority queue of vertices private IndexMinPQ&lt;Double&gt; pq; public DijkstraSP(EdgeWeightedDigraph digraph, int s) &#123; validateNegativeWeight(digraph); int vertex = digraph.vertex(); this.distTo = new double[vertex]; this.edgeTo = new DirectedEdge[vertex]; validateVertex(s); for (int v = 0; v &lt; vertex; v++) distTo[v] = Double.POSITIVE_INFINITY; distTo[s] = 0.0; // 将起点放入索引优先队列,并不断地进行松弛 pq = new IndexMinPQ&lt;&gt;(vertex); pq.insert(s, distTo[s]); while (!pq.isEmpty()) &#123; int v = pq.delMin(); // 对权值最小的非树顶点的所有邻接边集进行松弛操作 for (DirectedEdge e : digraph.adj(v)) relax(e); &#125; &#125; // relax edge e and update pq if changed private void relax(DirectedEdge e) &#123; int v = e.from(), w = e.to(); // s -&gt; v -&gt; w的权重 double weight = distTo[v] + e.weight(); if (distTo[w] &gt; weight) &#123; distTo[w] = weight; edgeTo[w] = e; if (pq.contains(w)) pq.decreaseKey(w, weight); else pq.insert(w, weight); &#125; &#125; private void validateNegativeWeight(EdgeWeightedDigraph digraph) &#123; for (DirectedEdge e : digraph.edges()) &#123; if (e.weight() &lt; 0) throw new IllegalArgumentException("Edge " + e + " has negative weight."); &#125; &#125; public double distTo(int v) &#123; validateVertex(v); return distTo[v]; &#125; public boolean hasPathTo(int v) &#123; validateVertex(v); return distTo[v] &lt; Double.POSITIVE_INFINITY; &#125; public Iterable&lt;DirectedEdge&gt; pathTo(int v) &#123; validateVertex(v); if (!hasPathTo(v)) return null; Stack&lt;DirectedEdge&gt; path = new Stack&lt;DirectedEdge&gt;(); for (DirectedEdge e = edgeTo[v]; e != null; e = edgeTo[e.from()]) &#123; path.push(e); &#125; return path; &#125; private void validateVertex(int v) &#123; int V = distTo.length; if (v &lt; 0 || v &gt;= V) throw new IllegalArgumentException("vertex " + v + " is not between 0 and " + (V - 1)); &#125;&#125; 上述的代码也可以用于处理加权无向图,但需要修改传入的对象类型.不管是无向图还是有向图它们对于最短路径问题是等价的. 无环加权有向图中的最短路径算法 如果是处理无环图的情况下,还会有一种比Dijkstra算法更快、更简单的算法.它的特点如下: 能够处理负权重的边. 能够在线性时间内解决单点最短路径问题. 在已知是一张无环图的情况下,它是找出最短路径效率最高的方法. 实现比Dijkstra算法更简单. 只需要将所有顶点按照拓扑排序的顺序来松弛边,就可以得到这个简单高效的算法. 12345678910111213141516171819202122232425262728293031323334353637383940public class AcyclicSP &#123; // distTo[v] = distance of shortest s-&gt;v path private double[] distTo; // edgeTo[v] = last edge on shortest s-&gt;v path private DirectedEdge[] edgeTo; public AcyclicSP(EdgeWeightedDigraph digraph, int s) &#123; int vertex = digraph.vertex(); distTo = new double[vertex]; edgeTo = new DirectedEdge[vertex]; validateVertex(s); for (int v = 0; v &lt; vertex; v++) distTo[v] = Double.POSITIVE_INFINITY; distTo[s] = 0.0; Topological topological = new Topological(digraph); if (!topological.hasOrder()) throw new IllegalArgumentException("Digraph is not acyclic."); // 按照拓扑排序的顺序进行放松操作 for (int v : topological.order()) &#123; for (DirectedEdge e : digraph.adj(v)) relax(e); &#125; &#125; private void relax(DirectedEdge e) &#123; int v = e.from(), w = e.to(); double weight = distTo[v] + e.weight(); if (distTo[w] &gt; weight) &#123; distTo[w] = weight; edgeTo[w] = e; &#125; &#125; &#125; 最长路径 要想找出一条最长路径,只需要把distTo[]的初始化变为Double.NEGATIVE_INFINITY,并更改relax()函数中的不等式的方向. 12345678910111213141516171819202122232425262728 public AcyclicLP(EdgeWeightedDigraph G, int s) &#123; distTo = new double[G.vertex()]; edgeTo = new DirectedEdge[G.vertex()]; validateVertex(s);// 全部初始化为负无穷 for (int v = 0; v &lt; G.vertex(); v++) distTo[v] = Double.NEGATIVE_INFINITY; distTo[s] = 0.0; Topological topological = new Topological(G); if (!topological.hasOrder()) throw new IllegalArgumentException("Digraph is not acyclic."); for (int v : topological.order()) &#123; for (DirectedEdge e : G.adj(v)) relax(e); &#125; &#125; private void relax(DirectedEdge e) &#123; int v = e.from(), w = e.to();// 改变不等式的方向 if (distTo[w] &lt; distTo[v] + e.weight()) &#123; distTo[w] = distTo[v] + e.weight(); edgeTo[w] = e; &#125; &#125; Bellman-Ford算法 我们已经知道了处理权重非负图的Dijkstra算法与处理无环图的算法,但如果遇见既含有环,权重也是负数的加权有向图该怎么办? Bellman-Ford算法就是用于处理有环且含有负权重的加权有向图的,它的原理是对图进行V-1次松弛操作,得到所有可能的最短路径. 要实现Bellman-Ford算法还需要以下数据结构: 队列: 用于保存即将被松弛的顶点. 布尔值数组: 用来标记该顶点是否已经存在于队列中,以防止重复插入. 我们将起点放入队列中,然后进入一个循环,每次循环都会从队列中取出一个顶点并对其进行松弛.为了保证算法在V轮后能够终止,需要能够动态地检测是否存在负权重环,如果找到了这个环则结束运行(也可以用一个变量动态记录轮数). 负权重环的检测 如果存在了一个从起点可达的负权重环,那么队列就永远不可能为空,为了从这个无尽的循环中解脱出来,算法需要能够动态地检测负权重环. Bellman-Ford算法也使用了edgeTo[]来存放最短路径树中的每一条边,我们根据edgeTo[]来复制一幅图并在该图中检测环. 1234567891011 private void findNegativeCycle() &#123; int V = edgeTo.length;// 根据edgeTo[]来创建一幅加权有向图 EdgeWeightedDigraph spt = new EdgeWeightedDigraph(V); for (int v = 0; v &lt; V; v++) if (edgeTo[v] != null) spt.addEdge(edgeTo[v]);// 判断该图有没有环 EdgeWeightedDirectedCycle finder = new EdgeWeightedDirectedCycle(spt); cycle = finder.cycle(); &#125; 实现代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class BellmanFordSP &#123; private double[] distTo; private DirectedEdge[] edgeTo; // 用于标记顶点是否在队列中 private boolean[] onQueue; // 存放下次进行松弛操作的顶点的队列 private Queue&lt;Integer&gt; queue; // 计算松弛操作的轮数 private int cost; // 负权重环 private Iterable&lt;DirectedEdge&gt; cycle; public BellmanFordSP(EdgeWeightedDigraph digraph, int s) &#123; int vertex = digraph.vertex(); this.distTo = new double[vertex]; this.edgeTo = new DirectedEdge[vertex]; this.onQueue = new boolean[vertex]; for (int v = 0; v &lt; vertex; v++) distTo[v] = Double.POSITIVE_INFINITY; distTo[s] = 0.0; // Bellman-Ford algorithm queue = new ArrayDeque&lt;&gt;(); queue.add(s); // 将起点放入队列 onQueue[s] = true; // 标记起点已在队列中 // 当队列为空时或者发现负权重环时结束循环 while (!queue.isEmpty() &amp;&amp; !hasNegativeCycle()) &#123; int v = queue.poll(); onQueue[v] = false; relax(digraph, v); &#125; &#125; private void relax(EdgeWeightedDigraph G, int v) &#123; for (DirectedEdge e : G.adj(v)) &#123; int w = e.to(); double weight = distTo[v] + e.weight(); if (distTo[w] &gt; weight) &#123; distTo[w] = weight; edgeTo[w] = e; // 将不在队列中的顶点w加到队列 if (!onQueue[w]) &#123; queue.add(w); onQueue[w] = true; &#125; &#125; // 动态检测负权重环, if (cost++ % G.vertex() == 0) &#123; findNegativeCycle(); if (hasNegativeCycle()) return; // found a negative cycle &#125; &#125; &#125;&#125; 总结 解决最短路径问题一直都是图论的经典问题,本文中介绍的算法适用于不同的环境,在应用中应该根据不同的环境选择不同的算法. 算法 局限性 路径长度的比较次数(增长的数量级) 空间复杂度 优势 Dijkstra 只能处理正权重 ElogV V 最坏情况下仍有较好的性能 拓扑排序 只适用于无环图 E+V V 实现简单,是无环图情况下的最优算法 Bellman-Ford 不能存在负权重环 E+V,最坏情况为VE V 适用广泛 参考文献 Algorithms, 4th Edition by Robert Sedgewick and Kevin Wayne Dijkstra’s algorithm - Wikipedia Bellman–Ford algorithm - Wikipedia 图的那点事儿 图的那点事儿(1)-无向图 图的那点事儿(2)-有向图 图的那点事儿(3)-加权无向图 图的那点事儿(4)-加权有向图]]></content>
      <categories>
        <category>Algorithms</category>
        <category>Graph</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的那点事儿(3)-加权无向图]]></title>
    <url>%2F2017%2F07%2F25%2F2017-07-25-Graph_WeightedUndirectedGraph%2F</url>
    <content type="text"><![CDATA[本文作者为: SylvanasSun.转载请务必将下面这段话置于文章开头处(保留超链接).本文转发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/07/25/2017-07-25-Graph_WeightedUndirectedGraph/ 加权无向图 所谓加权图,即每条边上都有着对应的权重,这个权重是正数也可以是负数,也不一定会和距离成正比.加权无向图的表示方法只需要对无向图的实现进行一下扩展. 在使用邻接矩阵的方法中,可以用边的权重代替布尔值来作为矩阵的元素. 在使用邻接表 的方法中,可以在链表的节点中添加一个权重域. 在使用邻接表的方法中,将边抽象为一个Edge类,它包含了相连的两个顶点和它们的权重,链表中的每个元素都是一个Edge. 我们使用第三种方法来实现加权无向图,它的数据表示如下图: Edge的实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class Edge implements Comparable&lt;Edge&gt; &#123; private final int v; private final int w; private final double weight; public Edge(int v, int w, double weight) &#123; validateVertexes(v, w); if (Double.isNaN(weight)) throw new IllegalArgumentException("Weight is NaN."); this.v = v; this.w = w; this.weight = weight; &#125; private void validateVertexes(int... vertexes) &#123; for (int i = 0; i &lt; vertexes.length; i++) &#123; if (vertexes[i] &lt; 0) &#123; throw new IllegalArgumentException( String.format("Vertex %d must be a nonnegative integer.", vertexes[i])); &#125; &#125; &#125; public double weight() &#123; return weight; &#125; public int either() &#123; return v; &#125; public int other(int vertex) &#123; if (vertex == v) return w; else if (vertex == w) return v; else throw new IllegalArgumentException("Illegal endpoint."); &#125; @Override public int compareTo(Edge that) &#123; return Double.compare(this.weight, that.weight); &#125; @Override public String toString() &#123; return String.format("%d-%d %.5f", v, w, weight); &#125;&#125; Edge类提供了either()与other()两个函数,在两个顶点都未知的情况下,可以调用either()获得顶点v,然后再调用other(v)来获得另一个顶点. 本文中的所有完整代码点我查看 加权无向图的实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116public class EdgeWeightedGraph &#123; private static final String NEWLINE = System.getProperty("line.separator"); private final int vertexes; private int edges; private Bag&lt;Edge&gt;[] adj; public EdgeWeightedGraph(int vertexes) &#123; validateVertexes(vertexes); this.vertexes = vertexes; this.edges = 0; adj = (Bag&lt;Edge&gt;[]) new Bag[vertexes]; for (int i = 0; i &lt; vertexes; i++) adj[i] = new Bag&lt;&gt;(); &#125; public EdgeWeightedGraph(Scanner scanner) &#123; this(scanner.nextInt()); int edges = scanner.nextInt(); validateEdges(edges); for (int i = 0; i &lt; edges; i++) &#123; int v = scanner.nextInt(); int w = scanner.nextInt(); double weight = scanner.nextDouble(); addEdge(new Edge(v, w, weight)); &#125; &#125; public int vertex() &#123; return vertexes; &#125; public int edge() &#123; return edges; &#125; public void addEdge(Edge e) &#123; int v = e.either(); int w = e.other(v); validateVertex(v); validateVertex(w); adj[v].add(e); adj[w].add(e); edges++; &#125; public Iterable&lt;Edge&gt; adj(int v) &#123; validateVertex(v); return adj[v]; &#125; public int degree(int v) &#123; validateVertex(v); return adj[v].size(); &#125; public Iterable&lt;Edge&gt; edges() &#123; Bag&lt;Edge&gt; list = new Bag&lt;Edge&gt;(); for (int v = 0; v &lt; vertexes; v++) &#123; int selfLoops = 0; for (Edge e : adj(v)) &#123; // 只添加一条边 if (e.other(v) &gt; v) &#123; list.add(e); &#125; // 只添加一条自环的边 else if (e.other(v) == v) &#123; if (selfLoops % 2 == 0) list.add(e); selfLoops++; &#125; &#125; &#125; return list; &#125; public String toString() &#123; StringBuilder s = new StringBuilder(); s.append(vertexes + " " + edges + NEWLINE); for (int v = 0; v &lt; vertexes; v++) &#123; s.append(v + ": "); for (Edge e : adj[v]) &#123; s.append(e + " "); &#125; s.append(NEWLINE); &#125; return s.toString(); &#125; private void validateVertexes(int... vertexes) &#123; for (int i = 0; i &lt; vertexes.length; i++) &#123; if (vertexes[i] &lt; 0) &#123; throw new IllegalArgumentException( String.format("Vertex %d must be a nonnegative integer.", vertexes[i])); &#125; &#125; &#125; private void validateEdges(int edges) &#123; if (edges &lt; 0) throw new IllegalArgumentException("Number of edges must be nonnegative."); &#125; // throw an IllegalArgumentException unless &#123;@code 0 &lt;= v &lt; V&#125; private void validateVertex(int v) &#123; if (v &lt; 0 || v &gt;= vertexes) throw new IllegalArgumentException("vertex " + v + " is not between 0 and " + (vertexes - 1)); &#125;&#125; 上述代码是对无向图的扩展,它将邻接表中的元素从整数变为了Edge,函数edges()返回了边的集合,由于是无向图所以每条边会出现两次,需要注意处理. 加权无向图的实现还拥有以下特点: 边的比较: Edge类实现了Comparable接口,它使用了权重来比较两条边的大小,所以加权无向图的自然次序就是权重次序. 自环: 该实现允许存在自环,并且edges()函数中对自环边进行了记录. 平行边: 该实现允许存在平行边,但可以用更复杂的方法来消除平行边,例如只保留平行边中的权重最小者. 最小生成树 最小生成树是加权无向图的重要应用.图的生成树是它的一棵含有其所有顶点的无环连通子图,最小生成树是它的一棵权值(所有边的权值之和)最小的生成树. 在给定的一幅加权无向图$G = (V,E)$中,$(u,v)$代表连接顶点u与顶点v的边,也就是$(u,v) \in E$,而$w(u,v)$代表这条边的权重,若存在T为E的子集,也就是$T \subseteq E$,且为无环图,使得$w(T) = \sum_{(u,v) \in T}w(u,v)$ 的 $w(T)$ 最小,则T为G的最小生成树. 最小生成树在一些情况下可能会存在多个,例如,给定一幅图G,当它的所有边的权重都相同时,那么G的所有生成树都是最小生成树,当所有边的权重互不相同时,将会只有一个最小生成树. 切分定理 切分定理将图中的所有顶点切分为两个集合(两个非空且不重叠的集合),检查两个集合的所有边并识别哪条边应属于图的最小生成树. 一种比较简单的切分方法即通过指定一个顶点集并隐式地认为它的补集为另一个顶点集来指定一个切分. 切分定理也表明了对于每一种切分,权重最小的横切边(一条连接两个属于不同集合的顶点的边)必然属于最小生成树. 切分定理是解决最小生成树问题的所有算法的基础,使用切分定理找到最小生成树的一条边,不断重复直到找到最小生成树的所有边. 这些算法可以说都是贪心算法,算法的每一步都是在找最优解(权值最小的横切边),而解决最小生成树的各种算法不同之处仅在于保存切分和判定权重最小的横切边的方式. Prim算法 Prim算法是用于解决最小生成树的算法之一,算法的每一步都会为一棵生长中的树添加一条边.一开始这棵树只有一个顶点,然后会一直添加到$V - 1$条边,每次总是将下一条连接树中的顶点与不在树中的顶点且权重最小的边加入到树中(也就是由树中顶点所定义的切分中的一条横切边). 实现Prim算法还需要借助以下数据结构: 布尔值数组: 用于记录顶点是否已在树中. 队列: 使用一条队列来保存最小生成树中的边,也可以使用一个由顶点索引的Edge对象的数组. 优先队列: 优先队列用于保存横切边,优先队列的性质可以每次取出权值最小的横切边. 延时实现 当我们连接新加入树中的顶点与其他已经在树中顶点的所有边都失效了(由于两个顶点都已在树中,所以这是一条失效的横切边).我们需要处理这种情况,即使实现对无效边采取忽略(不加入到优先队列中),而延时实现会把无效边留在优先队列中,等到要删除优先队列中的数据时再进行有效性检查. 上图为Prim算法延时实现的轨迹图,它的步骤如下: 将顶点0添加到最小生成树中,将它的邻接表中的所有边添加到优先队列中(将横切边添加到优先队列). 将顶点7和边0-7添加到最小生成树中,将顶点的邻接表中的所有边添加到优先队列中. 将顶点1和边1-7添加到最小生成树中,将顶点的邻接表中的所有边添加到优先队列中. 将顶点2和边0-2添加到最小生成树中,将边2-3和6-2添加到优先队列中,边2-7和1-2失效. 将顶点3和边2-3添加到最小生成树中,将边3-6添加到优先队列之中,边1-3失效. 将顶点5和边5-7添加到最小生成树中,将边4-5添加到优先队列中,边1-5失效. 从优先队列中删除失效边1-3,1-5,2-7. 将顶点4和边4-5添加到最小生成树中,将边6-4添加到优先队列中,边4-7,0-4失效. 从优先队列中删除失效边1-2,4-7,0-4. 将顶点6和边6-2添加到最小生成树中,和顶点6关联的其他边失效. 在添加V个顶点与V - 1条边之后,最小生成树就构造完成了,优先队列中剩余的边都为失效边. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class LazyPrimMST &#123; private final EdgeWeightedGraph graph; // 记录最小生成树的总权重 private double weight; // 存储最小生成树的边 private final Queue&lt;Edge&gt; mst; // 标记这个顶点在树中 private final boolean[] marked; // 存储横切边的优先队列 private final PriorityQueue&lt;Edge&gt; pq; public LazyPrimMST(EdgeWeightedGraph graph) &#123; this.graph = graph; int vertex = graph.vertex(); mst = new ArrayDeque&lt;&gt;(); pq = new PriorityQueue&lt;&gt;(); marked = new boolean[vertex]; for (int v = 0; v &lt; vertex; v++) if (!marked[v]) prim(v); &#125; private void prim(int s) &#123; scanAndPushPQ(s); while (!pq.isEmpty()) &#123; Edge edge = pq.poll(); // 取出权重最小的横切边 int v = edge.either(), w = edge.other(v); assert marked[v] || marked[w]; if (marked[v] &amp;&amp; marked[w]) continue; // 忽略失效边 mst.add(edge); // 添加边到最小生成树中 weight += edge.weight(); // 更新总权重 // 继续将非树顶点加入到树中并更新横切边 if (!marked[v]) scanAndPushPQ(v); if (!marked[w]) scanAndPushPQ(w); &#125; &#125; // 标记顶点到树中,并且添加横切边到优先队列 private void scanAndPushPQ(int v) &#123; assert !marked[v]; marked[v] = true; for (Edge e : graph.adj(v)) if (!marked[e.other(v)]) pq.add(e); &#125; public Iterable&lt;Edge&gt; edges() &#123; return mst; &#125; public double weight() &#123; return weight; &#125;&#125; 即时实现 在即时实现中,将v添加到树中时,对于每个非树顶点w,不需要在优先队列中保存所有从w到树顶点的边,而只需要保存其中权重最小的边,所以在将v添加到树中后,要检查是否需要更新这条权重最小的边(如果v-w的权重更小的话). 也可以认为只会在优先队列中保存每个非树顶点w的一条边(也是权重最小的那条边),将w和树顶点连接起来的其他权重较大的边迟早都会失效,所以没必要在优先队列中保存它们. 要实现即时版的Prim算法,需要使用两个顶点索引的数组edgeTo[]和distTo[]与一个索引优先队列,它们具有以下性质: 如果顶点v不在树中但至少含有一条边和树相连,那么edgeTo[v]是将v和树连接的最短边,distTo[v]为这条边的权重. 所有这类顶点v都保存在索引优先队列中,索引v关联的值是edgeTo[v]的边的权重. 索引优先队列中的最小键即是权重最小的横切边的权重,而和它相关联的顶点v就是下一个将要被添加到树中的顶点. 将顶点0添加到最小生成树之中,将它的邻接表中的所有边添加到优先队列中(这些边是目前唯一已知的横切边). 将顶点7和边0-7添加到最小生成树,将边1-7和5-7添加到优先队列中,将连接顶点4与树的最小边由0-4替换为4-7. 将顶点1和边1-7添加到最小生成树,将边1-3添加到优先队列. 将顶点2和边0-2添加到最小生成树,将连接顶点6与树的最小边由0-6替换为6-2,将连接顶点3与树的最小边由1-3替换为2-3. 将顶点3和边2-3添加到最小生成树. 将顶点5和边5-7添加到最小生成树,将连接顶点4与树的最小边4-7替换为4-5. 将顶点4和边4-5添加到最小生成树. 将顶点6和边6-2添加到最小生成树. 在添加了V - 1条边之后,最小生成树构造完成并且优先队列为空. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class PrimMST &#123; private final EdgeWeightedGraph graph; // 存放最小生成树中的边 private final Edge[] edgeTo; // 每条边对应的权重 private final double[] distTo; private final boolean[] marked; private final IndexMinPQ&lt;Double&gt; pq; public PrimMST(EdgeWeightedGraph graph) &#123; this.graph = graph; int vertex = graph.vertex(); this.edgeTo = new Edge[vertex]; this.marked = new boolean[vertex]; this.pq = new IndexMinPQ&lt;&gt;(vertex); this.distTo = new double[vertex]; // 将权重数组初始化为无穷大 for (int i = 0; i &lt; vertex; i++) distTo[i] = Double.POSITIVE_INFINITY; for (int v = 0; v &lt; vertex; v++) if (!marked[v]) prim(v); &#125; private void prim(int s) &#123; // 将起点设为0.0并加入到优先队列 distTo[s] = 0.0; pq.insert(s, distTo[s]); while (!pq.isEmpty()) &#123; // 取出权重最小的边,优先队列中存的顶点是与树相连的非树顶点, // 同时它也是下一次要加入到树中的顶点 int v = pq.delMin(); scan(v); &#125; &#125; private void scan(int v) &#123; // 将顶点加入到树中 marked[v] = true; for (Edge e : graph.adj(v)) &#123; int w = e.other(v); // 忽略失效边 if (marked[w]) continue; // 如果w与连接树顶点的边的权重小于其他w连接树顶点的边 // 则进行替换更新 if (e.weight() &lt; distTo[w]) &#123; distTo[w] = e.weight(); edgeTo[w] = e; if (pq.contains(w)) pq.decreaseKey(w, distTo[w]); else pq.insert(w, distTo[w]); &#125; &#125; &#125; public Iterable&lt;Edge&gt; edges() &#123; Queue&lt;Edge&gt; mst = new ArrayDeque&lt;&gt;(); for (int v = 0; v &lt; edgeTo.length; v++) &#123; Edge e = edgeTo[v]; if (e != null) &#123; mst.add(e); &#125; &#125; return mst; &#125; public double weight() &#123; double weight = 0.0; for (Edge e : edges()) weight += e.weight(); return weight; &#125;&#125; 不管是延迟实现还是即时实现,Prim算法的规律就是: 在树的生长过程中,都是通过连接一个和新加入的顶点相邻的顶点.当新加入的顶点周围没有非树顶点时,树的生长又会从另一部分开始. Kruskal算法 Kruskal算法的思想是按照边的权重顺序由小到大处理它们,将边添加到最小生成树,加入的边不会与已经在树中的边构成环,直到树中含有V - 1条边为止.这些边会逐渐由一片森林合并为一棵树,也就是我们需要的最小生成树. 与Prim算法的区别 Prim算法是一条边一条边地来构造最小生成树,每一步都会为树中添加一条边. Kruskal算法构造最小生成树也是一条边一条边地添加,但不同的是它寻找的边会连接一片森林中的两棵树.从一片由V棵单顶点的树构成的森林开始并不断地将两棵树合并(可以找到的最短边)直到只剩下一棵树,它就是最小生成树. 实现 要实现Kruskal算法需要借助Union-Find数据结构,它是一种树型的数据结构,用于处理一些不相交集合的合并与查询问题. 关于Union-Find的更多资料可以参考下面的链接: Union-Find简单实现 Disjoint-set data structure - Wikipedia 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class KruskalMST &#123; // 这条队列用于记录最小生成树中的边集 private final Queue&lt;Edge&gt; mst; private double weight; public KruskalMST(EdgeWeightedGraph graph) &#123; this.mst = new ArrayDeque&lt;&gt;(); // 创建一个优先队列,并将图的所有边添加到优先队列中 PriorityQueue&lt;Edge&gt; pq = new PriorityQueue&lt;&gt;(); for (Edge e : graph.edges()) &#123; pq.add(e); &#125; int vertex = graph.vertex(); // 创建一个Union-Find UF uf = new UF(vertex); // 一条一条地添加边到最小生成树,直到添加了 V - 1条边 while (!pq.isEmpty() &amp;&amp; mst.size() &lt; vertex - 1) &#123; // 取出权重最小的边 Edge e = pq.poll(); int v = e.either(); int w = e.other(v); // 如果这条边的两个顶点不在一个分量中(对于union-find数据结构中而言) if (!uf.connected(v, w)) &#123; // 将v和w归并(对于union-find数据结构中而言),然后将边添加进树中,并计算更新权重 uf.union(v, w); mst.add(e); weight += e.weight(); &#125; &#125; &#125; public Iterable&lt;Edge&gt; edges() &#123; return mst; &#125; public double weight() &#123; return weight; &#125;&#125; 上面代码实现的Kruskal算法使用了一条队列来保存最小生成树的边集,一条优先队列来保存还未检查的边,一个Union-Find来判断失效边. 性能比较 算法 空间复杂度 时间复杂度 Prim(延时) E ElogE Prim(即时) V ElogV Kruskal E ElogE 参考文献 Minimum spanning tree - Wikipedia Algorithms, 4th Edition by Robert Sedgewick and Kevin Wayne 图的那点事儿 图的那点事儿(1)-无向图 图的那点事儿(2)-有向图 图的那点事儿(3)-加权无向图 图的那点事儿(4)-加权有向图]]></content>
      <categories>
        <category>Algorithms</category>
        <category>Graph</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的那点事儿(2)-有向图]]></title>
    <url>%2F2017%2F07%2F23%2F2017-07-23-Graph_DirectedGraphs%2F</url>
    <content type="text"><![CDATA[本文作者为: SylvanasSun.转载请务必将下面这段话置于文章开头处(保留超链接).本文转发自SylvanasSun Blog,原文链接: https://sylvanassun.github.io/2017/07/23/2017-07-23-Graph_DirectedGraphs/ 有向图的性质 有向图与无向图不同,它的边是单向的,每条边所连接的两个顶点都是一个有序对,它们的邻接性是单向的. 在有向图中,一条有向边由第一个顶点指出并指向第二个顶点,一个顶点的出度为由该顶点指出的边的总数;一个顶点的入度为指向该顶点的边的总数. v-&gt;w表示一条由v指向w的边,在一幅有向图中,两个顶点的关系可能有以下四种(特殊图除外): 没有边相连. 存在一条从v到w的边: v-&gt;w. 存在一条从w到v的边: w-&gt;v. 既存在v-&gt;w,也存在w-&gt;v,也就是一条双向边. 当存在从v到w的有向路径时,称顶点w能够由顶点v达到.但在有向图中,由v能够到达w并不意味着由w也能到达v(但每个顶点都是能够到达它自己的). 有向图的实现 有向图的实现与无向图差不多,只不过在边的方向上有所不同.(本文中的所有完整代码可以在我的GitHub中查看) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159public class Digraph implements Graph &#123; private static final String NEWLINE = System.getProperty("line.separator"); // number of vertices in this digraph private final int vertex; // number of edges in this digraph private int edge; // adj[v] = adjacency list for vertex v private Bag&lt;Integer&gt;[] adj; // indegree[v] = indegree of vertex v private int[] indegree; public Digraph(int vertex) &#123; validateVertex(vertex); this.vertex = vertex; this.edge = 0; this.indegree = new int[vertex]; this.adj = (Bag&lt;Integer&gt;[]) new Bag[vertex]; for (int i = 0; i &lt; vertex; i++) adj[i] = new Bag&lt;Integer&gt;(); &#125; public Digraph(Scanner scanner) &#123; if (scanner == null) throw new IllegalArgumentException("Scanner must be not null."); try &#123; int vertex = scanner.nextInt(); validateVertex(vertex); this.vertex = vertex; this.indegree = new int[vertex]; this.adj = (Bag&lt;Integer&gt;[]) new Bag[vertex]; for (int i = 0; i &lt; vertex; i++) adj[i] = new Bag&lt;Integer&gt;(); int edge = scanner.nextInt(); validateEdge(edge); for (int i = 0; i &lt; edge; i++) &#123; int v = scanner.nextInt(); int w = scanner.nextInt(); addEdge(v, w); &#125; &#125; catch (NoSuchElementException e) &#123; throw new IllegalArgumentException("Invalid input format in Digraph constructor", e); &#125; &#125; public Digraph(Digraph digraph) &#123; this(digraph.vertex); this.edge = digraph.edge; for (int v = 0; v &lt; vertex; v++) this.indegree[v] = digraph.indegree(v); for (int v = 0; v &lt; vertex; v++) &#123; Stack&lt;Integer&gt; reverse = new Stack&lt;&gt;(); for (int w : digraph.adj(v)) reverse.push(w); for (int w : reverse) this.adj[v].add(w); &#125; &#125; @Override public int vertex() &#123; return vertex; &#125; @Override public int edge() &#123; return edge; &#125; /** * 注意这里与无向图不同,只在v的邻接表中添加了w */ @Override public void addEdge(int v, int w) &#123; validateVertex(v); validateVertex(w); adj[v].add(w); // w的入度+ 1 indegree[w]++; edge++; &#125; @Override public Iterable&lt;Integer&gt; adj(int v) &#123; validateVertex(v); return adj[v]; &#125; public int indegree(int v) &#123; validateVertex(v); return indegree[v]; &#125; /** * v的出度就是它邻接表中的顶点数 */ public int outdegree(int v) &#123; validateVertex(v); return adj[v].size(); &#125; @Override @Deprecated public int degree(int v) &#123; validateVertex(v); return adj[v].size(); &#125; /** * 它返回该有向图的一个副本,但所有边的方向都会被反转. */ public Digraph reverse() &#123; Digraph reverse = new Digraph(vertex); for (int v = 0; v &lt; vertex; v++) &#123; for (int w : adj[v]) &#123; reverse.addEdge(w, v); &#125; &#125; return reverse; &#125; @Override public String toString() &#123; StringBuilder sb = new StringBuilder(); sb.append(String.format("Vertexes: %s, Edges: %s", vertex, edge)); sb.append(NEWLINE); for (int v = 0; v &lt; vertex; v++) &#123; sb.append(String.format("vertex %d, ", v)); sb.append(String.format("indegree: %d, outdegree: %d", indegree(v), outdegree(v))); sb.append(NEWLINE); sb.append("adjacent point: "); for (int w : adj[v]) sb.append(w).append(" "); sb.append(NEWLINE); &#125; return sb.toString(); &#125; private void validateEdge(int edge) &#123; if (edge &lt; 0) throw new IllegalArgumentException("Number of edges in a Digraph must be nonnegative."); &#125; private void validateVertex(int vertex) &#123; if (vertex &lt; 0) throw new IllegalArgumentException("Number of vertex in a Digraph must be nonnegative."); &#125;&#125; 可达性 对于”是否存在一条从集合中的任意顶点到达给定顶点v的有向路径?”等类似问题,可以使用深度优先搜索或广度优先搜索(与无向图的实现一致,只不过传入的图的类型不同),有向图生成的搜索轨迹甚至要比无向图还要简单. 对于可达性分析的一个典型应用就是内存管理系统.例如,JVM使用多点可达性分析的方法来判断一个对象是否可以进行回收: 所有对象组成一幅有向图,其中有多个Root顶点(它是由JVM自己决定的)作为起点,如果一个对象从Root顶点不可达,那么这个对象就可以进行回收了. 环 在与有向图相关的实际应用中,有向环特别的重要.我们需要知道一幅有向图中是否包含有向环.在任务调度问题或其他许多问题中会不允许存在有向环,所以对于环的检测是很重要的. 使用深度优先搜索解决这个问题并不困难,递归调用隐式使用的栈表示的正是”当前”正在遍历的有向路径,一旦找到了一条边v-&gt;w且w已经存在于栈中,就等于找到了一个环(栈表示的是一条由w到v的有向路径,而v-&gt;w正好补全了这个环). 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class DirectedCycle &#123; private final Digraph digraph; // marked[v] = has vertex v been marked? private final boolean[] marked; // edgeTo[v] = previous vertex on path to v private final int[] edgeTo; // onStack[v] = is vertex on the stack? private final boolean[] onStack; // directed cycle (or null if no such cycle) private Stack&lt;Integer&gt; cycle; public DirectedCycle(Digraph digraph) &#123; this.digraph = digraph; int vertex = digraph.vertex(); this.marked = new boolean[vertex]; this.edgeTo = new int[vertex]; this.onStack = new boolean[vertex]; for (int v = 0; v &lt; vertex; v++) &#123; // 已经找到环时就不再需要继续搜索了 if (!marked[v] &amp;&amp; cycle == null) dfs(v); &#125; &#125; public boolean hasCycle() &#123; return cycle != null; &#125; public Iterable&lt;Integer&gt; cycle() &#123; return cycle; &#125; private void dfs(int vertex) &#123; marked[vertex] = true; onStack[vertex] = true; // 用于模拟递归调用栈 for (int w : digraph.adj(vertex)) &#123; if (cycle != null) return; else if (!marked[w]) &#123; edgeTo[w] = vertex; dfs(w); &#125; else if (onStack[w]) &#123; // 当w已被标记且在栈中时: 找到环 cycle = new Stack&lt;&gt;(); for (int x = vertex; x != w; x = edgeTo[x]) cycle.push(x); cycle.push(w); cycle.push(vertex); assert check(); &#125; &#125; // 这条路径已经到头,从栈中弹出 onStack[vertex] = false; &#125; // certify that digraph has a directed cycle if it reports one private boolean check() &#123; if (hasCycle()) &#123; // verify cycle int first = -1, last = -1; for (int v : cycle()) &#123; if (first == -1) first = v; last = v; &#125; if (first != last) &#123; System.err.printf("cycle begins with %d and ends with %d\n", first, last); return false; &#125; &#125; return true; &#125;&#125; 拓扑排序 拓扑排序等价于计算优先级限制下的调度问题的,所谓优先级限制的调度问题即是在给定一组需要完成的任务与关于任务完成的先后次序的优先级限制,需要在满足限制条件的前提下来安排任务. 拓扑排序需要的是一幅有向无环图,如果这幅图中含有环,那么它肯定不是拓扑有序的(一个带有环的调度问题是无解的). 在学习拓扑排序之前,需要先知道顶点的排序. 顶点排序 使用深度优先搜索来记录顶点排序是一个很好的选择(正好只会访问每个顶点一次),我们借助一些数据结构来保存顶点排序的顺序: 前序: 在递归调用之前将顶点加入队列. 后序: 在递归调用之后将顶点加入队列. 逆后序: 在递归调用之后将顶点压入栈. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114public class DepthFirstOrder &#123; private final Graph graph; // marked[v] = has v been marked in dfs? private final boolean[] marked; // pre[v] = preorder number of v private final int[] pre; // post[v] = postorder number of v private final int[] post; // vertices in preorder private final Queue&lt;Integer&gt; preorder; // vertices in postorder private final Queue&lt;Integer&gt; postorder; // counter or preorder numbering private int preCounter; // counter for postorder numbering private int postCounter; public DepthFirstOrder(Graph graph) &#123; this.graph = graph; int vertex = graph.vertex(); this.pre = new int[vertex]; this.post = new int[vertex]; this.preorder = new ArrayDeque&lt;&gt;(); this.postorder = new ArrayDeque&lt;&gt;(); this.marked = new boolean[vertex]; for (int v = 0; v &lt; vertex; v++) if (!marked[v]) dfs(v); assert check(); &#125; public int pre(int v) &#123; validateVertex(v); return pre[v]; &#125; public int post(int v) &#123; validateVertex(v); return post[v]; &#125; public Iterable&lt;Integer&gt; post() &#123; return postorder; &#125; public Iterable&lt;Integer&gt; pre() &#123; return preorder; &#125; // 逆后序,遍历后序队列并压入栈中 public Iterable&lt;Integer&gt; reversePost() &#123; Stack&lt;Integer&gt; reverse = new Stack&lt;Integer&gt;(); for (int v : postorder) reverse.push(v); return reverse; &#125; private void dfs(int vertex) &#123; marked[vertex] = true; // 前序 pre[vertex] = preCounter++; preorder.add(vertex); for (int w : graph.adj(vertex)) &#123; if (!marked[w]) dfs(w); &#125; // 后序 post[vertex] = postCounter++; postorder.add(vertex); &#125; // check that pre() and post() are consistent with pre(v) and post(v) private boolean check() &#123; // check that post(v) is consistent with post() int r = 0; for (int v : post()) &#123; if (post(v) != r) &#123; System.out.println("post(v) and post() inconsistent"); return false; &#125; r++; &#125; // check that pre(v) is consistent with pre() r = 0; for (int v : pre()) &#123; if (pre(v) != r) &#123; System.out.println("pre(v) and pre() inconsistent"); return false; &#125; r++; &#125; return true; &#125; // throw an IllegalArgumentException unless &#123;@code 0 &lt;= v &lt; V&#125; private void validateVertex(int v) &#123; int V = marked.length; if (v &lt; 0 || v &gt;= V) throw new IllegalArgumentException("vertex " + v + " is not between 0 and " + (V - 1)); &#125;&#125; 拓扑排序的实现 所谓拓扑排序就是无环有向图的逆后序,现在已经知道了如何检测环与顶点排序,那么实现拓扑排序就很简单了. 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class Topological &#123; // topological order private Iterable&lt;Integer&gt; order; // rank[v] = position of vertex v in topological order private int[] rank; public Topological(Digraph digraph) &#123; DirectedCycle directedCycle = new DirectedCycle(digraph); // 只有这幅图没有环时,才进行计算拓扑排序 if (!directedCycle.hasCycle()) &#123; DepthFirstOrder depthFirstOrder = new DepthFirstOrder(digraph); // 拓扑排序即是逆后序 order = depthFirstOrder.reversePost(); rank = new int[digraph.vertex()]; int i = 0; for (int v : order) rank[v] = i++; &#125; &#125; public Iterable&lt;Integer&gt; order() &#123; return order; &#125; public boolean hasOrder() &#123; return order != null; &#125; public int rank(int v) &#123; validateVertex(v); if (hasOrder()) return rank[v]; else return -1; &#125; // throw an IllegalArgumentException unless &#123;@code 0 &lt;= v &lt; V&#125; private void validateVertex(int v) &#123; int V = rank.length; if (v &lt; 0 || v &gt;= V) throw new IllegalArgumentException("vertex " + v + " is not between 0 and " + (V - 1)); &#125;&#125; 强连通性 在一幅无向图中,如果有一条路径连接顶点v和w,则它们就是连通的(既可以从w到达v,也可以从v到达w).但在有向图中,如果从顶点v有一条有向路径到达w,则w是从v可达的,但从w到达v的路径可能存在也可能不存在. 强连通性就是两个顶点v和w是互相可达的.有向图中的强连通性具有以下性质: 自反性: 任意顶点v和自己都是强连通性的(有向图中顶点都是自己可达的). 对称性: 如果v和w是强连通的,那么w和v也是强连通的. 传递性: 如果v和w是强连通的且w和x也是强连通的,那么v和x也是强连通的. 强连通性将所有顶点分为了一些等价类,每个等价类都是由相互为强连通的顶点的最大子集组成的.这些子集称为强连通分量,它的定义是基于顶点的,而非边. 一个含有V个顶点的有向图含有1 ~ V个强连通分量.一个强连通图只含有一个强连通分量,而一个有向无环图中则含有V个强连通分量. Kosaraju算法 Kosaraju算法是用于枚举图中每个强连通分量内的所有顶点,它主要有以下步骤: 在给定一幅有向图$G$中,取得它的反向图$G^R$. 利用深度优先搜索得到$G^R$的逆后序排列. 按照上述逆后序的序列进行深度优先搜索 同一个深度优先搜索递归子程序中访问的所有顶点都在同一个强连通分量内. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class KosarajuSharirSCC &#123; private final Digraph digraph; // marked[v] = has vertex v been visited? private final boolean[] marked; // id[v] = id of strong component containing v private final int[] id; // number of strongly-connected components private int count; public KosarajuSharirSCC(Digraph digraph) &#123; this.digraph = digraph; int vertex = digraph.vertex(); // compute reverse postorder of reverse graph DepthFirstOrder depthFirstOrder = new DepthFirstOrder(digraph.reverse()); // run DFS on G, using reverse postorder to guide calculation marked = new boolean[vertex]; id = new int[vertex]; for (int v : depthFirstOrder.reversePost()) &#123; if (!marked[v]) &#123; dfs(v); count++; &#125; &#125; // check that id[] gives strong components assert check(digraph); &#125; private void dfs(int v) &#123; marked[v] = true; id[v] = count; for (int w : digraph.adj(v)) &#123; if (!marked[w]) dfs(w); &#125; &#125; public int count() &#123; return count; &#125; public boolean stronglyConnected(int v, int w) &#123; validateVertex(v); validateVertex(w); return id[v] == id[w]; &#125; public int id(int v) &#123; validateVertex(v); return id[v]; &#125; // does the id[] array contain the strongly connected components? private boolean check(Digraph G) &#123; TransitiveClosure tc = new TransitiveClosure(G); for (int v = 0; v &lt; G.vertex(); v++) &#123; for (int w = 0; w &lt; G.vertex(); w++) &#123; if (stronglyConnected(v, w) != (tc.reachable(v, w) &amp;&amp; tc.reachable(w, v))) return false; &#125; &#125; return true; &#125; // throw an IllegalArgumentException unless &#123;@code 0 &lt;= v &lt; V&#125; private void validateVertex(int v) &#123; int V = marked.length; if (v &lt; 0 || v &gt;= V) throw new IllegalArgumentException("vertex " + v + " is not between 0 and " + (V - 1)); &#125;&#125; 传递闭包 在一幅有向图G中,传递闭包是由相同的一组顶点组成的另一幅有向图,在传递闭包中存在一条从v指向w的边且仅当在G中w是从v可达的. 由于有向图的性质,每个顶点对于自己都是可达的,所以传递闭包会含有V个自环. 通常将传递闭包表示为一个布尔值矩阵,其中v行w列的值为true代表当且仅当w是从v可达的. 传递闭包不适合于处理大型有向图,因为构造函数所需的空间与$V^2$成正比,所需的时间和$V(V+E)$成正比. 123456789101112131415161718192021222324public class TransitiveClosure &#123; private DirectedDFS[] tc; // tc[v] = reachable from v public TransitiveClosure(Digraph G) &#123; tc = new DirectedDFS[G.vertex()]; for (int v = 0; v &lt; G.vertex(); v++) tc[v] = new DirectedDFS(G, v); &#125; public boolean reachable(int v, int w) &#123; validateVertex(v); validateVertex(w); return tc[v].marked(w); &#125; // throw an IllegalArgumentException unless &#123;@code 0 &lt;= v &lt; V&#125; private void validateVertex(int v) &#123; int V = tc.length; if (v &lt; 0 || v &gt;= V) throw new IllegalArgumentException("vertex " + v + " is not between 0 and " + (V - 1)); &#125;&#125; 参考文献 Algorithms, 4th Edition by Robert Sedgewick and Kevin Wayne Kosaraju’s algorithm - Wikipedia Transitive closure - Wikipedia 图的那点事儿 图的那点事儿(1)-无向图 图的那点事儿(2)-有向图 图的那点事儿(3)-加权无向图 图的那点事儿(4)-加权有向图]]></content>
      <categories>
        <category>Algorithms</category>
        <category>Graph</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[电影中的"FaceMash"算法]]></title>
    <url>%2F2017%2F07%2F19%2F2017-07-19-FaceMash%2F</url>
    <content type="text"><![CDATA[最近在看&lt;&lt;社交网络&gt;&gt;时,发现了一个用于投票排名的算法,自己折腾实现了一下. 在影片中,卷西饰演的扎克伯格在被妹子甩了之后(其实是他自己直男癌),一气之下黑了附近女生宿舍的照片数据库打算做一个FaceMash(通过投票的方式来选出漂亮的女生,同时它也是Facebook的前身,后来这个网站由于流量太大,搞崩了哈佛大学的网络而被强行关闭了),并使用了他的好基友爱德华多用于计算国际象棋排名的算法. 这是一部很好看的电影,如果没有看过我强烈推荐去看一看. 公式 这个算法是用来计算期望胜率的,但影片中其实写的是错误的,正确的公式应该为: $$E_a = \frac{1} {1 + 10 ^ {(R_b - R_a) / 400}}$$ $E_a$就是a的期望胜率. $R_b,R_a$是b与a的Rank分数. 当$R_a,R_b$都相同时,它们的期望胜率都为0.5,即$E_a = \frac{1} {1+10^0} = 0.5$. 电影中只给出了计算期望胜率的算法,但我们还需要一个计算新的Rank分数的算法,公式如下: $$R_n = R_o + K(W - E)$$ $R_n$代表新的Rank,$R_o$自然就是旧的Rank了. K为一个定值,我把它设为10. W是胜负值,胜者为1,败者为0;E就是我们上面计算的期望胜率. 实现 有了这两个核心公式,我们就可以开始实现这个算法了,但在代码实现之前,我们先验证一下公式: 假设有两个女孩A与B,她们的基础Rank都为1400,通过上述的推论我们已经得知,当A,B的分值相同时,她们的期望胜率都为0.5. 如果,我选择了A,则A的胜负值变为1,B的胜负值为0,然后我们套用公式2可以得出: $R_a = 1400 + 10 * (1 - 0.5) = 1405$ $R_b = 1400 + 10 * (0 - 0.5) = 1395$ 由于她们的分数不再相同,所以套用公式1计算现在的期望胜率: $R_a = \frac{1} {1 + 10 ^ {(1395 - 1405) / 400}} \approx 0.51439 $ $R_b = \frac{1} {1 + 10 ^ {(1405 - 1395) / 400}} \approx 0.48561$ 下面是我用C写的一个小程序,它初始化了两个”女孩”,然后根据输入来判断哪个胜出,并动态计算Rank与期望胜率. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#include &lt;stdio.h&gt;#include &lt;math.h&gt;typedef struct &#123; const char *name; int rank; double expect_rate;&#125; girl;const int K = 10;void read_girl(girl g) &#123; printf("Girl name: %s, rank: %d, expect_rate: %.5f\n",g.name,g.rank,g.expect_rate);&#125;void compute_expect_rate(girl *a,girl *b) &#123; int a_rank = a-&gt;rank; int b_rank = b-&gt;rank; // expect rate formula // Ea = 1 / (1 + 10 ^ ((Rb-Ra) / 400)) double a_rank_differ = (double) (b_rank - a_rank) / 400; double a_rank_rate = pow(10,a_rank_differ); a-&gt;expect_rate = 1 / (1 + a_rank_rate); // Eb = 1 / (1 + 10 ^ ((Ra-Rb) / 400)) double b_rank_differ = (double) (a_rank - b_rank) / 400; double b_rank_rate = pow(10,b_rank_differ); b-&gt;expect_rate = 1 / (1 + b_rank_rate);&#125;// new rank formula: Rn = Ro + K(W - E)void compute_rank(girl *a,girl *b,int a_win_rate,int b_win_rate) &#123; a-&gt;rank = a-&gt;rank + K * (a_win_rate - a-&gt;expect_rate); b-&gt;rank = b-&gt;rank + K * (b_win_rate - b-&gt;expect_rate);&#125;int main(int argc,char *argv[]) &#123; char a_girl_name[20]; char b_girl_name[20]; girl a = &#123;.name = "A Gril",.rank = 1400&#125;; girl b = &#123;.name = "B Gril",.rank = 1400&#125;; compute_expect_rate(&amp;a,&amp;b); read_girl(a); read_girl(b); while (1) &#123; char choice[2]; printf("Choice A or B?\n"); scanf("%s",choice); if (choice[0] == 'A') &#123; compute_rank(&amp;a,&amp;b,1,0); compute_expect_rate(&amp;a,&amp;b); &#125; else if (choice[0] == 'B') &#123; compute_rank(&amp;a,&amp;b,0,1); compute_expect_rate(&amp;a,&amp;b); &#125; else &#123; printf("Invalid choice!\n"); break; &#125; read_girl(a); read_girl(b); &#125;&#125; 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>Algorithms</category>
        <category>Other</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图的那点事儿(1)-无向图]]></title>
    <url>%2F2017%2F07%2F18%2F2017-07-18-Graph_UndirectedGraph%2F</url>
    <content type="text"><![CDATA[在数学中,一个图(Graph)是表示物件与物件之间关系的方法,是图论的基本研究对象.一个图是由顶点(Vertex)与连接这些顶点的边(Edge)组成的. 图论作为数学领域中的一个重要分支已经有数百年的历史了.人们发现了图的许多重要而实用的性质,发明了许多重要的算法,给你一个图(Graph)你可以联想到许多问题: 两个顶点之间是否存在一条链接?如果存在,两个顶点之间最短的连接又是哪一条?…. 在生活中,到处都可以发现图论的应用: 地图: 在使用地图中,我们经常会想知道”从xx到xx的最短路线”这样的问题,要回答这些问题,就需要把地图抽象成一个图(Graph),十字路口就是顶点,公路就是边. 互联网: 整个互联网其实就是一张图,它的顶点为网页,边为超链接.而图论可以帮助我们在网络上定位信息. 任务调度: 当一些任务拥有优先级限制且需要满足前置条件时,如何在满足条件的情况下用最少的时间完成就需要用到图论. 社交网络: 在使用社交网站时,你就是一个顶点,你和你的朋友建立的关系则是边.分析这些社交网络的性质也是图论的一个重要应用. 图就是由一组顶点和一组能够将两个顶点相连的边组成的. 基本术语 相邻: 当两个顶点通过一条边相连接时,这两个顶点即为相邻的(也可以说这条边依附于这两个顶点). 度数: 某个顶点的度数即为依附于它的边的总数. 阶: 图G中的顶点集合V的大小称为G的阶. 自环: 一条连接一个顶点和其自身的边. 平行边: 连接同一对顶点的两条边称为平行边. 桥: 如果去掉一条边会使整个图变成非连通图,则该边称为桥. 路径: 当顶点v到顶点w是连通时,我们用v-&gt;x-&gt;y-&gt;w为一条v到w的路径,用v-&gt;x-&gt;y-&gt;v表示一条环. 子图: 也称作连通分量,它由一张图的所有边的一个子集组成的图(以及依附的所有顶点). 连通图: 连通图是一个整体,而非连通图则包含两个或多个连通分量. 稀疏图: 如果一张图中不同的边的数量在顶点总数V的一个小的常数倍内,那么该图就为稀疏图,否则为稠密图. 简单图与多重图: 含有平行边与自环的图称为多重图,而不含有平行边和自环的图称为简单图. 树 树是一张无环连通图,互不相连的树组成的集合称为森林.连通图的生成树是它的一张子图,它含有图中的所有顶点且是一棵树.图的生成树森林是它的所有连通分量的生成树的集合. 图G只要满足以下性质,那么它就是一棵树: G有V-1条边且不含有环. G有V-1条边且是连通的. G是连通的,但删除任意一条边都会使它不再连通. G是无环图,但添加任意一条边都会产生一条环. G中的任意一对顶点之间仅存在一条简单路径(一条没有重复顶点的路径). 二分图 二分图是一种能够将所有顶点分为两部分的图,其中图的每条边所连接的两个顶点都分别属于不同的部分. 设G = (V,E)为一张无向图,如果顶点V可以分割为两个互不相交的子集(U,V),且图中的每条边(x,y)所关联的两个顶点x,y分别属于这两个不同的顶点集合(x in U , y in V),则G为二分图. 也可以将(U,V)当做一张着色图: U中的所有顶点为蓝色,V中的所有顶点为绿色,每条边所关联的两个顶点颜色不同. 无向图 无向图是一种最简单的图模型,它的每条边都没有方向. 图的表示方法 实现一张图的API需要满足以下两个要求: 必须为可能在应用中碰到的各种类型的图预留出足够的空间. 图的实现一定要足够快(因为这是所有处理图的算法的基础结构). 有以下三种数据结构能够用来表示一张图: 邻接矩阵: 使用一个V * V的布尔矩阵.当顶点v和顶点w之间有相连接的边时,将v行w列的元素设为true,否则为false.这种方法不符合第一个条件,当图的顶点非常多时,邻接矩阵所需的空间将会非常大.且它无法表示平行边. 边的数组: 使用一个Edge类,它含有两个int成员变量来表示所依附的顶点.这种方法简单直接但不满足第二个条件(要实现查询邻接点的函数需要检查图中的所有边). 邻接表数组: 使用一个顶点为索引的链表数组,其中的每个元素都是和该顶点相邻的顶点列表(邻接点).这种方法同时满足了两个条件,我们会使用这种方法来实现图的数据结构. 实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138public interface Graph &#123; int vertex(); int edge(); void addEdge(int v, int w); Iterable&lt;Integer&gt; adj(int v); int degree(int v); String toString();&#125;public class UndirectedGraph implements Graph &#123; private static final String NEW_LINE_SEPARATOR = System.getProperty("line.separator"); private final int vertex; // 顶点 private int edge; // 边 private final Bag&lt;Integer&gt;[] adjacent; // 邻接表数组,Bag是一个没有实现删除操作的Stack public UndirectedGraph(int vertex) &#123; checkVertex(vertex); this.vertex = vertex; this.edge = 0; this.adjacent = (Bag&lt;Integer&gt;[]) new Bag[vertex]; for (int v = 0; v &lt; vertex; v++) adjacent[v] = new Bag&lt;Integer&gt;(); &#125; // 读取一个文件并初始化为无向图 public UndirectedGraph(Scanner scanner) &#123; if (scanner == null) throw new IllegalArgumentException("Specified input stream must not null!"); try &#123; // 文件的第一行为顶点数 this.vertex = scanner.nextInt(); checkVertex(this.vertex); // 文件的第二行为边数 int edge = scanner.nextInt(); checkEdge(this.edge); this.adjacent = (Bag&lt;Integer&gt;[]) new Bag[this.vertex]; for (int v = 0; v &lt; this.vertex; v++) adjacent[v] = new Bag&lt;Integer&gt;(); // 文件的剩余行为相连的顶点对 for (int i = 0; i &lt; edge; i++) &#123; int v = scanner.nextInt(); int w = scanner.nextInt(); addEdge(v, w); &#125; &#125; catch (NoSuchElementException e) &#123; throw new IllegalArgumentException("Invalid input format in Undirected Graph constructor", e); &#125; &#125; public UndirectedGraph(Graph graph) &#123; this(graph.vertex()); this.edge = graph.edge(); for (int v = 0; v &lt; this.vertex; v++) &#123; // reverse so that adjacency list is in same order as original Stack&lt;Integer&gt; stack = new Stack&lt;Integer&gt;(); for (int w : graph.adj(v)) stack.push(w); for (int w : stack) adjacent[v].add(w); &#125; &#125; private void checkVertex(int vertex) &#123; if (vertex &lt;= 0) throw new IllegalArgumentException("Number of vertices must be positive number!"); &#125; private void checkEdge(int edge) &#123; if (edge &lt; 0) throw new IllegalArgumentException("Number of edges must be positive number!"); &#125; public int vertex() &#123; return vertex; &#125; public int edge() &#123; return edge; &#125; // 添加一条连接v和w的边,由于是无向图所以这条边会出现两次 public void addEdge(int v, int w) &#123; validateVertex(v); validateVertex(w); adjacent[v].add(w); adjacent[w].add(v); edge++; &#125; public Iterable&lt;Integer&gt; adj(int v) &#123; validateVertex(v); return adjacent[v]; &#125; public int degree(int v) &#123; validateVertex(v); return adjacent[v].size(); &#125; private void validateVertex(int vertex) &#123; if (vertex &lt; 0 || vertex &gt;= this.vertex) throw new IllegalArgumentException("Vertex " + vertex + " is not between 0 and " + (this.vertex - 1)); &#125; @Override public String toString() &#123; StringBuilder sb = new StringBuilder(); sb.append("Vertices: ").append(vertex).append(" Edges: ").append(edge).append(NEW_LINE_SEPARATOR); for (int v = 0; v &lt; vertex; v++) &#123; sb.append(v).append(": "); for (int w : adjacent[v]) sb.append(w).append(" "); sb.append(NEW_LINE_SEPARATOR); &#125; return sb.toString(); &#125; public static void main(String[] args) throws FileNotFoundException &#123; InputStream inputStream = UndirectedGraph.class.getResourceAsStream("/graph_file/C4_1_UndirectedGraphs/" + args[0]); Scanner scanner = new Scanner(inputStream, "UTF-8"); Graph graph = new UndirectedGraph(scanner); System.out.println(graph); &#125;&#125; 上面的这个实现拥有以下特点: 使用的空间和V + E成正比. 添加一条边所需的时间为常数. 遍历顶点v的所有邻接点所需的时间和v的度数成正比(处理每个邻接点所需的时间为常数). 边的插入顺序决定了邻接表中顶点的出现顺序. 支持平行边与自环. 不支持添加或删除顶点的操作(如果想要支持这些操作需要使用一个符号表来代替由顶点索引构成的数组). 不支持删除边的操作(如果想要支持这个操作需要使用一个SET来代替Bag来实现邻接表,这种方法也叫邻接集). 每种图实现的性能复杂度如下表: 数据结构 所需空间 添加一条边v - w 检查w和v是否相邻 遍历v的所有邻接点 边的数组 E 1 E E 邻接矩阵 V^2 1 1 V 邻接表 E+V 1 degree(V) degree(V) 邻接集 E+V logV logV logV+degree(V) 本文中的所有完整代码可以到我的GitHub中查看. 深度优先搜索 处理图的基本问题: v 到 w是否是相连的?. 深度优先搜索就是用于解决这样问题的,它会沿着图的边寻找和起点连通的所有顶点. 如其名一样,深度优先搜素就是沿着图的深度来遍历顶点,它类似于走迷宫,会沿着一条路径一直走,直到走到尽头时再回退到上一个路口.为了防止迷路,还需要使用工具来标记已走过的路口(在我们的代码实现中使用一个布尔数组来进行标记). 递归实现 使用递归方法来实现深度优先搜索会很简洁,当遇到一个顶点时: 将它标记为已访问. 递归地访问它的所有没有被访问过的邻接点. 1234567891011121314151617181920212223242526272829303132333435363738394041public class DepthFirstSearch &#123; private final boolean[] marked; // 标记已访问过的顶点 private int count; // 记录起点连通的顶点数 private final Graph graph; public DepthFirstSearch(Graph graph, int originPoint) &#123; this.graph = graph; this.count = 0; this.marked = new boolean[graph.vertex()]; validateVertex(originPoint); // 从起点开始进行深度优先搜索 depthSearch(originPoint); &#125; public int count() &#123; return count; &#125; public boolean marked(int vertex) &#123; validateVertex(vertex); return marked[vertex]; &#125; private void depthSearch(int vertex) &#123; marked[vertex] = true; count++; for (int adj : graph.adj(vertex)) &#123; // 遍历邻接点,如果未访问则递归调用 if (!marked[adj]) depthSearch(adj); &#125; &#125; private void validateVertex(int vertex) &#123; int length = marked.length; if (vertex &lt; 0 || vertex &gt;= length) throw new IllegalArgumentException("Vertex " + vertex + " is not between 0 and " + (length - 1)); &#125;&#125; 非递归实现 如果是了解JVM中函数调用的小伙伴们应该知道,函数都会封装成一个个栈帧然后压入虚拟机栈,上述的递归实现其实就是在隐式的使用到了栈,要想实现非递归,我们需要显式使用栈这个数据结构. 12345678910111213141516171819202122232425262728293031323334353637383940414243public class NonrecursiveDFS &#123; private final boolean[] marked; private final Iterator&lt;Integer&gt;[] adj; public NonrecursiveDFS(Graph graph, int originPoint) &#123; int vertex = graph.vertex(); this.marked = new boolean[vertex]; validateVertex(originPoint); // 取出所有顶点的邻接表迭代器 adj = (Iterator&lt;Integer&gt;[]) new Iterator[vertex]; for (int v = 0; v &lt; vertex; v++) adj[v] = graph.adj(v).iterator(); dfs(originPoint); &#125; private void dfs(int originPoint) &#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); // 标记起点并放入栈 marked[originPoint] = true; stack.push(originPoint); while (!stack.isEmpty()) &#123; Integer v = stack.peek(); // 遍历栈顶顶点的邻接点 if (adj[v].hasNext()) &#123; int w = adj[v].next(); // 如果未被访问,进行标记并放入栈中 if (!marked[w]) &#123; marked[w] = true; stack.push(w); &#125; &#125; else &#123; // 当栈顶顶点的所有邻接点已经遍历完时,弹出栈 stack.pop(); &#125; &#125; &#125;&#125; 寻找路径 在图的应用中,找出v-w的可达路径也是常见的问题之一. 我们基于深度优先搜索实现寻找路径,并添加一个edgeTo[]整形数组来记录路径.例如,在由边v-w第一次访问任意w时,将edgeTo[w]设为v来记录这条路径(v-w是从起点到w的路径上最后一条已知的边).这样搜索到的路径就是一颗以起点为根的树,edgeTo[]是一颗由父链接表示的树. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class DepthFirstPaths &#123; private final Graph graph; private final boolean[] marked; private final int[] edgeTo; // 用于记录路径 private final int originPoint; public DepthFirstPaths(Graph graph, int originPoint) &#123; int vertex = graph.vertex(); this.graph = graph; this.originPoint = originPoint; this.marked = new boolean[vertex]; this.edgeTo = new int[vertex]; validateVertex(originPoint); dfs(originPoint); &#125; public boolean hasPathTo(int vertex) &#123; validateVertex(vertex); return marked[vertex]; &#125; public Iterable&lt;Integer&gt; pathTo(int vertex) &#123; validateVertex(vertex); Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); // 从指定顶点处向上遍历路径(直到起点) for (int x = vertex; x != originPoint; x = edgeTo[x]) stack.push(x); stack.push(originPoint); return stack; &#125; private void dfs(int vertex) &#123; marked[vertex] = true; for (int adj : graph.adj(vertex)) &#123; if (!marked[adj]) &#123; marked[adj] = true; // edgeTo[w] = v,记录了父链接 edgeTo[adj] = vertex; dfs(adj); &#125; &#125; &#125; &#125; 广度优先搜索 对于寻找一条最短路径,深度优先搜索没有什么作为,因为它遍历整个图的顺序和找出最短路径的目标没有任何关系.这种问题就需要用到广度优先搜索. 广度优先搜索是沿着宽度来进行搜索的.例如,要找到s到v的最短路径,从s开始,在所有由一条边就可以到达的顶点中寻找v,如果找不到就继续在与s距离两条边的所有顶点中寻找v,以此类推. 如果说深度优先搜索是一个人在走迷宫,那么广度优先搜索就是一群人一起朝着各个方向去走迷宫. 在广度优先搜索中,我们使用一个队列来保存所有已被标记过但邻接表还未被检查过的顶点.先将起点放入队列,然后重复以下步骤直到队列为空: 取出队列中的下一个顶点并标记. 将它相邻的所有未被标记过的顶点加入队列. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111public class BreadthFirstPaths &#123; private static final int INFINITY = Integer.MAX_VALUE; private final Graph graph; private final boolean[] marked; private final int[] edgeTo; private final int[] distTo; // 记录路径中经过的顶点数,起点为0,需要全部初始化为无穷大 public BreadthFirstPaths(Graph graph, int originPoint) &#123; this.graph = graph; int vertex = graph.vertex(); marked = new boolean[vertex]; edgeTo = new int[vertex]; distTo = new int[vertex]; for (int i = 0; i &lt; vertex; i++) distTo[i] = INFINITY; validateVertex(originPoint); bfs(originPoint); &#125; // 以一组顶点为起点 public BreadthFirstPaths(Graph graph, Iterable&lt;Integer&gt; sources) &#123; this.graph = graph; int vertex = graph.vertex(); this.marked = new boolean[vertex]; this.edgeTo = new int[vertex]; this.distTo = new int[vertex]; for (int i = 0; i &lt; vertex; i++) distTo[i] = INFINITY; validateVertices(sources); bfs(sources); &#125; public boolean hasPathTo(int vertex) &#123; validateVertex(vertex); return marked[vertex]; &#125; public int distTo(int vertex) &#123; validateVertex(vertex); return distTo[vertex]; &#125; public Iterable&lt;Integer&gt; pathTo(int vertex) &#123; validateVertex(vertex); Stack&lt;Integer&gt; path = new Stack&lt;&gt;(); int x; // 这里使用distTo[x] != 0来判断是否为起点 for (x = vertex; distTo[x] != 0; x = edgeTo[x]) path.push(x); path.push(x); return path; &#125; private void bfs(int vertex) &#123; Queue&lt;Integer&gt; queue = new ArrayDeque&lt;&gt;(); marked[vertex] = true; distTo[vertex] = 0; queue.add(vertex); searchAndMarkAdjacent(queue); &#125; private void bfs(Iterable&lt;Integer&gt; sources) &#123; Queue&lt;Integer&gt; queue = new ArrayDeque&lt;&gt;(); for (int v : sources) &#123; marked[v] = true; distTo[v] = 0; queue.add(v); &#125; searchAndMarkAdjacent(queue); &#125; // 广度优先搜索 private void searchAndMarkAdjacent(Queue&lt;Integer&gt; queue) &#123; while (!queue.isEmpty()) &#123; Integer v = queue.remove(); for (int adj : graph.adj(v)) &#123; // 将未标记过的邻接点加入队列并进行标记等操作 if (!marked[adj]) &#123; marked[adj] = true; edgeTo[adj] = v; distTo[adj] = distTo[v] + 1; queue.add(adj); &#125; &#125; &#125; &#125; private void validateVertex(int vertex) &#123; int length = marked.length; if (vertex &lt; 0 || vertex &gt;= length) throw new IllegalArgumentException("Vertex " + vertex + " is not between 0 and " + (length - 1)); &#125; private void validateVertices(Iterable&lt;Integer&gt; vertices) &#123; if (vertices == null) throw new IllegalArgumentException("Vertices is null."); int length = marked.length; for (int v : vertices) &#123; if (v &lt; 0 || v &gt;= length) throw new IllegalArgumentException("Vertex " + v + " is not between 0 and " + (length - 1)); &#125; &#125;&#125; 不管是深度优先搜索还是广度优先搜索,它们都是先将起点存入一个数据结构中,然后重复以下步骤直到数据结构被清空: 取其中的下一个顶点并标记它. 将它的所有相邻而又未被标记的顶点放入数据结构中. 这两种算法的不同之处仅在于从数据结构中获取下一个顶点的规则(对于广度优先搜索来说是最早加入的顶点,对于深度优先搜索来说是最晚加入的顶点). 深度优先搜索的方式是不断寻找离起点更远的顶点,直到碰见死胡同时才返回近处顶点. 广度优先搜索的方式是先覆盖起点附近的顶点,只有当邻接的所有顶点都被访问过之后才继续前进. 深度优先搜素的路径通常长且曲折,广度优先搜索的路径则短而直接.但不管是使用哪种算法,所有与起点连通的顶点和边都会被访问到. 连通分量 深度优先搜索的一个重要应用就是寻找出一幅图中的所有连通分量. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class ConnectedComponent &#123; private final Graph graph; private final boolean[] marked; // 顶点与它们所属的连通分量进行关联的数组 private final int[] id; // 记录每个连通分量中有多少顶点的数组 private final int[] size; // 连通分量数 private int count; public ConnectedComponent(Graph graph) &#123; this.graph = graph; int vertex = graph.vertex(); this.marked = new boolean[vertex]; this.id = new int[vertex]; this.size = new int[vertex]; for (int v = 0; v &lt; vertex; v++) &#123; if (!marked[v]) &#123; dfs(v); count++; // 一张连通图遍历完毕后,连通分量数 + 1 &#125; &#125; &#125; public int id(int vertex) &#123; validateVertex(vertex); return id[vertex]; &#125; public int size(int vertex) &#123; validateVertex(vertex); return size[id[vertex]]; &#125; public int count() &#123; return count; &#125; // 两个顶点是否处于一个连通分量中 public boolean connected(int v, int w) &#123; validateVertex(v); validateVertex(w); return id[v] == id[w]; &#125; private void dfs(int vertex) &#123; marked[vertex] = true; id[vertex] = count; size[count]++; for (int adj : graph.adj(vertex)) &#123; if (!marked[adj]) dfs(adj); &#125; &#125; &#125; 检测环与双色问题 深度优先搜索的应用远不于此,它还可以用来检测是否有环以及双色问题. 检测环 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public class Cyclic &#123; private final Graph graph; private boolean[] marked ; private int[] edgeTo; // 如果存在环则返回这条环路径 private Stack&lt;Integer&gt; cyclic; public Cyclic(Graph graph) &#123; this.graph = graph; // 先检测是否有自环 if (hasSelfLoop()) return; // 再检测是否有平行边 if (hasParallelEdges()) return; int vertex = graph.vertex(); this.marked = new boolean[vertex]; this.edgeTo = new int[vertex]; for (int v = 0; v &lt; vertex; v++) &#123; if (!marked[v]) dfs(v, -1); &#125; &#125; public boolean hasCyclic() &#123; return cyclic != null; &#125; public Iterable&lt;Integer&gt; cyclic() &#123; return cyclic; &#125; private boolean hasSelfLoop() &#123; for (int v = 0; v &lt; graph.vertex(); v++) &#123; for (int w : graph.adj(v)) &#123; // 如果v与w是同一个顶点,则代表有自环 if (v == w) &#123; cyclic = new Stack&lt;&gt;(); cyclic.push(v); cyclic.push(v); return true; &#125; &#125; &#125; return false; &#125; private boolean hasParallelEdges() &#123; int vertex = graph.vertex(); boolean[] marked = new boolean[vertex]; for (int v = 0; v &lt; vertex; v++) &#123; // check for parallel edges incident to v for (int w : graph.adj(v)) &#123; if (marked[w]) &#123; cyclic = new Stack&lt;&gt;(); cyclic.push(v); cyclic.push(w); cyclic.push(v); return true; &#125; marked[w] = true; &#125; // reset so marked[v] = false for all v for (int w : graph.adj(v)) marked[w] = false; &#125; return false; &#125; private void dfs(int v, int u) &#123; marked[v] = true; for (int w : graph.adj(v)) &#123; if (cyclic != null) return; if (!marked[w]) &#123; edgeTo[w] = v; dfs(w, v); &#125; else if (w != u) &#123; // check for cycle (but disregard reverse of edge leading to v) cyclic = new Stack&lt;&gt;(); for (int x = v; x != w; x = edgeTo[x]) cyclic.push(x); cyclic.push(w); cyclic.push(v); &#125; &#125; &#125;&#125; 检测双色 12345678910111213141516171819202122232425262728293031323334353637383940414243public class TwoColor &#123; private final Graph graph; private final boolean[] marked; private final boolean[] color; private boolean isTwoColorable = true; public TwoColor(Graph graph) &#123; this.graph = graph; int vertex = graph.vertex(); this.marked = new boolean[vertex]; this.color = new boolean[vertex]; for (int v = 0; v &lt; vertex; v++) &#123; if (!marked[v]) dfs(v); &#125; &#125; public boolean isBipartite() &#123; return isTwoColorable; &#125; private void dfs(int v) &#123; marked[v] = true; for (int w : graph.adj(v)) &#123; if (!marked[w]) &#123; // 将未被访问过的邻接点w设为v的反色 color[w] = !color[v]; dfs(w); &#125; else if (color[w] == color[v]) &#123; // 如果w已被访问且颜色与v相同,则代表这不是一张双色图 isTwoColorable = false; &#125; &#125; &#125;&#125; 符号图 在很多应用中,是使用字符串而非整数来表示顶点的,为了适应这种需求,需要拥有以下性质的输入格式: 顶点名是字符串. 用指定的分隔符来隔开顶点名 每一行都表示一组边的集合,每一条边都连接着这一行的第一个名称表示的顶点和其他名称所表示的顶点. 顶点集V与边集E都是隐式定义的. 要实现符号图还需要借助以下数据结构: 一个符号表,我这里使用的是TreeMap即红黑树,它的Key为String(顶点名),Value为Integer(顶点索引). 一个字符串数组,它用来与符号表作反向索引,保存每个顶点索引所对应的顶点名. 一个Graph对象,我们使用索引来生成这张图对象. 代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public class SymbolGraph &#123; private TreeMap&lt;String, Integer&gt; symbolTable; // string -&gt; index private String[] keys; // index -&gt; string private Graph graph; public SymbolGraph(String filename, String delimiter) &#123; symbolTable = new TreeMap&lt;&gt;(); // 第一次读取文件 String filePath = "/graph_file/C4_1_UndirectedGraphs/" + filename; InputStream inputStream = SymbolGraph.class.getResourceAsStream(filePath); Scanner scanner = new Scanner(inputStream, "UTF-8"); // 初始化符号表 while (scanner.hasNextLine()) &#123; String[] s = scanner.nextLine().split(delimiter); for (String key : s) &#123; if (!symbolTable.containsKey(key)) symbolTable.put(key, symbolTable.size()); &#125; &#125; System.out.printf("Done reading %s!\n", filename); // 初始化反向索引 keys = new String[symbolTable.size()]; for (String name : symbolTable.keySet()) keys[symbolTable.get(name)] = name; // 第二次读取文件,并生成图 graph = new UndirectedGraph(symbolTable.size()); Scanner create_graph_scanner = new Scanner(SymbolGraph.class.getResourceAsStream(filePath)); while (create_graph_scanner.hasNextLine()) &#123; String[] s = create_graph_scanner.nextLine().split(delimiter); // 将第一行的第一个顶点与其他顶点相连 int v = symbolTable.get(s[0]); for (int i = 1; i &lt; s.length; i++) &#123; int w = symbolTable.get(s[i]); graph.addEdge(v, w); &#125; &#125; &#125; public boolean contains(String s) &#123; return symbolTable.containsKey(s); &#125; public int indexOf(String s) &#123; return symbolTable.get(s); &#125; public String nameOf(int v) &#123; validateVertex(v); return keys[v]; &#125; public Graph graph() &#123; return graph; &#125;&#125; 参考资料 Graph (discrete mathematics) - Wikipedia Bipartite graph - Wikipedia Algorithms, 4th Edition by Robert Sedgewick and Kevin Wayne 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接. 图的那点事儿 图的那点事儿(1)-无向图 图的那点事儿(2)-有向图 图的那点事儿(3)-加权无向图 图的那点事儿(4)-加权有向图]]></content>
      <categories>
        <category>Algorithms</category>
        <category>Graph</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是动态规划?]]></title>
    <url>%2F2017%2F06%2F27%2F2017-06-27-DynamicProgramming%2F</url>
    <content type="text"><![CDATA[概述 动态规划(Dynamic Programming)是一种分阶段求解决策问题的数学思想,它通过把原问题分解为简单的子问题来解决复杂问题.动态规划在很多领域都有着广泛的应用,例如管理学,经济学,数学,生物学. 动态规划适用于解决带有最优子结构和子问题重叠性质的问题. 最优子结构 : 即是局部最优解能够决定全局最优解(也可以认为是问题可以被分解为子问题来解决),如果问题的最优解所包含的子问题的解也是最优的，我们就称该问题具有最优子结构性质. 子问题重叠 : 即是当使用递归进行自顶向下的求解时,每次产生的子问题不总是新的问题,而是已经被重复计算过的问题.动态规划利用了这种性质,使用一个集合将已经计算过的结果放入其中,当再次遇见重复的问题时,只需要从集合中取出对应的结果. 动态规划与分治算法的区别 相信了解过分治算法的同学会发现,动态规划与分治算法很相似,下面我们例举出一些它们的相同之处与不同之处. 相同点 分治算法与动态规划都是将一个复杂问题分解为简单的子问题. 分治算法与动态规划都只能解决带有最优子结构性质的问题. 不同点 分治算法一般都是使用递归自顶向下实现,动态规划使用迭代自底向上实现或带有记忆功能的递归实现. 动态规划解决带有子问题重叠性质的问题效率更加高效. 分治算法分解的子问题是相对独立的. 动态规划分解的子问题是互相带有关联且有重叠的. 斐波那契数列 斐波那契数列就很适合使用动态规划来求解,它在数学上是使用递归来定义的,公式为F(n) = F(n-1) + F(n-2). 普通递归实现一个最简单的实现如下. 12345678910public int fibonacci(int n) &#123; if (n &lt; 1) return 0; if (n == 1) return 1; if (n == 2) return 2; return fibonacci(n - 1) + fibonacci(n - 2);&#125; 但这种算法并不高效,它做了很多重复计算,它的时间复杂度为O(2^n). 动态规划递归实现使用动态规划来将重复计算的结果具有”记忆性”,就可以将时间复杂度降低为O(n). 12345678910111213141516public int fibonacci(int n) &#123; if (n &lt; 1) return 0; if (n == 1) return 1; if (n == 2) return 2; // 判断当前n的结果是否已经被计算,如果map存在n则代表该结果已经计算过了 if (map.containsKey(n)) return map.get(n); int value = fibonacci(n - 1) + fibonacci(n - 2); map.put(n, value); return value;&#125; 虽然降低了时间复杂度,但需要维护一个集合用于存放计算结果,导致空间复杂度提升了. 动态规划迭代实现通过观察斐波那契数列的规律,发现n只依赖于前2种状态,所以我们可以自底向上地迭代实现. 123456789101112131415161718192021public int fibonacci(int n) &#123; if (n &lt; 1) return 0; if (n == 1) return 1; if (n == 2) return 2; // 使用变量a,b来保存上次迭代和上上次迭代的结果 int a = 1; int b = 2; int temp = 0; for (int i = 3; i &lt;= n; i++) &#123; temp = a + b; a = b; b = temp; &#125; return temp;&#125; 这样不仅时间复杂度得到了优化,也不需要额外的空间复杂度. 参考资料 Wikipedia 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>Algorithms</category>
        <category>动态规划</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
        <tag>动态规划</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[红黑树那点事儿]]></title>
    <url>%2F2017%2F06%2F16%2F2017-06-16-RedBlackTree%2F</url>
    <content type="text"><![CDATA[概述 红黑树是一种自平衡二叉查找树,它相对于二叉查找树性能会更加高效(查找、删除、添加等操作需要O(log n),其中n为树中元素的个数),但实现较为复杂(需要保持自身的平衡). 性质 红黑树与二叉查找树不同,它的节点多了一个颜色属性,每个节点非黑即红,这也是它名字的由来. 红黑树的节点定义如以下代码: 123456789101112131415161718private static final boolean RED = true;private static final boolean BLACK = false;private Node root;private class Node &#123; private int size = 0; private boolean color = RED; //颜色 private Node parent, left, right; private int orderStatus = 0; private K key; private V value; public Node(K key, V value) &#123; this.key = key; this.value = value; this.size = 1; &#125;&#125; 完整的代码我已经放在了我的Gist中,点击查看完整代码. 红黑树需要保证以下性质: 每个节点的颜色非黑即红. 根节点的颜色为黑色. 所有叶子节点都为黑色(即NIL节点). 每个红色节点的两个子节点都必须为黑色(不能有两个连续的红节点). 从任一节点到其叶子的所有简单路径包含相同数量的黑色节点. 插入 红黑树的查找操作与二叉查找树一致(因为查找不会影响树的结构),而插入与删除操作需要在最后对树进行调整. 我们将新的节点的颜色设为红色(如果设为黑色会使根节点到叶子的一条路径上多了一个黑节点,违反了性质5,这个是很难调整的). 现在我们假设新节点为N,它的父节点为P(且P为G的左节点,如果为右节点则与其操作互为镜像),祖父节点为G,叔叔节点为U.插入一个节点会有以下种情况. 情况1N位于根,它没有父节点与子节点,这时候只需要把它重新设置为黑色即可,无需其他调整. 情况2P的颜色为黑色,这种情况下保持了性质4(N只有两个叶子节点,它们都为黑色)与性质5(N是一个红色节点,不会对其造成影响)的有效,所以无需调整. 情况3如果P与U都为红色,我们可以将它们两个重新绘制为黑色,然后将G绘制为红色(保持性质5),最后再从G开始继续向上进行调整. 情况4P为红色,U为黑色,且N为P的左子节点,这种情况下,我们需要在G处进行一次右旋转,结果满足了性质4与性质5,因为通过这三个节点中任何一个的所有路径以前都通过祖父节点G，现在它们都通过以前的父节点P. 关于旋转操作,可以查看这篇文章《Algorithms,4th Edition》读书笔记-红黑二叉查找树. 情况5P为红色,U为黑色,且N为P的右子节点,我们需要先在P处进行一次左旋转,这样就又回到了情况4. 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 private void fixAfterInsertion(Node x) &#123; while (x != null &amp;&amp; x != root &amp;&amp; colorOf(parentOf(x)) == RED) &#123; if (parentOf(x) == grandpaOf(x).left) &#123; x = parentIsLeftNode(x); &#125; else &#123; x = parentIsRightNode(x); &#125; fixSize(x); &#125; setColor(root, BLACK); &#125; private Node parentIsLeftNode(Node x) &#123; Node xUncle = grandpaOf(x).right;// 情况3 if (colorOf(xUncle) == RED) &#123; x = uncleColorIsRed(x, xUncle); &#125; else &#123; // 情况5 if (x == parentOf(x).right) &#123; x = parentOf(x); rotateLeft(x); &#125; // 情况4 rotateRight(grandpaOf(x)); &#125; return x; &#125; private Node parentIsRightNode(Node x) &#123; Node xUncle = grandpaOf(x).left; if (colorOf(xUncle) == RED) &#123; x = uncleColorIsRed(x, xUncle); &#125; else &#123; if (x == parentOf(x).left) &#123; x = parentOf(x); rotateRight(x); &#125; rotateLeft(grandpaOf(x)); &#125; return x; &#125; private Node uncleColorIsRed(Node x, Node xUncle) &#123; setColor(parentOf(x), BLACK); setColor(xUncle, BLACK); setColor(grandpaOf(x), RED); x = grandpaOf(x); return x; &#125; 删除我们只考虑删除节点只有一个子节点的情况,且只有后继节点与删除节点都为黑色(如果删除节点为红色,从根节点到叶子节点的每条路径上少了一个红色节点并不会违反红黑树的性质,而如果后继节点为红色,只需要将它重新绘制为黑色即可). 先将删除节点替换为后继节点,且后继节点定义为N,它的兄弟节点为S. 情况1N为新的根节点,在这种情况下只需要把根节点保持为黑色即可. 情况2S为红色,只需要在P进行一次左旋转,接下来则继续按以下情况进行处理(尽管路径上的黑色节点数量没有改变,但N有了一个黑色的兄弟节点与红色的父节点). 情况3S和它的子节点都是黑色的,而P为红色.这种情况下只需要将S与P的颜色进行交换 情况4S和它的子节点都是黑色的,这种情况下需要把S重新绘制为红色.这时不通过N的路径都将少一个黑色节点(通过N的路径因为删除节点是黑色的也都少了一个黑色节点),这让它们平衡了起来. 但现在通过P的路径比不通过P的路径都少了一个黑色节点,所以还需要在P上继续进行调整. 情况5S为黑色,它的左子节点为红色,右子节点为黑色.这种情况下,我们在S上做右旋转,这样S的左儿子成为S的父亲和N的新兄弟。我们接着交换S和它的新父亲的颜色。所有路径仍有同样数目的黑色节点，但是现在N有了一个右儿子是红色的黑色兄弟，所以我们进入了情况6。N和P都不受这个变换的影响。 情况6S是黑色，它的右子节点是红色,我们在N的父亲P上做左旋转.这样S成为N的父亲和S的右儿子的父亲。我们接着交换N的父亲和S的颜色，并使S的右儿子为黑色。子树在它的根上的仍是同样的颜色,但是,N现在增加了一个黑色祖先.所以,通过N的路径都增加了一个黑色节点.此时,如果一个路径不通过N,则有两种可能性: 它通过N的新兄弟.那么它以前和现在都必定通过S和N的父亲,而它们只是交换了颜色.所以路径保持了同样数目的黑色节点. 它通过N的新叔父,S的右儿子.那么它以前通过S、S的父亲和S的右儿子,但是现在只通过S,它被假定为它以前的父亲的颜色,和S的右儿子,它被从红色改变为黑色.合成效果是这个路径通过了同样数目的黑色节点. 在任何情况下,在这些路径上的黑色节点数目都没有改变.所以我们恢复了性质4.在示意图中的白色节点可以是红色或黑色,但是在变换前后都必须指定相同的颜色. 代码12345678910111213141516171819202122232425262728293031323334353637383940 private void fixAfterDeletion(Node x) &#123; while (x != null &amp;&amp; x != root &amp;&amp; colorOf(x) == BLACK) &#123; if (x == parentOf(x).left) &#123; x = successorIsLeftNode(x); &#125; else &#123; x = successorIsRightNode(x); &#125; &#125; setColor(x, BLACK); &#125; private Node successorIsLeftNode(Node x) &#123; Node brother = parentOf(x).right;// 情况2 if (colorOf(brother) == RED) &#123; rotateLeft(parentOf(x)); brother = parentOf(x).right; &#125;// 情况3,4 if (colorOf(brother.left) == BLACK &amp;&amp; colorOf(brother.right) == BLACK) &#123; x = brotherChildrenColorIsBlack(x, brother); &#125; else &#123; // 情况5 if (colorOf(brother.right) == BLACK) &#123; rotateRight(brother); brother = parentOf(x).right; &#125; // 情况6 setColor(brother.right, BLACK); rotateLeft(parentOf(x)); x = root; &#125; return x; &#125; private Node brotherChildrenColorIsBlack(Node x, Node brother) &#123; setColor(brother, RED); x = parentOf(x); return x; &#125; 参考资料 Wikipedia 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>Tree</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出排序算法(3)-快速排序]]></title>
    <url>%2F2017%2F06%2F14%2F2017-06-14-sort_algorithms_qucikSort%2F</url>
    <content type="text"><![CDATA[概述 快速排序与归并排序一样也是基于分治算法的排序算法.所以它的实现方法也与其他的分治算法一样,需要进行分解子任务,处理子任务,归并子任务这些步骤. 但快速排序与归并排序不同,它是一种原地排序算法(不需要额外的辅助数组),且快速排序不使用中间值来分解任务,而是使用划分函数. 算法过程 从数组中挑选出一个值,作为基准值 k. 重新排序序列,将所有小于k的值放到k前面,所有大于k的值放到k后面(也可以理解为将数组a切分为两个子数组a[begin...k-1],a[k+1...end],其中前一个子数组都小于k,后一个子数组都大于k). 递归地将两个子数组进行快速排序(递归到最底部时,子数组的大小是零或一,也就是已经排序好了.). 划分函数 划分函数就是上述步骤中的第二步,它将数组根据基准值进行重排序.根据基准值选择的位置不同,划分函数也有不同的实现方法,不过其根本思想都是将小于基准值的值放到前面,大于基准值的值放到后面. 使用末尾元素作为基准值 123456789101112131415161718// 使用末尾元素作为基准值来进行切分private static int partitionUseEnd(Comparable[] a, int begin, int end) &#123; Comparable pivot = a[end]; // 基准值,切分后的数组应满足左边都小于基准,右边都大于基准 int i = begin - 1; for (int j = begin; j &lt; end; j++) &#123; // 如果j小于基准值则与i交换 if (less(a[j], pivot)) &#123; i++; swap(a, i, j); &#125; &#125; // 将基准值交换到正确的位置上 int pivotLocation = i + 1; swap(a, pivotLocation, end); return pivotLocation;&#125; 使用首元素作为基准值 1234567891011121314151617181920212223242526272829// 使用首元素作为基准值来进行切分private static int partitionUseBegin(Comparable[] a, int begin, int end) &#123; Comparable pivot = a[begin]; int i = begin; int j = end + 1; while (true) &#123; // 从左向右扫描,直到找出一个大于等于基准的值 while (less(a[++i], pivot)) &#123; if (i &gt;= end) break; &#125; // 从右向左扫描,直到找出一个小于等于基准的值 while (less(pivot, a[--j])) &#123; if (j &lt;= begin) break; &#125; // 如果指针i与j发生碰撞则结束循环 if (i &gt;= j) break; // 将左边大于小于基准的值与右边小于等于基准的值进行交换 swap(a, i, j); &#125; // 将基准值交换到正确的位置上 swap(a, begin, j); return j;&#125; 代码实现 了解了划分函数的实现,剩下就只需要递归地调用快速排序不断地分解子任务即可. 注意,快速排序与归并排序不同,它不需要进行归并(划分后就已经是有序的了),并且是先进行划分函数,再分解任务. 123456789101112public static void sort(Comparable[] a) &#123; sort(a, 0, a.length - 1);&#125;private static void sort(Comparable[] a, int begin, int end) &#123; if (begin &gt;= end) return; int k = partitionUseEnd(a, begin, end); sort(a, begin, k - 1); sort(a, k + 1, end);&#125; 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>Algorithms</category>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出排序算法(2)-归并排序]]></title>
    <url>%2F2017%2F06%2F12%2F2017-06-12-sort_algorithmes_mergeSort%2F</url>
    <content type="text"><![CDATA[概述 归并排序是基于分治算法实现的一种排序算法,它将数组分割为两个子数组,然后对子数组进行排序,最终将子数组归并为有序的数组. 归并排序的时间复杂度为O(n log n),空间复杂度为O(1),并且它是稳定的排序算法(所谓稳定即是不影响值相等元素的相对次序). 算法过程 首先,归并排序需要将一个大小为n个元素的数组分解为各包含n/2个元素的子数组(这个分解的过程会不断进行,直到子数组元素个数为1). 当子数组的元素个数为1时,代表这个子数组已经有序,开始两两归并(将两个个数为1的子数组归并为一个个数为2的子数组,不断归并,直到所有子数组个数为2,然后继续将两个个数为2的子数组归并为一个个数为4的子数组….以此类推). 不断重复步骤2,直到整个数组有序. 归并 通过以上的了解,我们发现归并排序中最重要的步骤就是归并. 采用类似洗牌的方式来理解这个过程.想象辅助数组为一个空牌堆,两个子数组为两堆牌a和b.我们从a堆与b堆中各取出一张牌进行比较,然后将较小的牌放入空牌堆中,不断重复比较直到任一牌堆为空.最后,再将未空的牌堆全部放入空牌堆中. 1234567891011121314151617181920212223242526// 将两个子序列进行归并private static void merge(Comparable[] a, int lo, int mid, int hi) &#123; Comparable[] aux = new Comparable[a.length]; // 辅助数组 int i = lo, j = mid + 1; int count = lo; // 对[lo...mid] 与 [mid+1...hi] 两个子序列的首元素进行比较,将较小的元素放入辅助数组 while (i &lt;= mid &amp;&amp; j &lt;= hi) &#123; if (less(a[i], a[j])) aux[count++] = a[i++]; else aux[count++] = a[j++]; &#125; //将[lo...mid] 与 [mid+1...hi] 两个子序列中剩余的元素放入辅助数组 while (i &lt;= mid) &#123; aux[count++] = a[i++]; &#125; while (j &lt;= hi) &#123; aux[count++] = a[j++]; &#125; // 将辅助数组中的元素复制到源数组中 for (int k = lo; k &lt;= hi; k++) &#123; a[k] = aux[k]; &#125;&#125; 递归实现 只要理解了归并的过程,剩下就很容易实现了.归并排序的递归实现如下. 12345678910111213141516 public static void sort(Comparable[] a) &#123; sort(a, 0, a.length - 1); &#125; // 递归实现归并排序 private static void sort(Comparable[] a, int lo, int hi) &#123; if (lo &gt;= hi) return; int mid = (lo + hi) &gt;&gt;&gt; 1; // (lo + hi) / 2// 分解数组 sort(a, lo, mid); sort(a, mid + 1, hi);// 归并 merge(a, lo, mid, hi); &#125; 非递归实现 我们已经知道了归并排序中最小子数组的元素个数为1,非递归实现只需要从1开始自底向上地归并即可(递归实现的真实计算过程也是如此,这是由于递归调用是后进先出的). 123456789101112131415161718192021222324252627282930313233343536373839404142434445 // 非递归实现归并排序 private static void sortUnRecursive(Comparable[] a) &#123; int len = 1; // 自底向上实现归并排序,子序列的最小粒度为1 while (len &lt; a.length) &#123; for (int i = 0; i &lt; a.length; i += len &lt;&lt; 1) &#123; merge(a, i, len); &#125; len = len &lt;&lt; 1; // 子序列规模每次迭代时乘2 &#125; &#125;// 与递归实现的归并函数不同,需要注意边界检查 private static void merge(Comparable[] a, int lo, int hi) &#123; int length = a.length; Comparable[] aux = new Comparable[length]; int count = lo; // 子数组1 int i = lo; int i_bound = lo + hi; // 子数组2 int j = i_bound; int j_bound = j + hi; // 注意j的边界检查 while (i &lt; i_bound &amp;&amp; j &lt; j_bound &amp;&amp; j &lt; length) &#123; if (less(a[i], a[j])) aux[count++] = a[i++]; else aux[count++] = a[j++]; &#125; // i和j都有可能越界 while (i &lt; i_bound &amp;&amp; i &lt; length) &#123; aux[count++] = a[i++]; &#125; while (j &lt; j_bound &amp;&amp; j &lt; length) &#123; aux[count++] = a[j++]; &#125; int k = lo; while (k &lt; j &amp;&amp; k &lt; length) &#123; a[k] = aux[k]; k++; &#125; &#125; 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>Algorithms</category>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入浅出排序算法(1)-堆排序]]></title>
    <url>%2F2017%2F06%2F09%2F2017-06-09-sort_algorithms_heapSort%2F</url>
    <content type="text"><![CDATA[概述 堆排序即是利用堆这个数据结构来完成排序的.所以,要想理解堆排序就要先了解堆. 堆 堆(Heap)是一种数据结构,它可以被看做是一棵树的数组对象.一个二叉堆拥有以下性质. 父节点k的左子节点在数组中的索引位置为2 * k + 1. 父节点k的右子节点在数组中的索引位置为2 * k + 2. 子节点i的父节点在数组中的索引位置为(i - 1) / 2. 父节点k的任意子节点都必须小于(或大于)k. 根节点必须是最大节点(或最小节点). 最大堆代码实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class MaxHeap&lt;T extends Comparable&gt; &#123; T[] heap; private MaxHeap() &#123; &#125; public MaxHeap(T[] heap) &#123; this.heap = heap; buildHeap(); &#125; /** * 自底向上构建堆 */ private void buildHeap() &#123; int length = heap.length; // 当堆为空或者长度为1时不需要任何操作 if (length &lt;= 1) return; int root = (length - 2) &gt;&gt;&gt; 1; // (i - 1) / 2 while (root &gt;= 0) &#123; heapify(heap, length, root); root--; &#125; &#125; /** * 调整堆的结构 * * @param heap 堆 * @param length 堆的长度 * @param root 根节点索引 */ public void heapify(T[] heap, int length, int root) &#123; if (root &gt;= length) return; int largest = root; // 表示root,left,right中最大值的变量 int left = (root &lt;&lt; 1) + 1; // 左子节点,root * 2 + 1 int right = left + 1; // 右子节点,root * 2 + 2 // 找出最大值 if (left &lt; length &amp;&amp; greater(heap[left], heap[largest])) largest = left; if (right &lt; length &amp;&amp; greater(heap[right], heap[largest])) largest = right; // 如果largest发生变化,将largest与root交换 if (largest != root) &#123; T t = heap[root]; heap[root] = heap[largest]; heap[largest] = t; // 继续向下调整堆 heapify(heap, length, largest); &#125; &#125; private boolean greater(Comparable a, Comparable b) &#123; return a.compareTo(b) &gt; 0; &#125;&#125; 优先队列 普通的队列是基于先进先出的,也就是说最先入队的元素永远是在第一位,而优先队列中的每一个元素都是拥有优先级的,优先级最高的元素永远在第一位. 优先队列也是贪心算法的体现,所谓的贪心算法即是在问题求解的每一步中总是选择当前最好的结果. 堆就是用于实现优先队列的,因为堆的性质与优先队列十分吻合. 添加 往优先队列中添加元素时,我们只需要将元素添加到数组末尾并调整堆(以下例子均是以最大堆为例). 1234567891011121314151617181920212223242526272829 public boolean add(T t) &#123; if (t == null) throw new NullPointerException(); if (size == queue.length) resize(queue.length * 2); int i = size; // 如果当前队列为空,则不需要进行堆调整直接插入元素即可 if (i == 0) queue[0] = t; else swim(i, t); size++; return true; &#125;// 上浮调整 private void swim(int i, T t) &#123; Comparable&lt;? super T&gt; key = (Comparable) t; while (i &gt; 0) &#123; int parent = (i - 1) &gt;&gt;&gt; 1; T p = (T) queue[parent]; // 如果key小于他的父节点(符合最大堆规则)则结束调整 if (key.compareTo(p) &lt; 0) break; queue[i] = p; i = parent; &#125; queue[i] = key; &#125; 删除 删除操作要稍微麻烦一点,将优先队列中末尾的元素放到队头并进行堆调整. 12345678910111213141516171819202122232425262728293031323334 public T poll() &#123; if (isEmpty()) return null; int s = --size; Object result = queue[0]; Object end = queue[s]; queue[s] = null; if (s != 0) sink(0, (T) end); if (size &lt;= queue.length / 4) resize(queue.length / 2); return (T) result; &#125;// 下沉调整 private void sink(int i, T t) &#123; Comparable&lt;? super T&gt; key = (Comparable&lt;? super T&gt;) t; int half = size &gt;&gt;&gt; 1; while (i &lt; half) &#123; int child = (i &lt;&lt; 1) + 1; // 左子节点 int right = child + 1; // 右子节点 T max = (T) queue[child]; // find maximum element if (right &lt; size &amp;&amp; ((Comparable&lt;? super T&gt;) max).compareTo((T) queue[right]) &lt; 0) max = (T) queue[child = right]; // key大于它的最大子节点(符合最大堆规则)则结束调整 if (key.compareTo(max) &gt; 0) break; queue[i] = max; i = child; &#125; queue[i] = key; &#125; 点击查看优先队列完整代码 堆排序 实现堆排序有两种方法,一种是使用优先队列,另一种是直接使用堆. 直接使用堆实现堆排序 123456789101112// 使用最大堆实现堆排序private static void maxHeapSort(Comparable[] a) &#123; MaxHeap&lt;Comparable&gt; maxHeap = new MaxHeap&lt;&gt;(a); //不断地将最大堆中顶端元素(最大值)与最底部的元素(最小值)交换 for (int i = a.length - 1; i &gt; 0; i--) &#123; Comparable largest = a[0]; a[0] = a[i]; a[i] = largest; // 堆减少,并调整新的堆 maxHeap.heapify(a, i, 0); &#125;&#125; 使用优先队列实现堆排序12345678910// 使用优先队列实现堆排序private static void pqSort(Comparable[] a) &#123; MinPriorityQueue&lt;Comparable&gt; priorityQueue = new MinPriorityQueue&lt;&gt;(); for (int i = 0; i &lt; a.length; i++) &#123; priorityQueue.add(a[i]); &#125; for (int i = 0; i &lt; a.length; i++) &#123; a[i] = priorityQueue.poll(); &#125;&#125; 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>Algorithms</category>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IoC与AOP的那点事儿]]></title>
    <url>%2F2017%2F06%2F07%2F2017-06-07-IoC%26AOP%2F</url>
    <content type="text"><![CDATA[IoC 控制反转(Inversion of Control)是OOP中的一种设计原则,也是Spring框架的核心.大多数应用程序的业务逻辑代码都需要两个或多个类进行合作完成的,通过IoC则可以减少它们之间的耦合度. 实现方法 IoC的主要实现方法有两种,依赖注入与依赖查找. 依赖注入 : 应用程序被动的接收对象,IoC容器通过类型或名称等信息来判断将不同的对象注入到不同的属性中. 依赖注入主要有以下的方式: 基于set方法 : 实现特定属性的public set()方法,来让IoC容器调用注入所依赖类型的对象. 基于接口 : 实现特定接口以供IoC容器注入所依赖类型的对象. 基于构造函数 : 实现特定参数的构造函数,在创建对象时来让IoC容器注入所依赖类型的对象. 基于注解 : 通过Java的注解机制来让IoC容器注入所依赖类型的对象,例如Spring框架中的@Autowired. 依赖查找 : 它相对于依赖注入而言是一种更为主动的方法,它会在需要的时候通过调用框架提供的方法来获取对象,获取时需要提供相关的配置文件路径、key等信息来确定获取对象的状态. IoC的思想 在传统实现中,我们都是通过应用程序自己来管理依赖的创建,例如下代码. 123456789public class Person &#123; // 由Person自己管理Food类的创建 public void eat() &#123; Food food = new Chicken(); System.out.println("I am eating " + food.getName() + "..."); &#125;&#125; 而IoC则是通过一个第三方容器来管理并维护这些被依赖对象,应用程序只需要接收并使用IoC容器注入的对象而不需要关注其他事情. 1234567891011121314public class Person &#123; private Food food; // 通过set注入 public void setFood(Food food) &#123; this.food = food; &#125; // Person不需要关注Food,只管使用即可 public void eat() &#123; System.out.println("I am eating " + this.food.getName() + "..."); &#125;&#125; 通过以上的例子我们能够发现,控制反转其实就是对象控制权的转移,应用程序将对象的控制权转移给了第三方容器并通过它来管理这些被依赖对象,完成了应用程序与被依赖对象的解耦. AOP AOP(Aspect-Oriented Programming)即面向方面编程.它是一种在运行时,动态地将代码切入到类的指定方法、指定位置上的编程思想.用于切入到指定类指定方法的代码片段叫做切面,而切入到哪些类中的哪些方法叫做切入点. AOP是OOP的有益补充,OOP从横向上区分出了一个个类,AOP则从纵向上向指定类的指定方法中动态地切入代码.它使OOP变得更加立体. Java中的动态代理或CGLib就是AOP的体现. 案例分析 在OOP中,我们使用封装的特性来将不同职责的代码抽象到不同的类中.但是在分散代码的同时,也增加了代码的重复性. 例如,我们需要在两个或多个类中的方法都记录日志或执行时间,可能这些代码是完全一致的,但因为类与类无法联系造成代码重复. 12345678910111213141516171819202122232425public class A &#123; public void something () &#123; // 业务逻辑... recordLog(); &#125; private void recordLog() &#123; // 记录日志... &#125;&#125;public class B &#123; public void something () &#123; // 业务逻辑... recordLog(); &#125; private void recordLog() &#123; // 记录日志... &#125;&#125; 接下来,我们采取两种不同方案来改进这段代码. 将重复代码抽离到一个类中 12345678910111213141516171819202122232425public class A &#123; public void something () &#123; // 业务逻辑... Report.recordLog(); &#125;&#125;public class B &#123; public void something () &#123; // 业务逻辑... Report.recordLog(); &#125;&#125;public class Report &#123; public static void recordLog (String ...messages) &#123; // 记录日志... &#125;&#125; 这样看似解决了问题,但类之间已经耦合了.并且当这些外围业务代码(日志,权限校验等)越来越多时,它们的侵入(与核心业务代码混在一起)会使代码的整洁度变得混乱不堪. 使用AOP分离外围业务代码 我们使用AspectJ,它是一个AOP框架,扩展了Java语言,并定义了AOP语法(通过它实现的编译器). 使用AspectJ需要先安装并将lib中aspectjrt.jar添加进入classpath,下载地址. 12345678910111213141516171819202122232425262728public class Something &#123; public void say() &#123; System.out.println("Say something..."); &#125; public static void main(String[] args) &#123; Something something = new Something(); something.say(); &#125;&#125;public aspect SomethingAspect &#123; /** * 切入点,切入到Something.say() */ pointcut recordLog():call(* com.sun.sylvanas.application.hello_aop.Something.say(..)); /** * 在方法执行后执行 */ after():recordLog() &#123; System.out.println("[AFTER] Record log..."); &#125;&#125; AOP解决了代码的重复并将这些外围业务代码抽离到一个切面中,我们可以动态地将切面切入到切入点. 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>2017</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈如何实现一个非阻塞的线程安全的集合]]></title>
    <url>%2F2017%2F05%2F29%2F2017-5-29-cas_concurrent_stack%2F</url>
    <content type="text"><![CDATA[概述 众所周知,想要在java中要实现一个线程安全的类有很多方法.最简单直接的即是使用synchronized关键字或ReentrantLock. 但是,这两种同步方法都是基于锁的,基于锁的同步方法是阻塞的,即未争夺到锁的线程需要阻塞等待(或挂起)直到锁可用. 这种方法具有一些明显的缺点: 被阻塞的线程无法去做任何其他事情,如果这个线程是优先级较高的线程甚至会发生非常不好的结果(优先级倒置). 由于java的线程模型是基于内核线程实现的,挂起恢复线程需要来回地切换到内核态,性能开销很大. 当两个(或多个)线程都阻塞着等待另一方释放锁时,将会引发死锁. 那么有非阻塞的方法来实现同步吗?(volatile关键字也是非阻塞的,但它只保证了数据的可见性与有序性,并不保证原子性) 有!在jdk5中,java增加了大量的原子类来保证无锁下的操作原子性,可以说java.util.concurrent包下的所有类都几乎用到了这些原子类. CAS 这些原子类都是基于CAS实现的,CAS即是Compare And Swap,它的原理简单来讲就是在更新新值之前先去比较原值有没有发生变化,如果没发生变化则进行更新. java中的CAS是通过Unsafe类中的本地方法实现的,而这些本地方法需要通过现代处理器提供的CAS指令实现(在Intel处理器中该指令为cmpxchg). 所以我们发现,CAS操作的原子性是由处理器来保证的. 比较的过程在CAS操作中包含了三个数,V(内存位置),A(预期值),B(新值). 首先会将V与A进行匹配. 如果两个值相等,则使用B作为新值进行更新. 如果不相等,则不进行更新操作(一般的补救措施是继续进行请求). 与锁相比的优点 CAS操作是无锁的实现,所以它不会发生死锁情况. 虽然CAS操作失败需要不断的进行请求重试,但相对于不断地挂起或恢复线程来说,性能开销要低得多. CAS的粒度更细,操作也更加轻量与灵活. ConcurrentStack 我们通过实现一个简单的ConcurentStack来看看CAS操作是如何保证线程安全的. 完整代码请从作者的Gist中获取 节点的实现12345678910111213141516171819public class ConcurrentStack&lt;E&gt; implements Iterable&lt;E&gt; &#123; private AtomicReference&lt;Node&lt;E&gt;&gt; head = new AtomicReference&lt;&gt;(null); private AtomicInteger size = new AtomicInteger(0); /** * This internal static class represents the nodes in the stack. */ private static class Node&lt;E&gt; &#123; private final E value; private volatile Node&lt;E&gt; next; private Node(E value, Node&lt;E&gt; next) &#123; this.value = value; this.next = next; &#125; &#125;&#125; pushpush函数主要是通过观察头节点(这里的头节点即是V),然后构建一个新的节点(它代表B)放于栈顶,如果V没有发生变化,则进行更新.如果发生了变化(被其他线程修改),就重新尝试进行CAS操作. 12345678910111213141516171819202122/** * Insert a new element to the this stack. * * @return if &#123;@code true&#125; insert success,&#123;@code false&#125; otherwise * @throws IllegalArgumentException if &#123;@code value&#125; is null */public boolean put(E value) &#123; if (value == null) throw new IllegalArgumentException(); return putAndReturnResult(value);&#125;private boolean putAndReturnResult(E value) &#123; Node&lt;E&gt; oldNode; Node&lt;E&gt; newNode; do &#123; oldNode = head.get(); newNode = new Node&lt;E&gt;(value, oldNode); &#125; while (!head.compareAndSet(oldNode, newNode)); sizePlusOne(); return true;&#125; poppop函数中的CAS操作的思想基本与push函数一致. 1234567891011121314151617181920212223/** * Return the element of stack top and remove this element. * * @throws NullPointerException if this stack is empty */public E pop() &#123; if (isEmpty()) throw new NullPointerException(); return removeAndReturnElement();&#125;private E removeAndReturnElement() &#123; Node&lt;E&gt; oldNode; Node&lt;E&gt; newNode; E result; do &#123; oldNode = head.get(); newNode = oldNode.next; result = oldNode.value; &#125; while (!head.compareAndSet(oldNode, newNode)); sizeMinusOne(); return result;&#125; end 非阻塞的算法实现的复杂度要比阻塞算法复杂的多,但它能带来更少的性能开销,在jdk中,很多线程安全类都是在尽量地避免使用锁的基础上来实现线程安全. 本文作者为SylvanasSun(sylvanassun_xtz@163.com),转载请务必指明原文链接.]]></content>
      <categories>
        <category>后端</category>
        <category>多线程</category>
        <category>CAS</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>CAS</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Algorithms,4th Edition》读书笔记-散列表]]></title>
    <url>%2F2017%2F04%2F13%2F2017-4-13-hash_table%2F</url>
    <content type="text"><![CDATA[概述 散列表(Hash Table,也叫哈希表),它是根据键而直接访问在内存存储位置的数据结构.也可以说是用一个数组来实现的无序的符号表,将键作为数组的索引而数组中键i处存储的就是它对应的值. 散列表通过散列函数将键转化为数组的索引来访问数组中的键值对. 在散列表的算法中,最重要的两个操作如下. 使用散列函数将被查找的键转化为数组的一个索引. 处理散列表中的碰撞冲突问题. 性质 若关键字为k,则其值存放于的存储位置上.由此,不需要比较便可直接取得所查记录.称这个对应关系为散列函数,按照这个思想建立的符合表为散列表. 对不同的键可能会得到同一个散列地址,即,而,这种现象被称为碰撞冲突.具有相同函数值的键对该散列函数来说称做同义词.综上所述,根据散列函数和处理碰撞冲突的方法将一组键映射到一个有限的连续的地址集(区间)上,这种表称为散列表,这一映射过程称为散列,所得的存储位置称为散列地址. 若对于键集合中的任一个键,经散列函数映射到地址集合中任何一个地址的概率是相等的,则这个散列函数被称为均匀散列函数,它可以减少碰撞冲突. 散列函数 散列函数用于将键转化为数组的索引.如果我们有一个能够保存M个键值对的数组,那么我们就需要一个能够将任意键转化为该数组范围内的索引([0,M-1]范围内的整数)的散列函数 散列函数与键的类型有关,对于每种类型的键都需要一个与之对应的散列函数. 实现散列函数的几种方法 直接定址法 : 取key或者key的某个线性函数值为散列地址.即或其中a,b为常数(这种散列函数叫做自身函数). 数字分析法 : 假设key是以r为基的数,并且散列表中可能出现的key都是事先知道的,则可取key的若干数位组成散列地址. 平方取中法 : 取key平方后的中间几位为散列地址.通常在选定散列函数时不一定能知道key的全部情况,取其中的哪几位也不一定合适,而一个数平方后的中间几位数和数的每一位都相关,由此使随机分布的key得到的散列地址也是随机的.取的位数由表长决定. 折叠法 : 将key分割成位数相同的几部分(最后一部分的位数可以不同),然后取这几部分的叠加和(舍去进位)作为散列地址. 除留余数法 : 取key被某个不大于散列表长度m的数p除后所得的余数为散列地址.即,.不仅可以对key直接取模，也可在折叠法、平方取中法等运算之后取模。对p的选择很重要，一般取素数或m，若p选择不好，容易产生碰撞冲突. 正整数将正整数散列一般使用的是除留余数法.我们选择大小为素数M的数组,对于任意正整数k,计算k除以M的余数(即k%M).它能够有效地将key散布在0到M-1的范围内. 如果M不是素数,可能无法利用key中包含的所有信息,这可能导致无法均匀地散列散列值. 浮点数对浮点数进行散列一般是将key表示为二进制数然后再使用除留余数法. 字符串除留余数法也可以处理较长的key,例如字符串,我们只需将它们当成大整数即可. 12345int hash = 0;for (int i = 0; i &lt; s.length(); i++) &#123; hash = (R * hash + s.charAt(i)) % M;&#125; Java的charAt()函数能够返回一个char值,即一个非负16位整数.如果R比任何字符的值都大,这种计算相当于将字符串当作一个N位的R进制值,将它除以M并取余.只要R足够小,不造成溢出,那么结果就能够落在0至M-1之间.可以使用一个较小的素数,例如31. 组合键如果key的类型含有多个整型变量,我们可以和字符串类型一样将它们混合起来. 例如,key的类型为Date,其中含有几个整型的域 : day(两个数字表示的日),month(两个数字表示的月),year(四个数字表示的年).我们可以这样计算它的散列值: 1int hash = (((day * R + month) % M) * R + year) % M; Java中的约定在Java中如果要为自定义的数据类型定义散列函数,需要同时重写hashCode()和equals()两个函数,并要遵守以下规则. hashCode()与equals()的结果必须保持一致性.即a.equals(b)返回true,则a.hashCode()的返回值也必然和b.hashCode()的返回值相同. 但如果两个对象的hashCode()函数的返回值相同,这两个对象也有可能不同,还需要用equals()函数进行判断. 一个使用除留余数法的简单散列函数如下,它会将符号位屏蔽(将一个32位整数变为一个31位非负整数). 123private int hash(Key key) &#123; return (key.hashCode() &amp; 0x7fffffff) % M;&#125; 软缓存由于散列函数的计算有可能会很耗时,我们可以进行缓存优化,将每个key的散列值缓存起来(可以在每个key中使用一个hash变量来保存它的hashCode()的返回值). 当第一次调用hashCode()时,需要计算对象的散列值,但之后对hashCode()方法的调用会直接返回hash变量的值. 总结总之,要想实现一个优秀的散列函数需要满足以下的条件. 一致性,等价的key必然产生相等的散列值. .高效性,计算简便. 均匀性,均匀地散列所有的key. 基于拉链法的散列表 拉链法是解决碰撞冲突的一种策略,它的核心思想是 : 将大小为M的数组中的每个元素指向一条链表,链表中的每个节点都存储了散列值为该元素的索引的键值对. 拉链法的实现一般分为以下两种: 使用一个原始的链表数据类型来表示数组中的每个元素. 使用一个符号表实现来表示数组中的每个元素(这个方法实现简单但效率偏低). 12345678910111213141516171819202122232425262728public class SeparateChainingHashST&lt;K, V&gt; &#123; private static final int INIT_CAPACITY = 4; private int n; // the number of key-value pairs in the symbol table private int m; // the number of size of separate chaining table private Node&lt;K, V&gt;[] table; // array of linked-list symbol tables private class Node&lt;K, V&gt; &#123; private K key; private V value; private Node&lt;K,V&gt; next; public Node() &#123; &#125; public Node(K key, V value, Node next) &#123; this.key = key; this.value = value; this.next = next; &#125; &#125; private int hash(K key) &#123; return ((key.hashCode()) &amp; 0x7fffffff) % m; &#125;&#125; 查找、插入、删除基于拉链法的散列表的查找、插入、删除算法基本分为两步: 首先根据散列值找到对应的链表. 然后沿着这条链表进行相应的操作. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public V get(K key) &#123; if (key == null) throw new IllegalArgumentException("called get() with key is null."); int i = hash(key); Node x = table[i]; while (x != null) &#123; if (key.equals(x.key)) return (V) x.value; x = x.next; &#125; return null;&#125; public void put(K key, V value) &#123; if (key == null) throw new IllegalArgumentException("called put() with key is null."); if (value == null) &#123; remove(key); return; &#125; // double table size if average length of list &gt;= 10 if (n &gt;= 10 * m) resize(2 * m); int i = hash(key); Node x = table[i]; Node p = null; while (x != null) &#123; if (key.equals(x.key)) &#123; x.value = value; return; &#125; p = x; x = x.next; &#125; if (p == null) &#123; table[i] = new Node(key, value, null); n++; &#125; else &#123; p.next = new Node(key, value, null); n++; &#125;&#125; public V remove(K key) &#123; if (key == null) throw new IllegalArgumentException("called remove() with key is null."); if (isEmpty()) throw new NoSuchElementException("called remove() with empty symbol table."); if (!contains(key)) return null; int i = hash(key); Node x = table[i]; Node p = null; V oldValue = null; while (x != null) &#123; if (key.equals(x.key)) &#123; oldValue = (V) x.value; if (p == null) &#123; table[i] = x.next; &#125; else &#123; p.next = x.next; &#125; n--; break; &#125; p = x; x = x.next; &#125; // halve table size if average length of list &lt;= 2 if (m &gt; INIT_CAPACITY &amp;&amp; n &lt;= 2 * m) resize(m / 2); return oldValue;&#125; 基于线性探测法的散列表 解决碰撞冲突的另一种策略是使用线性探测法.它的核心思想是: 使用大小为M的数组保存N个键值对,其中M&gt;N.这种方法需要依靠数组中的空位来解决碰撞冲突,基于这种策略的所有方法被统称为开放地址散列表. 开放地址散列表中最简单的方法就是线性探测法: 当发生碰撞冲突时,我们直接检查散列表中的下一个位置(将索引值加1).它可能会产生三种结果: 命中,该位置的key和被查找的key相同. 未命中,key为空(该位置没有key). 继续查找,该位置的key和被查找的key不同. 我们使用散列函数找到key在数组中的索引,检查其中的key和被查找的key是否相同.如果不同则继续查找(将索引值加1,到达数组结尾时折回数组的开头),直到找到该key或者遇到一个空元素. 开放地址散列表的核心思想是: 与其将内存用作链表,不如将它们作为在散列表的空元素(这些空元素可以作为查找结束的标识). 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class LinearProbingHashST&lt;K, V&gt; &#123; private static final int INIT_CAPACITY = 4; private int n; // the number of key-value pairs in the symbol table private int m; // the number of size of linear probing table private K[] keys; // the keys private V[] vals; // the values /** * Initializes an empty symbol table. */ public LinearProbingHashST() &#123; this(INIT_CAPACITY); &#125; /** * Initializes an empty symbol table with the specified initial capacity. * * @param capacity the initial capacity */ public LinearProbingHashST(int capacity) &#123; m = capacity; n = 0; keys = (K[]) new Object[m]; vals = (V[]) new Object[m]; &#125; public V get(K key) &#123; if (key == null) throw new IllegalArgumentException("called get() with key is null."); for (int i = hash(key); keys[i] != null; i = (i + 1) % m) &#123; if (keys[i].equals(key)) return vals[i]; &#125; return null; &#125; public void put(K key, V value) &#123; if (key == null) throw new IllegalArgumentException("called put() with key is null."); if (value == null) &#123; delete(key); return; &#125; // double table size if 50% full if (n &gt;= m / 2) resize(2 * m); int i; for (i = hash(key); keys[i] != null; i = (i + 1) % m) &#123; if (keys[i].equals(key)) &#123; vals[i] = value; return; &#125; &#125; keys[i] = key; vals[i] = value; n++; &#125; &#125; 删除基于线性探测法的散列表的删除操作较为复杂,我们不能直接将key所在的位置设为null,这样会使在此位置之后的元素无法被查找到. 因此,我们需要将被删除键的右侧的所有键重新插入到散列表中. 1234567891011121314151617181920212223242526272829303132333435363738public V delete(K key) &#123; if (key == null) throw new IllegalArgumentException("called delete() with key is null."); if (isEmpty()) throw new NoSuchElementException("called delete() with empty symbol table."); if (!contains(key)) return null; // find position i of key int i = hash(key); while (!key.equals(keys[i])) &#123; i = (i + 1) % m; &#125; V oldValue = vals[i]; // delete key and associated value keys[i] = null; vals[i] = null; // rehash all keys in same cluster i = (i + 1) % m; while (keys[i] != null) &#123; // delete keys[i] an vals[i] and reinsert K keyToRehash = keys[i]; V valToRehash = vals[i]; keys[i] = null; vals[i] = null; n--; put(keyToRehash, valToRehash); i = (i + 1) % m; &#125; n--; // halves size of array if it's 12.5% full or less if (n &gt; 0 &amp;&amp; n &lt;= m / 8) resize(m / 2); assert check(); return oldValue;&#125; 键簇线性探测法的平均成本取决于元素在插入数组后聚集成的一组连续的条目,也叫作键簇. 显然,短小的键簇才能保证较高的效率.随着插入的key越来越多,这个要求会很难满足,较长的键簇会越来越多.长键簇的可能性要比短键簇更大,因为新键的散列值无论落在键簇的任何位置都会使它的长度加1. 总结 散列表使用了适度的空间和时间并在这两个极端之间找到了一种平衡,所以它可以在一般应用中实现拥有(均摊后)常数级别的查找和插入操作的符号表. 但散列表是很难实现有序操作的,这是因为散列最主要的目的在于均匀地将键散布开来,因此在计算散列后键的顺序信息就已经丢失了. 同时,散列表的性能也依赖于α=N/M的比值,其中α称为散列表的使用率.对于拉链法来说,α是每条链表的长度,因此一般大于1.对于线性探测法来说,α是表中已被占用的空间的比例,它是不可能大于1的. 散列表的性能虽然高效,但它也有以下的局限性: 每种类型的键都需要一个优秀的散列函数. 性能保证来自于散列函数的质量. 散列函数的计算可能复杂而且昂贵. 难以支持有序性相关的操作. end Author : SylvanasSun Email : sylvanassun_xtz@163.com 文中的完整实现代码见我的GitHub &amp; Gist 文中参考资料引用自&lt;&gt; &amp; Wikepedia]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>HashTable</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>HashTable</tag>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[平衡查找树之AVL树]]></title>
    <url>%2F2017%2F04%2F08%2F2017-4-08-avl_tree%2F</url>
    <content type="text"><![CDATA[概述 AVL树得名于它的发明者G.M. Adelson-Velsky和E.M. Landis,它是最先发明的自平衡二叉查找树. 在AVL树中任何节点的两个子树的高度最大差别为一.并且,查找、插入、删除等操作在平均和最坏情况下都是O(log n). AVL树的基本操作都与二叉查找树的算法一致,只有在插入、删除等这种会改变树的平衡性的操作需要使用一些旋转操作来修正树的平衡性. 平衡因子 节点的平衡因子一般是它的左子树的高度减去它的右子树的高度(相反也可以).带有平衡因子为1、0或-1的节点被认为是平衡的.带有平衡因子为-2或2的节点被认为是不平衡的. 计算树的高度与平衡因子的代码如下. 12345678910111213141516171819202122// calculate node x depthprivate int calcDepth(Node x) &#123; int depth = 0; if (x.left != null) depth = x.left.depth; if (x.right != null &amp;&amp; x.right.depth &gt; depth) depth = x.right.depth; // parent + left or right depth depth++; return depth;&#125;// calculate node x balance(left.depth - right.depth)private int calcBalance(Node x) &#123; int leftDepth = 0; int rightDepth = 0; if (x.left != null) leftDepth = x.left.depth; if (x.right != null) rightDepth = x.right.depth; return leftDepth - rightDepth;&#125; 旋转 旋转操作是用于修复树的平衡性的,它保证了树的有序性与平衡性(旋转操作的具体讲解可以参考《Algorithms,4th Edition》读书笔记-红黑二叉查找树). 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051private Node rotateLeft(Node x) &#123; Node t = x.right; x.right = t.left; t.left = x; if (x.parent != null) &#123; t.parent = x.parent; if (x.parent.left == x) x.parent.left = t; else x.parent.right = t; &#125; else &#123; t.parent = null; root = t; &#125; x.parent = t; // calculate depth and balance x.depth = calcDepth(x); x.balance = calcBalance(x); t.depth = calcDepth(t); t.balance = calcBalance(t); // calculate size t.size = x.size; x.size = 1 + size(x.left) + size(x.right); return t;&#125;private Node rotateRight(Node x) &#123; Node t = x.left; x.left = t.right; t.right = x; if (x.parent != null) &#123; t.parent = x.parent; if (x.parent.left == x) x.parent.left = t; else x.parent.right = t; &#125; else &#123; t.parent = null; root = t; &#125; x.parent = t; // calculate depth and balance x.depth = calcDepth(x); x.balance = calcBalance(x); t.depth = calcDepth(t); t.balance = calcBalance(t); // calculate size t.size = x.size; x.size = 1 + size(x.left) + size(x.right); return t;&#125; 平衡修正 当一个节点被认为是不平衡的时候,我们需要使用一些旋转操作来修正树的平衡,一般有以下情况需要进行旋转. 例如当前节点为x,对x进行平衡修正需要进行以下判断. 当x的平衡因子大于等于2时(左子树高度偏高),对其进行右旋转. 当x的左子树的平衡因子等于-1时(左子树的右子节点高度偏高),对x的左子树进行左旋转. 当x的平衡因子小于等于-2时(右子树高度偏高),对其进行左旋转. 当x的右子树的平衡因子等于1时(右子树的左子节点高度偏高),对x的右子树进行右旋转. 123456789101112131415161718192021222324private void balance(Node x) &#123; while (x != null) &#123; x.depth = calcDepth(x); x.balance = calcBalance(x); // if x left subtree high,rotateRight if (x.balance &gt;= 2) &#123; // if x.left.right high,rotateLeft if (x.left != null &amp;&amp; x.left.balance == -1) &#123; x.left = rotateLeft(x.left); &#125; x = rotateRight(x); &#125; // if x right subtree high,rotateLeft if (x.balance &lt;= -2) &#123; // if x.right.left high,rotateRight if (x.right != null &amp;&amp; x.right.balance == 1) &#123; x.right = rotateRight(x.right); &#125; x = rotateLeft(x); &#125; x.size = 1 + size(x.left) + size(x.right); x = x.parent; &#125;&#125; 插入 AVL树的插入和删除与二分查找树的算法一致,只不过在完成插入后需要自底向上的修复平衡性. 123456789101112131415161718192021222324252627282930313233343536373839public void put(K key, V value) &#123; if (key == null) throw new IllegalArgumentException("called put() with key is null."); if (value == null) &#123; remove(key); return; &#125; put(root, key, value);&#125;private void put(Node x, K key, V value) &#123; Node parent = x; int cmp = 0; while (x != null) &#123; parent = x; cmp = key.compareTo(x.key); if (cmp &lt; 0) &#123; x = x.left; &#125; else if (cmp &gt; 0) &#123; x = x.right; &#125; else &#123; x.value = value; return; &#125; &#125; // if not find key,create new node x = new Node(key, value, 1, parent); if (parent != null) &#123; if (cmp &lt; 0) parent.left = x; else parent.right = x; &#125; else &#123; root = x; &#125; // fixup balance balance(x);&#125; end Author : SylvanasSun Email : sylvanassun_xtz@163.com 文中的完整实现代码见我的GitHub &amp; Gist 本文参考资料引用自Wikipedia]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>Tree</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Algorithms,4th Edition》读书笔记-红黑二叉查找树]]></title>
    <url>%2F2017%2F03%2F30%2F2017-3-30-red_black_binary_search_tree%2F</url>
    <content type="text"><![CDATA[红黑二叉查找树是2-3查找树的简单表示方式,它的代码量并不大,并且保证了平衡性.阅读本文前需先了解 《Algorithms,4th Edition》读书笔记-2-3查找树 概述 红黑树是一种自平衡的二叉查找树,它的基本思想是用标准的二叉查找树(完全由2-节点构成)和一些额外的信息(替换3-节点)来表示2-3树. 可以说红黑树是2-3树的一种等同. 红黑树中的链接可以分为两种类型: 红链接 : 它将两个2-节点连接起来构成一个3-节点(也可以说是将3-节点表示为由一条红色左链接(两个2-节点其中之一是另一个的左子节点)相连的两个2-节点). 黑链接 : 表示2-3树中的普通链接. 这种表示方式带来的优点如下: 无需修改就可以直接使用标准的二叉查找树中的查找方法(其他与链接颜色不关联的方法也可以直接使用). 对于任意的2-3树,只要对节点进行转换,我们都可以立即派生出一棵对应的二叉查找树. 红黑树的性质 红黑树是含有红黑链接并满足下列条件的二叉查找树(满足这些条件的红黑树才是与相应的2-3树一一对应的). 红链接均为左链接(这条仅限于偏向左红链接实现的红黑树). 每个节点不是红色就是黑色的. 没有任何一个节点同时和两条红链接相连(不可以有两条连续的红链接). 该树是完美黑色平衡的,即任意空链接到根节点的路径上的黑链接数量相同. 根节点是黑色的. 所有叶子节点(即null节点)的颜色是黑色的. 与2-3树的对应关系 假如我们将一棵红黑树中的红链接画平,我们会发现所有的空链接到根节点的距离都将是相同的.如果再把由红链接相连的节点合并,得到的就是一棵2-3树. 相对的,如果将一棵2-3树中的3-节点画作由红色左链接相连的两个2-节点,那么不会存在能够和两条红链接相连的节点,且树必然是完美黑色平衡的,因为黑链接就是2-3树中的普通链接,根据定义这些链接必然是完美平衡的. 通过这些结论,我们可以发现红黑树即是二叉查找树,也是2-3树. 节点的实现 我们使用boolean类型的变量color来表示链接的颜色.如果指向它的链接为红色,则color变量为true,黑色则为false(空链接也为黑色). 并且定义了一个isRed()函数用于判断链接的颜色. 这里节点的颜色指的是指向该节点的链接的颜色. 12345678910111213141516171819202122232425 private static final boolean RED = true; private static final boolean BLACK = false; private Node root; // root node private class Node &#123; private K key; private V value; private Node left, right; // links to left and right subtress private boolean color; // color of parent link private int size; // subtree count public Node(K key, V value, boolean color, int size) &#123; this.key = key; this.value = value; this.color = color; this.size = size; &#125; &#125; // node x is red? if x is null return false. private boolean isRed(Node x) &#123; if (x == null) return false; return x.color == RED; &#125; 旋转 当我们在实现某些操作时,可能会产生一些红色右链接或者两条连续的红色左链接.这时就需要在操作完成前进行旋转操作来修复红黑树的平衡性(旋转操作会改变红链接的指向). 旋转操作保证了红黑树的两个重要性质 : 有序性和完美平衡性. 左旋转假设当前有一条红色右链接需要被修正旋转为左链接.这个操作叫做左旋转. 左旋转函数接受一条指向红黑树中的某个节点的链接作为参数.然后会对树进行必要的调整并返回一个指向包含同一组键的子树且其左链接为红色的根节点的链接. 也可以认为是将用两个键中的较小者作为根节点变为将较大者作为根节点(右旋转中逻辑相反). 旋转操作返回的链接可能是左链接也可能是右链接,这个链接可能是红色也可能是黑色的(在实现中我们使用x.color = h.color保留了它原本的颜色).这可能会产生两条连续的红链接,但算法会在后续操作中继续使用旋转操作修正这种情况. 旋转操作只影响了根节点(返回的节点的子树中的所有键和旋转前都相同,只有根节点发生了变化). 具体的实现如下图: 右旋转实现右旋转的逻辑基本与左旋转相同,只需要将left和right互换即可. 颜色转换 颜色转换操作也是用于保证红黑树的性质的.它将父节点的颜色由黑变红,将子节点的颜色由红变黑. 这项操作与旋转操作一样是局部变换,不会影响整棵树的黑色平衡性. 根节点总是为黑颜色转换可能会使根节点变为红色,但红色的根节点说明根节点是一个3-节点的一部分,实际情况并不是这样的.所以我们需要将根节点设为黑色. 每当根节点由红变黑时,树的黑链接高度就会加1. 插入 在红黑树中实现插入操作是比较复杂的,因为需要保持红黑树的平衡性.但只要利用好左旋转、右旋转、颜色转换这三个辅助操作,就能够保证插入操作后树的平衡性. 向单个2-节点中插入新键当一棵只含有一个键的红黑树只含有一个2-节点时,插入另一个键后需要马上进行旋转操作修正树的平衡性. 如果新键小于老键,只需要新增一个红色的节点即可(这时,新的红黑树等价于一个3-节点). 如果新键大于老键,那么新增的红色节点将会产生一条红色的右链接,这时就需要使用左旋转修正根节点的链接. 以上两种情况最终的结果均为一棵等价于单个3-节点的红黑树,它含有两个键,一条红链接,树的黑链接高度为1. 向树底部的2-节点插入新键和二叉查找树一样,向红黑树中插入一个新键会在树的底部新增一个节点,但在红黑树中总是用红链接将新节点和它的父节点相连. 如果它的父节点是一个2-节点,那么上一节讨论的方法依然适用. 如果指向新节点的是父节点的左链接,那么父节点就直接成为一个3-节点. 如果指向新节点的是父节点的右链接,那么就需要一次左旋转进行修正. 向一棵双键树(一个3-节点)中插入新键当向一个3-节点中插入新键时,会发生以下三种情况且每种情况都会产生一个同时连接到两条红链接的节点,我们需要修正这一点. 如果新键大于原树中的两个键 : 这是最容易处理的一种情况,这个键会被连接到3-节点的右链接.此时树是平衡的,根节点为中间大小的键,它有两条红链接分别和较小和较大的节点相连.只需要把这两条链接的颜色都由红变黑,那么就可以得到一棵由三个节点组成、高度为2的平衡树(其他两种情况最终也会转化为这样的树). 如果新键小于原树中的两个键 : 这个键会被连接到最左边的空链接,这样就产生了两条连续的红链接.此时只需要将上层的红链接右旋转即可得到第一种情况(中值键为根节点并和其他两个节点用红链接相连). 如果新键介于原树中的两个键之间 : 这种情况依然会产生两条连续的红链接:一条红色左链接接一条红色右链接.此时只需要将下层的红链接左旋转即可得到第二种情况(两条连续的红色左链接). 通过以上这三种情况可以总结出 : 我们只需要通过0次、1次、2次旋转以及颜色转换就可以完成对红黑树的修正. 将红链接向上传递当每次旋转操作之后都会进行颜色转换,它会使得中间节点变为红色.从父节点的角度来看,处理这样一个红色节点的方式和处理一个新插入的红色节点完全相同(继续将红链接转移到中间节点). 这个操作对应于2-3树中向3-节点进行插入的操作 : 即在一个3-节点下插入新键,需要创建一个临时的4-节点,将其分解并将中间键插入父节点(在红黑树中,是将红链接由中间键传递给它的父节点).重复这个过程,直至遇到一个2-节点或者根节点. 当根节点变为红色时需要将根节点的颜色转换为黑色(对应2-3树中的根节点分解). 实现插入操作的实现除了每次递归调用之后的对平衡性修正的操作,其他与二叉查找树中的插入操作没什么不同. 1234567891011121314151617181920212223242526272829public void put(K key, V val) &#123; if (key == null) throw new IllegalArgumentException("first argument to put() is null"); if (val == null) &#123; delete(key); return; &#125; root = put(root, key, val); root.color = BLACK; assert check();&#125;// insert the key-value pair in the subtree rooted at hprivate Node put(Node h, K key, V val) &#123; if (h == null) return new Node(key, val, RED, 1); int cmp = key.compareTo(h.key); if (cmp &lt; 0) h.left = put(h.left, key, val); else if (cmp &gt; 0) h.right = put(h.right, key, val); else h.value = val; // fix-up any right-leaning links if (isRed(h.right) &amp;&amp; !isRed(h.left)) h = rotateLeft(h); if (isRed(h.left) &amp;&amp; isRed(h.left.left)) h = rotateRight(h); if (isRed(h.left) &amp;&amp; isRed(h.right)) flipColors(h); h.size = size(h.left) + size(h.right) + 1; return h;&#125; 总结只要在沿着插入点到根节点的路径向上移动时在所经过的每个节点中顺序完成以下操作,就能够实现红黑树的插入操作. 如果右子节点是红色的而左子节点是黑色的,那么进行左旋转. 如果左子节点是红色的而且它的左子节点也是红色的,那么进行右旋转. 如果左右子节点都是红色的,那么进行颜色转换. 删除 删除操作也需要定义一系列局部变换来在删除一个节点的同时保持树的完美平衡性.然而,这个过程要比插入操作还要复杂,它不仅要在(为了删除一个节点而)构造临时4-节点时沿着查找路径向下进行变换,还要在分解遗留的4-节点时沿着查找路径向上进行变换(同插入操作). 自顶向下的2-3-4树2-3-4树是一种允许存在4-节点的树.它的插入算法就是一种沿着查找路径既能向上也能向下进行变换的算法. 沿查找路径向下进行变换(向下变换与2-3树中分解4-节点所进行的变换完全相同)是为了保证当前节点不是4-节点(这样树的底部才有足够的空间插入新的键). 沿查找路径向上进行变换是为了将之前创建的4-节点配平. 如果根节点是一个4-节点,就将它分解成三个2-节点,树的高度加1. 如果在向下查找的过程中,遇到了一个父节点为2-节点的4-节点,就将4-节点分解为两个2-节点并将中间键传递给它的父节点(这时父节点变为了一个3-节点). 如果遇到了一个父节点为3-节点的4-节点,将4-节点分解为两个2-节点并将中间键传递给它的父节点(这时父节点变为了一个4-节点). 不必担心遇见父节点为4-节点的4-节点,算法本身保证了不会出现这种情况,到达树的底部之后,只会遇到2-节点或者3-节点. 如果要使用红黑树来实现这个算法,需要以下步骤 : 将4-节点表示为由三个2-节点组成的一棵平衡的子树,根节点和两个子节点都用红链接相连. 在向下的过程中分解所有4-节点并进行颜色转换. 在向上的过程中使用旋转将4-节点配平. 只需要将插入一节中的put()实现方法里的flipColors语句(及其if语句)移动到递归调用之前(null判断和比较操作之间)就能实现2-3-4树的插入操作. 删除最小键从2-节点中删除一个键会留下一个空节点,一般会将它替换为一个空链接,但这样会破坏树的完美平衡性.所以在删除操作中,为了避免删除一个2-节点,我们沿着左链接向下进行变换时,需要确保当前节点不是2-节点. 根节点可能有以下两种情况: 如果根节点是一个2-节点且它的两个子节点都是2-节点,可以直接将这三个节点变成一个4-节点. 否则,需要保证根节点的左子节点不是2-节点,必要时可以从它右侧的兄弟节点借走一个键. 在沿着左链接向下的过程中,保证以下情况之一成立: 如果当前节点的左子节点不是2-节点. 如果当前节点的左子节点是2-节点而它的兄弟节点不是2-节点,将左子节点的兄弟节点中的一个键移动到左子节点中 如果当前节点的左子节点和它的兄弟节点都是2-节点,将左子节点、父节点中的最小键和左子节点最近的兄弟节点合并为一个4-节点,使父节点由3-节点变为2-节点(或是从4-节点变为3-节点). 只要保证了以上的条件,我们最终能够得到一个含有最小键的3-节点或4-节点(然后进行删除即可),之后再不断向上分解所有临时的4-节点. 代码实现在删除操作中,颜色转换的操作与插入操作中的实现略微有些不同(需要将父节点设为黑,而将两个子节点设为红). 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051private void flipColors(Node h) &#123; h.color = !h.color; h.left.color = !h.left.color; h.right.color = !h.right.color; &#125; // restore red-black tree invariant private Node balance(Node h) &#123; if (isRed(h.right)) h = rotateLeft(h); if (isRed(h.left) &amp;&amp; isRed(h.left.left)) h = rotateRight(h); if (isRed(h.left) &amp;&amp; isRed(h.right)) flipColors(h); h.size = size(h.left) + size(h.right) + 1; return h; &#125; // Assuming that h is red and both h.left and h.left.left // are black, make h.left or one of its children red. private Node moveRedLeft(Node h) &#123; flipColors(h); if (isRed(h.right.left)) &#123; h.right = rotateRight(h.right); h = rotateLeft(h); flipColors(h); &#125; return h; &#125; public void deleteMin() &#123; if (isEmpty()) throw new NoSuchElementException("RedBlackBST underflow."); // if both children of root are black, set root to red if (!isRed(root.left) &amp;&amp; !isRed(root.right)) root.color = RED; root = deleteMin(root); if (!isEmpty()) root.color = BLACK; &#125; // delete the key-value pair with the minimum key rooted at h private Node deleteMin(Node h) &#123; if (h.left == null) return null; if (!isRed(h.left) &amp;&amp; !isRed(h.left.left)) h = moveRedLeft(h); h.left = deleteMin(h.left); return balance(h); &#125; 删除最大键1234567891011121314151617181920212223242526272829303132333435363738// Assuming that h is red and both h.right and h.right.left// are black, make h.right or one of its children red.private Node moveRedRight(Node h) &#123; flipColors(h); if (isRed(h.left.left)) &#123; h = rotateRight(h); flipColors(h); &#125; return h;&#125; public void deleteMax() &#123; if (isEmpty()) throw new NoSuchElementException("RedBlackBST underflow."); // if both children of root are black, set root to red if (!isRed(root.left) &amp;&amp; !isRed(root.right)) root.color = RED; root = deleteMax(root); if (!isEmpty()) root.color = BLACK;&#125;// delete the key-value pair with the maximum key rooted at hprivate Node deleteMax(Node h) &#123; if (isRed(h.left)) h = rotateRight(h); if (h.right == null) return null; if (!isRed(h.right) &amp;&amp; !isRed(h.right.left)) h = moveRedRight(h); h.right = deleteMax(h.right); return balance(h);&#125; 删除操作同样也需要像删除最小键那样在查找路径上进行变换来保证查找过程中任意当前节点均不是2-节点.如果目标键在树的底部,可以直接删除它;如果不在,则需要将它和它的后继节点交换. 在删除操作之后需要向上变换分解余下的4-节点. 12345678910111213141516171819202122232425262728293031323334public void delete(K key) &#123; if (key == null) throw new IllegalArgumentException("called delete() with key is null."); if (!contains(key)) return; // if both children of root are black, set root to red if (!isRed(root.left) &amp;&amp; !isRed(root.right)) root.color = RED; root = delete(root, key); if (!isEmpty()) root.color = BLACK;&#125;// delete the key-value pair with the given key rooted at hprivate Node delete(Node h, K key) &#123; if (key.compareTo(h.key) &lt; 0) &#123; if (!isRed(h.left) &amp;&amp; !isRed(h.left.left)) h = moveRedLeft(h); h.left = delete(h.left, key); &#125; else &#123; if (isRed(h.left)) h = rotateRight(h); if (key.compareTo(h.key) == 0 &amp;&amp; (h.right == null)) return null; if (!isRed(h.right) &amp;&amp; !isRed(h.right.left)) h = moveRedRight(h); if (key.compareTo(h.key) == 0) &#123; Node x = min(h.right); h.key = x.key; h.value = x.value; h.right = deleteMin(h.right); &#125; else h.right = delete(h.right, key); &#125; return balance(h);&#125; 总结 无论键的插入顺序如何,红黑树都几乎是完美平衡的,基于它实现的有序符号表操作的运行时间均为对数级别(除了范围查询). 在红黑树的实现中复杂的代码仅限于put()和delete()方法,像get()这些不会涉及检查颜色的方法与二叉查找树中的实现一致(因为这些操作与平衡性无关). end Author : SylvanasSun Email : sylvanassun_xtz@163.com 文中的完整实现代码见我的GitHub &amp; Gist 本文参考资料引用自《Algorithms,4th Editio》]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>Tree</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Algorithms,4th Edition》读书笔记-2-3查找树]]></title>
    <url>%2F2017%2F03%2F28%2F2017-3-28-2_3tree%2F</url>
    <content type="text"><![CDATA[概述 由于二叉查找树的性能与树的高度(即根节点到底部节点的深度)相关,因此当高度较大时,二叉查找树的性能就会下降.为了更高效的性能,平衡查找树应运而生,它能保证无论键的插入顺序如何,树的高度都将是总键数的对数. 2-3查找树就是平衡树的一种. 性质 2-3查找树允许树中的一个节点保存多个键.我们可以将二叉查找树中的节点称为2-节点,而在2-3查找树中引入了3-节点,它含有两个键和三条链接. 一棵2-3查找树由以下节点组成: 2-节点 : 含有一个键(及其对应的值)和两条链接,左链接指向的2-3查找树中的键都小于该节点,右链接指向的2-3查找树中的键都大于该节点. 3-节点 : 含有两个键(及其对应的值)和三条链接,左链接指向的2-3查找树中的键都小于该节点,中链接指向的2-3查找树中的键都位于该节点的两个键之间,右链接指向的2-3查找树中的键都大于该节点. 一棵完美平衡的2-3查找树中的所有空链接到根节点的距离都应该是相同的. 查找 2-3查找树的查找算法与二叉查找树基本相似. 首先,要判断一个键需要先将它和根节点中的键进行比较. 如果它和其中任意一个相等,查找命中. 否则,根据比较的结果找到指向相应区间的链接,并在其指向的子树中递归地继续查找. 如果最后指向空链接,查找未命中. 插入 由于2-3查找树需要保持完美平衡性,所以它的插入算法并不像二叉查找树那么简单. 它的插入算法基本思想是 : 一直向上不断分解临时的4-节点并将中键插入更高层的父节点中,直至遇到一个2-节点并将它替换为一个不需要继续分解的3-节点,或是到达3-节点的根(分解根节点) 向2-节点中插入新键如果未命中的查找结束于一个2-节点,只需要把这个2-节点替换为一个3-节点,将要插入的键保存在其中即可. 向一棵只含有一个3-节点的树中插入新键如果我们需要向一棵只含有一个3-节点的树中插入一个新键(这棵树中唯一的节点已经没有可插入新键的空间了). 先临时将新键存入该节点中,使之成为一个4-节点(它扩展了以前的节点并含有3个键和4条链接). 将4-节点分解为一棵由3个2-节点组成的2-3查找树,其中一个节点(根)含有中键,一个节点含有3个键中的最小者(和根节点的左链接相连),一个节点含有3个键中的最大者(和根节点的右链接相连). 这时,这棵树既是一棵含有3个节点的二叉查找树,同时也是一棵完美平衡的2-3查找树. 向一个父节点为2-节点的3-节点中插入新键如果未命中的查找结束于一个3-节点,而它的父节点是一个2-节点.这种情况下,我们需要在维持树的完美平衡性的前提下为新键腾出空间. 构造一个临时的4-节点并将其分解(此时并不会为中键创建一个新节点). 将中键移动至父节点中(可以看做将指向3-节点的一条链接替换为新父节点中的原中键左右两边的两条链接,并分别指向两个新的2-节点). 向一个父节点为3-节点的3-节点中插入新键如果未命中的查找结束于一个父节点为3-节点且它本身也是一个3-节点时.我们可以构造一个临时的4-节点并分解它.将中键插入到它的父节点中. 但由于它的父节点也是一个3-节点,所以需要再用这个中键构造一个新的临时4-节点,然后在这个节点上进行相同的变换,即分解这个父节点并将它的中键插入到它的父节点中. 重复相同的变换直到遇到一个2-节点(将2-节点替换为一个3-节点)或者到达根节点(分解根节点). 分解根节点如果从插入节点到根节点的路径上全部都是3-节点.那么根节点最终会变成一个临时的4-节点,这时可以将4-节点分解为3个2-节点,同时树高加1(仍然保持了树的完美平衡性,因为它变换的是根节点). 具体实现 关于如何使用一个简单的数据结构来表达实现2-3查找树可以见此文 &lt;&gt;读书笔记-红黑二叉查找树 总结 2-3查找树的根本在于插入操作中的变换操作都是局部的,除了相关的节点和链接之外不必修改或者检查树的其他部分. 每次变换都会将4-节点中的一个键移动至它的父节点中,并重构相应的链接而不必涉及树的其他部分.且保持了树的完美平衡性,例如在变换之前根节点到所有空链接的路径长度为h,那么变换之后该长度仍然为h.只有进行根节点分解时,所有空链接到根节点的路径长度才会加1. 通过这些我们可以总结得出: 2-3查找树的生长是由下向上的. (标准的二叉查找树则是由上向下生长的) end Author : SylvanasSun Email : sylvanassun_xtz@163.com 本文参考资料引用自&lt;&gt;]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>Tree</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《Algorithms,4th Edition》读书笔记-二叉查找树]]></title>
    <url>%2F2017%2F03%2F26%2F2017-3-26-binary_search_tree%2F</url>
    <content type="text"><![CDATA[概述 二叉查找树是一颗有序的二叉树,它有以下性质: 若任意节点的左子树不为空,则左子树上所有节点的值均小于它的根节点的值. 若任意节点的右子树不为空,则右子树上所有节点的值均大于它的根节点的值. 可以将每个链接看做指向另一颗二叉查找树,而这棵树的根节点就是被指向的节点. 没有键值相等的节点. 使用二叉查找树实现的符号表结合了链表插入的灵活性和有序数组查找的高效性.通常采取二叉链表作为存储结构. 每个节点都含有一个键、一个值、一条左链接、一条右链接和一个节点计数器(用于统计其所有子节点数).左链接指向一棵由小于该节点的所有键组成的二叉查找树,右链接指向一棵由大于该节点的所有键组成的二叉查找树. 如果将一棵二叉查找树的所有键投影到一条直线上,保证一个节点的左子树中的键出现在它的左边,右子树种的键出现在它的右边,那么我们一定可以得到一条有序的键列. 可以说二叉查找树和快速排序很相似.树的根节点就是快速排序中的第一个基准数(切分元素),左侧的键都比它小,右侧的键都比它大. 基本实现 1234567891011121314151617181920212223242526272829303132333435363738394041public class BinarySearchTree&lt;K extends Comparable&lt;K&gt;, V&gt; &#123; private Node root; // root node private class Node &#123; private K key; private V value; private Node left, right; // left and right subtree private int size; // number of nodes in subtree public Node(K key, V value, int size) &#123; this.key = key; this.value = value; this.size = size; &#125; &#125; /** * Returns true if this symbol table is empty. * * @return &#123;@code true&#125; is this symbol table is empty, &#123;@code false&#125; otherwise. */ public boolean isEmpty() &#123; return size() == 0; &#125; /** * Returns the number of key-value pairs in this symbol table. * * @return the number of key-value pairs in this symbol table. */ public int size() &#123; return size(root); &#125; // return number of key-value pairs in binary search tree rooted at x private int size(Node x) &#123; if (x == null) return 0; else return x.size; &#125;&#125; 在以上代码中,使用私有嵌套类Node来表示一个二叉链表,每个Node对象都是一棵含有N个节点的子树的根节点.变量root指向二叉查找树的根节点(这棵树包含了符号表中的所有键值对). 查找与插入 查找1234567891011121314151617181920212223242526272829/** * Returns the value associated with the given key. * * @param key the key * @return the value associated with the given key if the key is in the symbol table * and &#123;@code null&#125; if the key is not in the symbol table. * @throws IllegalArgumentException if &#123;@code key&#125; is &#123;@code null&#125; */ public V get(K key) &#123; if (key == null) throw new IllegalArgumentException("called get() with a null key."); return get(root, key); &#125; private V get(Node x, K key) &#123; if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp &lt; 0) &#123; // if key &lt; x.key , search left subtree return get(x.left, key); &#125; else if (cmp &gt; 0) &#123; // if key &gt; x.key , search right subtree return get(x.right, key); &#125; else &#123; // hit target return x.value; &#125; &#125; 在二叉查找树中实现查找操作是很简单而简洁的,这也是二叉查找树的特性之一.-如果树为空,就返回null,如果被查找的键小于根节点的键,我们就继续在左子树中查找,否则在右子树中查找. 插入插入操作的逻辑与查找差不多,只不过需要在判定树为空时,返回一个含有该键值对的新节点,还需要重置搜索路径上每个父节点指向子节点的链接,并增加路径上每个节点中的子节点计数器的值. 1234567891011121314151617181920212223242526272829303132333435363738/** * Inserts the specified key-value pair into the symbol table. * overwriting the old value with the new value if the symbol table already contains * the specified key. * Deletes the specified key (and its associated value) from this symbol table * if the specified value is &#123;@code null&#125;. * * @param key the key * @param value the value * @throws IllegalArgumentException if &#123;@code key&#125; is &#123;@code null&#125;. */ public void put(K key, V value) &#123; if (key == null) throw new IllegalArgumentException("called put() with a null key."); if (value == null) &#123; delete(key); return; &#125; root = put(root, key, value); assert check(); &#125; private Node put(Node x, K key, V value) &#123; // if tree is empty, return a new node. if (x == null) return new Node(key, value, 1); int cmp = key.compareTo(x.key); if (cmp &lt; 0) &#123; x.left = put(x.left, key, value); &#125; else if (cmp &gt; 0) &#123; x.right = put(x.right, key, value); &#125; else &#123; // hit target,overwriting old value with the new value x.value = value; &#125; x.size = 1 + size(x.left) + size(x.right); // compute subtree node size return x; &#125; 最大键和最小键 如果根节点的左链接为空,那么一棵二叉查找树中最小的键就是根节点. 如果左链接非空,那么树中的最小键就是左子树中的最小键.找出最大键的逻辑也是类似的,只是变为查找右子树而已. 123456789101112131415161718192021222324252627282930313233/** * Returns the smallest key in the symbol table. * * @return the smallest key in the symbol table. * @throws NoSuchElementException if the symbol table is empty */ public K min() &#123; if (isEmpty()) throw new NoSuchElementException("called min() with empty symbol table."); return min(root).key; &#125; private Node min(Node x) &#123; if (x.left == null) return x; else return min(x.left); &#125; /** * Returns the largest key in the symbol table. * * @return the largest key in the symbol table. * @throws NoSuchElementException if the symbol table is empty */ public K max() &#123; if (isEmpty()) throw new NoSuchElementException("called max() with empty symbol table."); return max(root).key; &#125; private Node max(Node x) &#123; if (x.right == null) return x; else return max(x.right); &#125; 向上取整和向下取整 如果给定的键key小于二叉查找树的根节点的键,那么小于等于key的最大键floor(key)一定在根节点的左子树中. 如果给定的键key大于二叉查找树的根节点,那么只有当根节点右子树中存在小于等于key的节点时,小于等于key的最大键才会出现在右子树中,否则根节点就是小于等于key的最大键.(将左变为右,小于变为大于就是向上取整的实现逻辑) 12345678910111213141516171819202122232425262728293031323334353637383940414243public K floor(K key) &#123; if (key == null) throw new IllegalArgumentException("called floor() with a null key."); if (isEmpty()) throw new NoSuchElementException("called floor() with empty symbol table."); Node x = floor(root, key); if (x == null) return null; else return x.key; &#125; private Node floor(Node x, K key) &#123; if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp == 0) return x; if (cmp &lt; 0) return floor(x.left, key); Node t = floor(x.right, key); if (t != null) return t; else return x; &#125;public K ceiling(K key) &#123; if (key == null) throw new IllegalArgumentException("called ceiling() with a null key."); if (isEmpty()) throw new NoSuchElementException("called ceiling() with empty symbol table."); Node x = ceiling(root, key); if (x == null) return null; else return x.key; &#125; private Node ceiling(Node x, K key) &#123; if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp == 0) return x; if (cmp &lt; 0) &#123; Node t = ceiling(x.left, key); if (t != null) return t; else return x; &#125; return ceiling(x.right, key); &#125; 选择和排名 select假设我们想找到排名为k的键(即树中正好有k个小于它的键). 如果左子树中的节点数t大于k,那么我们就继续递归地在左子树中查找排名为k的键. 如果t等于k,我们就返回根节点中的键. 如果t小于k,我们就递归地在右子树中查找排名为(k-t-1)的键. 123456789101112131415public K select(int k) &#123; if (k &lt; 0 || k &gt;= size()) throw new IllegalArgumentException("called select() with invalid argument: " + k); return select(root, k).key;&#125;private Node select(Node x, int k) &#123; if (x == null) return null; int t = size(x.left); // if left subtree node size greater than k,in left subtree search if (t &gt; k) return select(x.left, k); // otherwise,in right subtree search else if (t &lt; k) return select(x.right, k - t - 1); else return x;&#125; rankrank()函数是select()函数的逆函数,它会返回指定键的排名. 如果给定的键和根节点的键相等,就返回左子树中的节点总数t. 如果给定的键小于根节点,就返回该键在左子树中的排名(递归计算). 如果给定的键大于根节点,就返回t+1(根节点)加上它在右子树中的排名(递归计算). 1234567891011121314151617public int rank(K key) &#123; if (key == null) throw new IllegalArgumentException("called rank() with a null key."); return rank(root, key);&#125;private int rank(Node x, K key) &#123; if (x == null) return 0; int cmp = key.compareTo(x.key); if (cmp &lt; 0) return rank(x.left, key); else if (cmp &gt; 0) return 1 + size(x.left) + rank(x.right, key); else return size(x.left);&#125; 删除最大键和删除最小键 对于删除最小键,需要不断深入根节点的左子树直至遇见一个空链接,然后将指向该节点的链接指向该节点的右子树(只需要在递归调用中返回它的右链接即可).此时已经没有任何链接指向要被删除的节点,因此它会被垃圾回收器gc掉. 删除最大键与其逻辑相似,只是方向相反. 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Removes the smallest key and associated value from the symbol table. * * @throws NoSuchElementException if the symbol table is empty. */public void deleteMin() &#123; if (isEmpty()) throw new NoSuchElementException("Symbol table underflow."); root = deleteMin(root); assert check();&#125;private Node deleteMin(Node x) &#123; // if the left link is empty,will link to the node of the right subtree. if (x.left == null) return x.right; x.left = deleteMin(x.left); x.size = size(x.left) + size(x.right) + 1; return x;&#125;/** * Removes the largest key and associated value from the symbol table. * * @throws NoSuchElementException if the symbol table is empty */public void deleteMax() &#123; if (isEmpty()) throw new NoSuchElementException("Symbol table underflow."); root = deleteMax(root); assert check();&#125;private Node deleteMax(Node x) &#123; // if the right link is empty,will link to the node of the left subtree. if (x.right == null) return x.left; x.right = deleteMax(x.right); x.size = size(x.left) + size(x.right) + 1; return x;&#125; 删除 删除操作是二叉查找树中较为复杂的操作,假设我们要删除节点x(它是一个拥有两个子节点的节点),基本的实现逻辑如下. 在删除节点x后用它的后继节点填补它的位置. 找出x的右子树中的最小节点(这样替换仍能保证树的有序性,因为x.key和它的后继节点的键之间不存在其他的键)做为x的后继节点. 将由此节点到根节点的路径上的所有节点的计数器减1(这里计数器的值仍然会被设为其所有子树中的节点总数加1). 具体的过程如以下例子: 将指向即将被删除的节点的链接保存为t. 将x指向它的后继节点min(t.right). 将x的右链接(原本指向一棵所有节点都大于x.key的二叉查找树)指向deleteMin(t.right),也就是在删除后所有的节点仍然都大于x.key的子二叉查找树. 将x的左链接(本为空)指向t.left(其下所有的键都小于被删除的节点和它的后继节点). 以上的实现逻辑有一个缺点,即是在某些实际应用场景下会产生性能问题,主要原因在于后继节点是一个随意的决定,且没有考虑树的对称性. 12345678910111213141516171819202122232425262728public void delete(K key) &#123; if (key == null) throw new IllegalArgumentException("called delete() with a null key."); root = delete(root, key); assert check();&#125;private Node delete(Node x, K key) &#123; if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp &lt; 0) x.left = delete(x.left, key); else if (cmp &gt; 0) x.right = delete(x.right, key); else &#123; if (x.right == null) return x.left; if (x.left == null) return x.right; Node t = x; x = min(t.right); x.right = deleteMin(t.right); x.left = t.left; &#125; x.size = size(x.left) + size(x.right) + 1; return x;&#125; 范围查找 实现一个范围查找的思路可以是:将所有落在给定范围以内的键放入一个队列Queue并跳过那些不可能含有所查找键的子树. 12345678910111213141516171819202122public Iterable&lt;K&gt; keys(K lo, K hi) &#123; if (lo == null) throw new IllegalArgumentException("called keys(lo,hi) first argument is null."); if (hi == null) throw new IllegalArgumentException("called keys(lo,hi) second argument is null."); Queue&lt;K&gt; queue = new Queue&lt;K&gt;(); keys(root, queue, lo, hi); return queue;&#125;private void keys(Node x, Queue&lt;K&gt; queue, K lo, K hi) &#123; if (x == null) return; int cmp_lo = lo.compareTo(x.key); int cmp_hi = hi.compareTo(x.key); if (cmp_lo &lt; 0) keys(x.left, queue, lo, hi); if (cmp_lo &lt;= 0 &amp;&amp; cmp_hi &gt;= 0) queue.enqueue(x.key); if (cmp_hi &gt; 0) keys(x.right, queue, lo, hi);&#125; 总结 二叉查找树的高度决定了它在最坏情况下的运行效率,但由于键的插入顺序不会是永远随机的,所以树的某一端高度可能会非常深,解决这个问题可以使用平衡二叉查找树,它能保证无论键的插入顺序如何,树的高度都将是总键数的对数. 本文中的实现皆采用递归的方式是为了提高可读性,二叉查找树可以使用非递归的方式实现且效率会更高. 二叉查找树结合了链表插入操作的灵活性和有序数组查找操作的高效性,且还有很多种优化的改进方案(例如平衡二叉查找树),总体来说二叉查找树是一种比较好的动态查找方法. end Author : SylvanasSun Email : sylvanassun_xtz@163.com 文中的完整实现代码见我的GitHub &amp; Gist 本文参考资料引用自&lt;&gt; &amp; WikiPedia]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>Tree</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
        <tag>Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈几个常用的排序算法]]></title>
    <url>%2F2017%2F03%2F20%2F2017-3-20-sorting_algorithm%2F</url>
    <content type="text"><![CDATA[最近在读&lt;&gt;时,了解到了很多常用的排序算法,故写一篇读书笔记记录下这些排序算法的思路和实现. 冒泡排序 冒泡排序是一种非常简单的初级排序算法,它每次比较相邻的两个元素,如果顺序错误就进行交换.由于最小的元素是经由不断交换慢慢浮到顶端的,所以叫做冒泡排序. 冒泡排序对n个元素需要O(n^2)次的比较次数,所以它对规模较大的数组进行排序是效率低下的. 运行过程 比较相邻的两个元素,如果第二个元素小于第一个元素,则进行交换(降序则相反). 对每一对相邻元素作同样的工作,从开始第一对直到最后一对.完成后,最后的元素将是最大的元素. 针对所有的元素重复以上步骤,除了最后一个元素. 持续地对每次越来越少的元素重复以上步骤,直到整个数组有序(即没有任何一对元素需要比较). 代码实现12345678910 // less与exch函数见完整代码public static void sort(Comparable[] a) &#123; for (int i = 0; i &lt; a.length - 1; i++) &#123; for (int j = 0; j &lt; a.length - 1 - i; j++) &#123; if (less(a[j + 1], a[j])) &#123; exch(a, j, j + 1); &#125; &#125; &#125;&#125; 完整代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * Bubble Sort * * @author SylvanasSun * */public class Bubble &#123; // This class should not be instantiated. private Bubble() &#123; &#125; /** * Rearranges the array in ascending order, using the natural order. * * @param a * a the array to be sorted */ public static void sort(Comparable[] a) &#123; for (int i = 0; i &lt; a.length - 1; i++) &#123; for (int j = 0; j &lt; a.length - 1 - i; j++) &#123; if (less(a[j + 1], a[j])) &#123; exch(a, j, j + 1); &#125; &#125; &#125; &#125; /** * Rearranges the array in ascending order, using a comparator. * * @param a * a the arry to be sorted * @param comparator * comparator the comparator specifying the order */ public static void sort(Object[] a, Comparator comparator) &#123; for (int i = 0; i &lt; a.length - 1; i++) &#123; for (int j = 0; j &lt; a.length - 1 - i; j++) &#123; if (less(comparator, a[j + 1], a[j])) &#123; exch(a, j, j + 1); &#125; &#125; &#125; &#125; // a &lt; b ? private static boolean less(Comparable a, Comparable b) &#123; return a.compareTo(b) &lt; 0; &#125; // a &lt; b ? private static boolean less(Comparator comparator, Object a, Object b) &#123; return comparator.compare(a, b) &lt; 0; &#125; // exchange a[i] and a[j] private static void exch(Object[] a, int i, int j) &#123; Object temp = a[i]; a[i] = a[j]; a[j] = temp; &#125; // print array elements to console public static void print(Comparable[] a) &#123; for (int i = 0; i &lt; a.length; i++) &#123; System.out.print(a[i] + " "); &#125; &#125; // test public static void main(String[] args) &#123; String[] s = new Scanner(System.in).nextLine().split("\\s+"); Bubble.sort(s); Bubble.print(s); &#125;&#125; 选择排序 选择排序也是一种非常简单直观的初级排序算法,它的思想是不断地选择剩余元素之中的最小者. 它有以下两个特点. 运行时间与输入模型无关 在选择排序中,为了找出最小元素而扫描一遍数组并不能为下一轮扫描提供什么信息,即使输入是一个已经有序的数组或者是主键全部相等的数组和一个元素随机排列无序的数组所用的排序时间是一样长的. 数据移动是最少的 如果元素处于正确的位置上,则它不会被移动.选择排序每次交换一对元素，它们当中至少有一个将被移到其最终位置上，因此对n个元素的表进行排序总共进行至多n-1次交换. 运行过程 首先,找到数组中最小的那个元素 其次,将它和数组的第一个元素交换位置(如果第一个元素就是最小元素则它就和自己交换) 再次,在剩下的元素中找到最小的元素,将它与数组第二个元素交换位置.如此往复,直到整个数组有序. 代码实现12345678910public static void sort(Comparable[] a) &#123; for (int i = 0; i &lt; a.length; i++) &#123; int min = i; // the smallest element index for (int j = i + 1; j &lt; a.length; j++) &#123; if (less(a[j], a[min])) min = j; exch(a, i, min); &#125; &#125; &#125; 插入排序 插入排序与选择排序一样,当前索引左边的所有元素都是有序的,但它们的最终位置并不是确定的.它构建了一个有序序列,对于未排序的元素,在有序序列中从后向前扫描,找到相应的位置并插入. 插入排序所需的时间取决于输入模型中元素的初始顺序.当输入模型是一个部分有序的数组时,插入排序的效率会高很多. 因此插入排序对于部分有序的数组十分高效,也很适合小规模的数组. 运行过程 从第一个元素开始,该元素可以认为已是有序的 取出下一个元素,在有序序列中从后向前进行扫描 如果该元素(已排序)大于新元素,则将该元素移到下一位置(右移) 重复步骤3,直到找到已排序的元素小于或者等于新元素的位置 将新元素插入到该位置后 重复步骤2~5 代码实现12345678public static void sort(Comparable[] a) &#123; for (int i = 0; i &lt; a.length; i++) &#123; // a[i] insert to a[i-1]、a[i-2]、a[i-3]... for (int j = i; j &gt; 0 &amp;&amp; less(a[j], a[j - 1]); j--) &#123; exch(a, j, j - 1); &#125; &#125; &#125; 优化插入排序还有很多可以优化的地方,这里例举两个案例. 采用二分查找法来减少比较操作的次数.123456789101112131415161718192021public static void sort(Comparable[] a) &#123; int length = a.length; for (int i = 1; i &lt; length; i++) &#123; // binary search to determine index j at which to insert a[i] Comparable v = a[i]; int lo = 0, hi = i; while (lo &lt; hi) &#123; int mid = lo + (hi - lo) / 2; if (less(v, a[mid])) hi = mid; else lo = mid + 1; &#125; // insertion sort with "half exchanges" // (insert a[i] at index j and shift a[j], ..., a[i-1] to right) for (int j = i; j &gt; lo; --j) a[j] = a[j - 1]; a[lo] = v; &#125;&#125; 在内循环中将较大的元素都向右移动而不总是交换两个元素(访问数组的次数能够减半)12345678910111213141516171819202122232425public static void sort(Comparable[] a) &#123; int length = a.length; // put smallest element in position to serve as sentinel int exchanges = 0; for (int i = length - 1; i &gt; 0; i--) &#123; if (less(a[i], a[i - 1])) &#123; exch(a, i, i - 1); exchanges++; &#125; &#125; if (exchanges == 0) return; // insertion sort with half-exchanges for (int i = 2; i &lt; length; i++) &#123; Comparable v = a[i]; int j = i; while (less(v, a[j - 1])) &#123; a[j] = a[j - 1]; j--; &#125; a[j] = v; &#125;&#125; 希尔排序 希尔排序,也称递减增量排序算法,它是基于插入排序的一种更高效的改进版本. 由于插入排序对于大规模乱序数组效率并不高,因为它只会交换相邻的元素,因此元素只能一点一点地从数组的一端移动到另一端. 而希尔排序为了加快速度简单地改进了插入排序,交换不相邻的元素以对数组的局部进行排序,并最终用插入排序将局部有序的数组排序. 希尔排序的思想是使数组中任意间隔为h的元素都是有序的,可以说一个h有序的数组就是h个互相独立的有序数组编织在一起组成的一个数组. 代码实现12345678910111213141516public static void sort(Comparable[] a) &#123; int h = 1; while (h &lt; a.length / 3) &#123; // h sequence 1,4,13,40,121,364,1093,... h = h * 3 + 1; &#125; while (h &gt;= 1) &#123; for (int i = h; i &lt; a.length; i++) &#123; // a[i] insert to a[i-h],a[i-2*h],a[i-3*h]... for (int j = i; j &gt;= h &amp;&amp; less(a[j], a[j - h]); j -= h) &#123; exch(a, j, j - h); &#125; &#125; h = h / 3; &#125; &#125; 归并排序 归并排序是分治算法的典型应用.所谓归并即是将两个有序的数组归并成一个更大的有序数组. 它有一个主要的缺点就是它需要额外的空间(辅助数组)并且所需的额外空间和N成正比. 合并过程 申请空间,使其大小为两个已有序序列之和,该空间用于存放合并后的序列 声明两个指针,最初位置分别为两个有序序列的起始位置 比较两个指针所指向的元素,选择相对小的元素放入合并空间中,并移动指针到下一个位置 重复步骤3直到某一指针到达序列尾部 将另一序列剩下的所有元素直接放入合并序列尾 自顶向下的归并排序自顶向下即是从顶部化整为零地递归解决问题. 例如:要对数组a[lo..hi]进行排序,需要先将它切分为a[lo..mid]与a[mid+1..hi]两部分,分别通过递归调用将它们单独排序,最后将有序的子数组归并为最终的排序结果. 1234567891011121314151617181920212223242526272829303132// stably merge a[lo .. mid] with a[mid+1 ..hi] using aux[lo .. hi] private static void merge(Comparable[] a, Comparable[] aux, int lo, int mid, int hi) &#123; // copy a[] to aux[] for (int k = lo; k &lt;= hi; k++) &#123; aux[k] = a[k]; &#125; // merge back to a[] int i = lo, j = mid + 1; for (int k = lo; k &lt;= hi; k++) &#123; if (i &gt; mid) &#123; a[k] = aux[j++]; &#125; else if (j &gt; hi) &#123; a[k] = aux[i++]; &#125; else if (less(aux[j], aux[i])) &#123; a[k] = aux[j++]; &#125; else &#123; a[k] = aux[i++]; &#125; &#125; &#125; // mergesort a[lo..hi] using auxiliary array aux[lo..hi] private static void sort(Comparable[] a, Comparable[] aux, int lo, int hi) &#123; if (hi &lt;= lo) return; int mid = lo + (hi - lo) / 2; sort(a, aux, lo, mid); sort(a, aux, mid + 1, hi); merge(a, aux, lo, mid, hi); &#125; 自底向上的归并排序自底向上则是循序渐进地解决问题. 实现思路是先归并那些微型数组,然后再成对归并得到的子数组,直到将整个数组归并在一起. 可以先进行两两归并(每个元素想象成一个大小为1的数组),然后进行四四归并(将两个大小为2的数组归并成一个有四个元素的数组),然后是八八归并…..(一直下去)在每一轮归并中,最后一次归并的第二个子数组可能比第一个子数组要小,如果不是的话所有归并中两个数组大小都应该一致. 123456789101112//merge函数与自顶向下中的一致public static void sort(Comparable[] a) &#123; int N = a.length; Comparable[] aux = new Comparable[N]; for (int len = 1; len &lt; N; len *= 2) &#123; for (int lo = 0; lo &lt; N - len; lo += len + len) &#123; int mid = lo + len - 1; int hi = Math.min(lo + len + len - 1, N - 1); merge(a, aux, lo, mid, hi); &#125; &#125; &#125; 优化 如果数组很小,那么频繁的递归调用效率会很差,所以可以使用插入排序(或选择排序等)来处理小规模的子数组. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private static void merge(Comparable[] src, Comparable[] dst, int lo, int mid, int hi) &#123; int i = lo, j = mid + 1; for (int k = lo; k &lt;= hi; k++) &#123; if (i &gt; mid) &#123; dst[k] = src[j++]; &#125; else if (j &gt; hi) &#123; dst[k] = src[i++]; &#125; else if (less(src[j], src[i])) &#123; dst[k] = src[j++]; &#125; else &#123; dst[k] = src[i++]; &#125; &#125;&#125;private static void sort(Comparable[] src, Comparable[] dst, int lo, int hi) &#123; // if (hi &lt;= lo) return; if (hi &lt;= lo + CUTOFF) &#123; insertionSort(dst, lo, hi); return; &#125; int mid = lo + (hi - lo) / 2; sort(dst, src, lo, mid); sort(dst, src, mid + 1, hi); // using System.arraycopy() is a bit faster than the above loop if (!less(src[mid + 1], src[mid])) &#123; System.arraycopy(src, lo, dst, lo, hi - lo + 1); return; &#125; merge(src, dst, lo, mid, hi);&#125;// using insertion sort handle small arrayprivate static void insertionSort(Comparable[] a, int lo, int hi) &#123; for (int i = lo; i &lt;= hi; i++) &#123; for (int j = i; j &gt; lo &amp;&amp; less(a[j], a[j - 1]); j--) &#123; exch(a, j, j - 1); &#125; &#125;&#125;public static void sort(Comparable[] a) &#123; Comparable[] aux = a.clone(); sort(aux, a, 0, a.length - 1);&#125; 快速排序 快速排序又称划分交换排序,它也是一种分治的排序算法. 快速排序有一个潜在的缺点,在切分不平衡时这个程序可能会极为低效,所以需要在快速排序前将数组随机排序来避免这种情况. 它将一个数组切分成两个子数组,将两部分独立地排序.它与归并排序不同的地方在于: 归并排序将数组分成两个子数组分别排序,最终将有序的子数组归并以致整个数组排序. 快速排序将数组排序的方式则是当两个子数组都有序时,整个数组也就是有序的了. 在归并排序中,递归调用发生在处理整个数组之前;而在快速排序中,递归调用发生在处理整个数组之后. 在归并排序中,一个数组会被等分为两半,而在快速排序中,切分的位置取决于数组的内容. 运行过程 先从数列中挑选出一个基准,可以为a[lo],它是被确认为排定的元素. 从数组的左端(左指针)开始向右扫描直到找到一个大于等于基准的元素. 从数组的右端(右指针)开始向左扫描直到找到一个小于等于基准的元素. 这两个元素即是没有排定的,交换它们的位置(保证了左指针i的左侧元素都不大于基准,右指针j的右侧元素都不小于基准). .当两个指针相遇时,将基准和左子数组最右侧的元素(a[j])交换然后返回j即可. 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960// partition the subarray a[lo..hi] so that a[lo..j-1] &lt;= a[j] &lt;= a[j+1..hi] // and return the index j. private static int partition(Comparable[] a, int lo, int hi) &#123; int i = lo; // left point int j = hi + 1; // right point Comparable v = a[lo]; // partition element while (true) &#123; // scan left point while (less(a[++i], v)) &#123; if (i == hi) break; &#125; // scan right point while (less(v, a[--j])) &#123; if (j == lo) break; &#125; // check if point cross if (i &gt;= j) break; exch(a, i, j); &#125; // put partition element v to a[j] exch(a, lo, j); // now a[lo..j-1] &lt;= a[j] &lt;= a[j+1..hi] return j; &#125; private static void sort(Comparable[] a, int lo, int hi) &#123; if (hi &lt;= lo) return; int j = partition(a, lo, hi); sort(a, lo, j - 1); sort(a, j + 1, hi); &#125; public static void sort(Comparable[] a) &#123; shuffle(a); sort(a, 0, a.length - 1); &#125; // random sort an array private static void shuffle(Object[] a) &#123; if (a == null) throw new IllegalArgumentException("array is null."); Random random = new Random(); int N = a.length; for (int i = 0; i &lt; N; i++) &#123; int j = i + random.nextInt(N - i); Object temp = a[i]; a[i] = a[j]; a[j] = temp; &#125; &#125; 三向切分的快速排序当存在大量重复元素的情况下,快速排序的递归性会使元素全部重复的子数组经常出现,这就有很大的改进潜力,将当前快速排序从线性对数级别的性能提升至线性级别. 一个简单的思路是将数组切分为三部分,分别对应小于、等于、大于切分元素的数组元素. 在实现中,维护一个左指针lt使得a[lo..lt-1]的元素都小于基准,右指针gt使得a[gt+1..hi]中的元素都大于基准,一个指针i使得a[lt..i-1]中的元素都等于基准,a[i..gt]中的元素都还未确定. a[i]小于基准,将a[lt]和a[i]交换,lt++&amp;i++. a[i]大于基准,将a[gt]和a[i]交换,gt–. a[i]等于基准,i++. 以上操作都会保证数组元素不变且缩小gt-i的值(这样循环才会结束).除非和切分元素相等,其他元素都会被交换. 12345678910111213141516171819202122// quicksort the subarray a[lo .. hi] using 3-way partitioning private static void sort(Comparable[] a, int lo, int hi) &#123; if (hi &lt;= lo) return; int lt = lo, i = lo + 1, gt = hi; Comparable v = a[lo]; // partition element // a[lo..lt-1] &lt; a[lt..gt] &lt; a[gt+1..hi] while (i &lt;= gt) &#123; int cmp = a[i].compareTo(v); if (cmp &lt; 0) &#123; exch(a, i++, lt++); &#125; else if (cmp &gt; 0) &#123; exch(a, i, gt--); &#125; else &#123; i++; &#125; &#125; sort(a, lo, lt - 1); sort(a, gt + 1, hi); &#125; 堆排序 堆排序是基于堆的优先队列实现的一种排序算法. 优先队列优先队列是一种支持删除最大(最小)元素和插入元素的数据结构,它的内部是有序的,任意优先队列都可以变成一种排序方法. 堆堆是一种数据结构,它通常可以被看作为一棵树的数组对象.将根节点作为最大数的叫做最大堆,反之,将根节点作为最小数的叫做最小堆. 堆是一个近似完全二叉树的结构,同时又满足了堆的性质:每个元素都要保证大于(小于)等于它的子节点的元素. 在一个堆中,根据根节点的索引位置不同,计算父节点与子节点位置的算法也不同. 当数组起始位置为0时,位置k的节点的父节点为(k - 1)/2,它的两个子节点为2k+1,2k+2. 当数组起始位置为1时(即不使用索引0),位置k的节点的父节点为k/2,它的两个子节点为2k,2k+1. 为了保证堆有序,需要支持两个操作用于打破堆的状态,然后再遍历堆并按照要求将堆的状态恢复,这个过程叫做堆的有序化. 由下至上的堆有序化(上浮) : 如果堆的有序状态因为某个节点变得比它的父节点更大而被打破时,那么就需要通过交换它和它的父节点来修复堆,将这个节点不断向上移动直到遇到了一个更大的父节点.(如果是最小堆,比较的逻辑相反). 1234567// 在本文中,均不使用数组的0索引 private void swim(int k) &#123; while (k &gt; 1 &amp;&amp; less(k/2, k)) &#123; exch(a,k, k/2); k = k/2; &#125; &#125; 由上至下的堆有序化(下沉) : 如果堆的有序状态因为某个节点变得比它的两个子节点或是其中之一更小了而被打破时,需要通过将它和它的两个子节点中的较大者交换来修复堆,将这个节点向下移动直到它的子节点都比它更小或是到达了堆的底部.(如果是最小堆,比较的逻辑想法) 12345678910// n为数组长度private void sink(int k) &#123; while (2*k &lt;= n) &#123; int j = 2*k; if (j &lt; n &amp;&amp; less(j, j+1)) j++; if (!less(a[k],a[j])) break; exch(a,k, j); k = j; &#125; &#125; 运行过程 堆排序可以分为两个阶段. 堆的构造阶段,将原始数组重新组织安排进一个堆中.从右至左用sink()函数,构造子堆,数组的每个位置都已经是一个子堆的根节点.只需要扫描数组中的一半元素,因为我们可以跳过大小为1的子堆.最后在位置1上调用sink()函数,结束扫描. 下沉排序阶段,从堆中按递减顺序取出所有元素并得到排序结果.将堆中的最大元素删除,然后放入堆缩小后数组中空出的位置. 代码实现123456789101112131415161718192021222324252627282930313233343536public static void sort(Comparable[] a) &#123; int N = a.length; // construction max heap for (int k = N / 2; k &gt;= 1; k--) &#123; sink(a, k, N); &#125; // sink sort while (N &gt; 1) &#123; // the biggest element (root) swap smallest element then heap shrink exch(a, 1, N--); // new root element sink sink(a, 1, N); &#125; &#125; private static void sink(Comparable[] pq, int k, int n) &#123; while (2 * k &lt;= n) &#123; int j = 2 * k; if (j &lt; n &amp;&amp; less(pq, j, j + 1)) j++; if (!less(pq, k, j)) break; exch(pq, k, j); k = j; &#125; &#125; private static boolean less(Comparable[] pq, int i, int j) &#123; return pq[i - 1].compareTo(pq[j - 1]) &lt; 0; &#125; private static void exch(Object[] pq, int i, int j) &#123; Object swap = pq[i - 1]; pq[i - 1] = pq[j - 1]; pq[j - 1] = swap; &#125; 总结 名称 是否稳定 是否为原地排序 时间复杂度 空间复杂度 备注 冒泡排序 是 是 O(N^2) O(1) （无序区，有序区）。从无序区通过交换找出最大元素放到有序区前端。 选择排序 否 是 O(N^2) O(1) （有序区，无序区）。在无序区里找一个最小的元素跟在有序区的后面。对数组：比较得多，换得少。 插入排序 是 是 介入N和N^2之间 O(1) （有序区，无序区）。把无序区的第一个元素插入到有序区的合适的位置。对数组：比较得少，换得多。 希尔排序 否 是 O(N log^2 N) O(1) 每一轮按照事先决定的间隔进行插入排序，间隔会依次缩小，最后一次一定要是1。 快速排序 否 是 O(N log N) O(logN) （小数，基准元素，大数）。在区间中随机挑选一个元素作基准，将小于基准的元素放在基准之前，大于基准的元素放在基准之后，再分别对小数区与大数区进行排序。 三向快速排序 否 是 介于N和NlogN之间 O(logN) 对含有大量重复元素的输入数据效率较高。 归并排序 是 否 O(N log N) O(N) 把数据分为两段，从两段中逐个选最小的元素移入新数据段的末尾。 堆排序 否 是 O(N log N) O(1) （最大堆，有序区）。从堆顶把根卸出来放在有序区之前，再恢复堆。 在大多数实际情况中,快速排序是最佳选择.如果稳定性很重要而空间又不是问题的情况下,归并排序可能是最好的.但是在运行时间至关重要的任何排序应用中应该认真地考虑使用快速排序. 在JDK中,Arrays.sort()选择了根据不同的参数类型,来使用不同的排序算法.如果是原始数据类型则使用三向切分的快速排序,对引用类型则使用归并排序. end Author : SylvanasSun Email : sylvanassun_xtz@163.com 文中的完整实现代码见我的GitHub &amp; Gist 本文参考资料引用自&lt;&gt; &amp; WikiPedia]]></content>
      <categories>
        <category>Algorithms</category>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>Algorithms</tag>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用链表做为Stack、Queue中的数据表示结构的基本思路]]></title>
    <url>%2F2017%2F03%2F06%2F2017-3-06-LinkedStack%26Queue%2F</url>
    <content type="text"><![CDATA[什么是链表? 链表是一种常见的基础数据结构(数组也是基础数据结构),它是一种递归的数据结构,由一系列节点(Node)组成,节点含有一个存储数据的数据域和一个指向下一个节点地址位置的引用. 链表是线性表的一种,但是它的物理存储结构是非连续、 非顺序的,元素的逻辑顺序是通过节点之间的链接确定的. 数据结构与数据类型的区别 数据类型是一组数据和一组对这些值进行操作的集合. 数据结构强调的是数据的存储和组织方式. 常见的数据结构有:数组、栈、队列、链表、树、图、堆、散列表. 链表是否可以替代数组? 由于创建数组需要预先知道数组的大小,所以想要动态的扩容需要不断地创建新数组,而链表则可以充分利用内存空间,实现较为灵活的动态扩展. 使用链表替代数组有优点也有缺点: 优点 在链表中进行插入操作或是删除操作都更加方便快速. 链表所需的空间总是和集合的大小成正比. 链表操作所需的时间总是和集合的大小无关. 缺点 无法像数组一样可以通过索引来进行随机访问. 由于每一个元素节点都是一个对象,所以需要的空间开销比较大. Stack 栈是一种基于后进先出(LIFO)的数据结构,其中的元素除了头尾之外,每个元素都有一个前驱和一个后继. 使用链表来表示栈内部的数据时,栈顶就是链表的头部,当push元素时将元素添加在表头,当pop元素时将元素从表头删除. 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788/** * 使用链表实现的可迭代的下压栈(后进先出) * &lt;p&gt; * Created by SylvanasSun on 2017/3/6. */public class Stack&lt;T&gt; implements Iterable&lt;T&gt; &#123; private Node first; //栈顶(链表头部) private int N; //元素个数 /** * 用于表示链表中的节点 */ private class Node &#123; T item; Node next; &#125; /** * 判断Stack是否为空 * * @return true代表Stack为空, false为未空 */ public boolean isEmpty() &#123; return first == null; //也可以用N==0来判断 &#125; /** * 返回Stack中的元素数量 * * @return 元素数量 */ public int size() &#123; return N; &#125; /** * 将元素t添加到栈顶 * * @param t 添加的元素 */ public void push(T t) &#123; Node oldFirst = first; first = new Node(); first.item = t; first.next = oldFirst; N++; &#125; /** * 将栈顶的元素弹出 * * @return 栈顶节点的item */ public T pop() &#123; T item = first.item; first = first.next; N--; return item; &#125; public Iterator&lt;T&gt; iterator() &#123; return new ListIterator(); &#125; /** * 迭代器,维护了一个实例变量current来记录链表的当前结点. * 这段代码可以在Stack和Queue之间复用,因为它们内部数据的数据结构是相同的, * 只是访问顺序分别为后进先出和先进先出而已. */ private class ListIterator implements Iterator&lt;T&gt; &#123; private Node current = first; public boolean hasNext() &#123; return current != null; &#125; public T next() &#123; T item = current.item; current = current.next; return item; &#125; public void remove() &#123; &#125; &#125;&#125; Queue 队列是一种基于先进先出(FIFO)的数据结构,元素的处理顺序就是它们被添加到队列中的顺序. 可以使用实例变量first指向队列的队头,实例变量last指向队列的队尾,当将一个元素入列时,就将这个元素添加到队尾,当要将一个元素出列时,就删除队头的节点. 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798/** * 使用链表实现的Queue,它与Stack的区别在于链表的访问顺序. * Queue的访问顺序是先进先出的. * &lt;p&gt; * Created by SylvanasSun on 2017/3/6. */public class Queue&lt;T&gt; implements Iterable&lt;T&gt; &#123; private Node first; //链表头部,即队头 private Node last; //链表尾部,即队尾 private int N; //size /** * 用于表示链表中的节点 */ private class Node &#123; T item; Node next; &#125; /** * 判断Queue是否为空 * * @return true代表Stack为空, false为未空 */ public boolean isEmpty() &#123; return first == null; &#125; /** * 返回Queue中的元素数量 * * @return 元素数量 */ public int size() &#123; return N; &#125; /** * 入队,向队尾添加新的元素 * * @param item 添加的元素 */ public void enqueue(T item) &#123; Node oldLast = last; last = new Node(); last.item = item; last.next = null; /** * 如果队列为空,队头指向队尾(队列中只有一个元素), * 否则将旧的队尾的next指向新的队尾 */ if (isEmpty()) first = last; else oldLast.next = last; N++; &#125; /** * 出队,将队头节点弹出队列 * * @return 队头节点的item */ public T dequeue() &#123; T item = first.item; first = first.next; N--; //如果队列为空,队尾则为null if (isEmpty()) last = null; return item; &#125; public Iterator&lt;T&gt; iterator() &#123; return new ListIterator(); &#125; /** * 迭代器,与Stack中的实现一致 */ private class ListIterator implements Iterator&lt;T&gt; &#123; private Node current = first; public boolean hasNext() &#123; return current != null; &#125; public T next() &#123; T item = current.item; current = current.next; return item; &#125; public void remove() &#123; &#125; &#125;&#125; end Author: SylvanasSun GitHub: https://github.com/SylvanasSun Email: sylvanassun_xtz@163.com Reference: 《Algorithms 4th edition》&amp; wiki]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>LinkedTable</category>
      </categories>
      <tags>
        <tag>2017</tag>
        <tag>数据结构</tag>
        <tag>LinkedTable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现一个简单的HashMap]]></title>
    <url>%2F2017%2F01%2F12%2F2017-01-12-SimpleHashMap%2F</url>
    <content type="text"><![CDATA[概述 &nbsp;&nbsp;HashMap是基于哈希表的Map接口的实现,以key-value的形式存在,在HashMap中,key-value会被当成一个整体(Entry)来处理,HashMap内部维护了一个链表数组,会根据hash算法来计算key-value在数组中的存储位置. HashMap的内部结构 &nbsp;&nbsp;HashMap内部使用了桶(bucket)来存储键值对,桶就是一个存储key-value的链表.而HashMap中维护了一个数组,这个数组的每个元素就是一个桶(bucket),在HashMap中,桶是使用链表实现的. HashMap使用散列函数(hash)将给定键转化为一个数组索引(桶号),不同的key会被转化为不同的索引,实际中有几率会把不同的键转化为同一个索引,这种情况叫做hash碰撞,HashMap使用了拉链法解决hash碰撞. 当发生hash碰撞时,会生成一个新的key-value对象,并将新对象挂在链表头部. 得到数组索引后,就可以遍历这个链表来进行各种操作了. &nbsp;&nbsp;在HashMap中有2个重要的参数:capaCity(容量),loadFactor(负载因子). 容量:它表示HashMap中桶的数量. 负载因子:它是HashMap在其容量自动扩容之前可以达到多满的一种尺度,它衡量的是一个散列表的空间使用程度,负载因子越大表示散列表的空间利用率越大,反之越小.对于使用拉链法的散列表来说,查找一个元素的平均时间是O(1+a),因此负载因子越大,对空间的利用越充分,但是查找效率就会越低,如果负载因子很小,那么散列表的空间利用率将会很小,对空间造成严重浪费,但是查找效率则会变快.HashMap中负载因子的默认值为0.75. 阈值:容量自动扩展的阈值,当HashMap中的键值对数量到达阈值时,将会进行自动扩容(当前容量x2),阈值通常的计算方法为 capaCity * loadFactor. 简单实现 construction12345678910111213141516171819202122232425/** * 构造一个空的SimpleHashMap,使用默认的capacity和负载因子 */public SimpleHashMap(int initialiCapacity, float loadFactor) &#123; if (initialiCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialiCapacity); if (initialiCapacity &gt; MAXIMUM_CAPACITY) initialiCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); //计算出大于initialCapacity的最小的2的n次方值 int capacity = 1; while (capacity &lt; initialiCapacity) &#123; capacity &lt;&lt;= 1; &#125; this.loadFactor = loadFactor; //设置HashMap的扩容阈值,当到达这个阈值时会进行自动扩容 threshold = (int) (capacity * loadFactor); //初始化table数组 table = new Node[capacity];&#125; &nbsp;&nbsp;其中table是一个Node数组,它是由链表实现的. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748static class Node&lt;K, V&gt; implements Map.Entry&lt;K, V&gt; &#123; final int hash; final K key; V value; Node&lt;K, V&gt; next; Node(int hash, K key, V value, Node&lt;K, V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?, ?&gt; e = (Entry&lt;?, ?&gt;) o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) &#123; return true; &#125; &#125; return false; &#125; &#125; put1234567891011121314151617181920212223public V put(K key, V value) &#123; //如果key为null,调用putForNullKey()向null key存入value if (key == null) return putForNullKey(value); //计算key的hash int hash = hash(key.hashCode()); ---1 //计算key的hash在table数组中的索引 int i = indexFor(hash, table.length); --2 //遍历table for (Node&lt;K, V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; //如果有相同的key,直接覆盖value,返回oldValue if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; return oldValue; &#125; &#125; modCount++; //修改次数++ //将key,value添加至i处 addEntry(hash, key, value, i); --3 return null; &#125; &nbsp;&nbsp;看以上代码1、2处,这2个函数计算了hash值和bucket索引. 1234567891011121314151617181920212223242526/** * 预处理hash值，避免较差的离散hash序列，导致桶没有充分利用. */static int hash(int h) &#123; h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125;/** * 返回对应hash值得索引 */static int indexFor(int h, int length) &#123; /** * 由于length是2的n次幂，所以h &amp; (length-1)相当于h % length。 * 对于length，其2进制表示为1000...0，那么length-1为0111...1。 * 那么对于任何小于length的数h，该式结果都是其本身h。 * 对于h = length，该式结果等于0。 * 对于大于length的数h，则和0111...1位与运算后， * 比0111...1高或者长度相同的位都变成0， * 相当于减去j个length，该式结果是h-j*length， * 所以相当于h % length。 * 其中一个很常用的特例就是h &amp; 1相当于h % 2。 * 这也是为什么length只能是2的n次幂的原因，为了优化。 */ return h % (length - 1);&#125; &nbsp;&nbsp;代码3的addEntry函数向数组添加了一对key-value. 123456789101112/** * 添加一对key-value,如果当前索引上已有桶(发生hash碰撞),则将新元素放入链表头 */void addEntry(int hash, K key, V value, int bucketIndex) &#123; //保存对应table的值 Node&lt;K, V&gt; e = table[bucketIndex]; //用新桶链住旧桶 table[bucketIndex] = new Node&lt;K, V&gt;(hash, key, value, e); //如果HashMap中元素的个数已经超过阈值,则扩容两倍 if (size++ &gt;= threshold) resize(2 * table.length);&#125; get12345678910111213141516public V get(Object key) &#123; //如果key为null,则调用getForNullkey()获得key为null的值 if (key == null) return getForNullKey(); //根据key的hashCode计算它的hash int hash = hash(key.hashCode()); //取出table中指定索引处的值 for (Node&lt;K, V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; //如果查找相同的key,返回其对应的value if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; return e.value; &#125; &#125; return null; &#125; all123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318/** * 一个简单的HashMap,内部使用拉链法解决hash碰撞. * &lt;p&gt; * Created by sylvanasp on 2017/1/12. */public class SimpleHashMap&lt;K, V&gt; extends AbstractMap&lt;K, V&gt; implements Map&lt;K, V&gt;, Cloneable, Serializable &#123; private static final long serialVersionUID = 6623475452522370065L; /** * 默认的容量(bucket数量),1 &lt;&lt; 4(16). */ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; /** * 最大的容量,1 &lt;&lt; 30 (2^30) */ static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; /** * 默认的负载因子,0.75. */ static final float DEFAULT_LOAD_FACTOR = 0.75f; /** * KV链表 */ static class Node&lt;K, V&gt; implements Map.Entry&lt;K, V&gt; &#123; final int hash; final K key; V value; Node&lt;K, V&gt; next; Node(int hash, K key, V value, Node&lt;K, V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?, ?&gt; e = (Entry&lt;?, ?&gt;) o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) &#123; return true; &#125; &#125; return false; &#125; &#125; /** * 链表数组,数组中的每一个元素代表了一个链表的头部. */ transient Node[] table; /** * 当前map的key-value映射数，也就是当前size */ transient int size; /** * 代表这个HashMap修改key-value的次数. */ transient int modCount; /** * 自动扩展的阈值(capacity * loadfactor) */ int threshold; /** * 负载因子: * 它是哈希表在其容量自动增加之前可以达到多满的一种尺度， * 它衡量的是一个散列表的空间的使用程度，负载因子越大表示散列表的装填程度越高， * 反之愈小。对于使用链表法的散列表来说，查找一个元素的平均时间是O(1+a)， * 因此如果负载因子越大，对空间的利用更充分，然而后果是查找效率的降低； * 如果负载因子太小，那么散列表的数据将过于稀疏，对空间造成严重浪费. */ final float loadFactor; /** * 构造一个空的SimpleHashMap,使用默认的capacity和负载因子 */ public SimpleHashMap(int initialiCapacity, float loadFactor) &#123; if (initialiCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialiCapacity); if (initialiCapacity &gt; MAXIMUM_CAPACITY) initialiCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); //计算出大于initialCapacity的最小的2的n次方值 int capacity = 1; while (capacity &lt; initialiCapacity) &#123; capacity &lt;&lt;= 1; &#125; this.loadFactor = loadFactor; //设置HashMap的扩容阈值,当到达这个阈值时会进行自动扩容 threshold = (int) (capacity * loadFactor); //初始化table数组 table = new Node[capacity]; &#125; public SimpleHashMap() &#123; this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR); &#125; /** * 预处理hash值，避免较差的离散hash序列，导致桶没有充分利用. */ static int hash(int h) &#123; h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); &#125; /** * 返回对应hash值得索引 */ static int indexFor(int h, int length) &#123; /** * 由于length是2的n次幂，所以h &amp; (length-1)相当于h % length。 * 对于length，其2进制表示为1000...0，那么length-1为0111...1。 * 那么对于任何小于length的数h，该式结果都是其本身h。 * 对于h = length，该式结果等于0。 * 对于大于length的数h，则和0111...1位与运算后， * 比0111...1高或者长度相同的位都变成0， * 相当于减去j个length，该式结果是h-j*length， * 所以相当于h % length。 * 其中一个很常用的特例就是h &amp; 1相当于h % 2。 * 这也是为什么length只能是2的n次幂的原因，为了优化。 */ return h % (length - 1); &#125; /** * 获得key为null的值 */ private V getForNullKey() &#123; //遍历table[0] for (Node&lt;K, V&gt; e = table[0]; e != null; e = e.next) &#123; //如果找到key为null,则返回对应的值 if (e.key == null) &#123; return e.value; &#125; &#125; return null; &#125; /** * 当Key为Null时如何放入值 */ private V putForNullKey(V value) &#123; //遍历table[0] for (Node&lt;K, V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; //取出oldValue,并存入newValue V oldValue = e.value; e.value = value; //返回oldValue return oldValue; &#125; &#125; modCount++; addEntry(0, null, value, 0); return null; &#125; /** * 添加一对key-value,如果当前索引上已有桶(发生hash碰撞),则将新元素放入链表头 */ void addEntry(int hash, K key, V value, int bucketIndex) &#123; //保存对应table的值 Node&lt;K, V&gt; e = table[bucketIndex]; //用新桶链住旧桶 table[bucketIndex] = new Node&lt;K, V&gt;(hash, key, value, e); //如果HashMap中元素的个数已经超过阈值,则扩容两倍 if (size++ &gt;= threshold) resize(2 * table.length); &#125; /** * 扩充容量 */ void resize(int newCapacity) &#123; //保存oldTable Node[] oldTable = table; //保存旧容量 int oldCapacity = oldTable.length; //如果旧的容量已经是系统默认最大容量了，那么将阈值设置成整形的最大值 if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; //根据newCapacity创建一个table Node[] newTable = new Node[newCapacity]; //将table转换为newTable transfer(newTable); table = newTable; //设置阈值 threshold = (int) (newCapacity * loadFactor); &#125; // 将所有格子里的桶都放到新的table中 void transfer(Node[] newTable) &#123; // 得到旧的table Node[] src = table; // 得到新的容量 int newCapacity = newTable.length; // 遍历src里面的所有格子 for (int j = 0; j &lt; src.length; j++) &#123; // 取到格子里的桶（也就是链表） Node&lt;K, V&gt; e = src[j]; // 如果e不为空 if (e != null) &#123; // 将当前格子设成null src[j] = null; // 遍历格子的所有桶 do &#123; // 取出下个桶 Node&lt;K, V&gt; next = e.next; // 寻找新的索引 int i = indexFor(e.hash, newCapacity); // 设置e.next为newTable[i]保存的桶（也就是链表连接上） e.next = newTable[i]; // 将e设成newTable[i] newTable[i] = e; // 设置e为下一个桶 e = next; &#125; while (e != null); &#125; &#125; &#125; @Override public V get(Object key) &#123; //如果key为null,则调用getForNullkey()获得key为null的值 if (key == null) return getForNullKey(); //根据key的hashCode计算它的hash int hash = hash(key.hashCode()); //取出table中指定索引处的值 for (Node&lt;K, V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; //如果查找相同的key,返回其对应的value if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; return e.value; &#125; &#125; return null; &#125; @Override public V put(K key, V value) &#123; //如果key为null,调用putForNullKey()向null key存入value if (key == null) return putForNullKey(value); //计算key的hash int hash = hash(key.hashCode()); //计算key的hash在table数组中的索引 int i = indexFor(hash, table.length); //遍历table for (Node&lt;K, V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; //如果有相同的key,直接覆盖value,返回oldValue if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; return oldValue; &#125; &#125; modCount++; //修改次数++ //将key,value添加至i处 addEntry(hash, key, value, i); return null; &#125; @Override public Set&lt;Entry&lt;K, V&gt;&gt; entrySet() &#123; return null; &#125; public static void main(String[] args) &#123; Map&lt;String, String&gt; map = new SimpleHashMap&lt;&gt;(); map.put("hello", "world"); System.out.println(map.get("hello")); &#125;&#125;]]></content>
      <categories>
        <category>Algorithms</category>
        <category>数据结构</category>
        <category>HashTable</category>
      </categories>
      <tags>
        <tag>HashTable</tag>
        <tag>Algorithms</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[探秘HotSpot虚拟机中的对象]]></title>
    <url>%2F2016%2F09%2F05%2F2016-09-5-HotSpotObject%2F</url>
    <content type="text"><![CDATA[对象的创建 &nbsp;&nbsp;在Java程序运行过程中无时无刻都有对象被创建出来.在语言层面上,创建对象通常仅仅是一个new关键字而已,而在虚拟机中,创建一个对象不像只需要new一下那么简单了. &nbsp;&nbsp;以下为虚拟机中对象创建的过程(仅限于普通Java对象,不包括数组和Class对象等). 虚拟机遇到一条new指令时,首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用,并且检查这个符号引用代表的类是否已被加载、解析和初始化过.如果没有,那必须先执行相应的类加载过程. 在类加载检查通过后,接下来虚拟机将为新生对象分配内存.对象所需内存的大小在类加载完成后便可完全确定,为对象分配空间的任务等同于把一块确定大小的内存从Java堆中划分出来. 如果Java堆中内存是绝对规整的,所有用过的内存都放在一边,中间放着一个指针作为分界点的指示器,那所分配内存就仅仅是把那个指针向空闲空间那边挪动一段与对象大小相等的距离,这种分配方式称为”指针碰撞(Bump the Pointer)”.在使用Serial、ParNew等待Compact过程的收集器时,系统采用的分配算法为指针碰撞. 如果Java堆中内存并不是规整的,已使用的内存和空闲的内存相互交错,那就没有办法简单地进行指针碰撞了,虚拟机就必须维护一个列表,记录上哪些内存块是可用的,在分配的时候从列表中找到一块足够大的空间划分给对象实例,并更新列表上的记录,这种分配方式称为”空闲列表(Free List)”.在使用CMS这种基于Mark-Sweep算法的收集器,通常采用空闲列表. 由于创建对象是一件非常频繁的事情,所以除了划分可用空间之外,虚拟机还要考虑线程安全的问题,可能出现正在给对象A分配内存,指针还没来得及修改,对象B又同时使用了原来的指针来分配内存的情况.解决这个问题有两种方案,一种是对分配内存空间的动作进行同步处理(虚拟机采用CAS配上失败重试的方式保证更新操作的原子性);另一种是把内存分配的动作按照线程划分在不同的空间之中进行,即每个线程在Java堆中预先分配一小块内存,称为本地线程分配缓冲(Thread Local Allocation Buffer TLAB).哪个线程要分配内存,就在哪个线程的TLAB上分配,只有TLAB用完并分配新的TLAB时,才需要同步锁定.虚拟机是否使用TLAB,可以通过-XX:+/-UseTLAB参数来设置. 在内存分配完成后,虚拟机需要将分配到的内存空间都初始化为零值(不包括对象头),如果使用TLAB,这一工作也可以提前至TLAB分配时进行.这一步操作保证了对象的实例字段在Java代码中可以不赋初始值就直接使用,程序能访问到这些字段的数据类型所对应的零值. 最后一步,虚拟机要对对象进行必要的设置,例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的GC分代年龄等信息.这些信息存放在对象的对象头(Object Header)之中.根据虚拟机当前的运作状态的不同,如是否启动偏向锁等,对象头会有不同的设置方式. 以上工作全部完成后,从虚拟机的角度来看,一个新的对象已经产生了,但从Java程序的视角来看,对象的创建才刚刚开始(init方法还没有执行,所有字段都还为零值).一般来说(由字节码中是否跟随invokespecial指令所决定),执行new指令之后会接着执行init()方法,把对象按照程序员的意愿进行初始化,这样一个真正可用的对象才算完全产生出来. 对象的内存布局 &nbsp;&nbsp;在HotSpot虚拟机中,对象在内存中存储的布局可以分为三块区域:对象头(Object Header)、实例数据(Instance Data)和对齐填充(Padding). 对象头&nbsp;&nbsp;对象头包括两部分信息,第一部分用于存储对象自身的运行时数据(例如哈希码、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等),这部分数据的长度在32位和64位的虚拟机(未开启压缩指针)中分别为32bit和64bit,官方称其为”Mark Word”.这些运行时数据很多,其实已经超过了32位、64位Bitmap结构所能记录的限度.考虑到虚拟机的空间效率,Mark Word被设计成一个非固定的数据结构以便在极小的空间内存储尽量多的信息,它会根据对象的状态复用自己的存储空间.例如,在32位的HotSpot虚拟机中,如果对象处于未被锁定的状态下,那么Mark Word的32bit空间中的25bit用于存储对象哈希码,4bit用于存储对象分代年龄,2bit用于存储锁标志位,1bit固定为0,而在其他状态(轻量级锁定、重量级锁定、GC标记、可偏向)下对象的存储内容参见下表. 存储内容 标志位 状态 对象哈希码、对象分代年龄 01 未锁定 指向锁记录的指针 00 轻量级锁定 指向重量级锁的指针 10 膨胀(重量级锁定) 空,不需要记录信息 11 GC标记 偏向线程ID、偏向时间戳、对象分代年龄 01 可偏向 &nbsp;&nbsp;对象头的另一部分为类型指针,即对象指向它的类元数据的指针,虚拟机通过这个指针来确定这个对象是哪个类的实例.并不是所有的虚拟机实现都必须在对象数据上保留类型指针,也可以认为,查找对象的元数据信息并不一定要经过对象本身.如果对象是一个Java数组,那在对象头中还必须有一块用于记录数组长度的数据,因为虚拟机可以通过普通Java对象的元数据信息确定Java对象的大小,但是从数组的元数据中无法确定数组的大小. 实例数据&nbsp;&nbsp;实例数据是对象真正存储的有效信息,也是在程序代码中所定义的各种类型的字段内容.无论是从父类继承下来的,还是子类中定义的,都需要记录起来. &nbsp;&nbsp;实例数据的存储顺序会受到虚拟机分配策略参数(FieldsAllocationStyle)和字段在Java源码中定义顺序的影响.HotSpot虚拟机默认的分配策略为longs/doubles、ints、shorts/chars、bytes/booleans、oops(Ordinary Object Pointers),从以上分配策略中可以看出,相同宽度的字段总是被分配到一起.在满足这个前提条件的情况下,父类中定义的变量会出现在子类之前.如果CompactFields参数值为true(默认为true),那么子类中较窄的变量也可能会插入到父类变量的空隙之中. 对齐填充&nbsp;&nbsp;对齐填充并不是必然存在的,它也没有特别的含义,只是用于当作占位符而已. &nbsp;&nbsp;因为HotSpot虚拟机的自动内存管理系统要求对象起始地址必须是8字节的整数值,即是对象的大小必须是8字节的整数倍.而对象头部分正好是8字节的倍数(1倍或2倍),所以,当对象实例数据部分没有对齐时,就需要通过对齐填充来补全. 对象的访问定位 &nbsp;&nbsp;Java程序需要通过栈上的reference数据来操作堆上的具体对象.由于reference类型在Java虚拟机规范中只规定了一个指向对象的引用,所以对象访问方式也是取决于虚拟机实现而定的. &nbsp;&nbsp;目前主流的访问方式为句柄和直接指针两种. 句柄 &nbsp;&nbsp;使用句柄访问对象,Java堆中将会划分出一块内存作为句柄池,reference中存储的就是对象的句柄地址,而句柄中包含了对象实例数据与类型数据各自的具体地址信息. &nbsp;&nbsp;使用句柄来访问的最大好处就是reference中存储的是稳定的句柄地址,在对象被移动时(垃圾收集时移动对象是非常普遍的行为)只会改变句柄中的实例数据指针,而reference本身不需要修改. 直接指针 &nbsp;&nbsp;使用直接指针访问对象,Java堆对象的布局中就必须考虑如何放置访问类型数据的相关信息,而reference中存储的直接就是对象地址. &nbsp;&nbsp;使用直接指针访问方式的最大好处就是速度更快,它节省了一次指针定位的时间开销,由于对象的访问在Java中非常频繁,积少成多后也是一项非常可观的执行成本.HotSpot虚拟机就是使用直接指针方式进行对象访问的. End 资料参考于 &lt;&lt;深入理解 Java虚拟机 第二版&gt;&gt;.]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java虚拟机中的内存区域]]></title>
    <url>%2F2016%2F09%2F04%2F2016-09-4-JavaMemoryRegion%2F</url>
    <content type="text"><![CDATA[概述 &nbsp;&nbsp;对于C/C++程序员来说需要自己负责每一个对象生命开始到终结的维护.而对于Java程序员来说,则可以在Java虚拟机自动内存管理机制的帮助下,不需要为每一个new的对象去写delete/free代码,由虚拟机管理内存可以让我们把注意力放在实现业务逻辑上. &nbsp;&nbsp;但也正是因为Java虚拟机接管了内存控制的权力,所以一旦出现内存泄漏或溢出方面的异常问题,就需要了解虚拟机如何使用与维护内存,这样才能更好的排查错误解决异常. 运行时数据区域 &nbsp;&nbsp;Java虚拟机在执行Java程序时会把它所管理的内存划分为若干个不同的数据区域.这些区域都有着各自的用途,以及创建和销毁的时间,有的区域则会随着虚拟机进程的启动而存在,有些区域则会依赖于用户线程的启动和结束而创建和销毁. &nbsp;&nbsp;根据&lt;&gt;的规定,Java虚拟机管理的内存将会包括以下图所示的几个运行时数据区域. 程序计数器&nbsp;&nbsp;程序计数器(Program Counter Register)可以看作为当前线程所执行的字节码的行号指示器,它占用的内存很小. &nbsp;&nbsp;字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令,分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖程序计数器来完成. &nbsp;&nbsp;在Java虚拟机中,多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的.在任何一个确定的时刻,一个处理器(或多核CPU中的一个内核)都只会执行一条线程中的指令.为了线程切换后能够恢复到正确的执行位置,每一条线程都需要有一个独立的程序计数器,各条线程之间计数器互不影响,独立存储,这种类型的内存区域为”线程私有”的内存. &nbsp;&nbsp;如果线程正在执行的是一个Java方法,程序计数器记录的是正在执行的虚拟机字节码指令的地址;如果正在执行的是一个Native方法,程序计数器值则为空(Undefined). &nbsp;&nbsp;程序计数器内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError异常情况的区域. 虚拟机栈&nbsp;&nbsp;Java虚拟机栈(Java Virtual Machine Stacks)与程序计数器一样,是线程私有的内存,它的生命周期与线程相同. &nbsp;&nbsp;虚拟机栈描述的是Java方法执行的内存模型:即每个Java方法在执行的同时都会创建一个栈帧(Stack Frame)用于存储局部变量表、操作数栈、动态链接、方法出口等信息.栈帧是方法运行时的基础数据结构. &nbsp;&nbsp;每一个方法从调用直至执行完成的过程,就对应着一个栈帧在虚拟机栈中入栈到出栈的过程. &nbsp;&nbsp;栈帧中的局部变量表存放了编译期可知的各种基本数据类型(boolean、byte、char、short、int、float、long、double)、对象引用类型(reference类型,它不等同于对象本身,可能是一个指向对象起始地址的引用指针,也可能是指向一个代表对象的句柄或其他与此对象相关的位置)和returnAddress类型(指向了一条字节码指令的地址). &nbsp;&nbsp;64位长度的long和double类型的数据会占用2个局部变量空间(Slot),其他的数据类型只占用1个.局部变量表所需的内存空间是在编译期间完成分配的.当进入一个方法时,这个方法需要在栈帧中分配多大的局部变量空间是完全确定的,在方法运行期间不会改变局部变量表的大小. &nbsp;&nbsp;Java虚拟机规范对虚拟机栈区域规定了两种异常情况: 如果线程请求的栈深度大于虚拟机所允许的深度,将会抛出StackOverflowError异常. 如果虚拟机可以动态扩展,并在扩展时无法申请到足够的内存时,将会抛出OutOfMemoryError异常. 本地方法栈&nbsp;&nbsp;本地方法栈(Native Method Stack)与虚拟机栈基本相似.它们之间的区别只不过是虚拟机栈为虚拟机执行Java方法(也就是字节码)服务,而本地方法栈则为虚拟机使用到的Native方法服务. &nbsp;&nbsp;Java虚拟机规范对本地方法栈没有强制的规定,具体的虚拟机可以自由实现它.例如Sun HotSpot虚拟机甚至将本地方法栈与虚拟机栈合二为一. &nbsp;&nbsp;与虚拟机栈相同,本地方法栈也会抛出StackOverflowError和OutOfMemoryError异常. Java堆&nbsp;&nbsp;在大多数情况下,Java堆(Java Heap)是Java虚拟机所管理的内存中最大的一块.它是被所有线程共享的一块内存区域,在虚拟机启动时创建.Java堆的唯一目的就是存放对象实例,几乎所有的对象实例都在这里分配内存. &nbsp;&nbsp;但随着JIT编译器的发展与逃逸分析技术逐渐成熟,栈上分配、标量替换优化技术将会导致一些微妙的变化发生,所有对象都分配在堆上也渐渐变得不是那么”绝对”了. &nbsp;&nbsp;Java堆也是垃圾收集器管理的主要区域,也可以称为GC堆(Garbage Collected Heap).由于现在垃圾收集器基本都采用分代收集算法,所以Java堆中可以细分为新生代和老年代(再细化的有Eden空间、Form Survivor空间、To Survivor空间等). &nbsp;&nbsp;从内存分配的角度来看,线程共享的Java堆中可能划分出多个线程私有的分配缓冲区(Thread Local Allocation Buffer TLAB). &nbsp;&nbsp;Java虚拟机规范规定,Java堆可以处于物理上不连续的内存空间中,只要逻辑上是连续的即可.在实现时,既可以实现是固定大小的,也可以是可扩展的,如果在堆中没有内存完成实例分配,并且堆也无法再扩展时,将会抛出OutOfMemoryError异常. 方法区&nbsp;&nbsp;方法区(Method Area)与Java堆一样是各个线程共享的内存区域,它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据.Java虚拟机规范中把方法区描述为堆的一个逻辑部分,但它有一个别名叫Non-Heap(非堆),用于与Java堆区分开来. &nbsp;&nbsp;在HotSpot虚拟机中,方法区被称作为”永久代(Permanent Generation)”.因为HotSpot将GC分代收集扩展至了方法区,或者说是使用永久代来实现方法区.这样HotSpot的垃圾收集器可以像管理Java堆一样管理这部分内存,能够省去编写方法区的内存管理代码工作.但这样做法会更容易遇到内存溢出问题.所以HotSpot官方决定放弃永久代并逐步改为采用Native Memory来实现方法区的规划.在JDK1.7的HotSpot中,已经把原本放在永久代的字符串常量池移出. &nbsp;&nbsp;Java虚拟机规范规定,当方法区无法满足内存分配需求时,将抛出OutOfMemoryError异常. 运行时常量池&nbsp;&nbsp;运行时常量池(Runtime Constant Pool)是方法区的一部分.Class文件中除了有类的版本、字段、方法、接口等描述信息外,还有一项信息是常量池(Constant Pool Table),它用于存放编译期生成的各种字面量和符号引用,这部分内容将在类加载后进入方法区的运行时常量池中存放.一般来说,除了保存Class文件中描述的符号引用外,还会把翻译出来的直接引用也存储在运行时常量池中. &nbsp;&nbsp;运行时常量池相对于Class文件中常量池的另外一个重要特征是具备动态性.Java语言并不要求常量一定只有编译期才能产生,运行期间也可能将新的常量放入池中,这种特性利用得比较多的便是String类的intern()方法. &nbsp;&nbsp;当运行时常量池无法申请到内存时会抛出OutOfMemoryError异常. 直接内存&nbsp;&nbsp;直接内存(Direct Memory)并不是虚拟机运行时数据区域的一部分,也不是Java虚拟机规范中定义的内存区域.但是部分内存也被频繁地使用,而且也可能导致OutOfMemoryError异常. &nbsp;&nbsp;在JDK1.4中新加入了NIO(New Input/Output)类,引入了一种基于通道(Channel)与缓冲区(Buffer)的I/O方式,它可以使用Native函数库直接分配堆外内存,然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作.这样避免了在Java堆中和Native堆中来回复制数据,提高了性能. &nbsp;&nbsp;由于本机直接内存不会受到Java堆大小的限制,但它还是会受到本机总内存大小以及处理器寻址空间的限制.当我们配置虚拟机参数时,会根据实际内存设置-Xmx等参数信息,但经常会忽略直接内存,导致各个内存区域总和大于物理内存限制,从而导致动态扩展时出现OutOfMemoryError异常. OutOfMemoryError异常案例 Java堆溢出1234567891011121314151617181920212223242526272829/** * Java堆内存溢出,Java堆内存的OOM异常是实际应用中常见的内存溢出异常情况. * * 堆内存用于存储对象实例,只要不断地创建对象,并且保证GC Roots到对象之间有可达路径来避免 * 垃圾回收机制清除这些对象,这样就可以在对象数量到达最大堆的容量限制后产生内存溢出异常. * * 以下启动参数中,-Xms20 -Xmx20m 将堆的最小值与最大值设置为一样的,既可避免自动扩展. * -XX:+HeapDumpOnOutOfMemoryError可以让虚拟机在出现内存溢出异常时Dump出当前的内存堆转储快照以便分析. * * VM Args: -Xms20m -Xmx20m -XX:+HeapDumpOnOutOfMemoryError * &lt;p&gt; * Created by sylvanasp on 2016/9/4. */public class HeapOOM &#123; static class OOMObject &#123; &#125; public static void main(String[] args) &#123; List&lt;OOMObject&gt; list = new ArrayList&lt;OOMObject&gt;(); while (true) &#123; list.add(new OOMObject()); &#125; &#125;&#125; 虚拟机栈溢出1234567891011121314151617181920212223242526272829303132333435363738/** * 在单线程中,以下两种方法均无法让虚拟机产生OutOfMemoryError异常,尝试的结果均为StackOverflowError异常. * * 1.使用-Xss参数减少栈内存容量.结果为StackOverflowError异常,出现时输出的堆栈深度相应缩小. * * VM Args: -Xss128k * * Created by sylvanasp on 2016/9/4. */public class JavaVMStackSOF &#123; private int stackLength = 1; /** * 定义大量的本地变量,增大此方法帧中本地变量表的长度. * 结果:抛出StackOverflowError异常时输出的堆栈深度相应缩小. */ public void stackLeak() &#123; stackLength++; stackLeak(); &#125; public static void main(String[] args) throws Throwable &#123; /** * 在单线程下,无论是栈帧太大还是虚拟机栈容量太小,当内存无法分配时, * 虚拟机抛出的都是StackOverflowError异常. */ JavaVMStackSOF javaVMStackSOF = new JavaVMStackSOF(); try &#123; javaVMStackSOF.stackLeak(); &#125; catch (Throwable e) &#123; System.out.println("stack length:" + javaVMStackSOF.stackLength); throw e; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940/** * 通过不断建立线程的方式产生内存溢出异常. * 但是这样产生的内存溢出异常与栈空间是否足够大并不存在任何联系. * 在这种情况下,为每个线程的栈分配的内存越大,反而越容易产生内存溢出异常. * 因为内存最后才由虚拟机栈和本地方法栈"瓜分". * 所以每个线程分配到的栈容量越大,可以建立的线程数量自然就越少,建立线程时就越容易把剩下的内存耗尽。 * * 如果是建立过多线程导致的内存溢出,在不能减少线程数或者更换64位虚拟机的情况下. * 就只能通过减少最大堆和减少栈容量来换取更多的线程. * * VM Args: -Xss2M * &lt;p&gt; * Created by sylvanasp on 2016/9/4. */public class JavaVMStackOOM &#123; private void dontStop() &#123; while (true) &#123; &#125; &#125; public void stackLeakByThread() &#123; while (true) &#123; Thread thread = new Thread(new Runnable() &#123; public void run() &#123; dontStop(); &#125; &#125;); thread.start(); &#125; &#125; public static void main(String[] args) &#123; JavaVMStackOOM javaVMStackOOM = new JavaVMStackOOM(); javaVMStackOOM.stackLeakByThread(); &#125;&#125; 方法区溢出123456789101112131415161718192021222324252627282930313233/** * 方法区内存溢出 * 方法区是用于存放Class的相关信息的,所以要让方法区产生内存溢出的基本思路为: * 在运行时产生大量的类去填满方法区,直到溢出. * 所以可以使用CGLib直接操作字节码运行时生成大量的动态类. * * VM Args : -XX:PermSize=10M -XX:MaxPermSize=10M * * Created by sylvanasp on 2016/9/4. */public class JavaMethodAreaOOM &#123; static class OOMObject &#123; &#125; public static void main(final String[] args) &#123; while (true) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(OOMObject.class); enhancer.setUseCache(false); enhancer.setCallback(new MethodInterceptor() &#123; public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; return methodProxy.invokeSuper(o,objects); &#125; &#125;); enhancer.create(); &#125; &#125;&#125; 运行时常量池溢出123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * 方法区和运行时常量池溢出 * 在JDK1.6及之前的版本中,由于常量池分配在永久代内. * 所以可以通过 -XX:PermSize和-XX:MaxPermSize限制方法区大小. * 从而间接限制其中常量池的容量. * * 而使用JDK1.7运行这段程序则不会得到相同的结果,while循环将一直进行下去. * * VM Args : -XX:PermSize=10M -XX:MaxPermSize=10M * * Created by sylvanasp on 2016/9/4. */public class RuntimeConstantPoolOOM &#123; public static void main(String[] args) &#123; // 使用List保持常量池的引用,避免Full GC回收常量池行为. List&lt;String&gt; list = new ArrayList&lt;String&gt;(); int i = 0; while (true) &#123; /** * String.intern()是一个Native方法. * 它的作用是: * 如果字符串常量池中已经包含了一个等于此String对象的字符串,则返回代表池中这个字符串的String对象. * 否则,将此String对象包含的字符串添加到常量池中,并且返回此String对象的引用. */ list.add(String.valueOf(i++).intern()); &#125; &#125; /** * 这段代码在JDK1.6中运行,会得到两个false,而在JDK1.7中运行,会得到一个true和一个false. * 产生差异的原因为: * 在JDK1.6中,intern()方法会把首次遇到的字符串实例复制到永久代中,返回的也是永久代中这个字符串实例的引用. * 而由StringBuilder创建的字符串实例在Java堆上,所以必然不是同一个引用.所以返回false. * * 在JDK1.7中(或部分其他虚拟机,如JRockit),intern()实现不会再复制实例. * 只是在常量池中记录首次出现的实例引用,因此intern()返回的引用和由StringBuilder创建的字符串实例为同一个. * 对str2比较返回false是因为"java"这个字符串在执行StringBuilder.toString()之前已经出现过, * 字符串常量池中已经有它的引用了,不符合"首次出现"的原则. * */ private void stringPool() &#123; String str1 = new StringBuilder("Hello").append("World哈哈").toString(); System.out.println(str1.intern() == str1); String str2 = new StringBuilder("ja").append("va").toString(); System.out.println(str2.intern() == str2); &#125;&#125; 直接内存溢出123456789101112131415161718192021222324252627282930313233343536/** * 直接内存溢出 * DirectMemory容量可以通过 -XX:MaxDirectMemorySize指定. * 如果不指定,则默认与Java堆最大值(-Xmx)一致. * * VM Args : -Xmx20M -XX:MaxDirectMemorySize=10M * * Created by sylvanasp on 2016/9/4. */public class DirectMemoryOOM &#123; private static final int _1MB = 1024 * 1024; /** * 越过DirectByteBuffer类,直接通过反射获取Unsafe实例进行内存分配. * Unsafe.getUnsafe()方法限制了只有引导类加载器才会返回实例, * 即只有rt.jar中的类才能使用Unsafe的功能. * * 使用DirectByteBuffer类分配内存虽然也会抛出内存溢出的异常, * 但它抛出异常时并没有真正的向操作系统申请分配内存,而是通过计算得知内存无法分配,于是手动抛出异常. * * 而unsafe.allocateMemory()则可以真正申请分配内存. * */ public static void main(String[] args) throws IllegalAccessException &#123; Field unsafeField = Unsafe.class.getDeclaredFields()[0]; unsafeField.setAccessible(true); Unsafe unsafe = (Unsafe) unsafeField.get(null); while (true) &#123; unsafe.allocateMemory(_1MB); &#125; &#125;&#125; End 资料参考于 &lt;&lt;深入理解 Java虚拟机 第二版&gt;&gt;.]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在CentOS7下手动编译JDK]]></title>
    <url>%2F2016%2F09%2F03%2F2016-09-3-CompilerOpenJDK%2F</url>
    <content type="text"><![CDATA[下载OpenJDK源码&nbsp;&nbsp;oepnjdk下载地址为 https://jdk7.java.net/source.html 安装编译依赖环境 安装alsa包. 1yum install alsa-lib-devel 安装cups-devel 1yum install cups-devel 安装X相关库 1yum install libX* 安装gcc-c++ 1yum install gcc gcc-c++ 安装freetype 12rpm -ivh freetype-2.4.11-9.el7.x86_64.rpm下载地址:http://rpm.pbone.net/index.php3/stat/4/idpl/26641422/dir/centos_7/com/freetype-2.4.11-9.el7.x86_64.rpm.html 安装Ant 12tar -zvxf apache-ant-1.9.6-bin.tar.gz下载地址:http://ant.apache.org/bindownload.cgi &nbsp;&nbsp;也可以使用以下命令一次性完成依赖安装: 12345yum -y install build-essential gawk m4openjkd-6-jkd libasound2-dev libcups2-dev libxrender-dev xorg-dev xutils-devxllproto-print-dev binutils libmotif3libmotif-dev ant 配置环境变量&nbsp;&nbsp;OpenJDK在编译时需要读取很多环境变量,但大部分都是有默认值的,必须设置的只有两个环境变量: LANG和ALT_BOOTDIR. &nbsp;&nbsp;LANG是用来设定语言选项的,必须设置为: export LANG=C. &nbsp;&nbsp;ALT_BOOTDIR则是用来设置Bootstrap JDK的位置,Bootstrap JDK就是一个可用的6u14以上版本的JDK,它是用来编译OpenJDK中部分Java实现的代码的. &nbsp;&nbsp;另外,如果之前设置了JAVA_HOME和CLASSPATH两个环境变量,在编译之前必须取消,否则在Makefile脚本中检查到有这两个变量存在,会有警告提示. 12unset JAVA_HOMEunset CLASSPATH &nbsp;&nbsp;其他环境变量如下: 12345678910111213141516171819202122232425262728293031323334353637383940#语言选项，这个必须设置，否则编译好后会出现一个HashTable的NPE错export LANG=C#Bootstrap JDK的安装路径。必须设置。 export ALT_BOOTDIR=/Library/Java/JavaVirtualMachines/jdk1.7.0_04.jdk/Contents/Home#允许自动下载依赖export ALLOW_DOWNLOADS=true#并行编译的线程数，设置为和CPU内核数量一致即可export HOTSPOT_BUILD_JOBS=6export ALT_PARALLEL_COMPILE_JOBS=6#比较本次build出来的映像与先前版本的差异。这个对我们来说没有意义，必须设置为false，否则sanity检查会报缺少先前版本JDK的映像。如果有设置dev或者DEV_ONLY=true的话这个不显式设置也行。 export SKIP_COMPARE_IMAGES=true#使用预编译头文件，不加这个编译会更慢一些export USE_PRECOMPILED_HEADER=true#要编译的内容export BUILD_LANGTOOLS=true #export BUILD_JAXP=false#export BUILD_JAXWS=false #export BUILD_CORBA=falseexport BUILD_HOTSPOT=true export BUILD_JDK=true#要编译的版本#export SKIP_DEBUG_BUILD=false#export SKIP_FASTDEBUG_BUILD=true#export DEBUG_NAME=debug#把它设置为false可以避开javaws和浏览器Java插件之类的部分的build。 BUILD_DEPLOY=false#把它设置为false就不会build出安装包。因为安装包里有些奇怪的依赖，但即便不build出它也已经能得到完整的JDK映像，所以还是别build它好了。BUILD_INSTALL=false#编译结果所存放的路径export ALT_OUTPUTDIR=/Users/IcyFenix/Develop/JVM/jdkBuild/openjdk_7u4/build 编译JDK &nbsp;&nbsp;全部设置好之后,可用在OpenJDK目录下使用 make sanity来检查设置是否正确,如果正确则会会输出：Sanity check passed. &nbsp;&nbsp;之后,输入make命令开始编译,(make不添加参数默认为编译make all). &nbsp;&nbsp;编译完成后,可以将目录复制到JAVA_HOME中,作为一个完整的JDK使用,编译出来的虚拟机,在-version命令中带有用户的机器名. &nbsp;&nbsp;如果只想单独编译HotSpot的话，那么使用hotspot/make目录下的MakeFile进行替换即可，其他参数与前面一致，这时候虚拟机的输出结果存放在build/hotspot/outputdir/linux_amd64_compiler2 中，里面对应了不同的优化级别的目录. &nbsp;&nbsp;在不同机器上,最后一个目录名称会有所差别,bad表示Mac OS系统(内核为FreeBSD),amd64表示是64位JDK(32位为x86),compiler2表示是Server VM(Client VM表示是compiler1). &nbsp;&nbsp;在运行虚拟机前，还要手工编辑目录下的env.sh文件，这个文件由编译脚本自动产生，用于设置虚拟机的环境变量，里面已经发布了”JAVA_HOME，CLASSPATH，HOTSPOT_BUILD_USER” 3个环境变量，还需要增加一个“LD_LIBRARY_CLASSPATH”,内容如下: - 12LD_LIBRARY_PATH=.:$&#123;JAVA_HOME&#125;/jre/lib/amd64/native_threads:$&#123;JAVA_HOME&#125;/jre/lib/amd64:export LD_LIBRARY_PATH &nbsp;&nbsp;然后执行以下命令启动虚拟机(这时的启动机器名为gamma). - 12. ./env.sh./gamma -version #有可能是test_gamma,这是自带的一段八皇后代码 Exception 报错: 1Error: time is more than 10 years from present: 1104530400000 when building java/openjdk* lists.freebsd.org 通过修改CurrencyData.properties文件, 把10年之前的时间修改为10年之内即可 Index: /usr/openjdk/jdk/src/share/classes/java/util/CurrencyData.properties把2006改掉就可以重新编译了. End 资料参考于 &lt;&lt;深入理解 Java虚拟机 第二版&gt;&gt;.]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot 部署测试与应用监控]]></title>
    <url>%2F2016%2F08%2F05%2F2016-08-5-Spring-monitor%2F</url>
    <content type="text"><![CDATA[Spring Boot部署 Jar包形式 &nbsp;&nbsp;如果在创建Spring Boot项目的时候选择的是jar包形式,则只需要使用maven将项目打成jar包即可. 1mvn pakage &nbsp;&nbsp;Spring Boot项目在打成jar后可以直接运行. War包形式 &nbsp;&nbsp;如果在创建Spring Boot项目的时候选择的是war包形式,则打包的方式与jar一样. 1mvn pakage &nbsp;&nbsp;之后将生成的war文件放在任意Servlet容器上运行即可. &nbsp;&nbsp;如果在创建Spring Boot项目的时候选择的是jar,在部署时又想以war包形式部署,则需要以下配置. 修改pom.xml文件的打包方式 添加以下依赖覆盖默认内嵌的Tomcat. 创建ServletInitializer. 注册为Linux服务 &nbsp;&nbsp;将软件注册为服务,可以通过命令开启、关闭、开机自启动等功能. &nbsp;&nbsp;如想注册为Linux服务,则先需要修改spring-boot-maven-plugin的配置: 1234567891011 &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;executable&gt;true&lt;/executable&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; &nbsp;&nbsp;主流的Linux大都使用init.d或systemd来注册服务. init.d方式 sudo ln -s /var/apps/spring-boot-demo.jar /etc/init.d/sdemo,其中sdemo就是服务名. 常用操作命令 1234service sdemo start 启动服务.service sdemo stop 停止服务.service sdemo status 服务状态.chkconfig sdemo on 开机启动. 项目日志存放于/var/log/sdemo.log中. systemd方式 在/etc/systemd/system/目录下新建文件sdemo.service.并写入以下内容: 1234567891011[Unit]Description=spring-boot-demoAfter=syslog.target[Service]ExecStart= /usr/bin/java -jar /var/apps/spring-boot-demo.jar[Install]WantedBy=multi-user.target#其中Description和ExecStart是可更改的.ExecStart是指定java运行需要运行的jar包. 常用操作命令12345systemctl start sdemo 启动服务.systemctl stop sdemo 停止服务.systemctl status sdemo 服务状态.systemctl enable sdemo 开机启动.journalctl -u sdemo 查看项目日志. 热部署 &nbsp;&nbsp;热部署即就是在应用正在运行的时候升级软件，却不需要重新启动应用. &nbsp;&nbsp;在Spring Boot项目中添加spring-boot-devtools依赖即可实现页面和类的热部署. 模板引擎热部署 &nbsp;&nbsp;Spring Boot默认模板引擎开启缓存,如果我们修改了页面后再刷新页面将得不到修改后的内容,我们可以在application.properties中关闭模板引擎的缓存. 12345678#Thymeleafspring.thymeleaf.cache=false#FreeMarkerspring.freemarker.cache=false#Groovyspring.groovy.template.cache=false#Velocityspring.velocity.cache=false 基于Docker部署 &nbsp;&nbsp;目前主流的PaaS平台都支持发布Docker镜像.而Docker镜像是使用Dockerfile文件来编译镜像的. Dockerfile FROM指令 FROM指令指明了当前镜像继承的基镜像.编译当前镜像时会自动下载基镜像. 1FROM java:8 MAINTAINER指令 MAINTAINER指令指明了当前镜像的作者 1MAINTAINER sun RUN指令 RUN指令可以在当前镜像上执行Linux命令并形成一个新的层.RUN指令是编译时(build)的动作. 1RUN /bin/bash -c &quot;echo helloworld&quot; CMD指令 CMD指令指明镜像启动时的默认行为.一个Dockerfile里只能有一个CMD指令.CMD指令设定的命令可以在运行镜像时使用参数覆盖.它是运行时(run)的动作. 12CMD echo &quot;hello&quot;可被 docker run -d image_name echo &quot;world&quot; 覆盖. EXPOSE指令 EXPOSE指令指明了镜像运行时的容器必须监听指定的端口. 1EXPOSE 8080 EVN指令 EVN指令用于设置环境变量. 1ENV myName=sun ADD指令 ADD指令用于从当前工作目录复制文件到镜像目录. 1ADD hello.sh /dir/ ENTRYPOINT指令 ENTRYPOINT指令可以让容器像一个可执行程序一样运行,这样镜像运行时可以像软件一样接收参数执行,ENTRYPOINT是运行时(run)的动作. 123ENTRYPOINT [&quot;/bin/echo&quot;]我们可以镜像传递参数:docker run -d image_name &quot;hello world&quot; Example 首先将打好包的demo上传到Linux服务器. 在项目同级目录下新建一个Dockerfile文件,如下: 123456789FROM java:8MAINTAINER=sunADD spring-boot-demo.jar app.jarEXPOSE 8080ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;] 编译镜像 1234在/var/apps/sdemo目录下执行以下命令docker build -t sun/sdemo .使用sun/sdemo作为镜像名称,sun为前缀. .是用来指明Dockerfile路径的,这里因为在当前目录下所以使用“.”. 运行镜像1docker run -d -p 8080:8080 --name sdemo sun/sdemo Spring Boot集成测试 &nbsp;&nbsp;Spring Boot的测试与Spring MVC类似,它提供了一个@SpringApplicationConfiguration替代@ContextConfiguration来指定配置类. Example Entity 123456789@Entitypublic class Person &#123; @Id @GeneratedValue private Long id; private String name; ...&#125; Dao 123public interface PersonRepositroy extends JpaRepository&lt;Person,Long&gt; &#123;&#125; Controller 12345678910111213@RestController@RequestMapping("/person")public class PersonController &#123; @Autowired PersonRepositroy personRepositroy; @RequestMapping(method = RequestMethod.GET,produces = &#123;MediaType.APPLICATION_JSON_VALUE&#125;) public List&lt;Person&gt; findAll() &#123; return personRepositroy.findAll(); &#125;&#125; Test 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354@RunWith(SpringJUnit4ClassRunner.class)//@ContextConfiguration(classes = &#123;SpringBootTestApplication.class&#125;)@SpringApplicationConfiguration(classes = SpringBootTestApplication.class)@WebAppConfiguration@Transactionalpublic class PersonApplicationTest &#123; @Autowired PersonRepositroy personRepositroy; MockMvc mockMvc; @Autowired WebApplicationContext webApplicationContext; String expectedJson; /** * 初始化 */ @Before public void setUp() throws JsonProcessingException &#123; Person p1 = new Person("sun"); Person p2 = new Person("sylvanas"); personRepositroy.save(p1); personRepositroy.save(p2); // 获得期待返回的JSON字符串. expectedJson = Obj2Json(personRepositroy.findAll()); // 初始化MockMvc. mockMvc = MockMvcBuilders.webAppContextSetup(webApplicationContext).build(); &#125; protected String Obj2Json(Object obj) throws JsonProcessingException &#123; ObjectMapper objectMapper = new ObjectMapper(); return objectMapper.writeValueAsString(obj); &#125; @Test public void testPersonController() throws Exception &#123; String url = "/person"; // 对/person发送请求,获得请求的执行结果. MvcResult result = mockMvc.perform(MockMvcRequestBuilders.get(url) .accept(MediaType.APPLICATION_JSON_VALUE)) .andReturn(); // 获得执行结果的状态 int status = result.getResponse().getStatus(); // 获得执行结果的内容 String content = result.getResponse().getContentAsString(); // 断言 Assert.assertEquals("错误:正确的返回值为200", 200, status); Assert.assertEquals("错误:返回值和预期返回值不一致", expectedJson, content); &#125;&#125; Spring Boot应用监控 &nbsp;&nbsp;Spring Boot提供了运行时的应用监控和管理的功能.我们可以通过HTTP、JMX、SSH来进行应用的监控. HTTP &nbsp;&nbsp;使用HTTP实现对应用的监控需要添加以下依赖: 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; &nbsp;&nbsp;端点: Endpoint Description actuator 所有EndPoint的列表,需要加入spring HATEOAS支持 autoconfig 当前应用的所有自动配置 beans 当前应用的所有Bean信息 configprops 当前应用中所有的配置信息 dump 显示当前应用线程状态信息 env 显示当前应用的环境信息 health 显示当前应用的健康情况 info 显示当前应用信息 metrics 显示当前应用的各项指标信息 mappings 显示所有的@RequestMapping映射的路径 shutdown 关闭当前应用(默认此项是关闭的) trace 默认最新的http请求 &nbsp;&nbsp;使用http访问应用监控只需要在url中加上/端点名即可. 例如: http://localhost:8080/actuator &nbsp;&nbsp;端点的开启关闭可以在application.properties中配置. &nbsp;&nbsp;如果想自定义端点,则需要一个继承AbstractEndpoint的实现类,并注册成Bean即可. &nbsp;&nbsp;如果想自定义HealthIndicator,则需要一个实现HealthIndicator接口的类,并注册成Bean即可. JMX &nbsp;&nbsp;可以使用Java内置的jconsole来实现JMX监控. SSH &nbsp;&nbsp;Spring Boot借助CraSH (http://www.crashub.org), 实现通过SSH或者TELNET监控和管理应用.我们只需要引入依赖spring-boot-starter-remote-shell即可. &nbsp;&nbsp;Spring Boot在spring-boot-starter-remote-shell的commands包下定制了命令,它使用的是Groovy语言来编写的.Groovy语言是由Spring主导的运行于JVM的动态语言. 自定义登录用户 &nbsp;&nbsp;Spring Boot支持在application.properties中自定义用户的账号密码.12shell.auth.simple.user.name=sunshell.auth.simple.user.password=sun end 资料参考于 JavaEE开发的颠覆者: Spring Boot实战]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Batch数据批处理]]></title>
    <url>%2F2016%2F08%2F04%2F2016-08-4-Spring-batch%2F</url>
    <content type="text"><![CDATA[概述 &nbsp;&nbsp;Spring Batch是一个轻量级的处理大量数据操作的框架,主要用于读取大量数据,然后进行一定处理后输出成指定的格式. &nbsp;&nbsp;Spring Batch 提供了大量可重用的组件,包括了日志、追踪、事务、任务作业统计、任务重启、跳过、重复、资源管理。对于大数据量和高性能的批处理任务,Spring Batch 同样提供了高级功能和特性来支持,比如分区功能、远程功能。总之,通过 Spring Batch 能够支持简单的、复杂的和大数据量的批处理作业. &nbsp;&nbsp;Spring Batch 是一个批处理应用框架,不是调度框架,但需要和调度框架合作来构建完成的批处理任务。它只关注批处理任务相关的问题,如事务、并发、监控、执行等,并不ﰁ供相应的调度功能。如果需要使用调用框架,在商业软件和开源软件中已经有很多优秀的企业级调度框架(如 Quartz、Tivoli、Control-M、Cron 等)可以使用. 应用场景 周期性的提交处理. 并行处理任务. 消息驱动应用分级处理. 手工或调度使任务失败之后重新启动. 有依赖步骤的顺序执行(使用工作流驱动扩展). 处理时跳过部分记录. 成批事务：为小批量的或有的存储过程/脚本的场景使用. 组成结构 Name Description Job 实际要执行的任务,包含一个或多个Step JobRepository 用于注册Job的容器 JobLauncher 用于启动Job的接口 Step Step步骤包含ItemReader,ItemProcessor,ItemWriter ItemReader 用于读取数据的接口 ItemProcessor 用于处理数据的接口 ItemWriter 用于输出数据的接口 &nbsp;&nbsp;以上组件只需要注册成Spring的Bean即可,并在配置类上使用@EnableBatchProcessing开启批处理的支持. Job Listener &nbsp;&nbsp;如果想要监听Job的执行情况,则需要定义一个实现了JobExecutionListener的类,并在定义Job的Bean上绑定该监听器. 数据处理和校验 &nbsp;&nbsp;数据的处理和校验都要通过ItemProcessor接口实现来完成的. 数据处理 &nbsp;&nbsp;实现ItemProcessor接口,并重写process方法,即可对数据进行处理.输入参数是从ItemReader读取到的数据,返回给ItemWriter. 数据校验 &nbsp;&nbsp;数据校验可以使用JSR-303的注解来校验ItemReader读取的数据是否符合要求. 参数后置绑定 &nbsp;&nbsp;实现参数后置绑定可以在JobParameters中绑定参数,在Bean定义的时候使用一个特殊的Bean生命周期注解@StepScope,然后通过@Value注入此参数. Spring Boot支持 &nbsp;&nbsp;Spring Boot自动初始化了Spring Batch存储批处理的数据库,并当程序启动时,会自动执行Job. &nbsp;&nbsp;Spring Boot使用以spring.batch为前缀的属性进行相关配置. 12345spring.batch.job.name=job1,job2 #启动时要执行的job,默认执行全部job.spring.batch.job.enabled=true #是否自动执行定义的job,默认为truespring.batch.initializer.enabled=true #是否初始化Spring Batch的数据库,默认为true.spring.batch.schema=spring.batch.table-prefix= #设置Spring Batch的数据库表的前缀. &nbsp;&nbsp;Spring Batch默认自动加载hsqldb驱动,如要使用其他数据库驱动则需要手动去除. Config Example 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899/** * 定义ItemProcessor */@Beanpublic ItemProcessor&lt;Person, Person&gt; processor() &#123; // 使用自定义的ItemProcessor CsvItemProcessor processor = new CsvItemProcessor(); // 指定校验器 processor.setValidator(csvBeanValidator()); return processor;&#125;/** * 定义Validator */@Beanpublic Validator&lt;Person&gt; csvBeanValidator() &#123; return new CsvBeanValidator&lt;Person&gt;();&#125;/** * 定义ItemWriter */@Beanpublic ItemWriter&lt;Person&gt; writer(DataSource dataSource) &#123; JdbcBatchItemWriter&lt;Person&gt; writer = new JdbcBatchItemWriter&lt;&gt;(); writer.setItemSqlParameterSourceProvider (new BeanPropertyItemSqlParameterSourceProvider&lt;Person&gt;()); // 设置要执行批处理的sql语句 String sql = "insert into person " + "(id,name,age,nation,address)" + "values(hibernate_sequence.nextval,:name,:age,:nation,:address)"; writer.setSql(sql); writer.setDataSource(dataSource); return writer;&#125;/** * 定义JobRepository */@Beanpublic JobRepository jobRepository(DataSource dataSource, PlatformTransactionManager transactionManager) throws Exception &#123; JobRepositoryFactoryBean jobRepositoryFactoryBean = new JobRepositoryFactoryBean(); jobRepositoryFactoryBean.setDataSource(dataSource); jobRepositoryFactoryBean.setTransactionManager(transactionManager); jobRepositoryFactoryBean.setDatabaseType("mysql"); return jobRepositoryFactoryBean.getObject();&#125;/** * 定义JobLauncher */@Beanpublic JobLauncher jobLauncher(DataSource dataSource, PlatformTransactionManager transactionManager) throws Exception &#123; SimpleJobLauncher jobLauncher = new SimpleJobLauncher(); jobLauncher.setJobRepository(jobRepository(dataSource, transactionManager)); return jobLauncher;&#125;/** * 定义Job */@Beanpublic Job importJob(JobBuilderFactory jobs, Step s1) &#123; return jobs.get("importJob") .incrementer(new RunIdIncrementer()) .flow(s1) // 为job指定step .end() .listener(csvJobListener()) // 绑定监听器 .build();&#125;/** * 定义Step */@Beanpublic Step step1(StepBuilderFactory stepBuilderFactory, ItemReader&lt;Person&gt; reader, ItemWriter&lt;Person&gt; writer, ItemProcessor&lt;Person, Person&gt; processor) &#123; return stepBuilderFactory .get("step1") .&lt;Person, Person&gt;chunk(65000) // 批处理每次提交65000条数据. .reader(reader) .processor(processor) .writer(writer) .build();&#125;/** * 定义自定义的Job监听器 */@Beanpublic CsvJobListener csvJobListener() &#123; return new CsvJobListener();&#125; end 资料参考于 JavaEE开发的颠覆者: Spring Boot实战]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot消息通信]]></title>
    <url>%2F2016%2F08%2F03%2F2016-08-3-Spring-message%2F</url>
    <content type="text"><![CDATA[SSL安全套接层 概述 &nbsp;&nbsp;SSL(Secure Sockets Layer)是专门用于网络通信安全及数据完整性的安全协议,它会在网络传输层对网络连接进行加密. &nbsp;&nbsp;SSL协议是位于TCP/IP协议与其他各种应用层协议之间的. &nbsp;&nbsp;SSL的体系结构中包含两个协议子层: SSL记录协议(SSL Record Protocol),它建立在可靠的传输协议(TCP)之上,为高层协议提供数据封装、压缩、加密等基本功能的支持.SSL纪录协议针对HTTP协议进行了特别的设计，使得超文本的传输协议HTTP能够在SSL运行. SSL握手协议(SSL Handshake Protocol),它建立在SSL记录协议之上.SSL握手协议层包括SSL握手协议（SSL HandShake Protocol）、SSL密码参数修改协议（SSL Change Cipher Spec Protocol）、应用数据协议（Application Data Protocol）和SSL告警协议（SSL Alert Protocol）.它主要用于在实际数据传输开始前,通信双方进行身份认证、协商加密算法、交换加密密钥等. &nbsp;&nbsp;在B/S应用中,是通过HTTPS实现SSL的,HTTPS即是在HTTP下加入SSL层,它的安全基础是SSL. 工作流程 客户端向服务器发送一个开始信息“Hello”以便开始一个新的会话连接. 服务器根据客户的信息确定是否需要生成新的主密钥，如需要则服务器在响应客户的“Hello”信息时将包含生成主密钥所需的信息. 客户根据收到的服务器响应信息，产生一个主密钥，并用服务器的公开密钥加密后传给服务器. 服务器回复该主密钥，并返回给客户一个用主密钥认证的信息，以此让客户认证服务器. Spring Boot中配置SSL &nbsp;&nbsp;在配置SSL之前需要生成一个证书,它可以是自签名的,也可以是从SSL证书授权中心获得的. 生成SSL证书&nbsp;&nbsp;可以在控制台中输入以下指令,生成自签名的SSL证书: 1keytool -genkey -alias tomcat &nbsp;&nbsp;之后会在当前目录生成一个.keystore文件,它就是SSL证书文件. 配置到项目中 将.keystore文件复制到src/main/resources/static下. 在application.properties中如下配置:12345server.port = 8090server.ssl.key-store = .keystoreserver.ssl.key-store-password = 123456server.ssl.keyStoreType = JKSserver.ssl.keyAlias : tomcat http自动转向https &nbsp;&nbsp;如果想要实现输入http自动转向到https,则需要配置TomcatEmbeddedServletContainerFactory,并添加Tomcat的connector来实现. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package cn.sun.sylvanas.spring_boot_security;import org.apache.catalina.Context;import org.apache.catalina.connector.Connector;import org.apache.tomcat.util.descriptor.web.SecurityCollection;import org.apache.tomcat.util.descriptor.web.SecurityConstraint;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.context.embedded.EmbeddedServletContainerFactory;import org.springframework.boot.context.embedded.tomcat.TomcatEmbeddedServletContainerFactory;import org.springframework.context.annotation.Bean;@SpringBootApplicationpublic class SpringBootSecurityApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootSecurityApplication.class, args); &#125; @Bean public EmbeddedServletContainerFactory servletContainerFactory() &#123; TomcatEmbeddedServletContainerFactory tomcat = new TomcatEmbeddedServletContainerFactory()&#123; @Override protected void postProcessContext(Context context) &#123; SecurityConstraint securityConstraint = new SecurityConstraint(); securityConstraint.setUserConstraint("CONFIDENTIAL"); SecurityCollection securityCollection = new SecurityCollection(); securityCollection.addPattern("/"); securityConstraint.addCollection(securityCollection); context.addConstraint(securityConstraint); &#125; &#125;; tomcat.addAdditionalTomcatConnectors(httpConnector()); return tomcat; &#125; @Bean public Connector httpConnector() &#123; Connector connector = new Connector("org.apache.coyote.http11.Http11NioProtocol"); connector.setScheme("http"); connector.setPort(8080); connector.setSecure(false); connector.setRedirectPort(8090); return connector; &#125;&#125; &nbsp;&nbsp;此时访问http://localhost:8080会自动转向到https://localhost:8090. WebSocket 概述 &nbsp;&nbsp;WebSocket protocol 是HTML5一种新的协议.它实现了浏览器与服务器全双工异步通信(full-duplex).即浏览器可以向服务端发送消息,服务端也可以向浏览器发送消息.WebSocket需要浏览器的支持,如IE 10+、Chrome 13+、Firefox 6+. &nbsp;&nbsp;WebSocket是通过一个socket来实现双工异步通信功能的.直接使用WebSocket协议开发程序会很繁琐,所以一般使用它的子协议STOMP,STOMP是一个更高级别的协议,它使用基于帧(frame)的格式来定义消息,具有一个类似@RequestMapping的@MessageMapping. Spring Boot支持 &nbsp;&nbsp;Spring Boot对内嵌的Tomcat(7、8)、Jetty9、Undertow提供了WebSocket支持.依赖为spring-boot-starter-websocket. &nbsp;&nbsp;配置WebSocket需要在配置类上使用@EnableWebSocketMessageBroker注解开启支持,并且继承AbstractWebSocketMessageBrokerConfigurer类,重写其方法进行配置. Example 广播&nbsp;&nbsp;广播即服务端有消息时,会将消息发送给所有连接了当前endpoint的浏览器. 配置 Controller 12345678910/** * @MessageMapping注解映射/hello这个地址,类似于@RequestMapping * 当服务端有消息时,会对订阅了@SendTo中的路径的浏览器发送消息. */ @MessageMapping("/hello") @SendTo("/topic/getHello") public String hello(String name) throws Exception &#123; Thread.sleep(3000); return "Hello " + name + "!"; &#125; js1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071 &lt;html xmlns:th="http://www.thymeleaf.org"&gt;&lt;head&gt; &lt;meta content="text/html;charset=UTF-8"/&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"/&gt; &lt;meta name="viewport" content="width=device-width,initial-scale=1"/&gt; &lt;title&gt;Spring Boot+WebSocket+广播式&lt;/title&gt;&lt;/head&gt;&lt;body onload="disconnect()"&gt;&lt;noscript&gt;&lt;h2 style="color: #ff0000"&gt;貌似你的浏览器不支持websocket&lt;/h2&gt;&lt;/noscript&gt;&lt;div&gt; &lt;div&gt; &lt;button id="connect" onclick="connect();"&gt;连接&lt;/button&gt; &lt;button id="disconnect" onclick="disconnect();"&gt;断开连接&lt;/button&gt; &lt;/div&gt; &lt;div id="conversationDiv"&gt; &lt;label&gt;输入你的名字&lt;/label&gt;&lt;input type="text" id="name"/&gt; &lt;button id="sendName" onclick="sendName();"&gt;发送&lt;/button&gt; &lt;p id="response"&gt;&lt;/p&gt; &lt;/div&gt;&lt;/div&gt;&lt;!-- 导入jQuery --&gt;&lt;script th:src="@&#123;jquery-3.1.0.min.js&#125;"/&gt;&lt;script th:src="@&#123;sockjs-1.0.0.min.js&#125;"/&gt;&lt;script th:src="@&#123;stomp.min.js&#125;"/&gt;&lt;script type="text/javascript"&gt; var stompClient = null; function setConnected(connected) &#123; document.getElementById('connect').disabled = connected; document.getElementById('disconnect').disabled = !connected; document.getElementById('conversationDiv').style.visibility = connected ? 'visible' : 'hidden'; $('#response').html(); &#125; function connect() &#123; var socket = new SockJS('/endpointSun'); stompClient = Stomp.over(socket); stompClient.connect(&#123;&#125;, function (frame) &#123; setConnected(true); console.log('Connected: ' + frame); // 订阅/topic/getHello stompClient.subscribe('/topic/getHello',function(response)&#123; showResponse(JSON.parse(response.body).responseMessage); &#125;); &#125;); &#125; function disconnect() &#123; if(stompClient != null)&#123; stompClient.disconnect(); &#125; setConnected(false); console.log("Disconnected"); &#125; function sendName() &#123; var name = $('#name').val(); // 发送到/hello stompClient.send("/hello",&#123;&#125;,JSON.stringify(&#123;'name':name&#125;)); &#125; function showResponse(message)&#123; var response = $("#response"); response.html(message); &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; P2P&nbsp;&nbsp;P2P即点对点,它不同于广播式,P2P需要指定消息由谁接收,且只能由一个人接收. 配置与广播式相同. Controller 1234567891011121314151617181920212223/** * 通过SimpMessagingTemplate向浏览器发送消息. */ @Autowired private SimpMessagingTemplate messagingTemplate; /** * Spring MVC可以直接在参数中注入principal对象,它包含当前用户的信息. */ @MessageMapping("/chat") public void handleChat(Principal principal, String msg) &#123; /** * 通过SimpMessagingTemplate.convertAndSendToUser向用户发送消息, * 第一个参数是接收者的用户,第二参数是浏览器订阅的地址,第三个参数是要发送的消息. */ if (principal.getName().equals("sun")) &#123; messagingTemplate.convertAndSendToUser("sylvanas", "/queue/notifications", principal.getName() + "-send:" + msg); &#125; else &#123; messagingTemplate.convertAndSendToUser("sun", "/queue/notifications", principal.getName() + "-send:" + msg); &#125; &#125; js 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 &lt;!DOCTYPE html&gt;&lt;!--suppress ALL --&gt;&lt;html xmlns="http://www.w3.org/1999/xhtml" xmlns:th="http://www.thymeleaf.org" xmlns:sec="http://www.thymeleaf.org/thymeleaf-extras-springsecurity3"&gt;&lt;head&gt; &lt;meta content="text/html;charset=UTF-8"/&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"/&gt; &lt;meta name="viewport" content="width=device-width,initial-scale=1"/&gt; &lt;script th:src="@&#123;jquery-3.1.0.min.js&#125;"/&gt; &lt;script th:src="@&#123;stomp.min.js&#125;"/&gt; &lt;script th:src="@&#123;sockjs-1.0.0.min.js&#125;"/&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt; 聊天室&lt;/p&gt;&lt;form id="chatForm"&gt; &lt;textarea rows="4" cols="60" name="text"&gt;&lt;/textarea&gt; &lt;input type="submit"/&gt;&lt;/form&gt;&lt;script th:inline="javascript"&gt; $('#chatForm').submit(function(e)&#123; e.preventDefault(); var text = $('#chatForm').find('textarea[name="text"]').val(); sendSpittle(text); &#125;); var socket = new SockJS("/endpointChat"); var stomp = Stomp.over(socket); stomp.connect('guest','guest',function(frame)&#123; // 与messagingTemplate.convertAndSendToUser中定义的订阅地址保持一致 // 但前面要多出一个/user,这个/user是必须的,只有使用了/user才会发送消息到指定的用户. stomp.subscribe("/user/queue/notifications",handleNotification); &#125;); function handleNotification(message)&#123; $("#output").append("&lt;b&gt;Received: " + message.body + "&lt;/b&gt;&lt;br/&gt;"); &#125; function sendSpittle(text)&#123; stomp.send("/chat",&#123;&#125;,text); &#125; $('#stop').click(function()&#123;socket.close()&#125;);&lt;/script&gt;&lt;div id="output"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 异步消息队列 &nbsp;&nbsp;异步消息队列主要用于各系统之间的通信与解耦,异步消息即为发送者无需关心消息接收者的处理和返回. &nbsp;&nbsp;异步消息的主要概念为消息代理(message broker)和目的地(destination).消息是由消息代理负责接管并传递到指定目的地的. &nbsp;&nbsp;目的地主要有2种: queue:用于P2P(point-to-point)的消息通信. topic:用于发布/订阅(publish/subscribe)的消息通信. Spring Boot支持 &nbsp;&nbsp;Spring对JMS和AMQP的支持来自于spring-jms、spring-rabbit.它们分别需要ConnectionFactory的实现来连接消息代理.并且提供了JmsTemplate和RabbitTemplate. JMS(Java Message Service)Java消息服务,它是基于JVM消息代理的规范. AMQP(Advanced Message Queuing Protocol)高级消息队列协议,它不仅支持JVM,还支持跨语言和平台.AMQP的主要实现有RabbitMQ. &nbsp;&nbsp;Spring Boot对JMS支持的实有ActiveMQ,HornetQ,Artemis.以ActiveMQ为例: Spring Boot自动配置了ActiveMQConnectionFactory与JmsTemplate. 通过spring.activemq为前缀的属性来配置ActiveMQ相关的属性. Spring Boot还自动开启了@EnableJms,即使用注解式消息监听的支持. &nbsp;&nbsp;Spring Boot对AMQP的实现RabbitMQ自动配置了如下内容: 自动配置了ConnectionFactory和RabbitTemplate. 通过spring.rabbitmq为前缀的属性来配置RabbitMQ相关的属性. 自动开启了@EnableRabbit,即使用注解式消息监听的支持. JMS Example &nbsp;&nbsp;添加依赖 123456789101112131415161718192021222324 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- 嵌入ActiveMQ --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-broker&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; &nbsp;&nbsp;在application.properties中配置activemq的地址. 1spring.activemq.broker-url=tcp://192.168.145.152:61616 &nbsp;&nbsp;定义JMS发送的消息需要实现MessageCreator接口.1234567891011public class Msg implements MessageCreator &#123; /** * 重写createMessage方法. */ @Override public Message createMessage(Session session) throws JMSException &#123; return session.createTextMessage("Hello World"); &#125;&#125; &nbsp;&nbsp;发送消息,定义目的地.123456789101112131415161718192021/** * Spring Boot提供了一个CommandLineRunner接口,用于程序启动后执行的代码 */@SuppressWarnings("SpringJavaAutowiringInspection")@SpringBootApplicationpublic class SpringBootActivemqApplication implements CommandLineRunner &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootActivemqApplication.class, args); &#125; @Autowired private JmsTemplate jmsTemplate; @Override public void run(String... strings) throws Exception &#123; // 向目的地my-destination发送消息 jmsTemplate.send("my-destination",new Msg()); &#125;&#125; &nbsp;&nbsp;接收消息 12345678910111213@Componentpublic class Receiver &#123; /** * @JmsListener是Spring 4.1提供的一个新特性,用来简化JMS开发. * destination指定要监听的目的地. */ @JmsListener(destination = "my-destination") public void receiveMessage(String message) &#123; System.out.println("接收到: &lt;" + message + "&gt;"); &#125;&#125; AMQP Example &nbsp;&nbsp;Spring Boot默认Rabbit主机位localhost,端口号为5672. &nbsp;&nbsp;添加以下依赖:123456789 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; &nbsp;&nbsp;在application.properties中配置RabbitMQ的地址.12spring.rabbitmq.host=192.168.145.152spring.rabbitmq.port=5672 &nbsp;&nbsp;发送消息,定义目的地. 1234567891011121314151617181920212223242526272829@SuppressWarnings("SpringJavaAutowiringInspection")@SpringBootApplicationpublic class SpringBootRabbitmqApplication implements CommandLineRunner &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootRabbitmqApplication.class, args); &#125; @Autowired private RabbitTemplate rabbitTemplate; /** * 定义目的地 */ @Bean public Queue myQueue() &#123; return new Queue("my-queue"); &#125; /** * 使用rabbitTemplate的convertAndSend方法向队列my-queue发送消息. */ @Override public void run(String... strings) throws Exception &#123; rabbitTemplate.convertAndSend("my-queue","Hello RabbitMQ!"); &#125;&#125; &nbsp;&nbsp;接收消息 123456789@Componentpublic class Receiver &#123; @RabbitListener(queues = "my-queue") public void receiveMessage(String message) &#123; System.out.println("Received &lt;" + message + "&gt;"); &#125;&#125; end 资料参考于 JavaEE开发的颠覆者: Spring Boot实战]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security安全控制]]></title>
    <url>%2F2016%2F08%2F02%2F2016-08-2-Spring-security%2F</url>
    <content type="text"><![CDATA[概述 &nbsp;&nbsp;Spring Security是Spring项目的安全框架,基于IoC和AOP来实现安全的功能. &nbsp;&nbsp;Spring Security有两个重要概念: 认证(Authentication),认证即确认用户可以访问当前系统. 授权(Authorization),授权即确定用户在当前系统下所拥有的权限. 配置 &nbsp;&nbsp;Spring Security使用过滤器来实现所有安全的功能,我们只需要注册一个特殊的DelegationFilterProxy过滤器到WebApplicationInitializer即可(WebApplicationInitializer是Spring提供用来配置Servlet3.0+配置的接口,从而替代web.xml,实现此接口的类会自动被SpringServletContainerInitializer加载,启动Servlet容器). &nbsp;&nbsp;也可以让自定义的Initializer类继承AbstractSecurityWebApplicationInitializer抽象类,这个抽象类实现了WebApplicationInitializer接口,并通过onStartup方法调用.它已经注册了DelegationFilterProxy过滤器. Spring Security配置类 &nbsp;&nbsp;只需要在配置类上使用@EnableWebSecurity注解,然后继承WebSecurityConfigurerAdapter.可以通过重写configure方法来配置相关的内容. &nbsp;&nbsp;在Spring Security中,通过重写以下方法放行静态资源: Authentication &nbsp;&nbsp;在Spring Security中,通过重写以下方法来完成认证的配置: &nbsp;&nbsp;认证用户需要用户数据的来源,AuthenticationManagerBuilder提供了内存中和JDBC的两种用户数据来源. 内存中 JDBC &nbsp;&nbsp;如果需要自定义用户数据来源,则可以通过实现UserDetailsService接口. Authorization &nbsp;&nbsp;在Spring Security中,通过重写以下方法来完成授权的配置: &nbsp;&nbsp;Spring Security使用2种匹配器用来匹配请求路径: antMatchers:使用Ant风格的路径匹配. regexMatchers:使用正则表达式匹配路径. &nbsp;&nbsp;Spring Security提供以下方法用于安全处理: Method Description anyRequest() 匹配所有请求路径. access(String) Spring EL表达式结果为true时可以访问. anonymous() 匿名可访问. denyAll() 用户不能访问. fullyAuthenticated() 用户完全认证时可访问. hasAnyAuthority(String…) 如果用户有参数,则其中任一权限可访问. hasAnyRole(String…) 如果用户有参数,则其中任一角色可访问. hasAuthority(String) 如果用户有参数,则其权限可访问. hasIpAddress(String) 如果用户来自参数中的IP则可访问. hasRole(String) 如果用户有参数中的角色可访问. permitAll() 用户可任意访问. rememberMe() 允许通过remember-me登陆的用户访问. authenticated() 用户登录后可访问 自定义登录实现 1234567891011121314151617181920@Override protected void configure(HttpSecurity http) throws Exception &#123; http.authorizeRequests() .anyRequest().authenticated() // 所有请求需要认证登陆后才能访问. .and() .formLogin() // 通过formLogin定制登录操作. .loginPage("/login") // 定制登录页面的访问地址. .defaultSuccessUrl("/index") // 登录成功后转向的页面. .failureUrl("/login?error") // 登录失败后转向的页面. .permitAll() .and() .rememberMe() // 开启cookie存储用户信息. .tokenValiditySeconds(1209600) // 指定cookie的有效时间,单位为秒. .key("userKey") // 指定cookie中的私钥. .and() .logout() // 通过logout()定制注销操作. .logoutUrl("/logout") // 指定注销的URL路径. .logoutSuccessUrl("/index") // 指定注销成功后转向的页面. .permitAll(); &#125; Spring Boot支持 &nbsp;&nbsp;Spring Boot主要通过SecurityAutoConfiguration和SecurityProperties来完成自动配置. &nbsp;&nbsp;SecurityAutoConfiguration导入了SpringBootWebSecurityConfiguration中的配置,我们可以获得如下的自动配置: 自动配置了一个内存中的用户,账号为user,密码在程序启动时print在控制台中. 放行了/css/**,/js/**,/images/**,/**/favicon.ico等静态文件存放的路径. 自动配置的securityFilterChainRegistration的Bean. 使用以security为前缀的属性配置Security相关的配置.1234567security.user.name=#内存中的默认用户账号,默认为user.security.user.password=#默认用户的密码.security.user.role=#默认用户的角色.security.require-ssl=false #是否需要ssl支持,默认为false.security.enable-csrf=false #是否开启“跨站请求伪造”支持,默认false.security.ignored= #用逗号隔开需要放行的路径..... 当我们需要自定义扩展配置的时候,只需要配置类继承WebSecurityConfigurerAdapter即可,不需要使用@EnableWebSecurity注解开启支持. end 资料参考于 JavaEE开发的颠覆者: Spring Boot实战]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data 事务&缓存]]></title>
    <url>%2F2016%2F08%2F01%2F2016-08-1-Spring-data-transaction%2F</url>
    <content type="text"><![CDATA[Spring Transaction &nbsp;&nbsp;Spring的事务机制是用统一的机制来处理不同数据访问计数的事务处理.它提供了一个PlatformTransactionManager接口,不同的数据访问技术使用不同的接口实现. 数据访问技术 接口实现 JDBC DataSourceTransactionManager JPA JpaTransactionManager Hibernate HibernateTransactionManager JDO JdoTransactionManager 分布式事务 JtaTransactionManager 声明式事务 &nbsp;&nbsp;Spring使用@Transactional注解在方法上表明事务支持.被注解的方法在被调用时,会开启一个新的事务,当方法无异常完成提交后.Spring会提交事务. &nbsp;&nbsp;@Transactional注解也可以用在类上,表明这个类下的所有方法都有事务支持.如果类和方法都使用了@Transactional,则使用方法上的注解覆盖类级别注解. &nbsp;&nbsp;@Transactional注解是基于AOP的实现操作. &nbsp;&nbsp;注意:@Transactional注解是由Spring提供的,而不是来自javax.transaction. &nbsp;&nbsp;Spring提供@EnableTransactionManagerment注解在配置类上开启声明式事务,Spring容器会自动扫描带有注解@Transactional的类和方法. Spring Data JPA支持 &nbsp;&nbsp;Spring Data JPA对所有方法默认开启了事务支持,查询类事务默认启用readOnly-true属性. Spring Boot支持 &nbsp;&nbsp;Spring Boot会自动配置事务管理器. 当使用JDBC时,Spring Boot会自动配置DataSourceTransactionManager. 当使用JPA时,Spring Boot会自动配置JpaTransactionManager. &nbsp;&nbsp;在Spring Boot中,不需要显式开启@EnableTransactionManagement注解. Spring Cache &nbsp;&nbsp;Spring提供了CacheManager和Cache接口用来统一各种不同的数据缓存技术. CacheManager是各种缓存技术抽象接口. Cache接口包含各种缓存操作. CacheManager 描述 SimpleCacheManager 使用简单的Collection存储缓存,主要用于测试. NoOpCacheManager 不会实际存储缓存. EhCacheCacheManager 使用EhCache缓存技术. ConcurrentMapCacheManager 使用ConcurrentMap存储缓存. GuavaCacheManager 使用Google Guava的GuavaCache缓存技术. HazelcastCacheManager 使用Hazelcast缓存技术. JCacheCacheManager 支持JCache(JSR-107)规范的实现作为缓存技术. RedisCacheManager 使用Redis作为缓存技术. &nbsp;&nbsp;不管使用什么缓存技术,都需要注册一个该实现的CacheManager的Bean. 声明式缓存 &nbsp;&nbsp;Spring提供了以下注解用来声明式缓存.它与@Transactional注解一样是基于AOP操作的实现. Annotation Description @CachePut 不管什么情况,都会把方法的返回值存入缓存中.@CachePut的属性与@Cacheable保持一致. @Cacheable Spring会先查看缓存中是否存有数据,如果有,则直接返回缓存数据,如果没有,则将调用方法的返回值存入缓存中. @Caching 可以通过@Caching注解组合多个注解策略在一个方法上. @CacheEvict 将一条或多条缓存数据从缓存中删除. &nbsp;&nbsp;开启声明式缓存需要在配置类上使用注解@EnableCaching Example Spring Boot支持 &nbsp;&nbsp;Spring Boot自动配置了CacheManager的各种实现,默认情况下使用的是ConcurrentMapCacheManager.支持以spring.cache为前缀的属性来配置缓存相关的配置. 12345678spring.cache.type= #缓存技术的类型,可选ehcache,guava,simple,none,generic,hazelcast,infinispan,jcache,redis.spring.cache.cache-name=#程序启动时创建缓存名称spring.cache.ehcache.config=#ehcache配置文件的地址.spring.cache.hazelcast.config=#hazelcast配置文件的地址.spring.cache.infinispan.config=#infinispan配置文件的地址.spring.cache.jcache.config=#jcache配置文件的地址.spring.cache.jcache.provider=#当多个jcache实现在类路径中的时候,指定jcache实现.spring.cache.guava.spec=# guava specs &nbsp;&nbsp;使用Spring Boot只需要导入相关缓存技术的依赖,并在配置类使用@EnableCaching注解开启缓存支持即可. end 资料参考于 JavaEE开发的颠覆者: Spring Boot实战]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring数据访问方案-Spring Data]]></title>
    <url>%2F2016%2F07%2F31%2F2016-07-31-Spring-data%2F</url>
    <content type="text"><![CDATA[概述&nbsp;&nbsp;Spring Data是Spring用来解决数据访问的解决方案,它包含了大量关系型数据库以及NoSQL数据库的数据持久层访问解决方案. | Spring Data Commons&nbsp;&nbsp;Spring Data提供了统一的API来对各种数据存储技术进行数据访问操作,这是通过Spring Data Commons来实现的,它是所有Spring Data子项目的依赖. Spring Data Repository&nbsp;&nbsp;Spring Data Repository是数据访问的统一标准,它是抽象的,不同的数据访问技术有不同的Repository,它的顶级接口为Repository接口. &nbsp;&nbsp;Repository用一个实体类型与ID类型作为泛型. &nbsp;&nbsp;Repository的子接口CrudRepository定义了CRUD操作的相关内容: &nbsp;&nbsp;CrudRepository的子接口PagingAndSortingRepository定义了分页与排序的相关内容: Spring Data JPAJPA规范 &nbsp;&nbsp;JPA是Java Persistence API的缩写,它是一个基于O/R(Object-Relational Mapping)映射的标准规范.例如Hibernate就是JPA规范的实现. 数据访问层 在Spring Data JPA中定义数据访问层首先需要继承JpaRepository接口. 之后可以通过@EnableJpaRepository注解开启Spring Data JPA的支持,@EnableJpaRepository接收的value参数用于扫描数据访问层所在包下的接口定义. 查询方法 &nbsp;&nbsp;Spring Data JPA支持通过定义在Repository接口中的方法名来定义查询,方法名是根据实体类的属性名来确定的. &nbsp;&nbsp;其中findBy关键字可以用find、read、readBy、query、queryBy、get、getBy替代. &nbsp;&nbsp;Spring Data JPA可以使用top和first关键字查询指定数量的数据. 查询关键字 关键字 示例 同功能JPQL And findByNameAndAge where x.name = ?1 and x.age = ?2 Or findByNameOrAge where x.name = ?1 or x.age = ?2 Is findByNameIs where x.name = ?1 Equals findByNameEquals where x.name = ?1 Between findByAgeBetween where x.age between ?1 and ?2 LessThan findByAgeLessThan where x.age &lt; ?1 LessThanEqual findByAgeLessThanEqual where x.age &lt;= ?1 GreaterThan findByAgeGreaterThan where x.age &gt; ?1 GreaterThanEqual findByAgeGreaterThanEqual where x.age &gt;= ?1 After findByStartDateAfter where x.startDate &gt; ?1 Before findByStartDateBefore where x.startDate &lt; ?1 IsNull findByNameIsNull where x.name is null IsNotNull&amp;NotNull findByName(Is)NotNull where x.name not null Like findByNameLike where x.name like ?1 NotLike findByNameNotLike where x.name not like ?1 StartingWith findByNameStartingWith where x.name like ?1(参数前面加%) EndingWith findByNameEndingWith where x.name like ?1(参数后面加%) Containing findByNameContaining where x.name like ?1(参数前后都加%) OrderBy findByNameOrderByAgeDesc where x.name = ?1 order by x.age desc Not findByNameNot where x.name &lt;&gt; ?1 In findByAgeIn(Collection age) where x.age in ?1 NotIn findByAgeNotIn(Collection age) where x.age not in ?1 True findByActiveTrue() where x.active = true False findByActiveFalse() where x.active = false IgnoreCase findByNameIgnoreCase where UPPER(x.name) = UPPER(?1) @Query&amp;@NamedQuery查询 &nbsp;&nbsp;Spring Data JPA支持使用@Query注解在接口的方法上实现查询. &nbsp;&nbsp;Spring Data JPA支持使用@NamedQuery定义查询方法,一个名称映射一条查询语句. 更新查询 &nbsp;&nbsp;Spring Data JPA支持使用@Modifying注解和@Query注解组合进行数据更新操作. Specification &nbsp;&nbsp;Spring Data JPA提供了一个Specification接口可以让我们快速地构造基于准则的查询.通过重写Specification接口的toPredicate方法用来构造查询条件. 数据访问接口需继承JpaSpecificationExecutor接口. 构造查询条件类 调用条件查询 排序&amp;分页 &nbsp;&nbsp;Spring Data JPA提供了Sort类和Page接口及Pageable接口完成排序和分页. 自定义Repository &nbsp;&nbsp;如果我们想自定义Repository,可以继承Repository的子接口PagingAndSortingRepository. 定义自定义的Repository接口. 定义自定义的Repository接口实现. 定义RepositoryFactoryBean 开启自定义支持需要使用@EnableJpaRepositories注解的repositoryFactoryBeanClass指定FactoryBean. Spring Boot支持 Spring Boot使用spring.datasource前缀用来配置dataSource. Spring Boot自动开启了注解事务支持(@EnableTransactionManagement),并配置了jdbcTemplate. Spring Boot提供了初始化数据的功能,在类路径下的schema.sql文件会自动初始化表结构;在类路径下的data.sql文件会自动插入表数据. Spring Boot为我们自动配置了transactionManager、jpaVendorAdapter、entityManagerFactory等Bean.JpaBaseConfiguration还有一个getPackagesToScan方法用于自动扫描带有注解@Entity的实体类 Spring Boot自动配置了OpenEntityManagerInViewInterceptor拦截器,并注册到了Spring MVC的拦截器中.解决了页面访问数据时会话连接已关闭的错误. Spring Boot自动开启了对Spring Data JPA的支持,无需再配置类中显式声明@EnableJpaRepositories. &nbsp;&nbsp;在Spring Boot下使用Spring Data JPA,只需要添加依赖spring-boot-stater-data-jpa,然后定义DataSource、实体类、数据访问层即可,无需其他配置. Spring Data REST &nbsp;&nbsp;Spring Data REST支持将Spring Data JPA,Spring Data MongoDB,Spring Data Neo4j,Spring Data GernFile,Spring Data Cassandra的Repository自动转换成REST服务. &nbsp;&nbsp;Spring Data REST的配置是定义在RepositoryRestMvcConfiguration配置类中的,我们可以通过继承这个配置类或者使用@Import注解导入此配置类来使用Spring Data REST. Spring Boot支持 &nbsp;&nbsp;Spring Boot已经自动配置了RepositoryRestMvcConfiguration,所以只需要引入依赖spring-boot-starter-data-rest,不需要任何其他配置. Spring Boot使用spring.data.rest前缀用来配置RepositoryRestMvcConfiguration的属性. 如果想在自定义的领域类Repository中将方法暴露为REST资源,则需要使用@RestResource注解. 如需要分页,则可以使用参数page=?&amp;size=?来实现分页.例:http://localhost:8080/persons/?page=1&amp;size=10. 如需要排序,则可以使用参数sort来实现排序.例:http://localhost:8080/persons/?sort=age,desc. 自定义根路径需要在application.properties中设置spring.data.rest.base-path属性. Spring Data REST的节点路径是默认在实体类之后加s,如果需要自定义节点路径则要在领域类Repository上使用@RepositoryRestResource注解的path属性进行设置. Spring Data MongoDB &nbsp;&nbsp;MongoDB是一个基于Document文档的NoSQL数据库,它使用面向对象的思想,每一条记录都是一个文档对象. &nbsp;&nbsp;Spring Data MongoDB提供了以下的注解用来定义领域类: 注解 描述 @Document 映射领域对象与MongoDB的一个文档 @Id 映射当前属性为ID @DbRef 当前属性将参考其他文档 @Field 为文档的属性定义名称 @Version 将当前属性作为版本 &nbsp;&nbsp;Spring Data MongoDB还提供了一个MongoTemplate封装了数据访问的方法,我们还需要为MongoClient和MongoDbFactory来配置数据库连接属性.开启MongoDB的Repository需要在配置类上使用注解@EnableMongoRepositories. &nbsp;&nbsp;定义Spring Data MongoDB的Repository只需要继承MongoRepository接口即可. Spring Boot支持 &nbsp;&nbsp;使用Spirng Boot主要配置数据库连接、MongoTemplate.可以使用spring.data.mongodb配置MongoDB相关的属性. Spring Boot自动开启了@EnableMongoRepositories注解. Spring Boot提供了一些默认配置:默认MongoDB端口为27017,服务器为localhost,数据库为test. 在Spring Boot下使用MongoDB只需要引入依赖spring-boot-starter-data-mongodb,不需要其他配置. Spring Data Redis &nbsp;&nbsp;Spring Data Redis提供了ConnectionFactory和RedisTemplate. 根据Redis不同的JavaClient,Spring Data Redis提供了不同的ConnectionFactory. JedisConnectionFactory:使用Jedis作为客户端. JredisConnectionFactory:使用Jredis作为客户端. LettuceConnectionFactory:使用Lettuce作为客户端. SrpConnectionFactory:使用Spullara/redis-protoccol作为客户端. 配置ConnectionFactory和RedisTemplate如下: &nbsp;&nbsp;Spring Data Redis提供了RedisTemplate和StringRedisTemplate两个模板对象进行数据操作.StringRedisTemplate只针对键值都是字符类型的数据进行操作. 数据操作方法 描述 opsForValue() 操作简单属性的数据 opsForList() 操作list数据 opsForSet() 操作set数据 opsForZSet() 操作ZSet(有序的set)数据 opsForHash() 操作hash散列的数据 Serializer &nbsp;&nbsp;当我们进行存储操作的时候,键值对都是通过Spring提供的Serializer序列化到数据库的. RedisTemplate默认使用的是JdkSerializationRedisSerizlizer. StringRedisTemplate默认使用的是StringRedisSerializer. Spring Boot支持 &nbsp;&nbsp;Spring Boot默认配置了JedisConnectionFactory、RedisTemplate和StringRedisTemplate. &nbsp;&nbsp;Spring Boot使用spring.redis为前缀在application.properties中配置Redis相关的属性. end 资料参考于 JavaEE开发的颠覆者: Spring Boot实战]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈Spring Boot自动配置的运作原理]]></title>
    <url>%2F2016%2F07%2F29%2F2016-07-29-Spring-boot-autoconfigure%2F</url>
    <content type="text"><![CDATA[Spring Boot&nbsp;&nbsp;Spring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。它使用“习惯优于配置”的理念可以让你的项目快速运行部署。使用Spring Boot可以不用或者只需要很少的Spring配置。 &nbsp;&nbsp;而Spring Boot核心的功能就是自动配置。它会根据在类路径中的jar、类自动配置Bean,当我们需要配置的Bean没有被Spring Boot提供支持时,也可以自定义自动配置。 自动配置的运作原理&nbsp;&nbsp;Spring Boot自动配置其实是基于Spring 4.x提供的条件配置(Conditional)实现的。 &nbsp;&nbsp;有关自动配置的源码在spring-boot-autoconfigure-1.x.x.jar内,如下图: 如何查看当前项目已启动和未启动的自动配置 在application.propertie中设置debug=true属性. 在运行jar时添加–debug指令. &nbsp;&nbsp;当使用以上两种任意一种方法后,启动项目会在控制台输出已启动和未启动的自动配置日志. @SpringBootApplication&nbsp;&nbsp;生成Spring Boot项目时,会自动生成一个入口类.入口类使用了@SpringBootApplication注解,它是Spring Boot的核心注解,它是一个组合注解,核心功能由@EnableAutoConfiguration注解提供. @EnableAutoConfiguration @Import注解提供导入配置的功能,它导入了EnableAutoConfigurationImportSelector. EnableAutoConfigurationImportSelector使用函数SpringFactoriesLoader.loadFactoryNames扫描META-INF/spring.factories文件中声明的jar包. spring.factories文件在spring-boot-autoconfigure-1.x.x.jar中. spring.factories中声明的类基本上都使用了@Conditional注解. Conditional&nbsp;&nbsp;Spring Boot在org.springframework.boot.autoconfigure.condition包下定义了以下注解. 注解名 作用 @ConditionalOnJava 基于JVM版本作为判断条件. @ConditionalOnBean 当容器中有指定的Bean的条件下. @ConditionalOnClass 当类路径下游指定的类的条件下. @ConditionalOnExpression 基于SpEL表达式作为判断条件. @ConditionalOnJndi 在JNDI存在的条件下查找指定的位置. @ConditionalOnMissingBean 当容器中没有指定Bean的情况下. @ConditionalOnMissingClass 当类路径下没有指定的类的情况下. @ConditionalOnNotWebApplication 当前项目不是web项目的条件下. @ConditionalOnProperty 指定的属性是否有指定的值. @ConditionalOnResource 类路径是否有指定的值. @ConditionalOnSingleCandidate 当指定Bean在容器中只有一个,或者虽然有多个但是指定首选的Bean. @ConditionalOnWebApplication 当前项目是web项目的条件下. &nbsp;&nbsp;以上这些注解都组合了@Conditional元注解. 分析@ConditionalOnNotWebApplication &nbsp;&nbsp;@ConditionalOnNotWebApplication使用的条件类是OnWebApplicationCondition. 1234567891011121314151617181920212223242526272829303132333435363738394041package org.springframework.boot.autoconfigure.condition;import org.springframework.boot.autoconfigure.condition.ConditionOutcome;import org.springframework.boot.autoconfigure.condition.ConditionalOnWebApplication;import org.springframework.boot.autoconfigure.condition.SpringBootCondition;import org.springframework.context.annotation.ConditionContext;import org.springframework.core.annotation.Order;import org.springframework.core.type.AnnotatedTypeMetadata;import org.springframework.util.ClassUtils;import org.springframework.util.ObjectUtils;import org.springframework.web.context.WebApplicationContext;import org.springframework.web.context.support.StandardServletEnvironment;@Order(-2147483628)class OnWebApplicationCondition extends SpringBootCondition &#123; private static final String WEB_CONTEXT_CLASS = "org.springframework.web.context.support.GenericWebApplicationContext"; OnWebApplicationCondition() &#123; &#125; public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; boolean webApplicationRequired = metadata.isAnnotated(ConditionalOnWebApplication.class.getName()); ConditionOutcome webApplication = this.isWebApplication(context, metadata); return webApplicationRequired &amp;&amp; !webApplication.isMatch()?ConditionOutcome.noMatch(webApplication.getMessage()):(!webApplicationRequired &amp;&amp; webApplication.isMatch()?ConditionOutcome.noMatch(webApplication.getMessage()):ConditionOutcome.match(webApplication.getMessage())); &#125; private ConditionOutcome isWebApplication(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; if(!ClassUtils.isPresent("org.springframework.web.context.support.GenericWebApplicationContext", context.getClassLoader())) &#123; return ConditionOutcome.noMatch("web application classes not found"); &#125; else &#123; if(context.getBeanFactory() != null) &#123; String[] scopes = context.getBeanFactory().getRegisteredScopeNames(); if(ObjectUtils.containsElement(scopes, "session")) &#123; return ConditionOutcome.match("found web application \'session\' scope"); &#125; &#125; return context.getEnvironment() instanceof StandardServletEnvironment?ConditionOutcome.match("found web application StandardServletEnvironment"):(context.getResourceLoader() instanceof WebApplicationContext?ConditionOutcome.match("found web application WebApplicationContext"):ConditionOutcome.noMatch("not a web application")); &#125; &#125;&#125; &nbsp;&nbsp;OnWebApplicationCondition在isWebApplication函数中进行条件判断. 判断GenericWebApplicationContext是否在类路径中. 判断容器中是否存在名为session的scope. 判断当前容器的Environment是否为StandardServletEnvironment. 判断当前的ResourceLoader是否为WebApplicationContext. 最后通过ConditionOutcome.isMatch函数返回布尔值确定条件. 分析自动配置的实现&nbsp;&nbsp;以http编码为例,如果在常规项目中则需要在web.xml中配置一个filter.而Spring Boot内置了http编码的自动配置,无需配置filter. properties配置类 自动配置Bean &nbsp;&nbsp;@ConditionalOnProperty:当设置spring.http.encoding=enabled的情况下,如果没有设置则默认为true,即符合条件. &nbsp;&nbsp;characterEncodingFilter()返回OrderedCharacterEncodingFilter这个对象,并根据注入的HttpEncodingProperties配置类设置参数. &nbsp;&nbsp; @ConditionalOnMissingBean({CharacterEncodingFilter.class}):在容器中没有这个Bean的时候则新建这个Bean. end 资料参考于 JavaEE开发的颠覆者: Spring Boot实战]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring Boot</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记(6)-Storm]]></title>
    <url>%2F2016%2F07%2F19%2F2016-07-19-Hadoop06-Storm%2F</url>
    <content type="text"><![CDATA[概述&nbsp;&nbsp;Storm是一个开源的分布式实时计算系统，可以简单、可靠的处理大量的数据流。被称作“实时的hadoop”。Storm有很多使用场景：如实时分析，在线机器学习，持续计算， 分布式RPC，ETL等等。Storm支持水平扩展，具有高容错性，保证每个消息都会得到处理，而且处理速度很快。Storm的部署和运维都很便捷，而且更为重要的是可以使用任意编程语言来开发应用。 Storm的特点简单的编程模型 &nbsp;&nbsp;在大数据处理方面相信大家对hadoop已经耳熟能详，基于Google Map/Reduce来实现的Hadoop为开发者提供了map、reduce原语，使并行批处理程序变得非常地简单和优美。 &nbsp;&nbsp;同样，Storm也为大数据 的实时计算提供了一些简单优美的原语，这大大降低了开发并行实时处理的任务的复杂性，帮助你快速、高效的开发应用。 水平扩展 &nbsp;&nbsp;在Storm集群中真正运行topology的主要有三个实体：工作进程、线程和任务。Storm集群中的每台机器上都可以运行多个工作进程，每个 工作进程又可创建多个线程，每个线程可以执行多个任务，任务是真正进行数据处理的实体，我们开发的spout、bolt就是作为一个或者多个任务的方式执行的。 &nbsp;&nbsp;计算任务在多个线程、进程和服务器之间并行进行，支持灵活的水平扩展。 支持多种编程语言 &nbsp;&nbsp;你可以在Storm之上使用各种编程语言。默认支持Clojure、Java、Ruby和Python。要增加对其他语言的支持，只需实现一个简单的Storm通信协议即可。 高可靠性 &nbsp;&nbsp;Storm保证每个消息至少能得到一次完整处理。任务失败时，它会负责从消息源重试消息。 &nbsp;&nbsp;spout发出的消息后续可能会触发产生成千上万条消息，可以形象的理解为一棵消息树，其中spout发出的消息为树根，Storm会跟踪这棵消息树的处理情况，只有当这棵消息树中的所有消息都被处理了，Storm才会认为spout发出的这个消息已经被“完全处理”。如果这棵消息树中的任何一个消息处理失败了，或者整棵消息树在限定的时间内没有“完全处理”，那么spout发出的消息就会重发。 高容错性 &nbsp;&nbsp;Storm会管理工作进程和节点的故障。 &nbsp;&nbsp;如果在消息处理过程中出了一些异常，Storm会重新安排这个出问题的处理单元。Storm保证一个处理单元永远运行（除非你显式杀掉这个处理单元）。 &nbsp;&nbsp;当然，如果处理单元中存储了中间状态，那么当处理单元重新被Storm启动的时候，需要应用自己处理中间状态的恢复。 本地模式 &nbsp;&nbsp;Storm有一个“本地模式”，可以在处理过程中完全模拟Storm集群。这让你可以快速进行开发和单元测试。 Storm架构 &nbsp;&nbsp;Storm集群由一个主节点和多个工作节点组成。主节点运行了一个名为“Nimbus”的守护进程，用于分配代码、布置任务及故障检测。每个工作节 点都运行了一个名为“Supervisor”的守护进程，用于监听工作，开始并终止工作进程。Nimbus和Supervisor都能快速失败，而且是无状态的，这样一来它们就变得十分健壮，两者的协调工作是由ApacheZooKeeper来完成的。 Stream&nbsp;&nbsp;Stream是一个数据流的抽象。这是一个没有边界的Tuple序列,而这些Tuple序列会以一种分布式的方式并行地创建和处理。 &nbsp;&nbsp;对消息流的定义主要就是对消息流里面的tuple 进行定义，为了更好地使用tuple，需要给tuple 里的每个字段取一个名字，并且不同的tuple 字段对应的类型要相同，即两个tuple 的第一个字段类型相同，第二个字段类型相同，但是第一个字段和第二个字段的类型可以不同。默认情况下，tuple 的字段类型可以为integer、long、short、byte、string、double、float、boolean 和byte array 等基本类型，也可以自定义类型，只需要实现相应的序列化接口。 &nbsp;&nbsp;每一个消息流在定义的时候需要被分配一个id，最常见的消息流是单向的消息流，在Storm 中OutputFieldsDeclarer 定义了一些方法，让你可以定义一个Stream 而不用指定这个id。在这种情况下，这个Stream 会有个默认的id: 1。 Topologies&nbsp;&nbsp;Topology是由Stream Grouping连接起来的Spout和Bolt节点网络。 &nbsp;&nbsp;在 Storm 中，一个实时计算应用程序的逻辑被封装在一个称为Topology 的对象中，也称为计算拓扑。Topology 有点类似于Hadoop 中的MapReduce Job，但是它们之间的关键区别在于，一个MapReduce Job 最终总是会结束的，然而一个Storm 的Topology 会一直运行。在逻辑上，一个Topology 是由一些Spout（消息的发送者）和Bolt（消息的处理者）组成图状结构，而链接Spouts 和Bolts 的则是Stream Groupings。 Spouts&amp;Bolts Spouts &nbsp;&nbsp;Spouts 是Storm集群中一个计算任务（Topology）中消息流的生产者，Spouts一般是从别的数据源（例如，数据库或者文件系统）加载数据，然后向Topology中发射消息。 &nbsp;&nbsp;Spouts即可以是可靠的,也可以是不可靠的。 &nbsp;&nbsp;在一个Topology中存在两种Spouts，一种是可靠的Spouts，一种是非可靠的Spouts，可靠的Spouts 在一个tuple 没有成功处理的时候会重新发射该tuple，以保证消息被正确地处理。不可靠的Spouts 在发射一个tuple 之后，不会再重新发射该tuple，即使该tuple 处理失败。每个Spouts 都可以发射多个消息流，要实现这样的效果，可以使用OutFieldsDeclarer.declareStream 来定义多个Stream，然后使用SpoutOutputCollector 来发射指定的Stream。 &nbsp;&nbsp;在Storm 的编程接口中，Spout 类最重要的方法是nextTuple()方法，使用该方法可以发射一个消息tuple 到Topology 中，或者简单地直接返回，如果没有消息要发射。需要注意的是，nextTuple 方法的实现不能阻塞Spout，因为Storm在同一线程上调用Spout 的所有方法。Spout 类的另外两个重要的方法是ack()和fail()，一个tuple 被成功处理完成后，ack()方法被调用，否则就调用fail()方法。注意，只有对于可靠的Spout，才会调用ack()和fail()方法。 Bolts &nbsp;&nbsp;所有消息处理的逻辑都在Bolt 中完成，在Bolt 中可以完成如过滤、分类、聚集、计算、查询数据库等操作。Bolt 可以做简单的消息处理操作，例如，Bolt 可以不做任何操作，只是将接收到的消息转发给其他的Bolt。Bolt 也可以做复杂的消息流的处理，从而需要很多个Bolt。在实际使用中，一条消息往往需要经过多个处理步骤，例如，计算一个班级中成绩在前十名的同学，首先需要对所有同学的成绩进行排序，然后在排序过的成绩中选出前十名的成绩的同学。所以在一个Topology 中，往往有很多个Bolt，从而形成了复杂的流处理网络。 &nbsp;&nbsp;Bolts可以发射多条消息流。 使用OutputFieldsDeclarer.declareStream定义Stream。 使用OutputCollector.emit来选择要发射的Stream。 &nbsp;&nbsp;Bolts的主要方法是execute。 &nbsp;&nbsp;Bolts以Tuple作为输入,使用OutputCollector来发射Tuple,通过调用OutputCollector.ack()通知这个Tuple的发射者Spout。 &nbsp;&nbsp;Bolts一般流程。 &nbsp;&nbsp;处理一个输入Tuple,发射0个或多个Tuple,然后调用ack()通知Storm自己已经处理过这个Tuple了。Storm提供了一个IBasicBolt会自动调用ack()。 Stream Groupings&nbsp;&nbsp;定义一个 Topology 的其中一步是定义每个Bolt 接收什么样的流作为输入。Stream Grouping 就是用来定义一个Stream 应该如何分配给Bolts 上面的多个Tasks。 &nbsp;&nbsp;Storm里有7种类型的Stream Grouping。 Shuffle Grouping 随机分组,随机派发Stream里面的Tuple,保证每个Bolt接收到的Tuple数量大致相同。 Fields Grouping 按字段分组,以id举例。具有相同id的Tuple会被分到相同的Bolt中的一个Task,而不同id的Tuple会被分到不同的Bolt中的Task。 All Grouping 广播,对于每一个Tuple,所有的Bolts都会收到。 Global Grouping 全局分组,这个Tuple被分配到Storm中的一个Bolt的其中一个Task。具体一点就是分配给id值最低的那个Task。 Non Grouping 不分组,Stream不关心到底谁会收到它的Tuple。目前这种分组和Shuffle Grouping是一样的效果,有一点不同的是Storm会把这个Bolt放到这个Bolt的订阅者同一个线程中去执行。 Direct Grouping 直接分组,这是一种比较特别的分组方法,用这种分组意味着消息的发送者指定由消息接收者的哪个Task处理这个消息。只有被声明为Direct Stream的消息流可以声明这种分组方法。而且这种消息Tuple必须使用emitDirect方法来发射。消息处理者可以通过TopologyContext来获取处理它的消息的Task的id(OutputCollector.emit方法也会返回Task的id)。 Local or Shuffle Grouping 如果目标Bolt有一个或者多个Task在同一个工作进程中,Tuple将会被随机发射给这些Tasks。否则,和普通的Shuffle Grouping行为一致。 Workers 每个Supervisor中运行着多个Workers进程。 每个Worker进程中运行着多个Executor线程。 每个Executor线程中运行着若干个相同的Task(Spout/Bolt)。 &nbsp;&nbsp;一个 Topology 可能会在一个或者多个工作进程里面执行，每个工作进程执行整个Topology 的一部分。比如，对于并行度是300 的Topology 来说，如果我们使用50 个工作进程来执行，那么每个工作进程会处理其中的6 个Tasks（其实就是每个工作进程里面分配6 个线程）。Storm 会尽量均匀地把工作分配给所有的工作进程。 Task&nbsp;&nbsp;在 Storm 集群上，每个Spout 和Bolt 都是由很多个Task 组成的，每个Task对应一个线程，流分组策略就是定义如何从一堆Task 发送tuple 到另一堆Task。在实现自己的Topology 时可以调用TopologyBuilder.setSpout() 和TopBuilder.setBolt()方法来设置并行度，也就是有多少个Task。 Storm安装部署 安装jdk。 搭建Zookeeper集群。 下载并解压Storm。 修改storm.yaml配置文件。 storm.zookeeper.servers: Storm集群使用的Zookeeper集群地址。例如:storm.zookeeper.servers:-“192.168.145.141”-“192.168.145.142” 如果Zookeeper没有使用默认端口,那么还需要修改storm.zookeeper.port。 storm.local.dir Nimbus和Supervisor进程用于存储少量状态,如jars、confs等的本地磁盘目录,需要提前创建该目录并给予足够的访问权限。然后在storm.yaml中配置该目录,例如:storm.local.dir:”/home/application/storm/workdir” 注意事项&nbsp;&nbsp;启动Storm后台进程时,需要对conf/storm.yaml配置文件中设置的storm.local.dir目录具有写权限。 &nbsp;&nbsp;Storm后台进程被启动时,将在Storm安装目录下的logs/子目录下生成各个进程的日志文件。 &nbsp;&nbsp;Storm UI必须和Storm Nimbus部署在同一台机器上,否则UI无法正常工作,因为UI进程会检查本机是否存在Nimbus链接。 常用命令 命令描述 格式 例子 启动Nimbus storm nimbus storm nimbus 启动Supervisor storm supervisor storm supervisor 启动UI storm ui storm ui 提交Topologies格式 storm jar 【jar路径】 【拓扑包名.拓扑类名】【stormIP地址】【storm端口】【拓扑名称】【参数】 Example 123storm jar /home/storm/hello.jarstorm.hello.WordCountTopology wordcountTop提交hello.jar到远程集群,并启动wordcountTop拓扑 停止Topologies格式 storm kill [拓扑名称] Example 1storm kill wordcountTop APISpouts&nbsp;&nbsp; Spout是Stream的消息产生源， Spout组件的实现可以通过继承BaseRichSpout类或者其他*Spout类来完成，也可以通过实现IRichSpout接口来实现。 open &nbsp;&nbsp;当一个Task被初始化的时候会调用open()。一般都会在此方法中对发送Tuple的对象SpoutOutputCollector和配置对象TopologyContext初始化。 getComponentConfiguration &nbsp;&nbsp;此方法用于声明针对当前组件的特殊的Configuration配置。 nextTuple &nbsp;&nbsp;这是Spout类中最重要的一个方法。发射一个Tuple到Topology都是通过这个方法来实现的。 declareOutputFields &nbsp;&nbsp;此方法用于声明当前Spout的Tuple发送流。Stream的定义是通过OutputFieldsDeclare.declare方法完成的,其中的参数包括了发送的Fields。 &nbsp;&nbsp;另外，除了上述几个方法之外，还有ack、fail和close方法等。 &nbsp;&nbsp;Storm在监测到一个Tuple被成功处理之后会调用ack方法，处理失败会调用fail方法。这两个方法在BaseRichSpout等类中已经被隐式的实现了。 Bolts&nbsp;&nbsp; Bolt类接收由Spout或者其他上游Bolt类发来的Tuple，对其进行处理。Bolt组件的实现可以通过继承BasicRichBolt类或者IRichBolt接口来完成。 prepare &nbsp;&nbsp;此方法与Spouts的open方法类似,为Bolt提供了OutputCollector,用来从Bolt中发射Tuple。Bolt中Tuple的发射可以在prepare中、execute中、cleanup等方法中进行,一般都是在execute中。 getComponentConfiguration &nbsp;&nbsp;与Spouts类似。 execute &nbsp;&nbsp; 这是Bolt中最关键的一个方法，对于Tuple的处理都可以放到此方法中进行。具体的发送也是在execute中通过调用emit方法来完成的。 &nbsp;&nbsp;emit有两种情况，一种是emit方法中有两个参数，另一个种是有一个参数。 emit有一个参数：此唯一的参数是发送到下游Bolt的Tuple，此时，由上游发来的旧的Tuple在此隔断，新的Tuple和旧的Tuple不再属于同一棵Tuple树。新的Tuple另起一个新的Tuple树。 emit有两个参数：第一个参数是旧的Tuple的输入流，第二个参数是发往下游Bolt的新的Tuple流。此时，新的Tuple和旧的Tuple是仍然属于同一棵Tuple树，即，如果下游的Bolt处理Tuple失败，则会向上传递到当前Bolt，当前Bolt根据旧的Tuple流继续往上游传递，申请重发失败的Tuple。保证Tuple处理的可靠性。 declareOutputFields &nbsp;&nbsp;用于声明当前Bolt发送的Tuple中包含的字段。 Topology Example12345678910111213141516171819202122232425262728293031public class RandomWordSpout extends BaseRichSpout &#123; // 初始化数据字典 private final static String[] words = &#123; "java", "c", "c++", "c#", "python", "go", "javascript", "swift" &#125;; private SpoutOutputCollector collector; @Override public void nextTuple() &#123; Random random = new Random(); // 获取随机的单词 String word = words[random.nextInt(words.length)]; // 发射消息 this.collector.emit(new Values(word)); // 休息2秒 Utils.sleep(2000); &#125; @Override public void open(Map arg0, TopologyContext arg1, SpoutOutputCollector collector) &#123; this.collector = collector; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; // 声明字段名 declarer.declare(new Fields("initName")); &#125;&#125; 123456789101112131415161718public class UpperBolt extends BaseBasicBolt &#123; @Override public void execute(Tuple tuple, BasicOutputCollector collector) &#123; // 获得上个bolt传入的initName String initName = tuple.getString(0); // 将initName转为大写 String upperCase = initName.toUpperCase(); // 发射消息 collector.emit(new Values(upperCase)); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("upperName")); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435public class PrefixBolt extends BaseBasicBolt &#123; private FileWriter fileWriter; @Override public void prepare(Map stormConf, TopologyContext context) &#123; // 初始化fileWriter try &#123; this.fileWriter = new FileWriter("/home/storm/output/" + UUID.randomUUID()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void execute(Tuple tuple, BasicOutputCollector collector) &#123; String upperName = tuple.getString(0); // 添加前缀 String finalName = "hello-" + upperName; // write try &#123; this.fileWriter.write(finalName); this.fileWriter.write("\n"); this.fileWriter.flush(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; &#125;&#125; 12345678910111213141516171819public class TopologyMain &#123; public static void main(String[] args) throws Exception &#123; TopologyBuilder topologyBuilder = new TopologyBuilder(); // 设置Spout topologyBuilder.setSpout("randomWordSpout", new RandomWordSpout()); // 设置Bolt topologyBuilder.setBolt("upperBolt", new UpperBolt()).shuffleGrouping("randomWordSpout"); topologyBuilder.setBolt("prefixBolt", new PrefixBolt()).shuffleGrouping("upperBolt"); Config config = new Config(); // 设置Workers数量 config.setNumWorkers(4); config.setDebug(true); // 提交Topology StormSubmitter.submitTopology("randomTopo", config, topologyBuilder.createTopology()); &#125;&#125;]]></content>
      <categories>
        <category>后端</category>
        <category>大数据</category>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记(5)-Hive]]></title>
    <url>%2F2016%2F07%2F18%2F2016-07-18-Hadoop05-Hive%2F</url>
    <content type="text"><![CDATA[概述&nbsp;&nbsp;Hive是建立在Hadoop上的数据仓库基础架构。它提供了一系列的工具，用来进行数据提取、转换、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据机制。可以把Hadoop下结构化数据文件映射为一张成Hive中的表，并提供类sql查询功能，除了不支持更新、索引和事务，sql其它功能都支持。可以将sql语句转换为MapReduce任务进行运行，作为sql到MapReduce的映射器。提供shell、JDBC/ODBC、Thrift、Web等接口。 &nbsp;&nbsp;Hive 并不适合那些需要低延迟的应用，例如，联机事务处理（OLTP）。Hive 查询操作过程严格遵守Hadoop MapReduce 的作业执行模型，Hive 将用户的HiveQL 语句通过解释器转换为MapReduce 作业提交到Hadoop 集群上，Hadoop 监控作业执行过程，然后返回作业执行结果给用户。Hive 并非为联机事务处理而设计，Hive 并不提供实时的查询和基于行级的数据更新操作。Hive 的最佳使用场合是大数据集的批处理作业，例如，网络日志分析。 元数据存储&nbsp;&nbsp;Hive将元数据存储在RDBMS中，有三种方式可以连接到数据库。 内嵌模式：元数据保持在内嵌数据库的Derby，一般用于单元测试，只允许一个会话连接。 多用户模式：在本地安装Mysql，把元数据放到Mysql内。 远程模式：元数据放置在远程的Mysql数据库。 数据存储 &nbsp;&nbsp;Hive没有专门的数据存储格式，也没有为数据建立索引，用于可以非常自由的组织Hive中的表，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符。 &nbsp;&nbsp;Hive中所有的数据都存储在HDFS中，Hive中包含4中数据模型：Tabel、ExternalTable、Partition、Bucket。 Table &nbsp;&nbsp;类似与传统数据库中的Table，每一个Table在Hive中都有一个相应的目录来存储数据。例如：一个表zz，它在HDFS中的路径为：/wh/zz，其中wh是在hive-site.xml中由${hive.metastore.warehouse.dir}指定的数据仓库的目录，所有的Table数据（不含External Table）都保存在这个目录中。 Partition &nbsp;&nbsp;类似于传统数据库中划分列的索引。在Hive中，表中的一个Partition对应于表下的一个目录，所有的Partition数据都存储在对应的目录中。例如：zz表中包含ds和city两个Partition，则对应于ds=20140214，city=beijing的HDFS子目录为：/wh/zz/ds=20140214/city=Beijing。 ExternalTable &nbsp;&nbsp;指向已存在HDFS中的数据，可创建Partition。和Table在元数据组织结构相同，在实际存储上有较大差异。Table创建和数据加载过程，可以用统一语句实现，实际数据被转移到数据仓库目录中，之后对数据的访问将会直接在数据仓库的目录中完成。删除表时，表中的数据和元数据都会删除。ExternalTable只有一个过程，因为加载数据和创建表是同时完成。时间数据是存储在Location后面指定的HDFS路径中的，并不会移动到数据仓库中。 Bcuket &nbsp;&nbsp;对指定列计算的hash，根据hash值切分数据，目的是为了便于并行，每一个Buckets对应一个文件。将user列分数至32个Bucket上，首先对user列的值计算hash，比如，对应hash=0的HDFS目录为：/wh/zz/ds=20140214/city=Beijing/part-00000;对应hash=20的，目录为：/wh/zz/ds=20140214/city=Beijing/part-00020。 Hive常用优化方法 join连接时的优化：当三个或多个以上的表进行join操作时，如果每个on使用相同的字段连接时只会产生一个mapreduce。 join连接时的优化：当多个表进行查询时，从左到右表的大小顺序应该是从小到大。原因：hive在对每行记录操作时会把其他表先缓存起来，直到扫描最后的表进行计算。 在where字句中增加分区过滤器。 当可以使用left semi join 语法时不要使用inner join，前者效率更高。原因：对于左表中指定的一条记录，一旦在右表中找到立即停止扫描。 如果所有表中有一张表足够小，则可置于内存中，这样在和其他表进行连接的时候就能完成匹配，省略掉reduce过程。设置属性即可实现，set hive.auto.covert.join=true; 用户可以配置希望被优化的小表的大小 set hive.mapjoin.smalltable.size=2500000; 如果需要使用这两个配置可置入$HOME/.hiverc文件中。 同一种数据的多种处理：从一个数据源产生的多个数据聚合，无需每次聚合都需要重新扫描一次。例如:insert overwrite table student select from employee; insert overwrite table person select from employee;可以优化成 from employee insert overwrite table student select insert overwrite table person select limit调优：limit语句通常是执行整个语句后返回部分结果。set hive.limit.optimize.enable=true; 开启并发执行。某个job任务中可能包含众多的阶段，其中某些阶段没有依赖关系可以并发执行，开启并发执行后job任务可以更快的完成。设置属性：set hive.exec.parallel=true; hive提供的严格模式，禁止3种情况下的查询模式。 当表为分区表时，where字句后没有分区字段和限制时，不允许执行。 当使用order by语句时，必须使用limit字段，因为order by 只会产生一个reduce任务。 限制笛卡尔积的查询。 合理的设置map和reduce数量。 jvm重用。可在hadoop的mapred-site.xml中设置jvm被重用的次数。 安装Hive 解压Hive。 将mysql的驱动jar包copy到${HIVE_HOME}/lib目录下。 cp hive-default.xml.template hive-size.xml。 配置hive-size.xml。12345678910111213141516171819 &lt;configuration&gt; &lt;!-- 指定数据库URL --&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://192.168.145.148:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Hive QL文件格式&nbsp;&nbsp;Hive创建表可以指定四种文件格式。 文本格式的数据是Hadoop中经常碰到的。如TextFile 、XML和JSON。 文本格式除了会占用更多磁盘资源外，对它的解析开销一般会比二进制格式高几十倍以上，尤其是XML 和JSON，它们的解析开销比Textfile 还要大，因此强烈不建议在生产系统中使用这些格式进行储存。如果需要输出这些格式，请在客户端做相应的转换操作。 文本格式经常会用于日志收集，数据库导入，Hive默认配置也是使用文本格式，而且常常容易忘了压缩，所以请确保使用了正确的格式。另外文本格式的一个缺点是它不具备类型和模式，比如销售金额、利润这类数值数据或者日期时间类型的数据，如果使用文本格式保存，由于它们本身的字符串类型的长短不一，或者含有负数，导致MR没有办法排序，所以往往需要将它们预处理成含有模式的二进制格式，这又导致了不必要的预处理步骤的开销和储存资源的浪费。 SequenceFile是Hadoop API 提供的一种二进制文件，它将数据以的形式序列化到文件中。这种二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。它与Hadoop API中的MapFile 是互相兼容的。Hive 中的SequenceFile 继承自Hadoop API 的SequenceFile，不过它的key为空，使用value 存放实际的值， 这样是为了避免MR 在运行map 阶段的排序过程。如果你用Java API 编写SequenceFile，并让Hive 读取的话，请确保使用value字段存放数据，否则你需要自定义读取这种SequenceFile 的InputFormat class 和OutputFormat class。 RCFile是Hive推出的一种专门面向列的数据格式。 它遵循“先按列划分，再垂直划分”的设计理念。当查询过程中，针对它并不关心的列时，它会在IO上跳过这些列。需要说明的是，RCFile在map阶段从远端拷贝仍然是拷贝整个数据块，并且拷贝到本地目录后RCFile并不是真正直接跳过不需要的列，并跳到需要读取的列， 而是通过扫描每一个row group的头部定义来实现的，但是在整个HDFS Block 级别的头部并没有定义每个列从哪个row group起始到哪个row group结束。所以在读取所有列的情况下，RCFile的性能反而没有SequenceFile高。 Avro是一种用于支持数据密集型的二进制文件格式。它的文件格式更为紧凑，若要读取大量数据时，Avro能够提供更好的序列化和反序列化性能。并且Avro数据文件天生是带Schema定义的，所以它不需要开发者在API 级别实现自己的Writable对象。 其他格式:Hadoop实际上支持任意文件格式，只要能够实现对应的RecordWriter和RecordReader即可。其中数据库格式也是会经常储存在Hadoop中，比如Hbase，Mysql，Cassandra，MongoDB。 这些格式一般是为了避免大量的数据移动和快速装载的需求而用的。他们的序列化和反序列化都是由这些数据库格式的客户端完成，并且文件的储存位置和数据布局(Data Layout)不由Hadoop控制，他们的文件切分也不是按HDFS的块大小（blocksize）进行切割。 create table12345678create table test_user(id int,name string) // 注释 comment &apos;This is the test table&apos; row format delimited // 指定切分格式规则 fields terminated by &apos;,&apos; // 指定文件格式 stored as textfile; insert select12//使用select语句来批量插入数据insert overwrite table test_user select * from tab_user; load data12345//从本地导入数据到hive的表中（实质就是将文件上传到hdfs中hive管理目录下）load data local inpath &apos;/home/hadoop/test.txt&apos; into table test_user;//从hdfs上导入数据到hive表中（实质就是将文件从原始目录移动到hive管理的目录下）load data inpath &apos;hdfs://ns1/data.log&apos; into table test_user; external table1234567//LOCATION指定的是hdfs路径//如果LOCATION路径有数据,则可以直接映射数据建表CREATE EXTERNAL TABLE test_user_external(id int, name string) ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE LOCATION &apos;/external/user&apos;; CTAS1234567//CTAS是通过查询,然后根据查询的结果来建立表格的一种方式。//CTAS会根据SELECT语句创建表结构,并把数据一并复制过来。CREATE TABLE test_user_ctas ASSELECT id new_id, name new_nameFROM test_userSORT BY new_id; Partition123456789101112//创建一个分区表,以year年份作为分区字段create table test_user_part(id int,name string) partitioned by (year string) row format delimited fields terminated by &apos;,&apos;;//将data.log导入到test_user_part表中,并设置分区为1990 load data local inpath &apos;/home/hadoop/data.log&apos; overwrite into table test_user_part partition(year=&apos;1990&apos;); load data local inpath &apos;/home/hadoop/data2.log&apos; overwrite into table test_user_part partition(year=&apos;2000&apos;); Array&amp;&amp;Map&nbsp;&nbsp;hive中的列支持使用struct、map和array集合数据类型。大多数关系型数据库中不支持这些集合数据类型，因为它们会破坏标准格式。关系型数据库中为实现集合数据类型是由多个表之间建立合适的外键关联来实现。在大数据系统中，使用集合类型的数据的好处在于提高数据的吞吐量，减少寻址次数来提高查询速度。 12345678910111213141516171819//array create table tab_array(a array&lt;int&gt;,b array&lt;string&gt;)row format delimitedfields terminated by &apos;\t&apos;collection items terminated by &apos;,&apos;;select a[0] from tab_array;select * from tab_array where array_contains(b,&apos;word&apos;);insert into table tab_array select array(0),array(name,ip) from tab_ext t; //mapcreate table tab_map(name string,info map&lt;string,string&gt;)row format delimitedfields terminated by &apos;\t&apos;collection items terminated by &apos;,&apos;map keys terminated by &apos;:&apos;;load data local inpath &apos;/home/hadoop/hivetemp/tab_map.txt&apos; overwrite into table tab_map;insert into table tab_map select name,map(&apos;name&apos;,name,&apos;ip&apos;,ip) from tab_ext; UDF&nbsp;&nbsp;UDF即用户自定义函数(User Defined Function),Hive支持UDF进行自定义函数的编写。 &nbsp;&nbsp;需要先使用Java代码开发UDF,然后再把jar包导入到Hive中。 123456789101112131415161718192021public class FindRegionByPhone extends UDF &#123; //使用map模拟数据库 private static HashMap&lt;String,String&gt; dataDictionary = new HashMap&lt;String,String&gt;(); static&#123; dataDictionary.put("136","beijing"); dataDictionary.put("137","guangzhou"); dataDictionary.put("138","shenzhen"); dataDictionary.put("139","shanghai"); &#125; public String evaluate(String phone) &#123; // 如果没有匹配到对应的区域则返回"other" return areaMap.get(phone.substring(0, 3)) == null ? "other" : areaMap .get(phone.substring(0, 3)); &#125; &#125; 添加jar包到Hive12345方式1:添加到hivehive&gt; add jar /root/MyUDF.jar;方式2:添加到hdfs,调用时需要指定jar包地址hdfs -dfs -put MyUDF.jar &apos;hdfs:///user/hadoop/hiveUDF&apos; 创建临时函数&nbsp;&nbsp;临时函数只在当前session中有效,临时函数不能指定库。 1create temporary function testUDF as &apos;cn.sylvanas.hive.udf.FindRegionByPhone&apos; using jar &apos;hdfs:///user/hadoop/hiveUDF/MyUDF.jar&apos; 创建永久函数格式 CREATE FUNCTION [db_name.]function_name AS class_name[USING JAR|FILE|ARCHIVE ‘file_uri’ [, JAR|FILE|ARCHIVE ‘file_uri’] ]; 例如 123create function test.testUDF as &apos;cn.sylvanas.hive.udf.FindRegionByPhone&apos; using jar &apos;hdfs:///user/hadoop/hiveUDF/MyUDF.jar&apos;函数需要属于某个库,如这里是’test’,当其他库调用时,需要加上库名,如’test.testUDF’.]]></content>
      <categories>
        <category>后端</category>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记(4)-HBase]]></title>
    <url>%2F2016%2F07%2F17%2F2016-07-17-Hadoop04-HBase%2F</url>
    <content type="text"><![CDATA[概述&nbsp;&nbsp;HBase是一个分布式的、面向列的开源数据库，该技术来源于 Fay Chang 所撰写的Google论文“Bigtable：一个结构化数据的分布式存储系统”。就像Bigtable利用了Google文件系统（File System）所提供的分布式数据存储一样，HBase在Hadoop之上提供了类似于Bigtable的能力。HBase是Apache的Hadoop项目的子项目。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。另一个不同的是HBase基于列的而不是基于行的模式。 &nbsp;&nbsp;与FUJITSU Cliq等商用大数据产品不同，HBase是Google Bigtable的开源实现，类似Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MapReduce来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据；Google Bigtable利用 Chubby作为协同服务，HBase利用Zookeeper作为对应。 HBase的特点 Hbase可以往数据里面insert，也可以update一些数据，但update的实际上也是insert，只是插入一个新的时间戳的一行。delete数据，也是insert，只是insert一行带有delete标记的一行。Hbase的所有操作都是追加插入操作。Hbase是一种日志集数据库。它的存储方式，像是日志文件一样。它是批量大量的往硬盘中写，通常都是以文件形式的读写。这个读写速度，取决于硬盘与机器之间的传输有多快。 Hbase中数据可以保存许多不同时间戳的版本（即同一数据可以复制许多不同的版本，准许数据冗余，也是优势）。数据按时间排序，因此Hbase特别适合寻找按照时间排序寻找Top n的场景。找出某个人最近浏览的消息，最近写的N篇博客，N种行为等等，因此Hbase在互联网应用非常多。 Hbase只有主键索引，因此在建模的时候会遇到了问题。例如，在一张表中，很多的列我都想做某种条件的查询。但却只能在主键上建快速查询。 Hbase是列式数据库,列式数据库的优势在于数据分析。 Hbase中的数据都是字符串，没有其他类型。 行式数据库与列式数据库的区别 行式数据库 &nbsp;&nbsp;以Oracle为例，数据文件的基本组成单位：块/页。块中数据是按照一行行写入的。这就存在一个问题，当我们要读一个块中的某些列的时候，不能只读这些列，必须把这个块整个的读入内存中，再把这些列的内容读出来。换句话就是：为了读表中的某些列，必须要把整个表的行全部读完，才能读到这些列。这就是行数据库最糟糕的地方。 列式数据库 &nbsp;&nbsp;列式数据库是以列作为元素存储的。同一个列的元素会挤在一个块。当要读某些列，只需要把相关的列块读到内存中，这样读的IO量就会少很多。通常，同一个列的数据元素通常格式都是相近的。这就意味着，当数据格式相近的时候，数据就可以做大幅度的压缩。所以，列式数据库在数据压缩方面有很大的优势，压缩不仅节省了存储空间，同时也节省了IO。（这一点，可利用在当数据达到百万、千万级别以后，数据查询之间的优化，提高性能，示场景而定） HBase架构 &nbsp;&nbsp;HBase采用Master/Slave架构搭建集群，它隶属于Hadoop生态系统，由以下类型节点组成：HMaster节点、HRegionServer节点、ZooKeeper集群，而在底层，它将数据存储于HDFS中，因而涉及到HDFS的NameNode、DataNode等节点。 Zookeeper &nbsp;&nbsp;Zookeeper Quorum存储-ROOT-表地址、HMaster地址。HRegionServer把自己以Ephedral方式注册到Zookeeper中，HMaster随时感知各个HRegionServer的健康状况。 HMaster &nbsp;&nbsp;HMaster没有单点问题,HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master在运行。 &nbsp;&nbsp;HMaster主要负责Table和Region的管理工作 实现DDL操作（Data Definition Language，namespace和table的增删改，column familiy的增删改等）。 管理HRegionServer的负载均衡，调整Region分布。 管理和分配HRegion，比如在HRegion split时分配新的HRegion；在HRegionServer退出时迁移其内的HRegion到其他HRegionServer上。 权限控制（ACL）。 HRegionServer &nbsp;&nbsp;HBase中最核心的模块，主要负责响应用户I/O请求，向HDFS文件系统中读写数据。 存放和管理本地HRegion。 读写HDFS，管理Table中的数据。 Client直接通过HRegionServer读写数据（从HMaster中获取元数据，找到RowKey所在的HRegion/HRegionServer后）。 HRegion &nbsp;&nbsp;HBase使用RowKey将表水平切割成多个HRegion，从HMaster的角度，每个HRegion都纪录了它的StartKey和EndKey（第一个HRegion的StartKey为空，最后一个HRegion的EndKey为空），由于RowKey是排序的，因而Client可以通过HMaster快速的定位每个RowKey在哪个HRegion中。HRegion由HMaster分配到相应的HRegionServer中，然后由HRegionServer负责HRegion的启动和管理，和Client的通信，负责数据的读(使用HDFS)。 HBase集群搭建&nbsp;&nbsp;如果HDFS是HA集群,需要把HDFS的core-site.xml和hdfs-site.xml copy到conf下。 hbase-env.sh12//告诉hbase使用外部的zookeeperexport HBASE_MANAGES_ZK=false hbase-site.xml12345678910111213141516171819202122&lt;configuration&gt; &lt;!-- 指定hbase在HDFS上存储的路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://ns1/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hbase是分布式的 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk的地址，多个用“,”分割 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;datanode01:2181,datanode02:2181,datanode03:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.maxclockskew&lt;/name&gt; &lt;value&gt;180000&lt;/value&gt; &lt;description&gt;Time difference of regionserver from master&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; regionservers&nbsp;&nbsp;配置regionserver的节点,为了尽量实现数据本地化,可以与DataNode在同一个节点上。 HBase常用Shell命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139进入hbase命令行./hbase shell显示hbase中的表list创建user表，包含info、data两个列族create &apos;user&apos;, &apos;info1&apos;, &apos;data1&apos;create &apos;user&apos;, &#123;NAME =&gt; &apos;info&apos;, VERSIONS =&gt; &apos;3&apos;&#125;向user表中插入信息，row key为rk0001，列族info中添加name列标示符，值为zhangsanput &apos;user&apos;, &apos;rk0001&apos;, &apos;info:name&apos;, &apos;zhangsan&apos;向user表中插入信息，row key为rk0001，列族info中添加gender列标示符，值为femaleput &apos;user&apos;, &apos;rk0001&apos;, &apos;info:gender&apos;, &apos;female&apos;向user表中插入信息，row key为rk0001，列族info中添加age列标示符，值为20put &apos;user&apos;, &apos;rk0001&apos;, &apos;info:age&apos;, 20向user表中插入信息，row key为rk0001，列族data中添加pic列标示符，值为pictureput &apos;user&apos;, &apos;rk0001&apos;, &apos;data:pic&apos;, &apos;picture&apos;获取user表中row key为rk0001的所有信息get &apos;user&apos;, &apos;rk0001&apos;获取user表中row key为rk0001，info列族的所有信息get &apos;user&apos;, &apos;rk0001&apos;, &apos;info&apos;获取user表中row key为rk0001，info列族的name、age列标示符的信息get &apos;user&apos;, &apos;rk0001&apos;, &apos;info:name&apos;, &apos;info:age&apos;获取user表中row key为rk0001，info、data列族的信息get &apos;user&apos;, &apos;rk0001&apos;, &apos;info&apos;, &apos;data&apos;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; [&apos;info&apos;, &apos;data&apos;]&#125;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; [&apos;info:name&apos;, &apos;data:pic&apos;]&#125;获取user表中row key为rk0001，列族为info，版本号最新5个的信息get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; &apos;info&apos;, VERSIONS =&gt; 2&#125;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; &apos;info:name&apos;, VERSIONS =&gt; 5&#125;get &apos;user&apos;, &apos;rk0001&apos;, &#123;COLUMN =&gt; &apos;info:name&apos;, VERSIONS =&gt; 5, TIMERANGE =&gt; [1392368783980, 1392380169184]&#125;获取user表中row key为rk0001，cell的值为zhangsan的信息get &apos;people&apos;, &apos;rk0001&apos;, &#123;FILTER =&gt; &quot;ValueFilter(=, &apos;binary:图片&apos;)&quot;&#125;获取user表中row key为rk0001，列标示符中含有a的信息get &apos;people&apos;, &apos;rk0001&apos;, &#123;FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;&#125;put &apos;user&apos;, &apos;rk0002&apos;, &apos;info:name&apos;, &apos;fanbingbing&apos;put &apos;user&apos;, &apos;rk0002&apos;, &apos;info:gender&apos;, &apos;female&apos;put &apos;user&apos;, &apos;rk0002&apos;, &apos;info:nationality&apos;, &apos;中国&apos;get &apos;user&apos;, &apos;rk0002&apos;, &#123;FILTER =&gt; &quot;ValueFilter(=, &apos;binary:中国&apos;)&quot;&#125;查询user表中的所有信息scan &apos;user&apos;查询user表中列族为info的信息scan &apos;user&apos;, &#123;COLUMNS =&gt; &apos;info&apos;&#125;scan &apos;user&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, RAW =&gt; true, VERSIONS =&gt; 5&#125;scan &apos;persion&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, RAW =&gt; true, VERSIONS =&gt; 3&#125;查询user表中列族为info和data的信息scan &apos;user&apos;, &#123;COLUMNS =&gt; [&apos;info&apos;, &apos;data&apos;]&#125;scan &apos;user&apos;, &#123;COLUMNS =&gt; [&apos;info:name&apos;, &apos;data:pic&apos;]&#125;查询user表中列族为info、列标示符为name的信息scan &apos;user&apos;, &#123;COLUMNS =&gt; &apos;info:name&apos;&#125;查询user表中列族为info、列标示符为name的信息,并且版本最新的5个scan &apos;user&apos;, &#123;COLUMNS =&gt; &apos;info:name&apos;, VERSIONS =&gt; 5&#125;查询user表中列族为info和data且列标示符中含有a字符的信息scan &apos;user&apos;, &#123;COLUMNS =&gt; [&apos;info&apos;, &apos;data&apos;], FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;&#125;查询user表中列族为info，rk范围是[rk0001, rk0003)的数据scan &apos;people&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, STARTROW =&gt; &apos;rk0001&apos;, ENDROW =&gt; &apos;rk0003&apos;&#125;查询user表中row key以rk字符开头的scan &apos;user&apos;,&#123;FILTER=&gt;&quot;PrefixFilter(&apos;rk&apos;)&quot;&#125;查询user表中指定范围的数据scan &apos;user&apos;, &#123;TIMERANGE =&gt; [1392368783980, 1392380169184]&#125;删除数据删除user表row key为rk0001，列标示符为info:name的数据delete &apos;people&apos;, &apos;rk0001&apos;, &apos;info:name&apos;删除user表row key为rk0001，列标示符为info:name，timestamp为1392383705316的数据delete &apos;user&apos;, &apos;rk0001&apos;, &apos;info:name&apos;, 1392383705316清空user表中的数据truncate &apos;people&apos;修改表结构首先停用user表（新版本不用）disable &apos;user&apos;添加两个列族f1和f2alter &apos;people&apos;, NAME =&gt; &apos;f1&apos;alter &apos;user&apos;, NAME =&gt; &apos;f2&apos;启用表enable &apos;user&apos;###disable &apos;user&apos;(新版本不用)删除一个列族：alter &apos;user&apos;, NAME =&gt; &apos;f1&apos;, METHOD =&gt; &apos;delete&apos; 或 alter &apos;user&apos;, &apos;delete&apos; =&gt; &apos;f1&apos;添加列族f1同时删除列族f2alter &apos;user&apos;, &#123;NAME =&gt; &apos;f1&apos;&#125;, &#123;NAME =&gt; &apos;f2&apos;, METHOD =&gt; &apos;delete&apos;&#125;将user表的f1列族版本号改为5alter &apos;people&apos;, NAME =&gt; &apos;info&apos;, VERSIONS =&gt; 5启用表enable &apos;user&apos;删除表disable &apos;user&apos;drop &apos;user&apos;get &apos;person&apos;, &apos;rk0001&apos;, &#123;FILTER =&gt; &quot;ValueFilter(=, &apos;binary:中国&apos;)&quot;&#125;get &apos;person&apos;, &apos;rk0001&apos;, &#123;FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;&#125;scan &apos;person&apos;, &#123;COLUMNS =&gt; &apos;info:name&apos;&#125;scan &apos;person&apos;, &#123;COLUMNS =&gt; [&apos;info&apos;, &apos;data&apos;], FILTER =&gt; &quot;(QualifierFilter(=,&apos;substring:a&apos;))&quot;&#125;scan &apos;person&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, STARTROW =&gt; &apos;rk0001&apos;, ENDROW =&gt; &apos;rk0003&apos;&#125;scan &apos;person&apos;, &#123;COLUMNS =&gt; &apos;info&apos;, STARTROW =&gt; &apos;20140201&apos;, ENDROW =&gt; &apos;20140301&apos;&#125;scan &apos;person&apos;, &#123;COLUMNS =&gt; &apos;info:name&apos;, TIMERANGE =&gt; [1395978233636, 1395987769587]&#125;delete &apos;person&apos;, &apos;rk0001&apos;, &apos;info:name&apos;alter &apos;person&apos;, NAME =&gt; &apos;ffff&apos;alter &apos;person&apos;, NAME =&gt; &apos;info&apos;, VERSIONS =&gt; 10get &apos;user&apos;, &apos;rk0002&apos;, &#123;COLUMN =&gt; [&apos;info:name&apos;, &apos;data:pic&apos;]&#125;]]></content>
      <categories>
        <category>后端</category>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记(3)-HA高可用集群搭建]]></title>
    <url>%2F2016%2F07%2F15%2F2016-07-15-Hadoop03-HA%2F</url>
    <content type="text"><![CDATA[概述&nbsp;&nbsp;HA(High Available), 高可用性集群，是保证业务连续性的有效解决方案，一般有两个或两个以上的节点，且分为活动节点及备用节点。通常把正在执行业务的称为活动节点，而作为活动节点的一个备份的则称为备用节点。当活动节点出现问题，导致正在运行的业务（任务）不能正常运行时，备用节点此时就会侦测到，并立即接续活动节点来执行业务。从而实现业务的不中断或短暂中断。 &nbsp;&nbsp;在hadoop2.0之前,每个集群只有一个NameNode,如果那台机器坏掉,集群作为一个整体将不可用,所以为了解决这个问题Hadoop2.0引入了HA机制,可以通过在同一集群上配置运行两个冗余的NameNodes，做到主动/被动的热备份。这将允许当一个机器宕机时，快速转移到一个新的NameNode，或管理员进行利用故障转移达到优雅的系统升级的目的。 &nbsp;&nbsp;HA一共有二种解决方案，一种是NFS（Network File System）方式，另外一种是QJM（Quorum Journal Manager）方式。 HA架构 &nbsp;&nbsp;一个典型的HA集群，NameNode会被配置在两台独立的机器上.在任何的时间上，一个NameNode处于活动状态，而另一个在备份状态，活动状态的NameNode会响应集群中所有的客户端，同时备份的只是作为一个副本，保证在必要的时候提供一个快速的转移。 &nbsp;&nbsp;为了使备份的节点和活动的节点保持一致，两个节点通过一个特殊的守护线程相连，这个线程叫做“JournalNodes”（JNs）。当活动状态的节点修改任何的命名空间，他都会通过这些JNs记录日志，备用的节点可以监控edit日志的变化，并且通过JNs读取到变化。备份节点查看edits可以拥有专门的namespace。在故障转移的时候备份节点将在切换至活动状态前确认他从JNs读取到的所有edits。这个确认的目的是为了保证Namespace的状态和迁移之前是完全同步的。 &nbsp;&nbsp;为了提供一个快速的转移，备份NameNode要求保存着最新的block在集群当中的信息。为了能够得到这个，DataNode都被配置了所有的NameNode的地址，并且发送block的地址信息和心跳给两个node。 &nbsp;&nbsp;保证只有一个活跃的NameNode在集群当中是一个十分重要的一步。否则namespace状态在两个节点间不同会导致数据都是或者其他一些不正确的结果。为了确保这个,防止所谓split - brain场景,JournalNodes将只允许一个NameNode进行写操作。故障转移期间,NameNode成为活跃状态的时候会接管JournalNodes的写权限,这会有效防止其他NameNode持续处于活跃状态,允许新的活动节点安全进行故障转移。 搭建HA集群&nbsp;&nbsp;hadoop-2.2.0中依然存在一个问题，就是ResourceManager只有一个，存在单点故障，hadoop-2.4.1解决了这个问题，可以有两个ResourceManager，一个是Active，一个是Standby，状态由zookeeper进行协调。 测试集群规划&nbsp;&nbsp;实验使用7台虚拟机,规划如下: HostName IP Software Process datanode01 192.168.145.140 jdk、hadoop、zookeeper DataNode、NodeManager、JournalNode、QuorumPeerMain datanode02 192.168.145.141 jdk、hadoop、zookeeper DataNode、NodeManager、JournalNode、QuorumPeerMain datanode03 192.168.145.142 jdk、hadoop、zookeeper DataNode、NodeManager、JournalNode、QuorumPeerMain namenode01 192.168.145.143 jdk、hadoop NameNode、DFSZKFailoverController(ZKFC) namenode02 192.168.145.144 jdk、hadoop NameNode、DFSZKFailoverController(ZKFC) yarn01 192.168.145.145 jdk、hadoop ResourceManager yarn02 192.168.145.146 jdk、hadoop ResourceManager ssh免密登陆 namenode01需要配置所有datanode、yarn、namenode的免密登陆。 namenode02需要配置namenode01的免密登陆。 yarn01需要配置所有nodemanager与resourcemanager的免密登陆。 1234567891011121314151617181920212223#在namenode01上生成密匙ssh-keygen#namenode01拷贝密匙(包括自己)ssh-copy-id namenode01ssh-copy-id namenode02ssh-copy-id datanode01ssh-copy-id datanode02ssh-copy-id datanode03ssh-copy-id yarn01ssh-copy-id yarn02#在namenode02上生成密匙ssh-keygenssh-copy-id namenode01ssh-copy-id namenode02#在yarn01上生成密匙ssh-keygen#yarn01拷贝密匙ssh-copy-id yarn02ssh-copy-id datanode01ssh-copy-id datanode02ssh-copy-id datanode03 安装zookeeper 解压zookeeper 重命名zookeeper/conf下的zoo_sample.cfg为zoo.cfg : mv zoo_sample.cfg zoo.cfg 在zoo.cfg中修改dataDir=$ZOOKEEPERHOME/data 这个文件需要自己创建例如:dataDir=/home/application/zookeeper-3.4.5/data 在zoo.cfg中最后添加 server.id=ip:2888:3888例如:server.1=datanode01:2888:3888 server.2=datanode02:2888:3888 server.3=datanode03:2888:3888 在$ZOOKEEPERHOME/data目录中创建一个myid文件并写入id。例如: echo 1 &gt;/home/application/zookeeper-3.4.5/data/myidid需要跟zoo.cfg中配置的一致。 core-site.xml1234567891011121314151617&lt;configuration&gt; &lt;!-- 指定hdfs的nameservice为ns1 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ns1/&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定hadoop临时目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/application/hadoop-2.6.0/tmp&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zookeeper地址 --&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;datanode01:2181,datanode02:2181,datanode03:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;configuration&gt; &lt;!--指定hdfs的nameservice为ns1，需要和core-site.xml中的保持一致. 这个名字是逻辑名字,可以是任意的,它将被用来配置在集群中作为HDFS的绝对路径组件。--&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns1&lt;/value&gt; &lt;/property&gt; &lt;!-- ns1下面有两个NameNode，分别是nn1，nn2 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ns1&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.nn1&lt;/name&gt; &lt;value&gt;namenode01:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.nn1&lt;/name&gt; &lt;value&gt;namenode01:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns1.nn2&lt;/name&gt; &lt;value&gt;namenode02:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns1.nn2&lt;/name&gt; &lt;value&gt;namenode02:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://datanode01:8485;datanode02:8485;datanode03:8485/ns1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/application/hadoop-2.6.0/journaldata&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启NameNode失败自动切换 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns1&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制方法，多个机制用换行分割，即每个机制暂用一行--&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt; sshfence shell(/bin/true) &lt;/value&gt; &lt;/property&gt; &lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置sshfence隔离机制超时时间 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapred-site.xml1234567&lt;configuration&gt; &lt;!-- 指定mr框架为yarn方式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn-site.xml1234567891011121314151617181920212223242526272829303132333435&lt;configuration&gt; &lt;!-- 开启RM高可用 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的cluster id 这是一个逻辑名称,可以是任意的 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;yarncluster&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定RM的名字 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;!-- 分别指定RM的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;yarn01&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;yarn02&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定zk集群地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;datanode01:2181,datanode02:2181,datanode03:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; slaves&nbsp;&nbsp;slaves指定子节点(DataNode)位置,因为yarn与HDFS分开启动,所以在yarn01中slaves指定的是NodeManager的位置。 启动HA集群 启动zookeeper集群 启动JournalNode,一旦JNs启动，必须进行一次初始化同步在两个HA的NameNode，主要是为了元数据。sbin/hadoop-daemon.sh start journalnode 格式化HDFS。在namenode01上执行命令 hdfs namenode -format格式化后会在根据core-site.xml中的hadoop.tmp.dir配置生成个文件,为了同步元数据,需要将tmp文件夹copy到namenode02上。scp -r tmp/ namenode02:/home/application/hadoop-2.6.0/也可以使用命令 hdfs namenode -bootstrapStandby 格式化ZKFC在namenode01上执行命令 hdfs zkfc -formatZK 启动HDFS在namenode01上执行命令 sbin/start-dfs.sh 启动Yarn在yarn01上执行命令 sbin/start-yarn.sh。 因为自带的start-yarn.sh脚本并不会远程启动第二个RM,所以需要在yarn02上单独启动一个RM。在yarn02上执行命令 sbin/yarn-daemon.sh start resourcemanager 管理命令1234567Usage: DFSHAAdmin [-ns &lt;nameserviceId&gt;] [-transitionToActive &lt;serviceId&gt;] [-transitionToStandby &lt;serviceId&gt;] [-failover [--forcefence] [--forceactive] &lt;serviceId&gt; &lt;serviceId&gt;] [-getServiceState &lt;serviceId&gt;] [-checkHealth &lt;serviceId&gt;] [-help &lt;command&gt;] &nbsp;&nbsp;描述了常用的命令，每个子命令的详细信息你应该运行”hdfs haadmin -help “. transitionToActive &amp;&amp; transitionToStandby &nbsp;&nbsp;切换NameNode的状态（Active或者Standby),这些子命令会使NameNode分别转换状态。 failover &nbsp;&nbsp;启动两个NameNode之间的故障迁移。 &nbsp;&nbsp;这个子命令会从第一个NameNode迁移到第二个，如果第一个NameNode处于备用状态,这个命令只是没有错误的转换第二个节点到活动状态。如果第一个NameNode处于活跃状态,试图将优雅地转换到备用状态。如果失败,过滤方法(如由dfs.ha.fencing.methods配置)将尝试过滤直到成功。只有在这个过程之后第二个NameNode会转换为活动状态，如果没有过滤方法成功，第二个nameNode将不会活动并返回一个错误。 getServiceState &nbsp;&nbsp;连接到NameNode，去判断现在的状态打印“standby”或者“active”去标准的输出。这个子命令可以被corn jobs或者是监控脚本使用，为了针对不同状态的NameNode采用不同的行为。 checkHealth &nbsp;&nbsp;连接NameNode检查健康，NameNode能够执行一些诊断,包括检查如果内部服务正在运行。如果返回0表明NameNode健康，否则返回非0.可以使用此命令用于监测目的。 &nbsp;&nbsp;注意：这个功能实现的不完整，目前除了NameNode完全的关闭，其他全部返回成功。]]></content>
      <categories>
        <category>后端</category>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记(2)-Mapreduce]]></title>
    <url>%2F2016%2F07%2F14%2F2016-07-14-Hadoop02-MapReduce%2F</url>
    <content type="text"><![CDATA[什么是MapReduce&nbsp;&nbsp;MapReduce是一种分布式计算模型，由Google提出，主要用于搜索领域，解决海量数据的计算问题。 &nbsp;&nbsp;MapReduce是处理大量半结构化数据集合的编程模型。编程模型是一种处理并结构化特定问题的方式。例如，在一个关系数据库中，使用一种集合语言执行查询，如SQL。告诉语言想要的结果，并将它提交给系统来计算出如何产生计算。还可以用更传统的语言(C++，Java)，一步步地来解决问题。这是两种不同的编程模型，MapReduce就是另外一种。 &nbsp;&nbsp;MapReduce和Hadoop是相互独立的，实际上又能相互配合工作得很好。 Yarn概述&nbsp;&nbsp;Yarn是一个分布式的资源管理系统，用以提高分布式的集群环境下的资源利用率，这些资源包括内存、IO、网络、磁盘等。其产生的原因是为了解决原MapReduce框架的不足。最初MapReduce的committer们还可以周期性的在已有的代码上进行修改，可是随着代码的增加以及原MapReduce框架设计的不足，在原MapReduce框架上进行修改变得越来越困难，所以MapReduce的committer们决定从架构上重新设计MapReduce,使下一代的MapReduce(MRv2/Yarn)框架具有更好的扩展性、可用性、可靠性、向后兼容性和更高的资源利用率以及能支持除了MapReduce计算框架外的更多的计算框架。 原MapReduce架构的不足 JobTracker是集群事务的集中处理点，存在单点故障。 JobTracker需要完成的任务太多，既要维护job的状态又要维护job的task的状态，造成过多的资源消耗。 在taskTracker端，用map/reduce task作为资源的表示过于简单，没有考虑到CPU、内存等资源情况，当把两个需要消耗大内存的task调度到一起，很容易出现OOM(Out Of Memory内存不足)。 把资源强制划分为map/reduce slot,当只有map task时，reduce slot不能用；当只有reduce task时，map slot不能用，容易造成资源利用不足。 MRv2/Yarn工作流程Yarn架构&nbsp;&nbsp;Yarn/MRv2最基本的想法是将原JobTracker主要的资源管理和job调度/监视功能分开作为两个单独的守护进程。 &nbsp;&nbsp;有一个全局的ResourceManager(RM)和每个Application有一个ApplicationMaster(AM)，Application相当于map-reduce job或者DAG jobs。 &nbsp;&nbsp;ResourceManager和NodeManager(NM)组成了基本的数据计算框架。ResourceManager协调集群的资源利用，任何client或者运行着的applicatitonMaster想要运行job或者task都得向RM申请一定的资源。ApplicatonMaster是一个框架特殊的库，对于MapReduce框架而言有它自己的AM实现，用户也可以实现自己的AM，在运行的时候，AM会与NM一起来启动和监视tasks。 ResourceManager ResourceManager作为资源的协调者有两个主要的组件：Scheduler和ApplicationsManager(AsM)。 Scheduler负责分配最少但满足application运行所需的资源量给Application。Scheduler只是基于资源的使用情况进行调度，并不负责监视/跟踪application的状态，当然也不会处理失败的task。RM使用resource container概念来管理集群的资源，resource container是资源的抽象，每个container包括一定的内存、IO、网络等资源，不过目前的实现只包括内存一种资源。 ApplicationsManager负责处理client提交的job以及协商第一个container以供applicationMaster运行，并且在applicationMaster失败的时候会重新启动applicationMaster。下面阐述RM具体完成的一些功能。 资源调度：Scheduler从所有运行着的application收到资源请求后构建一个全局的资源分配计划，然后根据application特殊的限制以及全局的一些限制条件分配资源。 资源监视：Scheduler会周期性的接收来自NM的资源使用率的监控信息，另外applicationMaster可以从Scheduler得到属于它的已完成的container的状态信息。 Application提交： client向AsM获得一个applicationIDclient将application定义以及需要的jar包. client将application定义以及需要的jar包文件等上传到hdfs的指定目录，由yarn-site.xml的yarn.app.mapreduce.am.staging-dir指定. client构造资源请求的对象以及application的提交context发送给AsM. AsM接收application的提交context. AsM根据application的信息向Scheduler协商一个Container供applicationMaster运行，然后启动applicationMaster. 向该container所属的NM发送launchContainer信息启动该container,也即启动applicationMaster、AsM向client提供运行着的AM的状态信息. AM的生命周期：AsM负责系统中所有AM的生命周期的管理。AsM负责AM的启动，当AM启动后，AM会周期性的向AsM发送heartbeat，默认是1s，AsM据此了解AM的存活情况，并且在AM fail时负责重启AM，若是一定时间过后(默认10分钟)没有收到AM的heartbeat，AsM就认为该AM已经fail。 NodeManager &nbsp;&nbsp;NM主要负责启动RM分配给AM的container以及代表AM的container，并且会监视container的运行情况。在启动container的时候，NM会设置一些必要的环境变量以及将container运行所需的jar包、文件等从hdfs下载到本地，也就是所谓的资源本地化；当所有准备工作做好后，才会启动代表该container的脚本将程序启动起来。启动起来后，NM会周期性的监视该container运行占用的资源情况，若是超过了该container所声明的资源量，则会kill掉该container所代表的进程。 &nbsp;&nbsp;NM还提供了一个简单的服务以管理它所在机器的本地目录。Applications可以继续访问本地目录即使那台机器上已经没有了属于它的container在运行。例如，Map-Reduce应用程序使用这个服务存储map output并且shuffle它们给相应的reduce task。 &nbsp;&nbsp;NM上还可以扩展自己的服务，yarn提供了一个yarn.nodemanager.aux-services的配置项，通过该配置，用户可以自定义一些服务，例如Map-Reduce的shuffle功能就是采用这种方式实现的。 NM在本地为每个运行着的application生成如下的目录结构： Container目录下的目录结构如下： &nbsp;&nbsp;在启动一个container的时候，NM就执行该container的default_container_executor.sh，该脚本内部会执行launch_container.sh。launch_container.sh会先设置一些环境变量，最后启动执行程序的命令。对于MapReduce而言，启动AM就执行org.apache.hadoop.mapreduce.v2.app.MRAppMaster；启动map/reduce task就执行org.apache.hadoop.mapred.YarnChild。 ApplicationMaster &nbsp;&nbsp;ApplicationMaster是一个框架特殊的库，对于Map-Reduce计算模型而言有它自己的ApplicationMaster实现，对于其他的想要运行在yarn上的计算模型而言，必须得实现针对该计算模型的ApplicationMaster用以向RM申请资源运行task，比如运行在yarn上的spark框架也有对应的ApplicationMaster实现，归根结底，yarn是一个资源管理的框架，并不是一个计算框架，要想在yarn上运行应用程序，还得有特定的计算框架的实现。 工作流程 JobClient向ResourceManager(AsM)申请提交一个job。 RM返回jobId和job提交路径。 JobClient提交job相关的文件。 向RM汇报提交完成。 RM将job写入Job Queue。 NodeManager(NM)向Job Queue领取任务。 ApplicationMaster(AM)启动,向RM进行注册。 RM向AM返回资源信息。 AM启动map。 当所有map任务完成后,AM启动reduce。 AM监视运行着的task直到完成,当task失败时,申请新的container运行失败的task。 当每个map/reduce task完成后,AM运行MR OutputCommitter的cleanup 代码，进行一些收尾工作。 当所有的map/reduce完成后,AM运行OutputCommitter的必要的job commit或者abort APIs。 AM注销自己。 Shuffle过程 Map 每个输入分片会让一个map任务来处理，默认情况下，以HDFS的一个块的大小（默认为64M）为一个分片，当然我们也可以设置块的大小。map输出的结果会暂且放在一个环形内存缓冲区中（该缓冲区的大小默认为100M，由io.sort.mb属性控制），当该缓冲区快要溢出时（默认为缓冲区大小的80%，由io.sort.spill.percent属性控制），会在本地文件系统中创建一个溢出文件，将该缓冲区中的数据写入这个文件。 在写入磁盘之前，线程首先根据reduce任务的数目将数据划分为相同数目的分区，也就是一个reduce任务对应一个分区的数据。这样做是为了避免有些reduce任务分配到大量数据，而有些reduce任务却分到很少数据，甚至没有分到数据的尴尬局面。其实分区就是对数据进行hash的过程。然后对每个分区中的数据进行排序，如果此时设置了Combiner，将排序后的结果进行Combia操作，这样做的目的是让尽可能少的数据写入到磁盘。 当map任务输出最后一个记录时，可能会有很多的溢出文件，这时需要将这些文件合并。合并的过程中会不断地进行排序和combia操作，目的有两个： 尽量减少每次写入磁盘的数据量； 尽量减少下一复制阶段网络传输的数据量。最后合并成了一个已分区且已排序的文件。 为了减少网络传输的数据量，这里可以将数据压缩，只要将mapred.compress.map.out设置为true就可以了。 将分区中的数据拷贝给相对应的reduce Task。有人可能会问：分区中的数据怎么知道它对应的reduce是哪个呢？其实map任务一直和其父TaskTracker保持联系，而TaskTracker又一直和JobTracker保持心跳。所以JobTracker中保存了整个集群中的宏观信息。只要reduce任务向JobTracker获取对应的map输出位置就ok了哦。 Reduce Reduce会接收到不同map任务传来的数据，并且每个map传来的数据都是有序的。如果reduce端接受的数据量相当小，则直接存储在内存中（缓冲区大小由mapred.job.shuffle.input.buffer.percent属性控制，表示用作此用途的堆空间的百分比），如果数据量超过了该缓冲区大小的一定比例（由mapred.job.shuffle.merge.percent决定），则对数据合并后溢写到磁盘中。 随着溢写文件的增多，后台线程会将它们合并成一个更大的有序的文件，这样做是为了给后面的合并节省时间。其实不管在map端还是reduce端，MapReduce都是反复地执行排序，合并操作。 合并的过程中会产生许多的中间文件（写入磁盘了），但MapReduce会让写入磁盘的数据尽可能地少，并且最后一次合并的结果并没有写入磁盘，而是直接输入到reduce函数。 WordCount案例Mapper1234567891011121314public class MyWordCountMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 读取一行的value String line = value.toString(); // 按照规则切分 String[] words = line.split(" "); // 按照&lt;单词,1&gt;的格式输出 for (String word : words) &#123; context.write(new Text(word), new LongWritable(1)); &#125; &#125;&#125; Reducer123456789101112131415public class MyWordCountReducer extends Reducer&lt;Text, LongWritable, Text, LongWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;LongWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 初始化计数器 long count = 0; // 迭代values,累加计数器计算出总次数 for (LongWritable value : values) &#123; count += value.get(); &#125; // 输出&lt;单词,总次数&gt; context.write(key, new LongWritable(count)); &#125;&#125; Main1234567891011121314151617181920212223242526272829303132public class MyWordCountDriver &#123; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); // 构造一个job对象 Job wordCountJob = Job.getInstance(conf); // 指定job用到的jar包位置,这里使用当前类 wordCountJob.setJarByClass(MyWordCountDriver.class); // 指定mapper wordCountJob.setMapperClass(MyWordCountMapper.class); // 指定reducer wordCountJob.setReducerClass(MyWordCountReducer.class); // 指定mapper输出key/value的类型 wordCountJob.setMapOutputKeyClass(Text.class); wordCountJob.setMapOutputValueClass(LongWritable.class); // 指定reducer输出key/value的类型 wordCountJob.setOutputKeyClass(Text.class); wordCountJob.setOutputValueClass(LongWritable.class); // 指定输入数据的路径 FileInputFormat.setInputPaths(wordCountJob, new Path(args[0])); // 指定输出结果的路径 FileOutputFormat.setOutputPath(wordCountJob, new Path(args[1])); // 通过yarn客户端进行提交,参数2为是否打印到控制台 wordCountJob.waitForCompletion(true); &#125;&#125; 启动MapReduce&nbsp;&nbsp;方式1: 将程序打成jar包,上传到hadoop中执行。hadoop jar [mainClass] args… &nbsp;&nbsp;方式2: 将程序打成jar包,在本地IDE上直接运行(需要代码指定jar)。 自定义Sortbean123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class FlowBean implements WritableComparable&lt;FlowBean&gt; &#123; private Long upFlow; private Long downFlow; private Long sumFlow; public void setAll(Long upFlow, Long downFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; public Long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(Long upFlow) &#123; this.upFlow = upFlow; &#125; public Long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(Long downFlow) &#123; this.downFlow = downFlow; &#125; public Long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(Long sumFlow) &#123; this.sumFlow = sumFlow; &#125; @Override public String toString() &#123; return "FlowBean [upFlow=" + upFlow + ", downFlow=" + downFlow + ", sumFlow=" + sumFlow + "]"; &#125; /** * 序列化 */ @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; /** * 反序列化 */ @Override public void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); downFlow = in.readLong(); sumFlow = in.readLong(); &#125; /** * 降序排序 -1 大于 0 等于 1小于 */ @Override public int compareTo(FlowBean o) &#123; // 如果当前类的总和大于其他类的总和 则返回-1(大于) false 1(小于) return this.sumFlow &gt; o.getSumFlow() ? -1 : 1; &#125;&#125; main1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class FlowSummarySort &#123; /** * 因为只有key才能进行排序,所以输出key为FlowBean * * @author sylvanasp * @version 1.0 */ public static class FlowSummarySortMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] fields = StringUtils.split(line, "\t"); String phoneNum = fields[0]; Long upFlow = Long.parseLong(fields[1]); Long downFlow = Long.parseLong(fields[2]); FlowBean flowBean = new FlowBean(); flowBean.setAll(upFlow, downFlow); context.write(flowBean, new Text(phoneNum)); &#125; &#125; /** * 因为在mapper中已经完成了排序,所以reducer中需要将phoneNum重新设置为key * * @author sylvanasp * @version 1.0 */ public static class FlowSummarySortReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; &#123; @Override protected void reduce(FlowBean bean, Iterable&lt;Text&gt; phoneNum, Context context) throws IOException, InterruptedException &#123; // 因为每个bean都是完全独立的,所以Iterable中只有一个数据 for (Text phoneNumKey : phoneNum) &#123; context.write(phoneNumKey, bean); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(FlowSummarySort.class); job.setMapperClass(FlowSummarySortMapper.class); job.setReducerClass(FlowSummarySortReducer.class); job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); int result = job.waitForCompletion(true) ? 0 : 1; System.exit(result); &#125;&#125; 自定义Partitionpartitioner123456789101112131415161718192021222324public class MyPartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; // 使用map模拟数据库 private static HashMap&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;(); // 初始化分区规则 static &#123; map.put("136", 0); map.put("137", 1); map.put("138", 2); map.put("139", 3); &#125; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; // 获取手机号前3位 String phonePrefix = key.toString().substring(0, 3); // 根据手机号前缀获得对应的分区编号 Integer partitionId = map.get(phonePrefix); // 如果手机号不在分区规则内,则分配到分区4。 return partitionId == null ? 4 : partitionId; &#125;&#125; main123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class FlowSummaryPartition &#123; public static class FlowSummaryPartitionMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] fields = StringUtils.split(line, "\t"); String phoneNum = fields[1]; Long upFlow = Long.parseLong(fields[fields.length - 3]); Long downFlow = Long.parseLong(fields[fields.length - 2]); FlowBean flowBean = new FlowBean(); flowBean.setAll(upFlow, downFlow); context.write(new Text(phoneNum), flowBean); &#125; &#125; public static class FlowSummaryPartitionReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; beans, Context context) throws IOException, InterruptedException &#123; long upSum = 0; long downSum = 0; for (FlowBean bean : beans) &#123; upSum += bean.getUpFlow(); downSum += bean.getDownFlow(); &#125; FlowBean flowBean = new FlowBean(); flowBean.setAll(upSum, downSum); context.write(key, flowBean); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(FlowSummaryPartition.class); job.setMapperClass(FlowSummaryPartitionMapper.class); job.setReducerClass(FlowSummaryPartitionReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 设置分区器 job.setPartitionerClass(MyPartitioner.class); // 设置Reducer Task 实例数量 (与分区数一致) job.setNumReduceTasks(5); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); int result = job.waitForCompletion(true) ? 0 : 1; System.exit(result); &#125;&#125; END 部分资料来源于http://blog.sina.com.cn/s/blog_829a682d0101lc9d.html&amp;http://weixiaolu.iteye.com/blog/1474172]]></content>
      <categories>
        <category>后端</category>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop学习笔记(1)-HDFS]]></title>
    <url>%2F2016%2F07%2F12%2F2016-07-12-Hadoop-HDFS%2F</url>
    <content type="text"><![CDATA[Hadoop概述Hadoop是一个由Apache基金会所开发的分布式系统基础架构。 &nbsp;&nbsp;Hadoop 由许多元素构成。其最底部是 Hadoop Distributed File System（HDFS），它存储 Hadoop 集群中所有存储节点上的文件。HDFS的上一层是MapReduce 引擎，该引擎由 JobTrackers 和 TaskTrackers 组成。 HDFS &nbsp;&nbsp;对外部客户机而言，HDFS就像一个传统的分级文件系统。可以创建、删除、移动或重命名文件，等等。但是 HDFS 的架构是基于一组特定的节点构建的，这是由它自身的特点决定的。这些节点包括 NameNode（仅一个），它在 HDFS 内部提供元数据服务；DataNode，它为 HDFS 提供存储块。由于仅存在一个 NameNode，因此这是 HDFS 的一个缺点（单点失败）。 &nbsp;&nbsp;存储在 HDFS 中的文件被分成块，然后将这些块复制到多个计算机中（DataNode）。这与传统的 RAID 架构大不相同。块的大小（通常为 64MB）和复制的块数量在创建文件时由客户机决定。NameNode 可以控制所有文件操作。HDFS 内部的所有通信都基于标准的 TCP/IP 协议。 &nbsp;&nbsp;HDFS和现有的分布式文件系统有很多共同点。但同时，它和其他的分布式文件系统的区别也是很明显的。HDFS是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用。HDFS放宽了一部分POSIX约束，来实现流式读取文件系统数据的目的。HDFS在最开始是作为Apache Nutch搜索引擎项目的基础架构而开发的。HDFS是Apache Hadoop Core项目的一部分。 NameNode &nbsp;&nbsp;NameNode 是一个通常在HDFS实例中的单独机器上运行的软件。它负责管理文件系统名称空间和控制外部客户机的访问。NameNode 决定是否将文件映射到 DataNode 上的复制块上。对于最常见的 3 个复制块，第一个复制块存储在同一机架的不同节点上，最后一个复制块存储在不同机架的某个节点上。 &nbsp;&nbsp;实际的 I/O事务并没有经过 NameNode，只有表示 DataNode 和块的文件映射的元数据经过 NameNode。当外部客户机发送请求要求创建文件时，NameNode 会以块标识和该块的第一个副本的 DataNode IP 地址作为响应。这个 NameNode 还会通知其他将要接收该块的副本的 DataNode。 &nbsp;&nbsp;NameNode 在一个称为FsImage的文件中存储所有关于文件系统名称空间的信息。这个文件和一个包含所有事务的记录文件（这里是 EditLog）将存储在 NameNode 的本地文件系统上。FsImage 和 EditLog 文件也需要复制副本，以防文件损坏或 NameNode 系统丢失。 &nbsp;&nbsp;NameNode本身不可避免地具有SPOF（Single Point Of Failure）单点失效的风险，主备模式并不能解决这个问题，通过Hadoop Non-stop namenode才能实现100% uptime可用时间。 DataNode &nbsp;&nbsp;DataNode 也是一个通常在 HDFS实例中的单独机器上运行的软件。Hadoop 集群包含一个 NameNode 和大量 DataNode。DataNode通常以机架的形式组织，机架通过一个交换机将所有系统连接起来。Hadoop 的一个假设是：机架内部节点之间的传输速度快于机架间节点的传输速度。 &nbsp;&nbsp;DataNode 响应来自 HDFS 客户机的读写请求。它们还响应来自 NameNode 的创建、删除和复制块的命令。NameNode 依赖来自每个 DataNode 的定期心跳（heartbeat）消息。每条消息都包含一个块报告，NameNode 可以根据这个报告验证块映射和其他文件系统元数据。如果 DataNode 不能发送心跳消息，NameNode 将采取修复措施，重新复制在该节点上丢失的块。 HDFS体系结构 NameNode:唯一的master节点,管理HDFS的名称空间和数据块映射信息、配置副本策略和处理客户端请求。 Secondary NameNode:辅助NameNode，分担NameNode工作，定期合并fsimage和edits并推送给NameNode，紧急情况下可辅助恢复NameNode。 DataNode:Slave节点，实际存储数据、执行数据块的读写并汇报存储信息给NameNode。 FSImage:元数据镜像文件。 Edits:元数据的操作日志。 HDFSWriteOperation &nbsp;&nbsp;在分布式文件系统中，需要确保数据的一致性。对于HDFS来说，直到所有要保存数据的DataNodes确认它们都有文件的副本时，数据才被认为写入完成。因此，数据一致性是在写的阶段完成的。一个客户端无论选择从哪个DataNode读取，都将得到相同的数据。 客户端请求NameNode,表示写入文件。 NameNode响应客户端,并告诉客户端将文件保存到DataNodeA、B、D。 客户端连接DataNodeA写入文件,DataNode集群内完成复制。 DataNodeA将文件副本发送给DataNodeB。 DataNodeB将文件副本发送给DataNodeD。 DataNodeD返回确认消息给DataNodeB。 DataNodeB返回确认消息给DataNodeA。 DataNodeA返回确认消息给客户端,写入完成。 Client调用DistributedFileSystem的create()函数创建新文件。 DistributedFileSystem使用RPC调用NameNode创建一个没有block关联的新文件,NameNode在创建之前将进行校验,如果校验通过,NameNode则创建一个新文件并记录一条记录,否则抛出IO异常。 前两步成功后,将会返回一个DFSOutputStream对象,DFSOutputStream可以协调NameNode与DataNode,当客户端写入数据到DFSOutputStream,DFSOutputStream会将数据分割为一个一个Packet(数据包),并写入数据队列。 DataStreamer处理数据队列,它会先去询问NameNode存储到哪几个DataNode,例如Replication为3,则会去找到3个最适合的DataNode。DataStreamer会将DataNode排成一个Pipeline,它会将Packet按队列输出到管道中的第一个DataNode,第一个DataNode又会把Packet输出到第二个DataNode,直到最后一个DataNode。 DataStreamer中还有一个Ack Queue,Ack Queue之中也含有Packet。Ack Queue负责接收DataNode的确认响应,当Pipeline中的所有DataNode都确认完毕后,Ack Queue将移除对应的Packet。 Client完成数据写入,关闭流。 DataStreamer等待Ack Queue信息,当收到最后一个信息时,通知NameNode把文件标记为完成。 HDFSReadOperation 客户端请求NameNode,表示读取文件。 NameNode响应客户端,将block(数据块)的信息发送给客户端。 客户端检查数据块信息,连接相关的DataNode。 DataNodeA将block1发送给客户端。 DataNodeB将block2发送给客户端。 拼接数据,读取完成。 Client调用FileSystem的open()函数打开希望读取的文件。 DistributedFileSystem使用RPC调用NameNode确定文件起始块的位置，同一Block按照重复数会返回多个位置，这些位置按照Hadoop集群拓扑结构排序，距离客户端近的排在前面。 前两步成功后,将会返回一个DFSInputStream对象,DFSInputStream可以协调NameNode与DataNode。客户端对DFSInputStream输入流调用read()函数。 DFSInputStream连接距离最近的DataNode，通过对数据流反复调用read()函数，可以将数据从DataNode传输到客户端。 当到达Block的末端时，DFSInputStream会关闭与该DataNode的连接，然后寻找下一个Block的最佳DataNode，这些操作对客户端来说是透明的。 客户端完成读取，对FSDataInputStream调用close()关闭文件读取。 HDFSShell命令&nbsp;&nbsp;既然 HDFS 是存取数据的分布式文件系统，那么对 HDFS 的操作，就是文件系统的基本 操作，比如文件的创建、修改、删除、修改权限等，文件夹的创建、删除、重命名等。对 HDFS 的操作命令类似于 Linux 的 shell 对文件的操作，如 ls、mkdir、rm 等。 我们执行以下操作的时候，一定要确定 hadoop 是正常运行的，使用 jps 命令确保看到 各个 hadoop 进程。 命令名 格式 含义 -ls -ls&lt;路径&gt; 查看指定路径的当前目录结构 -lsr -lsr&lt;路径&gt; 递归查看指定路径的目录结构 -du -du&lt;路径&gt; 统计目录下个文件大小 -dus -dus&lt;路径&gt; 汇总统计目录下文件(夹)大小 -count -count[-q]&lt;路径&gt; 统计文件(夹)数量 -mv -mv&lt;源路径&gt;&lt;目的路径&gt; 移动 -cp -cp&lt;源路径&gt;&lt;目的路径&gt; 复制 -rm -rm[-skipTrash]&lt;路径&gt; 删除文件/空白文件夹 -rmr -rmr[-skipTrash]&lt;路径&gt; 递归删除 -put -put&lt;多个 linux 上的文件&gt; 上传文件 -copyFromLocal -copyFromLocal&lt;多个 linux 上的文件&gt; 从本地复制 -moveFromLocal -moveFromLocal&lt;多个 linux 上的文件&gt; 从本地移动 -getmerge -getmerge&lt;源路径&gt; 合并到本地 -cat -cat 查看文件内容 -text -text 查看文件内容 -copyToLocal -copyToLocal[-ignoreCrc][-crc][hdfs 源路 径][linux 目的路径] 复制到本地 -moveToLocal -moveToLocal[-crc] 移动到本地 -mkdir -mkdir 创建空白文件夹 -setrep -setrep[-R][-w]&lt;副本数&gt;&lt;路径&gt; 修改副本数量 -touchz -touchz&lt;文件路径&gt; 创建空白文件 -stat -stat[format]&lt;路径&gt; 显示文件统计信息 -tail -tail[-f]&lt;文件&gt; 查看文件尾部信息 -chmod -chmod[-R]&lt;权限模式&gt;[路径] 修改权限 -chown -chown[-R][属主][:[属组]] 路径 修改属主 -chgrp -chgrp[-R] 属组名称 路径 修改属组 -help -help[命令选项] 帮助 使用JAVA操作HDFS123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169public class HdfsTest &#123; private Configuration conf = null; private FileSystem fs = null; private FSDataInputStream DFSInputStream = null; /** * 初始化FlieSystem * * @throws IOException * @throws InterruptedException */ @Before public void init() throws IOException, InterruptedException &#123; conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://192.168.145.145:9000"); fs = fs.get(URI.create("hdfs://192.168.145.145:9000"), conf, "root"); &#125; /** * 读取文件 * * @throws IOException * @throws IllegalArgumentException */ @Test public void testReadAsOpen() throws IllegalArgumentException, IOException &#123; Path path = null; try &#123; path = new Path("/test"); if (fs.exists(path)) &#123; DFSInputStream = fs.open(path); IOUtils.copyBytes(DFSInputStream, System.out, conf); &#125; &#125; finally &#123; IOUtils.closeStream(DFSInputStream); fs.close(); &#125; &#125; /** * 上传本地文件 * * @throws IOException */ @Test public void testUpload() throws IOException &#123; Path src = null; Path dst = null; try &#123; src = new Path("f:/saber_by_wlop-d8tjwa5.jpg");// 原路径 dst = new Path("/saber.jpg");// 目标路径 // 参数1为是否删除原文件,true为删除,默认为false fs.copyFromLocalFile(false, src, dst); // 打印文件路径 System.out.println("Upload to " + conf.get("fs.default.name")); System.out.println("----------------------------------------"); FileStatus[] fileStatus = fs.listStatus(dst); for (FileStatus file : fileStatus) &#123; System.out.println(file.getPath()); &#125; &#125; finally &#123; fs.close(); &#125; &#125; /** * 下载文件 * * @throws IOException */ @Test public void testDownload() throws IOException &#123; Path src = null; Path dst = null; try &#123; if (fs.exists(src)) &#123; src = new Path("/saber.jpg"); dst = new Path("D:/temp/"); fs.copyToLocalFile(false, src, dst); // 打印文件路径 System.out.println("Download from " + conf.get("fs.default.name")); System.out.println("--------------------------------------------"); // 迭代路径,参数2为是否递归迭代 RemoteIterator&lt;LocatedFileStatus&gt; iterator = fs.listFiles(src, true); while (iterator.hasNext()) &#123; LocatedFileStatus fileStatus = iterator.next(); System.out.println(fileStatus.getPath()); &#125; &#125; &#125; finally &#123; fs.close(); &#125; &#125; /** * 创建目录 * * @throws IOException */ @Test public void testMkdir() throws IOException &#123; Path path = null; try &#123; path = new Path("/create01"); // 判断目录是否已存在 boolean exists = fs.exists(path); if (!exists) &#123; // 创建目录 boolean mkdirs = fs.mkdirs(path); if (mkdirs) &#123; System.out.println("create dir success!"); &#125; else &#123; System.out.println("create dir failure!"); &#125; &#125; &#125; finally &#123; fs.close(); &#125; &#125; /** * 重命名文件 * * @throws IOException */ @Test public void testRename() throws IOException &#123; Path oldPath = null; Path newPath = null; try &#123; oldPath = new Path("/saber.jpg"); newPath = new Path("/saber01.jpg"); if (fs.exists(oldPath)) &#123; boolean rename = fs.rename(oldPath, newPath); if (rename) &#123; System.out.println("rename success!"); &#125; else &#123; System.out.println("rename failure!"); &#125; &#125; &#125; finally &#123; fs.close(); &#125; &#125; /** * 删除文件 * * @throws IOException */ @Test public void testDelete() throws IOException &#123; Path path = null; try &#123; path = new Path("/saber01.jpg"); if (fs.exists(path)) &#123; boolean delete = fs.deleteOnExit(path); if (delete) &#123; System.out.println("delete success!"); &#125; else &#123; System.out.println("delete failure!"); &#125; &#125; &#125; finally &#123; fs.close(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>后端</category>
        <category>大数据</category>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyCat快速入门]]></title>
    <url>%2F2016%2F07%2F09%2F2016-07-09-MyCat%2F</url>
    <content type="text"><![CDATA[MyCat概述 &nbsp;&nbsp;MyCat是基于Cobar二次开发的数据库中间件。它可以低成本的将现有的单机数据库和应用平滑迁移到“云”端，解决数据存储和业务规模迅速增长情况下的数据瓶颈问题。 &nbsp;&nbsp;从定义和分类来看，它是一个开源的分布式数据库系统，是一个实现了MySQL协议的的Server，前端用户可以把 它看作是一个数据库代理，用MySQL客户端工具和命令行访问，而其后端可以用MySQL原生（Native）协议与多个MySQL服务 器通信，也可以用JDBC协议与大多数主流数据库服务器通信，其核心功能是分表分库，即将一个大表水平分割为N个小表，存储 在后端MySQL服务器里或者其他数据库里。 &nbsp;&nbsp;Mycat发展到目前的版本，已经不是一个单纯的MySQL代理了，它的后端可以支持MySQL、SQL Server、Oracle、DB2、 PostgreSQL等主流数据库，也支持MongoDB这种新型NoSQL方式的存储，未来还会支持更多类型的存储。而在最终用户看 来，无论是那种存储方式，在Mycat里，都是一个传统的数据库表，支持标准的SQL语句进行数据的操作，这样一来，对前端业 务系统来说，可以大幅降低开发难度，提升开发速度，在测试阶段，可以将一个表定义为任何一种Mycat支持的存储方式，比如 MySQL的MyASIM表、内存表、或者MongoDB、LevelDB以及号称是世界上最快的内存数据库MemSQL上。试想一下，用户表 存放在MemSQL上，大量读频率远超过写频率的数据如订单的快照数据存放于InnoDB中，一些日志数据存放于MongoDB中， 而且还能把Oracle的表跟MySQL的表做关联查询，你是否有一种不能呼吸的感觉？而未来，还能通过Mycat自动将一些计算分析 后的数据灌入到Hadoop中，并能用Mycat+Storm/Spark Stream引擎做大规模数据分析，看到这里，你大概明白了，Mycat是 什么？Mycat就是BigSQL，Big Data On SQL Database。 MyCat特点 支持SQL92标准。 支持Mysql集群,可以作为Proxy使用。 支持JDBC连接ORACLE、DB2、SQL Server。 支持galera for mysql集群，percona-cluster或者mariadb cluster，提供高可用性数据分片集群。 支持自动故障切换,实现高可用。 支持读写分离,Mysql双主多从,以及一主多从模式。 支持全局表。 支持独有的基于E-R关系分片策略,实现了高效的表关联查询。 支持多平台,部署简单。 MyCat原理&nbsp;&nbsp;Mycat的原理中最重要的一个动词是“拦截”，它拦截了用户发送过来的SQL语句，首先对SQL语句做了一些特定的分析：如分片分析、路由分析、读写分离分析、缓存分析等，然后将此SQL发往后端的真实数据库，并将返回的结果做适当的处理，最终再返回给用户。 分片策略 MyCat支持横向分片与纵向分片。 横向分片:一个表的数据分割到多个节点上,按照行分隔。 纵向分片:一个数据库中有多个表A、B、C,A存储到节点1,B存储到节点2,C存储到节点3。 MyCat通过定义表的分片规则来实现分片,每个表可以捆绑一个分片规则,每个分片规则指定一个分片字段并绑定一个函数,实现动态分片算法。 schema:逻辑库,一个逻辑库中定义了所包含的Table。 table 逻辑表:既然有逻辑库，那么就会有逻辑表，分布式数据库中，对应用来说，读写数据的表就是逻辑表。逻辑表，可以是数据切分后，分布在一个或多个分片库中，也可以不做数据切分，不分片，只有一个表构成。 分片表:指那些原有的很大数据的表，需要切分到多个数据库的表，这样，每个分片都有一部分数据，所有分片构成了完整的 数据。 例如在mycat配置中的t_node就属于分片表，数据按照规则被分到dn1,dn2两个分片节点(dataNode)上。 1&lt;table name=&quot;t_node&quot; primaryKey=&quot;vid&quot; autoIncrement=&quot;true&quot; dataNode=&quot;dn1,dn2&quot; rule=&quot;rule1&quot; /&gt; **非分片表**:如果一个数据库中并不是所有的表都有很大的数据,某些表是可以不用进行切分的, 非分片表是相对于分片表来说的,就是不需要进行数据切分的表。 例如下面配置的t_node,只存在于一个分片节点dn1上。 1&lt;table name=&quot;t_node&quot; primaryKey=&quot;vid&quot; autoIncrement=&quot;true&quot; dataNode=&quot;dn1&quot; / **ER表**:关系型数据库是基于实体关系模型(Entity-RelationshipModel)之上的,通过其描述了真实世界中的事物与关系。MyCat提出了基于E-R关系的数据分片策略,子表的记录与所关联的父表记录存放在同一个数据分片中,即子表依赖于父表,通过表分组(TableGroup)保证数据Join不会跨库操作。 **全局表**:一个真实的业务系统中，往往存在大量的类似字典表的表，这些表基本上很少变动，字典表具有以下几个特性： - 变动不频繁。 - 数据量总体变化不大。 - 数据规模不大。 对于这类的表，在分片的情况下，当业务表因为规模而进行分片以后，业务表与这些附属的字典表之间的关联，就成了比较棘手的问题，所以Mycat中通过数据冗余来解决这类表的join，即所有的分片都有一份数据的拷贝，所有将字典表或者符合字典表特 性的一些表定义为全局表。 dataNode:分片节点。数据切分后,一个大表被切分到不同的分片数据库上面,每个表分片所在的数据库就是分片节点。 dataHost:节点主机。数据切分后，每个分片节点（dataNode）不一定都会独占一台机器，同一机器上面可以有多个分片数据库，这样一个或多个分片节点（dataNode）所在的机器就是节点主机（dataHost）,为了规避单节点主机并发数限制，尽量将读写压力高的分片节点 （dataNode）均衡的放在不同的节点主机（dataHost）. rule:分片规则。按照某种业务规则把数据分到某个分片的规则就是分片规则,数据切分选择合适的分片规则非常重要,将极大的避免后续数据处理的难度。 sequence:全局序列号。数据切分后，原有的关系数据库中的主键约束在分布式条件下将无法使用，因此需要引入外部机制保证数据唯一性标识，这种保证全局性的数据唯一标识的机制就是全局序列号（sequence）。 快速入门&amp;&nbsp;&nbsp;MyCat是是使用JAVA语言开发的,所以需要先安装JAVA运行环境,并且要求JDK版本在1.7以上。 1.环境准备JDK下载地址:http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html MySQL下载地址:http://dev.mysql.com/downloads/mysql/5.5.html#downloads MyCat下载地址:https://github.com/MyCATApache/Mycat-download 2.MyCat的安装 将下载的MyCat压缩包上传到linux服务器。 解压缩MyCat压缩包。 3.MyCat目录结构 bin程序目录 存放了window版本和linux版本，除了提供封装成服务的版本之外，也提供了nowrap的shell脚本命令，方便大 家选择和修改，进入到bin目录： Windows下运行：运行: mycat.bat console 在控制台启动程序，也可以装载成服务，若此程序运行有问题，也可以运行 startup_nowrap.bat，确保java命令可以在命令执行. Windows下将MyCAT做成系统服务：MyCAT提供warp方式的命令，可以将MyCAT安装成系统服务并可启动和停止。 进入bin目录下执行命令 mycat install 执行安装mycat服务. 输入 mycat start 启动mycat服务. conf目录存放配置文件，server.xml是Mycat服务器参数调整和用户授权的配置文件，schema.xml是逻辑库定义和表以及分片定义的配置文件，rule.xml是分片规则的配置文件，分片规则的具体一些参数信息单独存放为文件，也在这个目录下，配置文件修改，需要重启Mycat或者通过9066端口reload. lib目录主要存放mycat依赖的一些jar文件. 日志存放在logs/mycat.log中，每天一个文件，日志的配置是在conf/log4j.xml中，根据自己的需要，可以调整输出级别为 debug，debug级别下，会输出更多的信息，方便排查问题. 4.服务启动 MyCAT在Linux中部署启动时，首先需要在Linux系统的环境变量中配置MYCAT_HOME,操作方式如下： vi /etc/profile,在系统环境变量文件中增加 MYCAT_HOME=/usr/local/Mycat 执行 source /etc/profile 命令，使环境变量生效。 如果是在多台linux系统中组建的MyCat集群,则需要在MyCat Server所在的服务器上配置对其他Ip和主机名的映射。 vi /etc/hosts 例如192.168.145.1 test_1192.168.145.2 test_2 配置完毕后,可以cd到/usr/local/mycat/bin目录下执行 ./mycat start 启动服务。注:MyCat的默认服务端口为8066. MyCat切分数据1.配置schema.xmlSchema.xml作为MyCat中重要的配置文件之一，管理着MyCat的逻辑库、表、分片规则、DataNode以及DataSource。弄懂这些配置，是正确使用MyCat的前提。这里就一层层对该文件进行解析。 schema标签 属性名 值 数量限制 dataNode String 0..1 checkSQLschema Boolean 1 sqlMaxLimit Integer 1 dataNode该属性用于绑定逻辑库到某个具体的database上，如果定义了这个属性，那么这个逻辑库就不能工作在分库分表模式下了。也就是说对这个逻辑库的所有操作会直接作用到绑定的dataNode上，这个schema就可以用作读写分离和主从切换，具体如下配置: 123&lt;schema name=&quot;USERDB&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot; dataNode=&quot;dn1&quot;&gt; &lt;!—这里不能配置任何逻辑表信息--&gt; &lt;/schema&gt; 那么现在USERDB就绑定到dn1所配置的具体database上，可以直接访问这个database。当然该属性只能配置绑定到一个 database上，不能绑定多个dn。 checkSQLschema当该值设置为 true 时，如果我们执行语句select * from TESTDB.travelrecord;则MyCat会把语句修改为select * from travelrecord;。即把表示schema的字符去掉，避免发送到后端数据库执行时报（ERROR 1146 (42S02): Table ‘testdb.travelrecord’ doesn’t exist）。 不过，即使设置该值为 true ，如果语句所带的是并非是schema指定的名字，例如：select * from db1.travelrecord; 那么 MyCat并不会删除db1这个字段，如果没有定义该库的话则会报错，所以在提供SQL语句的最好是不带这个字段。 sqlMaxLimit当该值设置为某个数值时。每条执行的SQL语句，如果没有加上limit语句，MyCat也会自动的加上所对应的值。例如设置值为 100，执行select fromTESTDB.travelrecord;的效果为和执行select from TESTDB.travelrecord limit 100;相同。 不设置该值的话，MyCat默认会把查询到的信息全部都展示出来，造成过多的输出。所以，在正常使用中，还是建议加上一个 值，用于减少过多的数据返回。当然SQL语句中也显式的指定limit的大小，不受该属性的约束。需要注意的是，如果运行的schema为非拆分库的，那么该属性不会生效。需要手动添加limit语句。 &nbsp;&nbsp;schema 标签用于定义MyCat实例中的逻辑库，MyCat可以有多个逻辑库，每个逻辑库都有自己的相关配置。可以使用 schema 标 签来划分这些不同的逻辑库。 如果不配置 schema 标签，所有的表配置，会属于同一个默认的逻辑库。 注意：若是LINUX版本的MYSQL，则需要设置为Mysql大小写不敏感，否则可能会发生表找不到的问题。在MySQL的配置文件中my.ini [mysqld] 中增加一行 lower_case_table_names = 1 table标签 1&lt;table name=&quot;travelrecord&quot; dataNode=&quot;dn1,dn2,dn3&quot; rule=&quot;auto-sharding-long&quot; &gt;&lt;/table&gt; Table 标签定义了MyCat中的逻辑表，所有需要拆分的表都需要在这个标签中定义。 name属性 逻辑表的表名,同个schema标签中定义的名字必须唯一。 dataNode属性 定义这个逻辑表所属的dataNode,该属性的值需要和dataNode标签中name属性的值相互对应。如果需要定义的dn过多可以使 用如下的方法减少配置： 1234567&lt;table name=&quot;travelrecord&quot; dataNode=&quot;multipleDn$0-99,multipleDn2$100-199&quot; rule=&quot;auto-sharding-long&quot; &gt;&lt;/table&gt;&lt;dataNode name=&quot;multipleDn&quot; dataHost=&quot;localhost1&quot; database=&quot;db$0-99&quot; &gt;&lt;/dataNode&gt;&lt;dataNode name=&quot;multipleDn2&quot; dataHost=&quot;localhost1&quot; database=&quot; db$0-99&quot; &gt;&lt;/dataNode&gt;这里需要注意的是database属性所指定的真实database name需要在后面添加一个，例如上面的例子中，我需要在真实的mysql 上建立名称为dbs0到dbs99的database。 rule属性 该属性用于指定逻辑表要使用的规则名字，规则名字在rule.xml中定义，必须与tableRule标签中name属性属性值一一对应。 ruleRequired属性 该属性用于指定表是否绑定分片规则，如果配置为true，但没有配置具体rule的话 ，程序会报错。 primaryKey属性 该逻辑表对应真实表的主键，例如：分片的规则是使用非主键进行分片的，那么在使用主键查询的时候，就会发送查询语句到所有配置的DN上，如果使用该属性配置真实表的主键。那么MyCat会缓存主键与具体DN的信息，那么再次使用非主键进行查询的 时候就不会进行广播式的查询，就会直接发送语句给具体的DN，但是尽管配置该属性，如果缓存并没有命中的话，还是会发送语 句给具体的DN，来获得数据。 type属性 该属性定义了逻辑表的类型，目前逻辑表只有“全局表”和”普通表”两种类型。对应的配置： 全局表 global 普通表 不指定该值为global的所有表。 autoIncrement属性 mysql对非自增长主键，使用last_insert_id()是不会返回结果的，只会返回0。所以，只有定义了自增长主键的表才可以用 last_insert_id()返回主键值。 mycat目前提供了自增长主键功能，但是如果对应的mysql节点上数据表，没有定义auto_increment，那么在mycat层调用 last_insert_id()也是不会返回结果的。 由于insert操作的时候没有带入分片键，mycat会先取下这个表对应的全局序列，然后赋值给分片键。这样才能正常的插入到数据 库中，最后使用last_insert_id()才会返回插入的分片键值。 如果要使用这个功能最好配合使用数据库模式的全局序列。 使用autoIncrement=“true”指定这个表有使用自增长主键，这样mycat才会不抛出分片键找不到的异常。使用autoIncrement=“false”来禁用这个功能，当然你也可以直接删除掉这个属性。默认就是禁用的。 needAddLimit属性 指定表是否需要自动的在每个语句后面加上limit限制。由于使用了分库分表，数据量有时会特别巨大。这时候执行查询语句，如果恰巧又忘记了加上数量限制的话。那么查询所有的数据出来，也够等上一小会儿的。 所以，mycat就自动的为我们加上LIMIT 100。当然，如果语句中有limit，就不会在次添加了。 这个属性默认为true,你也可以设置成false`禁用掉默认行为。 childTable标签 childTable标签用于定义E-R分片的子表。通过标签上的属性与父表进行关联。 属性名 值 数量限制 描述 name String 1 定义子表的表名。 joinKey String 1 插入子表的时候会使用这个列的值查找父表存储的数据节点。 parentKey String 1 属性指定的值一般为与父表建立关联关系的列名。程序首先获取joinkey的值，再通过parentKey属性指定的列名产生查询语 句，通过执行该语句得到父表存储在哪个分片上。从而确定子表存储的位置。 primaryKey String 0..1 同table标签所描述的。 needAddLimit Boolean 0..1 同table标签所描述的。 dataNode标签 dataNode 标签定义了MyCat中的数据节点，也就是我们通常说所的数据分片。一个dataNode 标签就是一个独立的数据分 片。 属性名 值 数量限制 描述 name String 1 定义数据节点的名字，这个名字需要是唯一的，我们需要在table标签上应用这个名字，来建立表与分片对应的关系。 dataHost String 1 该属性用于定义该分片属于哪个数据库实例的，属性值是引用dataHost标签上定义的name属性。 database String 1 该属性用于定义该分片属性哪个具体数据库实例上的具体库，因为这里使用两个纬度来定义分片，就是：实例+具体的库。因为 每个库上建立的表和表结构是一样的。所以这样做就可以轻松的对表进行水平拆分。 dataHost标签 作为Schema.xml中最后的一个标签，该标签在mycat逻辑库中也是作为最底层的标签存在，直接定义了具体的数据库实例、读 写分离配置和心跳语句。 name属性 唯一标识dataHost标签，供上层的标签使用。 maxCon属性 指定每个读写实例连接池的最大连接。也就是说，标签内嵌套的writeHost、readHost标签都会使用这个属性的值来实例化出连接池的最大连接数。 minCon属性 指定每个读写实例连接池的最小连接，初始化连接池的大小。 balance属性 负载均衡类型，目前的取值有3种： balance=“0”, 所有读操作都发送到当前可用的writeHost上。 balance=“1”，所有读操作都随机的发送到readHost。 balance=“2”，所有读操作都随机的在writeHost、readhost上分发。 writeType属性 负载均衡类型，目前的取值有3种： writeType=“0”, 所有写操作都发送到可用的writeHost上。 writeType=“1”，所有写操作都随机的发送到readHost。 writeType=“2”，所有写操作都随机的在writeHost、readhost分上发。 dbType属性 指定后端连接的数据库类型，目前支持二进制的mysql协议，还有其他使用JDBC连接的数据库。例如：mongodb、oracle、 spark等。 dbDriver属性 指定连接后端数据库使用的Driver，目前可选的值有native和JDBC。使用native的话，因为这个值执行的是二进制的mysql协 议，所以可以使用mysql和maridb。其他类型的数据库则需要使用JDBC驱动来支持。如果使用JDBC的话需要将符合JDBC 4标准的驱动JAR包放到MYCAT\lib目录下，并检查驱动JAR包中包括如下目录结构的文 件：META-INF\services\java.sql.Driver。在这个文件内写上具体的Driver类名，例如：com.mysql.jdbc.Driver。 heartbeat标签 这个标签内指明用于和后端数据库进行心跳检查的语句。例如,MYSQL可以使用select user()，Oracle可以使用select 1 from dual等。这个标签还有一个connectionInitSql属性，主要是当使用Oracla数据库时，需要执行的初始化SQL语句就这个放到这里面来。例 如：alter session set nls_date_format=’yyyy-mm-dd hh24:miss’ writeHost标签、readHost标签 这两个标签都指定后端数据库的相关配置给mycat，用于实例化后端连接池。唯一不同的是，writeHost指定写实例、readHost 指定读实例，组着这些读写实例来满足系统的要求。 在一个dataHost内可以定义多个writeHost和readHost。但是，如果writeHost指定的后端数据库宕机，那么这个writeHost绑 定的所有readHost都将不可用。另一方面，由于这个writeHost宕机系统会自动的检测到，并切换到备用的writeHost上去。 这两个标签的属性相同，这里就一起介绍。 属性名 值 数量限制 描述 host String 1 用于标识不同实例，一般writeHost我们使用M1，readHost我们用S1。 url String 1 后端实例连接地址，如果是使用native的dbDriver，则一般为address:port这种形式。用JDBC或其他的dbDriver，则需要特殊 指定。当使用JDBC时则可以这么写：jdbc:mysql://localhost:3306/。 password String 1 后端存储实例需要的用户名字 user String 1 后端存储实例需要的密码 2.配置server.xmlserver.xml几乎保存了所有mycat需要的系统配置信息。最常用的是在此配置用户名、密码及权限。 例如:给TESTDB配置一个test用户。 12345&lt;user name=&quot;test&quot;&gt; &lt;property name=&quot;password&quot;&gt;test&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;TESTDB&lt;/property&gt; &lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt;&lt;/user&gt; 3.配置rule.xmlrule.xml里面就定义了我们对表进行拆分所涉及到的规则定义。我们可以灵活的对表使用不同的分片算法，或者对表使用相同的算法但具体的参数不同。这个文件里面主要有tableRule和function这两个标签。在具体使用过程中可以按照需求添加tableRule和function。 MyCat读写分离数据库读写分离对于大型系统或者访问量很高的互联网应用来说，是必不可少的一个重要功能。对于MySQL来说，标准的读写分离是主从模式，一个写节点Master后面跟着多个读节点，读节点的数量取决于系统的压力，通常是1-3个读节点的配置。 1.Mysql主从复制 Mysql主从配置需要注意的地方: 主DBServer和从DBServer需要版本一致。 主DBServer和从DBServer数据一致。 主DB server开启二进制日志,主DB server和从DB server的server_id都必须唯一。 2.Mysql主服务器配置修改/etc路径下的my.cnf文件,在[mysqld]段中添加: 123456binlog-do-db=db1binlog-ignore-db=mysql#启用二进制日志log-bin=mysql-bin#服务器唯一ID，一般取IP最后一段server-id=138 修改后,重启mysql服务service mysqld restart 创建一个账户并授权slave。 123mysql&gt;GRANT FILE ON *.* TO &apos;backup&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;mysql&gt;GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* to &apos;backup&apos;@&apos;%&apos; identified by &apos;123456&apos;; #一般不用root帐号，“%”表示所有客户端都可能连，只要帐号，密码正确，此处可用具体客户端IP代替，如192.168.145.226，加强安全。 之后刷新权限:FLUSH PRIVILEGES; 可以使用show master status;命令 查询主服务器状态。 3.Mysql从服务器配置修改/etc路径下的my.cnf文件,在[mysqld]段中添加一个serverid。 配置从服务器 12mysql&gt;change master to master_host=&apos;192.168.145.138&apos;,master_user=&apos;backup&apos;,master_password=&apos;123456&apos;, master_log_file=&apos;mysql-bin.000002&apos;,master_log_pos=679; 注意语句中间不要断开，master_port为mysql服务器端口号(无引号)，master_user为执行同步操作的数据库账户，“120”无单引号(此处的120就是show master status 中看到的position的值，这里的mysql-bin.000001就是file对应的值)。 之后启动从服务器复制功能 mysql&gt;start slave; 检查从服务器状态 show slave status; 注：Slave_IO及Slave_SQL进程必须正常运行，即YES状态，否则都是错误的状态(如：其中一个NO均属错误)。 如果出现此错误：Fatal error: The slave I/O thread stops because master and slave have equal MySQL server UUIDs; these UUIDs must be different for replication to work.因为是mysql是克隆的系统所以mysql的uuid是一样的，所以需要修改。 解决方法:删除/var/lib/mysql/auto.cnf文件，重新启动服务。 4.MyCat配置Mycat 1.4 支持MySQL主从复制状态绑定的读写分离机制，让读更加安全可靠，配置如下： 123456789101112&lt;dataNode name=&quot;dn1&quot; dataHost=&quot;localhost1&quot; database=&quot;db1&quot; /&gt; &lt;dataNode name=&quot;dn2&quot; dataHost=&quot;localhost1&quot; database=&quot;db2&quot; /&gt; &lt;dataNode name=&quot;dn3&quot; dataHost=&quot;localhost1&quot; database=&quot;db3&quot; /&gt; &lt;dataHost name=&quot;localhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;1&quot; writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;2&quot; slaveThreshold=&quot;100&quot;&gt; &lt;heartbeat&gt;show slave status&lt;/heartbeat&gt; &lt;writeHost host=&quot;hostM&quot; url=&quot;192.168.25.138:3306&quot; user=&quot;root&quot; password=&quot;root&quot;&gt; &lt;readHost host=&quot;hostS&quot; url=&quot;192.168.25.166:3306&quot; user=&quot;root&quot; password=&quot;root&quot; /&gt; &lt;/writeHost&gt; &lt;/dataHost&gt; readHost是从属于writeHost的，即意味着它从那个writeHost获取同步数据，因此，当它所属的writeHost宕机了，则它也不会再参与到读写分离中来，即“不工作了”，这是因为此时，它的数据已经“不可靠”了。基于这个考虑，目前mycat 1.3和1.4版本中，若想支持MySQL一主一从的标准配置，并且在主节点宕机的情况下，从节点还能读取数据，则需要在Mycat里配置为两个writeHost并设置banlance=1。 设置 switchType=”2” 与slaveThreshold=”100” switchType 目前有三种选择： -1：表示不自动切换 1 ：默认值，自动切换 2 ：基于MySQL主从同步的状态决定是否切换 Mycat心跳检查语句配置为 show slave status ，dataHost 上定义两个新属性: switchType=”2” 与slaveThreshold=”100”，此时意味着开启MySQL主从复制状态绑定的读写分离与切换机制。 Mycat心跳机制通过检测 show slave status 中的 “Seconds_Behind_Master”, “Slave_IO_Running”, “Slave_SQL_Running” 三个字段来确定当前主从同步的状态以及Seconds_Behind_Master主从复制时延。]]></content>
      <categories>
        <category>后端</category>
        <category>Database</category>
        <category>MySql</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>Database</tag>
        <tag>MyCat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ActiveMQ消息队列]]></title>
    <url>%2F2016%2F07%2F03%2F2016-07-03-activemq%2F</url>
    <content type="text"><![CDATA[介绍&nbsp;&nbsp;ActiveMQ 是Apache出品，最流行的，能力强劲的开源消息总线。ActiveMQ 是一个完全支持JMS1.1和J2EE 1.4规范的 JMS Provider实现,尽管JMS规范出台已经是很久的事情了,但是JMS在当今的J2EE应用中间仍然扮演着特殊的地位。 主要特点： 多种语言和协议编写客户端。语言: Java, C, C++, C#, Ruby, Perl, Python, PHP。应用协议: OpenWire,Stomp REST,WS Notification,XMPP,AMQP 完全支持JMS1.1和J2EE 1.4规范 (持久化,XA消息,事务) 对Spring的支持,ActiveMQ可以很容易内嵌到使用Spring的系统里面去,而且也支持Spring2.0的特性 通过了常见J2EE服务器(如 Geronimo,JBoss 4, GlassFish,WebLogic)的测试,其中通过JCA 1.5 resource adaptors的配置,可以让ActiveMQ可以自动的部署到任何兼容J2EE 1.4 商业服务器上 支持多种传送协议:in-VM,TCP,SSL,NIO,UDP,JGroups,JXTA 支持通过JDBC和journal提供高速的消息持久化 从设计上保证了高性能的集群,客户端-服务器,点对点 支持Ajax 支持与Axis的整合 可以很容易得调用内嵌JMS provider,进行测试 什么是JMS规范&nbsp;&nbsp;JMS的全称是Java MessageService，即Java消息服务。用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。&nbsp;&nbsp;它主要用于在生产者和消费者之间进行消息传递，生产者负责产生消息，而消费者负责接收消息。把它应用到实际的业务需求中的话我们可以在特定的时候利用生产者生成一消息，并进行发送，对应的消费者在接收到对应的消息后去完成对应的业务逻辑。&nbsp;&nbsp;对于消息的传递有两种类型：一种是点对点的，即一个生产者和一个消费者一一对应；另一种是发布/订阅模式，即一个生产者产生消息并进行发送后，可以由多个消费者进行接收。JMS定义了五种不同的消息正文格式，以及调用的消息类型，允许你发送并接收以一些不同形式的数据，提供现有消息格式的一些级别的兼容性。 StreamMessage – Java原始值的数据流 MapMessage–一套名称-值对 TextMessage–一个字符串对象 ObjectMessage–一个序列化的 Java对象 BytesMessage–一个字节的数据流 JMS应用程序接口&nbsp;&nbsp;ConnectionFactory &nbsp;&nbsp;&nbsp;&nbsp;用户用来创建到JMS提供者的连接的被管对象。JMS客户通过可移植的接口访问连接，这样当下层的实现改变时，代码不需要进行修改。 管理员在JNDI名字空间中配置连接工厂，这样，JMS客户才能够查找到它们。根据消息类型的不同，用户将使用队列连接工厂，或者主题连接工厂。 &nbsp;&nbsp;Connection &nbsp;&nbsp;&nbsp;&nbsp;连接代表了应用程序和消息服务器之间的通信链路。在获得了连接工厂后，就可以创建一个与JMS提供者的连接。根据不同的连接类型，连接允许用户创建会话，以发送和接收队列和主题到目标。 &nbsp;&nbsp;Destination &nbsp;&nbsp;&nbsp;&nbsp;目标是一个包装了消息目标标识符的被管对象，消息目标是指消息发布和接收的地点，或者是队列，或者是主题。JMS管理员创建这些对象，然后用户通过JNDI发现它们。和连接工厂一样，管理员可以创建两种类型的目标，点对点模型的队列，以及发布者／订阅者模型的主题。 &nbsp;&nbsp;MessageProducer &nbsp;&nbsp;&nbsp;&nbsp;由会话创建的对象，用于发送消息到目标。用户可以创建某个目标的发送者，也可以创建一个通用的发送者，在发送消息时指定目标。 &nbsp;&nbsp;MessageConsumer &nbsp;&nbsp;&nbsp;&nbsp;由会话创建的对象，用于接收发送到目标的消息。消费者可以同步地（阻塞模式），或异步（非阻塞）接收队列和主题类型的消息。 &nbsp;&nbsp;Message &nbsp;&nbsp;&nbsp;&nbsp;是在消费者和生产者之间传送的对象，也就是说从一个应用程序创送到另一个应用程序。一个消息有三个主要部分： 消息头（必须）：包含用于识别和为消息寻找路由的操作设置。 一组消息属性（可选）：包含额外的属性，支持其他提供者和用户的兼容。可以创建定制的字段和过滤器（消息选择器）。 一个消息体（可选）：允许用户创建五种类型的消息（文本消息，映射消息，字节消息，流消息和对象消息）。 消息接口非常灵活，并提供了许多方式来定制消息的内容。 &nbsp;&nbsp;Session &nbsp;&nbsp;&nbsp;&nbsp;表示一个单线程的上下文，用于发送和接收消息。由于会话是单线程的，所以消息是连续的，就是说消息是按照发送的顺序一个一个接收的。会话的好处是它支持事务。如果用户选择了事务支持，会话上下文将保存一组消息，直到事务被提交才发送这些消息。在提交事务之前，用户可以使用回滚操作取消这些消息。一个会话允许用户创建消息生产者来发送消息，创建消息消费者来接收消息。 JMS消息发送模式 &nbsp;&nbsp;在P2P模型下，一个生产者向一个特定的队列发布消息，一个消费者从该队列中读取消息。这里，生产者知道消费者的队列，并直接将消息发送到消费者的队列。这种模式被概括为：只有一个消费者将获得消息。生产者不需要在接收者消费该消息期间处于运行状态，接收者也同样不需要在消息发送时处于运行状态。每一个成功处理的消息都由接收者签收。 &nbsp;&nbsp;publish/subscribe模型支持向一个特定的消息主题发布消息。0或多个订阅者可能对接收来自特定消息主题的消息感兴趣。在这种模型下，发布者和订阅者彼此不知道对方。这种模式好比是匿名公告板。这种模式被概括为：多个消费者可以获得消息.在发布者和订阅者之间存在时间依赖性。发布者需要建立一个订阅（subscription），以便客户能够购订阅。订阅者必须保持持续的活动状态以接收消息，除非订阅者建立了持久的订阅。在那种情况下，在订阅者未连接时发布的消息将在订阅者重新连接时重新发布。 安装ActiveMQ 首先到官网 http://activemq.apache.org/ 下载ActiveMQ. 因为ActiveMQ是JAVA开发的,所以依赖jdk环境。 解压ActiveMQ。 在ActiveMQ/bin目录中,./activemq start 开启ActiveMQ 在ActiveMQ/bin目录中,./activemq stop 关闭ActiveMQ 访问后台 http://ip:8161/admin ActiveMQ的默认后台端口为8161,Message端口为61616 使用ActiveMQ&nbsp;&nbsp;使用ActiveMQ需要先引入ActiveMQ的jar包。 12345 &lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-all&lt;/artifactId&gt; &lt;version&gt;5.11.2&lt;/version&gt;&lt;/dependency&gt; 以下示例使用Queue模式,如要使用Topic模式只需要将Destination改成Topic即可。 1.Producer12345678910111213141516171819202122232425262728293031323334353637383940414243444546 @Testpublic void testProducer() throws JMSException&#123; // 创建连接工厂 ConnectionFactory connectionFactory = new ActiveMQConnectionFactory("tcp://192.168.145.137:61616"); // 声明Connection Connection connection = null; // 声明Session Session session = null; // 声明Producer MessageProducer producer = null; try&#123; // 从连接工厂中获得连接 connection = connectionFactory.createConnection(); // 开启连接 connection.start(); /* * 从连接中获得会话 * 参数1:transacted boolean型 * 当设置为true时,将忽略参数2,acknowledgment mode被jms服务器设置 SESSION_TRANSACTED。 * 当一个事务被提交时,消息确认就会自动发生。 * 当设置为false时,需要设置参数2 * Session.AUTO_ACKNOWLEDGE为自动确认，当客户成功的从receive方法返回的时候，或者从 * MessageListener.onMessage方法成功返回的时候，会话自动确认客户收到的消息。 * Session.CLIENT_ACKNOWLEDGE 为客户端确认。客户端接收到消息后，必须调用javax.jms.Message的 * acknowledge方法。jms服务器才会删除消息。（默认是批量确认） */ session = connection.createSession(false,Session.AUTO_ACKNOWLEDGE); // 创建一个Destination目的地 Queue或者Topic Queue queue = session.createQueue("testMessage"); // 创建一个Producer生产者 producer = session.createProducer(queue); // 创建message ActiveMQTextMessage textMessage = new ActiveMQTextMessage(); textMessage.setText("test"); // 发送message producer.send(textMessage); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; // 回收资源 producer.close(); session.close(); connection.close(); &#125;&#125; 2.Consumer&nbsp;&nbsp;消费者有两种消费方式: 同步消费。通过调用消费者的receive方法从目的地中显式提取消息。receive方法可以一直阻塞到消息到达。 异步消费。客户可以为消费者注册一个消息监听器，以定义在消息到达时所采取的动作。 实现MessageListener接口，在MessageListener（）方法中实现消息的处理逻辑。 同步消费 123456789101112131415161718192021222324252627282930313233343536373839@Testpublic void testSyncConsumer() throws JMSException&#123; // 创建连接工厂 ConnectionFactory connectionFactory = new ActiveMQConnectionFactory("tcp://192.168.145.137:61616"); Connection connection = null; Session session = null; MessageConsumer consumer = null; try&#123; // 获得连接 connection = connectionFactory.createConnection(); // 开启连接 connection.start(); // 获得Session session = connection.createSession(false,Session.AUTO_ACKNOWLEDGE); // 创建一个目的地 Queue queue = session.createQueue("testSynchronization"); // 创建消费者 consumer = session.createConsumer(queue); // 使用receive同步消费 while(true)&#123; // 设置接收信息的时间,单位为毫秒 Message message = consumer.receive(10000); if(message != null)&#123; System.out.println(message); &#125;else&#123; // 超时,结束循环 break; &#125; &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; // 回收资源 consumer.close(); session.close(); connection.close(); &#125;&#125; 异步消费 1234567891011121314151617181920212223242526272829303132333435363738394041424344 @Testpublic void testAsyncConsumer() throws JMSException&#123; // 创建连接工厂 ConnectionFactory connectionFactory = new ActiveMQConnectionFactory("tcp://192.168.145.137:61616"); Connection connection = null; Session session = null; MessageConsumer consumer = null; try&#123; // 获得连接 connection = connectionFactory.createConnection(); // 开启连接 connection.start(); // 获得Session session = connection.createSession(false,Session.AUTO_ACKNOWLEDGE); // 设置目的地 Queue queue = session.createQueue("testAsynchronization"); // 创建Consumer consumer = session.createConsumer(queue); // 异步消费 session.setMessageListener(new MessageListener() &#123; @Override public void onMessage(Message message) &#123; if(message instanceof TextMessage)&#123; try &#123; String text = ((TextMessage) message).getText(); System.out.println(text); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;); System.in.read(); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125;finally&#123; // 回收资源 consumer.close(); session.close(); connection.close(); &#125;&#125; 整合Spring1.配置ConnectionFactory123456789101112131415161718192021222324&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:context="http://www.springframework.org/schema/context" xmlns:p="http://www.springframework.org/schema/p" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:jms="http://www.springframework.org/schema/jms" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd http://www.springframework.org/schema/jms http://www.springframework.org/schema/jms/spring-jms-4.0.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util-4.0.xsd"&gt; &lt;!-- ActiveMQ提供的ConnectionFactory --&gt; &lt;bean id="targetConnectionFactory" class="org.apache.activemq.ActiveMQConnectionFactory"&gt; &lt;property name="brokerURL" value="tcp://192.168.145.137:61616" /&gt; &lt;/bean&gt; &lt;!-- Spring的ConnectionFactory需要注入ActiveMQ的ConnectionFactory --&gt; &lt;bean id="connectionFactory" class="org.springframework.jms.connection.SingleConnectionFactory"&gt; &lt;!-- 目标ConnectionFactory对应真实的可以产生JMS Connection的ConnectionFactory --&gt; &lt;property name="targetConnectionFactory" ref="targetConnectionFactory" /&gt; &lt;/bean&gt;&lt;/beans&gt; 2.配置生产者12345678910111213141516 &lt;!-- 配置生产者 --&gt;&lt;!-- Spring提供的JMS工具类，它可以进行消息发送、接收等 --&gt;&lt;bean id="jmsTemplate" class="org.springframework.jms.core.JmsTemplate"&gt; &lt;!-- 注入Spring的连接工厂 --&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt;&lt;/bean&gt;&lt;!--P2P模式的Destination --&gt;&lt;bean id="queueDestination" class="org.apache.activemq.command.ActiveMQQueue"&gt; &lt;constructor-arg&gt; &lt;value&gt;queue&lt;/value&gt; &lt;/constructor-arg&gt;&lt;/bean&gt;&lt;!-- publish/subscribe模式的Destination --&gt;&lt;bean id="topicDestination" class="org.apache.activemq.command.ActiveMQTopic"&gt; &lt;constructor-arg value="topic" /&gt;&lt;/bean&gt; 3.发送消息1234567891011121314151617public void testSend()&#123; // 读取Spring配置文件 ApplicationContext applicationContext = new ClassPathXmlApplicationContext("applicationContext.xml"); // 获得JmsTemplate JmsTemplate jmsTemplate = applicationContext.getBean(JmsTemplate.class); // 获得Destination ActiveMQQueue queue = applicationContext.getBean(ActiveMQQueue.class); // 发送消息 jmsTemplate.send(queue, new MessageCreator() &#123; @Override public Message createMessage(Session session) throws JMSException &#123; return session.createTextMessage("send-spring"); &#125; &#125;); &#125; 4.配置消费者&nbsp;&nbsp;Spring通过MessageListenerContainer接收信息,并把接收到的信息分发给MessageListener进行处理。每个消费者对应每个目的地都需要有对应的MessageListenerContainer。 1234567891011121314151617181920212223242526272829303132333435&lt;!-- ActiveMQ提供的ConnectionFactory --&gt;&lt;bean id="targetConnectionFactory" class="org.apache.activemq.ActiveMQConnectionFactory"&gt; &lt;property name="brokerURL" value="tcp://192.168.145.137:61616" /&gt;&lt;/bean&gt;&lt;!-- Spring的ConnectionFactory需要注入ActiveMQ的ConnectionFactory --&gt;&lt;bean id="connectionFactory" class="org.springframework.jms.connection.SingleConnectionFactory"&gt; &lt;!-- 目标ConnectionFactory对应真实的可以产生JMS Connection的ConnectionFactory --&gt; &lt;property name="targetConnectionFactory" ref="targetConnectionFactory" /&gt;&lt;/bean&gt; &lt;!-- 配置生产者 --&gt;&lt;!-- Spring提供的JMS工具类，它可以进行消息发送、接收等 --&gt;&lt;bean id="jmsTemplate" class="org.springframework.jms.core.JmsTemplate"&gt; &lt;!-- 注入Spring的连接工厂 --&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt;&lt;/bean&gt;&lt;!--P2P模式的Destination --&gt;&lt;bean id="queueDestination" class="org.apache.activemq.command.ActiveMQQueue"&gt; &lt;constructor-arg&gt; &lt;value&gt;queue&lt;/value&gt; &lt;/constructor-arg&gt;&lt;/bean&gt;&lt;!-- publish/subscribe模式的Destination --&gt;&lt;bean id="topicDestination" class="org.apache.activemq.command.ActiveMQTopic"&gt; &lt;constructor-arg value="topic" /&gt;&lt;/bean&gt;&lt;!-- 配置监听器 --&gt;&lt;bean id="myMessageListener" class="com.activemq.MyMessageListener" /&gt;&lt;!-- 消息监听容器 --&gt;&lt;bean id="jmsContainer" class="org.springframework.jms.listener.DefaultMessageListenerContainer"&gt; &lt;property name="connectionFactory" ref="connectionFactory" /&gt; &lt;property name="destination" ref="queueDestination" /&gt; &lt;property name="messageListener" ref="myMessageListener" /&gt;&lt;/bean&gt; 监听器需要实现MessageListener接口。 123456public class MyMessageListener implements MessageListener &#123; @Override public void onMessage(Message message) &#123; System.out.println(message); &#125;&#125; Exception&nbsp;&nbsp;启动ActiveMQ时,如果发生java.net.UnknownHostException异常。解决方法:修改 /etc/hosts 文件 添加一行 192.168.1.1(主机IP) 主机名.localdomain 主机名例: 192.168.145.137 ActiveMQ.localdomain ActiveMQ]]></content>
      <categories>
        <category>后端</category>
        <category>消息队列</category>
      </categories>
      <tags>
        <tag>ActiveMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SolrCloud初体验]]></title>
    <url>%2F2016%2F06%2F30%2F2016-06-30-solrcloud%2F</url>
    <content type="text"><![CDATA[什么是SolrCloudSolrCloud是基于Solr和Zookeeper的分布式搜索方案,它的主要思想是使用Zookeeper作为集群的配置信息中心。 SolrCloud的特点1.近实时搜索立即推送式的replication（也支持慢推送）。可以在秒内检索到新加入索引。 2.自动容错SolrCloud对索引分片,并对每个分片创建多个Replication。每个Replication都可以对外提供服务。一个Replication挂掉不会影响索引服务。更强大的是，它还能自动的在其它机器上帮你把失败机器上的索引Replication重建并投入使用。 3.查询时自动负载均衡SolrCloud索引的多个Replication可以分布在多台机器上,均衡查询压力。如果查询压力大，可以通过扩展机器，增加Replication来减缓。 4.集中式的配置信息SolrCloud可以将配置文件上传到Zookeeper,由Zookeeper对配置文件进行管理。 结构分析为了减少处理压力,SolrCloud需要由多台服务器共同完成索引和搜索。 1.实现思路SolrCloud将索引数据进行分片(Shard),每个分片由多台服务器共同完成。 2.结构 物理结构SolrCloud由三个Solr服务器组成,每个Solr服务器包含2个Core。 逻辑结构一个Collection包含2个Shard,每个Shard由3个core组成(一个Leader,两个Replication)。 CollectionCollection是一个在逻辑意义上完整的索引结构,它常常被划分为一个或多个Shard分片,它们使用相同的配置信息。如果Shard数超过一个，它就是分布式索引，SolrCloud让你通过Collection名称引用它，而不需要关心分布式检索时需要使用的和Shard相关参数。 Core一个Solr中包含一个或者多个Solr Core，每个Solr Core可以独立提供索引和查询功能，每个Solr Core对应一个索引或者Collection的Shard，Solr Core的提出是为了增加管理灵活性和共用资源。在SolrCloud中有个不同点是它使用的配置是在Zookeeper中的，传统的Solr core的配置文件是在磁盘上的配置目录中。 ShardCollection的逻辑分片。每个Shard被化成一个或者多个replicas，通过选举确定哪个是Leader。 Replication在master-slave结构中,Replication是一个从节点,同一个Shard下主从节点存储的数据是一致的。 Leader在master-slave结构中,Leader是一个主节点,Leader是赢得选举的Replication。选举可以发生在任何时间，但是通常它们仅在某个Solr实例发生故障时才会触发。当索引documents时，SolrCloud会传递它们到此Shard对应的Leader，Leader再分发它们到全部Shard的Replication。 SolrCloud的搭建1.ZookeeperSolrCloud需要Zookeeper进行管理,所以需要先安装Zookeeper。 .解压缩zookeeper.tar.gz,并复制出3个Zookeeper实例。 .进入zookeeper01目录,创建一个data文件夹,并在data中创建一个myid文件,内容为1(其他Zookeeper实例为2和3)。 进入conf文件夹,将zoo_sample.cfg改名为zoo.cfg vim zoo.cfg 修改dataDir=data文件夹所在的目录,添加:server.myid的值=ip:每个Zookeeper服务器之间的通讯端口:Zookeeper与其他应用的通讯端口。(每个Zookeeper实例都需要添加这行内容)例如: 2.Solr实例 安装一个单机的Solr实例,并复制成4份,分别对应4个SolrHome。 修改SolrHome的solr.xml文件 将配置文件上传到Zookeeper,当配置文件发生改变时,需要重新上传。 java -classpath .:/usr/local/solr-cloud/solr-lib/* org.apache.solr.cloud.ZkCLI -cmd upconfig -zkhost 127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183 -confdir /usr/local/solr-cloud/solrhome01/collection1/conf -confname solr-conf -cmd upconfig 上传配置文件命令 -zkhost Zookeeper集群的ip与端口 -confdir 配置文件的目录 -confname 上传到Zookeeper后的文件夹名称 其中参数/usr/local/solr-cloud/solr-lib/可以自己创建，内容如下： 复制tomcat/webapps/solr/WEB-INF/lib下所有jar包 复制example/lib/ext下所有jar包 复制example/resources/log4j.properties 通知Solr实例Zookeeper的地址,需要修改tomcat/bin/catalina.sh添加一行:JAVA_OPTS=”-DzkHost=Zookeeper集群的地址列表” 3.设置Shard1http://web容器/solr/admin/collections?action=CREATE&amp;name=Collection名称&amp;numShards=Shard个数&amp;replicationFactor=Replication个数 例: 12http://192.168.145.150:8080/solr/admin/collections?action=CREATE&amp;name=collection2&amp;numShards=2&amp;replicationFactor=2上面的命令为 创建一个name为collection2的Collection,并分成了2个Shard,每个Shard有2个Replication 删除一个Collection 12http://192.168.145.150:8080/solr/admin/collections?action=DELETE&amp;name=collection1上面的命令为 删除一个name为collection1的Collection Spring整合SolrCloud1234567&lt;!-- SolrCloud --&gt; &lt;bean id=&quot;cloudSolrServer&quot; class=&quot;org.apache.solr.client.solrj.impl.CloudSolrServer&quot;&gt; &lt;constructor-arg name=&quot;zkHost&quot; value=&quot;192.168.145.136:2181,192.168.145.136:2182,192.168.145.136:2183&quot; /&gt; &lt;!-- 设置默认搜索的Collection --&gt; &lt;property name=&quot;defaultCollection&quot; value=&quot;collection2&quot; /&gt; &lt;/bean&gt;]]></content>
      <categories>
        <category>后端</category>
        <category>全文检索</category>
        <category>Solr</category>
      </categories>
      <tags>
        <tag>全文检索引擎</tag>
        <tag>Solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何搭建与维护一个Redis集群]]></title>
    <url>%2F2016%2F06%2F27%2F2016-06-27-redis-cluster01%2F</url>
    <content type="text"><![CDATA[Redis集群架构 每一个Redis节点使用PING-PONG的形式互相通信。 当半数以上的节点fail时,则整个集群失效。 客户端不需要连接集群所有节点,只要连接集群中任意一个节点即可。 Redis集群中内置了16384个哈希槽,当操作数据时,Redis会对key使用crc16算法算出一个结果,并把结果对16384取余,每个key都会对应0-16383之间的哈希槽,Redis会根据节点数量平均的将哈希槽映射到不同的节点上。 Redis投票机制 Redis集群中每一个节点都会参与投票,如果当半数以上的节点认为一个节点通信超时,则该节点fail。 当集群中任意节点的master(主机)挂掉,且这个节点没有slave(从机),则整个集群进入fail状态。 搭建Redis集群1.安装Ruby环境因为redis-trib.rb脚本依赖ruby环境,所以需要先安装ruby。 12yum install ruby yum install rubygems 安装ruby和redis的接口程序 1gem install /usr/local/redis-3.0.0.gem 2.搭建Redis集群一个Redis集群最少需要3组主从机,即6个Redis。 修改redis.conf配置文件,将集群开关开启。 启动Redis实例 123456789101112131415161718cd redis01./redis-server redis.confcd ..cd redis02./redis-server redis.confcd ..cd redis03./redis-server redis.confcd ..cd redis04./redis-server redis.confcd ..cd redis05./redis-server redis.confcd ..cd redis06./redis-server redis.confcd .. 使用redis-trib.rb创建集群 1./redis-trib.rb create --replicas 1 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 127.0.0.1:7006 3.添加主节点如果想添加一个port为7007的Redis实例,可以使用以下命令。 1./redis-trib.rb add-node 127.0.0.1:7007 127.0.0.1:7001 当添加了一个主节点后,需要重新分配哈希槽。 1./redis-trib.rb reshard 127.0.0.1:7001 4.添加从节点添加一个port为7008的Redis实例做为7007的从节点。 1./redis-trib.rb add-node --slave --master-id cad9f7413ec6842c971dbcc2c48b4ca959eb5db4 127.0.0.1:7008 127.0.0.1:7001 命令格式: ./redis-trib.rb add-node –slave –master-id 主节点id 新加从节点的ip和端口 集群中节点的ip和端口(任意一个节点)。 主节点id可以在client中使用 cluster nodes 命令查询。 注意: 如果原来该结点在集群中的配置信息已经生成到cluster-config-file指定的配置文件中（如果cluster-config-file没有指定则默认为nodes.conf),则会报错: 1[ERR] Node XXXXXX is not empty. Either the node already knows other nodes (check with CLUSTER NODES) or contains some key in database 0 解决方法: 删除生成的配置文件 nodes.conf,再执行./redis-trib.rb add-node命令。 5.删除节点./redis-trib.rb del-node 要删除的节点的ip和端口 节点id注意: 如果这个节点已经占有哈希槽,则无法删除,需要先将哈希槽分配出去。 Jedis连接集群使用Jedis连接集群需要先创建JedisCluster对象。代码如下: 123456789101112131415161718public void test01()&#123; Set&lt;HostAndPort&gt; nodes = new HashSet&lt;HostAndPort&gt;(); nodes.add(new HostAndPort("192.168.145.134", 7001)); nodes.add(new HostAndPort("192.168.145.134", 7002)); nodes.add(new HostAndPort("192.168.145.134", 7003)); nodes.add(new HostAndPort("192.168.145.134", 7004)); nodes.add(new HostAndPort("192.168.145.134", 7005)); nodes.add(new HostAndPort("192.168.145.134", 7006)); // 创建JedisCluster JedisCluster jedisCluster = new JedisCluster(nodes); // 操作redis String set = jedisCluster.set("hello", "helloWorld"); String hello = jedisCluster.get("hello"); System.out.println(set); System.out.println(hello); // 关闭JedisCluster jedisCluster.close();&#125; 在Spring容器中维护: 123456789101112131415161718192021222324252627282930 &lt;bean class=&quot;redis.clients.jedis.JedisCluster&quot;&gt; &lt;constructor-arg name=&quot;nodes&quot;&gt; &lt;set&gt; &lt;bean class=&quot;redis.clients.jedis.HostAndPort&quot;&gt; &lt;constructor-arg name=&quot;host&quot; value=&quot;192.168.145.134&quot;/&gt; &lt;constructor-arg name=&quot;port&quot; value=&quot;7001&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;redis.clients.jedis.HostAndPort&quot;&gt; &lt;constructor-arg name=&quot;host&quot; value=&quot;192.168.145.134&quot;/&gt; &lt;constructor-arg name=&quot;port&quot; value=&quot;7002&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;redis.clients.jedis.HostAndPort&quot;&gt; &lt;constructor-arg name=&quot;host&quot; value=&quot;192.168.145.134&quot;/&gt; &lt;constructor-arg name=&quot;port&quot; value=&quot;7003&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;redis.clients.jedis.HostAndPort&quot;&gt; &lt;constructor-arg name=&quot;host&quot; value=&quot;192.168.145.134&quot;/&gt; &lt;constructor-arg name=&quot;port&quot; value=&quot;7004&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;redis.clients.jedis.HostAndPort&quot;&gt; &lt;constructor-arg name=&quot;host&quot; value=&quot;192.168.145.134&quot;/&gt; &lt;constructor-arg name=&quot;port&quot; value=&quot;7005&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;redis.clients.jedis.HostAndPort&quot;&gt; &lt;constructor-arg name=&quot;host&quot; value=&quot;192.168.145.134&quot;/&gt; &lt;constructor-arg name=&quot;port&quot; value=&quot;7006&quot;/&gt; &lt;/bean&gt; &lt;/set&gt; &lt;/constructor-arg&gt;&lt;/bean&gt;]]></content>
      <categories>
        <category>后端</category>
        <category>Database</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何安装与搭建一个Nginx服务器]]></title>
    <url>%2F2016%2F06%2F24%2F2016-06-24-nginx-initiation%2F</url>
    <content type="text"><![CDATA[Nginx介绍Nginx是一款轻量级的Web 服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器，并在一个BSD-like 协议下发行。由俄罗斯的程序设计师Igor Sysoev所开发，供俄国大型的入口网站及搜索引擎Rambler使用。 Nginx应用场景 作为http服务器使用并独立提供http服务。 虚拟主机。 用于反向代理与负载均衡,当并发量非常大时,可以使用Nginx对服务器集群进行反向代理与负载均衡,提高吞吐量。 反向代理 反向代理是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。 Nginx安装流程 Nginx依赖于以下4个库 gcc 安装nginx需要先将官网下载的源码进行编译，编译依赖gcc环境 **yum install gcc-c++** pcre PCRE(Perl Compatible Regular Expressions)是一个Perl库，包括 perl 兼容的正则表达式库。nginx的http模块使用pcre来解析正则表达式，所以需要在linux上安装pcre库。 yum install -y pcre pcre-devel zlib zlib库提供了很多种压缩和解压缩的方式，nginx使用zlib对http包的内容进行gzip，所以需要在linux上安装zlib库。 yum install -y zlib zlib-devel openssl OpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及SSL协议，并提供丰富的应用程序供测试或其它目的使用。nginx不仅支持http协议，还支持https（即在ssl协议上传输http），所以需要在linux安装openssl库。 yum install -y openssl openssl-devel 一键安装依赖包 yum -y install zlib zlib-devel openssl openssl–devel pcre pcre-devel 解压Nginx源码包 使用configure创建makefile 123456789101112./configure \--prefix=/usr/local/nginx \--pid-path=/var/run/nginx/nginx.pid \--lock-path=/var/lock/nginx.lock \--error-log-path=/var/log/nginx/error.log \--http-log-path=/var/log/nginx/access.log \--with-http_gzip_static_module \--http-client-body-temp-path=/var/temp/nginx/client \--http-proxy-temp-path=/var/temp/nginx/proxy \--http-fastcgi-temp-path=/var/temp/nginx/fastcgi \--http-uwsgi-temp-path=/var/temp/nginx/uwsgi \--http-scgi-temp-path=/var/temp/nginx/scgi make &amp; make install cd /usr/local/nginx/sbin 目录 ./nginx 开启Nginx ./nginx -s stop 关闭Nginx ./nginx -s reload 重新加载Nginx配置文件 Nginx配置文件 nginx.conf是Nginx的主要配置文件,它的主要结构如下 worker_process表示工作进程的数量，一般设置为cpu的核数 worker_connections表示每个工作进程的最大连接数 server{} 定义了一个虚拟机,如果要添加一个虚拟机,则添加一个server{}即可。 listen监听的端口 server_name监听的域名 location{}配置匹配的URI root 指定URI查找的资源路径,为相对路径。 index 指定首页的名称,可以配置多个。 proxy_pass 指定反向代理转发的路径。 Nginx配置负载均衡 对upstream test{}中的2个web服务器配置了负载均衡,weight设置权值,权值越高的服务器承载的压力就越大。]]></content>
      <categories>
        <category>后端</category>
        <category>负载均衡</category>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Lucene实现全文检索]]></title>
    <url>%2F2016%2F06%2F19%2F2016-06-19-lucene%2F</url>
    <content type="text"><![CDATA[1. Lucene介绍 Lucene是apache下的一个开源的全文检索引擎工具包,但它不是一个完整的全文检索引擎,而是一个全文检索引擎的架构,提供了完整的查询引擎和索引引擎,部分文本分析引擎（英文与德文两种西方语言）。它为软件开发人员提供一个简单易用的工具包（类库）,以方便的在目标系统中实现全文检索的功能。 全文检索 全文检索首先将要查询的目标文档中的词提取出来，组成索引，通过查询索引达到搜索目标文档的目的。这种先建立索引，再对索引进行搜索的过程就叫全文检索（Full-text Search）。 Lucene的优点 索引文件格式独立于应用平台。Lucene定义了一套以8位字节为基础的索引文件格式，使得兼容系统或者不同平台的应用能够共享建立的索引文件。 在传统全文检索引擎的倒排索引的基础上，实现了分块索引，能够针对新的文件建立小文件索引，提升索引速度。然后通过与原有索引的合并，达到优化的目的。 优秀的面向对象的系统架构，使得对于Lucene扩展的学习难度降低，方便扩充新功能。 设计了独立于语言和文件格式的文本分析接口，索引器通过接受Token流完成索引文件的创立，用户扩展新的语言和文件格式，只需要实现文本分析的接口。 已经默认实现了一套强大的查询引擎，用户无需自己编写代码即可使系统可获得强大的查询能力，Lucene的查询实现中默认实现了布尔操作、模糊查询（Fuzzy Search[11]）、分组查询等等。 2. Lucene检索流程 创建索引流程:Gather Data(采集数据) –&gt; 构造文档对象 –&gt; 分词 –&gt; 创建索引并存入索引库 搜索流程:用户发起搜索请求 –&gt; 创建查询对象 –&gt; 从索引库中搜索 –&gt; 渲染并返回搜索结果 3. 索引的逻辑结构 一个非结构化的数据统一格式为document文档,一个document可以有多个field。 当用户搜索时,Lucene会从索引域中搜索,并找到对应的document,将document中的filed进行分词,然后根据分词创建索引。 4. 创建索引创建索引的流程 IndexWriter是核心对象,它可以完成创建索引、更新索引、删除索引等操作。 Directory负责对索引进行存储,它是一个抽象类,子类为FSDirectory(文件中存储)、RAMDirectory(内存中存储) 创建索引1234567891011121314151617181920212223242526272829303132333435363738394041424344@Testpublic void createIndex() &#123; // 获得原始数据 ItemsDao itemsDao = new ItemsDaoImpl(); List&lt;Items&gt; items = itemsDao.findAllItems(); // 创建Document List&lt;Document&gt; documents = new ArrayList&lt;Document&gt;(); Document document = null; // 遍历原始数据并封装到field for (Items item : items) &#123; document = new Document(); /** * 参数1:field的域名 参数2:要封装的数据 参数3:是否存储 */ Field name = new TextField("name", item.getName(), Store.YES); Field id = new StringField("id", item.getId().toString(), Store.YES); Field price = new TextField("price", item.getPrice().toString(), Store.YES); // 将field封装到document document.add(name); document.add(id); document.add(price); documents.add(document); &#125; // 创建一个标准分析器 Analyzer analyzer = new StandardAnalyzer(); // 索引库目录 Directory directory = FSDirectory.open(new File("D:\\Repository\\indexDatabase\\test")); // 创建IndexWriterConfig IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer); IndexWriter indexWriter = null; try &#123; // 创建IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); for (Document doc : documents) &#123; indexWriter.addDocument(doc); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 关闭IndexWriter indexWriter.close(); &#125;&#125; 5. 搜索索引使用QueryParse12345678910111213141516171819202122232425262728293031323334353637@Testpublic void searchIndex() throws ParseException &#123; // 创建标准分析器 Analyzer analyzer = new StandardAnalyzer(); // 创建QueryParser,c // 参数1:Field域名 参数2:分析器 QueryParser queryParser = new QueryParser("name", analyzer); // 创建Query 查找name为冰箱的 Query query = queryParser.parse("name:冰箱"); // 索引库目录 Directory directory = FSDirectory.open(new File("D:\\Repository\\indexDatabase\\test")); IndexReader indexReader = null; try &#123; // 创建IndexReader indexReader = DirectoryReader.open(directory); // 创建IndexSearcher IndexSearcher indexSearcher = new IndexSearcher(indexReader); // 执行搜索,并返回TopDoc对象 参数1:query对象 参数2:最大记录数 TopDocs topDocs = indexSearcher.search(query, 10); // 获得TopDocs中的记录对象 ScoreDoc[] scoreDocs = topDocs.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; // 获得doc的ID int docId = scoreDoc.doc; // 根据id查找到doc Document doc = indexSearcher.doc(docId); // 打印数据 System.out.println("商品名: " + doc.get("name")); System.out.println("价格: " + doc.get("price")); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 关闭reader indexReader.close(); &#125;&#125; 使用Query的子类TermQuery TermQuery使用搜索关键词进行查询。 1234567891011121314151617181920212223242526272829303132@Testpublic void searcher(Query query) &#123; // 索引库 Directory directory = FSDirectory.open(new File("D:\\Repository\\indexDatabase\\test")); // IndexReader IndexReader reader = null; try &#123; reader = DirectoryReader.open(directory); // IndexSearcher IndexSearcher searcher = new IndexSearcher(reader); TopDocs topDocs = searcher.search(query, 10); ScoreDoc[] scoreDocs = topDocs.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; int docId = scoreDoc.doc; Document document = searcher.doc(docId); System.out.println("商品id :" + document.get("id")); System.out.println("商品名称 :" + document.get("name")); System.out.println("商品价格 :" + document.get("price")); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; reader.close(); &#125;&#125;@Testpublic void termQuery() &#123; // 创建TermQuery 查询name中有冰箱的 等效于 name:冰箱 Query query = new TermQuery(new Term("name", "冰箱")); searcher(query);&#125; NumericRangeQuery 数字范围查询 123456789101112@Testpublic void numericRangeQuery()&#123; // 创建查询 // 第一个参数：域名 // 第二个参数：最小值 // 第三个参数：最大值 // 第四个参数：是否包含最小值 // 第五个参数：是否包含最大值 Query query = NumericRangeQuery.newLongRange("price", l00, 1000, true,true); // 2、 执行搜索 searcher(query);&#125; BooleanQuery 布尔查询,用于组合条件查询。 1234567891011121314151617 @Testpublic void booleanQuery() throws Exception &#123; BooleanQuery query = new BooleanQuery(); Query query1 = new TermQuery(new Term("id", "3")); Query query2 = NumericRangeQuery.newFloatRange("price", 10f, 200f, true, true); //MUST：查询条件必须满足，相当于AND //SHOULD:查询条件可选，相当于OR //MUST_NOT：查询条件不能满足，相当于NOT非 query.add(query1, Occur.MUST); query.add(query2, Occur.SHOULD); System.out.println(query); searcher(query);&#125; MUST和MUST表示“与”的关系，即“交集”。 MUST和MUST_NOT前者包含后者不包含。 MUST_NOT和MUST_NOT没有结果,没有意义。 SHOULD与MUST表示MUST，SHOULD失去意义。 SHOUlD与MUST_NOT相当于MUST与MUST_NOT。 SHOULD与SHOULD表示“或”的概念。 6. 删除索引12345678910111213@Test public void deleteIndex()&#123; // 指定索引库 Directory directory = FSDirectory.open(new File("D:\\Repository\\indexDatabase\\test")); // 创建IndexWriterConfig IndexWriterConfig indexWriterConfig = new IndexWriterConfig(new StandardAnalyzer()); // 创建IndexWriter IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); // 删除指定的索引 indexWriter.deleteDocuments(new Term("name","冰箱")); // 关闭indexWriter indexWriter.close(); &#125; 7. 修改索引在Lucene中修改索引即是替换索引,先将原来的索引删除,再保存新的索引。 12345678910111213141516@Testpublic void modifyIndex() &#123; // 指定索引库 Directory directory = FSDirectory.open(new File("D:\\Repository\\indexDatabase\\test")); // IndexWriterConfig IndexWriterConfig indexWriterConfig = new IndexWriterConfig(new StandardAnalyzer()); // IndexWriter IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); // 创建一个Document Document document = new Document(); Field name = new TextField("name", "比利海灵顿", Store.YES); document.add(name); // 修改索引 indexWriter.updateDocument(new Term("name", "冰箱"), document); indexWriter.close();&#125; 8. 相关度排序Lucene通过计算Term的权重,对查询关键字和索引文档的相关度进行打分,分越高的就排在越前面。 影响Term的权重有两个因素: Term Frequency:Term在文档中的出现频率,次数越多,则代表这个Term对该文档越重要,即权重越高。 Document Frequency:指多少文档包含这个Term的频率,频率越高,则代表这个Term越不重要,即权重越低。 手动设置权值在创建索引时设置权值 1234567891011121314151617 for (Items item : items) &#123; document = new Document(); /** * 参数1:field的域名 参数2:要封装的数据 参数3:是否存储 */ Field name = new TextField("name", item.getName(), Store.YES); Field id = new StringField("id", item.getId().toString(), Store.YES); Field price = new TextField("price", item.getPrice().toString(), Store.YES); // 给name域增加权值 name.setBoost(100f); // 将field封装到document document.add(name); document.add(id); document.add(price); documents.add(document);&#125; 在搜索时设置权值 1234567891011121314@Test public void setBoosts() throws Exception &#123; // 搜索的域名数组 String[] fields = &#123; "name", "price" &#125;; // 设置权值 Map&lt;String, Float&gt; boosts = new HashMap&lt;String, Float&gt;(); // 给name域设置权重 boosts.put("name", 100f); // 创建MultiFieldQueryParse MultiFieldQueryParser multiFieldQueryParser = new MultiFieldQueryParser(fields, new StandardAnalyzer(), boosts); // 创建Query Query query = multiFieldQueryParser.parse("冰箱"); searcher(query); &#125;]]></content>
      <categories>
        <category>后端</category>
        <category>全文检索</category>
        <category>Lucene</category>
      </categories>
      <tags>
        <tag>全文检索引擎</tag>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring MVC进阶应用]]></title>
    <url>%2F2016%2F06%2F17%2F2016-06-17-springmvc-02%2F</url>
    <content type="text"><![CDATA[Spring MVC 进阶应用参数绑定 客户端发送请求时传递的参数默认是键值对格式,Spring MVC通过参数绑定组件将请求的参数串进行类型转换。 Spring MVC使用Controller方法的形参接收请求传来的参数。 1. Spring MVC默认内置的形参类型 HttpServletRequest HttpServletResponse HttpSession Model 一个接口,可以将数据填充到request域对象中。 ModelMap Model接口的实现。 这5个对象可以直接通过形参注入并使用。 2. 简单数据类型绑定前端页面表单中的name或者URL中的key与Controller方法中的形参名称一致,即可完成绑定。如果名称不一致,可以使用@RequestParam注解指定名称完成绑定。例如： 12345URL: http://xxxxx?SID=1Controller:// 将SID的数据封装到形参id中。public String update (@RequestParam("SID") Integer id,Model model) RequestParam注解 value:参数名,即传入参数的名称。 required:默认为true,表示请求中要有相应的参数,否则报错:Status 400 - Required Integer parameter ‘XXXX’ is not present defaultValue:默认值,没有同名参数时,则使用默认值。 3. JavaBean对象类型绑定前端页面表单中的name或者URL中的key与JavaBean中的属性名进行匹配。如果参数名称一致,则将参数绑定到JavaBean中的属性上。如果JavaBean中包装了对象类型,传入参数的名称则需要按照 对象.属性 的格式编写,并且属性要与JavaBean中包装的对象中的属性名称一致。 4. 集合类型绑定数组类型绑定使用checkbox复选框时,会将参数绑定到一个数组中。例如: 1234567前端页面:&lt;input type="checkbox" name="cId" value="1"&gt;&lt;input type="checkbox" name="cId" value="2"&gt;Controller:// 使用一个数组接收复选框的参数 public String delete (Integer[] cId,Model model) List集合类型绑定Controller不能直接在形参中定义List,需要在包装类中定义List。页面中的编写格式: list名[index].属性名例如: 12345&lt;c:forEach items="$&#123;list&#125;" var="user" varStatus="status"&gt; &lt;tr&gt; &lt;td&gt;&lt;input type="text" name="list[$&#123;status.index&#125;].name" value="$&#123;user.name&#125;" /&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/c:forEach&gt; Map集合类型绑定Controller不能直接在形参中定义Map,需要在包装类中定义Map。页面中的编写格式:map名[‘key’]例如: 12345&lt;c:forEach items="$&#123;list&#125;" var="user" varStatus="status"&gt; &lt;tr&gt; &lt;td&gt;&lt;input type="text" name="map['name']" value="$&#123;user.name&#125;" /&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/c:forEach&gt; Converter当请求参数中含有日期类型时,需要自定义一个类型转换器转换成我们需要的日期格式。创建一个类实现Converter接口就可以自定义一个类型转换器。 1234567891011121314151617// Converter&lt;需要转换的数据的类型,转换的类型&gt;public class DateConverter implements Converter&lt;String, Date&gt; &#123; @Override public Date convert(String source) &#123; try&#123; // 转换日期格式 SimpleDateFormat format = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss"); return format.parse(source); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; // 出现异常,则返回null return null; &#125;&#125; 之后需要在处理器适配器中注册Converter 123456789101112&lt;!-- 注册注解方式的适配器与映射器 --&gt;&lt;mvc:annotation-driven conversion-service="conversionService" /&gt;&lt;bean id="conversionService" class="org.springframework.format.support.FormattingConversionServiceFactoryBean"&gt; &lt;property name="converters"&gt; &lt;list&gt; &lt;!-- 日期类型转换器 --&gt; &lt;bean class="com.sun.ssm.converter.DateConverter"&gt;&lt;/bean&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; ValidationSpring MVC本身没有实现表现层校验的部分，它本身支持JSR 303校验规范，而这个规范的官方参考实现是hibernate validator。 所以Spring MVC想要实现校验,需要导入Hibernate-validator jar包。 在配置文件中配置validator校验器 1234567891011121314151617181920212223&lt;!-- 配置校验器 --&gt;&lt;bean id="validator" class="org.springframework.validation.beanvalidation.LocalValidatorFactoryBean"&gt; &lt;!-- 校验器提供者 --&gt; &lt;property name="providerClass" value="org.hibernate.validator.HibernateValidator" /&gt; &lt;!-- 指定校验使用的资源文件，在文件中配置校验错误信息， 如果不指定，默认使用classpath下的ValidationMessages.properties --&gt; &lt;property name="validationMessageSource" ref="messageSource" /&gt;&lt;/bean&gt; &lt;!-- 校验错误信息的资源文件 --&gt;&lt;bean id="messageSource" class="org.springframework.context.support.ReloadableResourceBundleMessageSource"&gt; &lt;!-- 指定文件路径 --&gt; &lt;property name="basenames"&gt; &lt;list&gt; &lt;value&gt;classpath:validationMessages&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- 指定文件的编码 --&gt; &lt;property name="fileEncodings" value="utf8"/&gt; &lt;!-- 资源文件内容缓存的时间，单位秒 --&gt; &lt;property name="cacheSeconds" value="120" /&gt;&lt;/bean&gt; 在处理器适配器中注册Validator 12&lt;!-- 注册注解方式的适配器与映射器 --&gt;&lt;mvc:annotation-driven conversion-service="conversionService" validator="validator" /&gt; 在JavaBean中可以使用注解制定校验规则。 123456ValidationMessages.properties: user.username.size.error=用户名长度格式为1-10之间JavaBean: @Size(min=1,max=10,message="&#123;user.username.size.error&#125;") private String username; Controller中在需要检验的形参前使用@Validated注解,并在形参中注入一个BindingResult对象接收校验错误信息。@Validated注解和BindingResult顺序是固定的。(@Validated在前) 12345678910111213@RequestMapping("/updateUser")public String updateUser(@Validated User user,BindingResult bindingResult,Model model) &#123; // 判断是否有校验错误信息,如果有则代表校验未通过 if(bindingResult.hasErrors())&#123; // 获得校验错误信息集合 List&lt;ObjectError&gt; allErrors = bindingResult.getAllErrors(); // 将校验错误信息集合存入request域中 model.addAttribute("errors", allErrors); //检验未通过,重新跳转到修改页面 return "updateUser"; &#125;&#125; 分组校验 因为校验规则是在JavaBean中定义的,所以当同一个JavaBean需要被多个Controller使用时,可能根据需求需要不同的校验。 在这种情况下,可以使用一个标识作用的接口进行分组。 定义一个空接口作为分组的标识: 123public interface ValidationGroup &#123; &#125; 在JavaBean中使用分组: 12@Size(min=1,max=10,message="&#123;user.username.size.error&#125;",groups=&#123;ValidationGroup.class&#125;)private String username; 在Controller中使用分组校验: 1234567891011121314@RequestMapping("/updateUser")public String updateUser(@Validated(value=&#123;ValidationGroup.class&#125;) User user, BindingResult bindingResult,Model model) &#123; // 判断是否有校验错误信息,如果有则代表校验未通过 if(bindingResult.hasErrors())&#123; // 获得校验错误信息集合 List&lt;ObjectError&gt; allErrors = bindingResult.getAllErrors(); // 将校验错误信息集合存入request域中 model.addAttribute("errors", allErrors); //检验未通过,重新跳转到修改页面 return "updateUser"; &#125;&#125; 主要的验证注解如下: 注解 适用的数据类型 说明 @AssertFalse Boolean, boolean 验证注解的元素值是false @AssertTrue Boolean, boolean 验证注解的元素值是true @DecimalMax（value=x） BigDecimal, BigInteger, String, byte,short, int, long and the respective wrappers of the primitive types. Additionally supported by HV: any sub-type of Number andCharSequence. 验证注解的元素值小于等于@ DecimalMax指定的value值 @DecimalMin（value=x） BigDecimal, BigInteger, String, byte,short, int, long and the respective wrappers of the primitive types. Additionally supported by HV: any sub-type of Number andCharSequence. 验证注解的元素值小于等于@ DecimalMin指定的value值 @Digits(integer=整数位数, fraction=小数位数) BigDecimal, BigInteger, String, byte,short, int, long and the respective wrappers of the primitive types. Additionally supported by HV: any sub-type of Number andCharSequence. 验证注解的元素值的整数位数和小数位数上限 @Future java.util.Date, java.util.Calendar; Additionally supported by HV, if theJoda Time date/time API is on the class path: any implementations ofReadablePartial andReadableInstant. 验证注解的元素值（日期类型）比当前时间晚 @Max（value=x） BigDecimal, BigInteger, byte, short,int, long and the respective wrappers of the primitive types. Additionally supported by HV: any sub-type ofCharSequence (the numeric value represented by the character sequence is evaluated), any sub-type of Number. 验证注解的元素值小于等于@Max指定的value值 @Min（value=x） BigDecimal, BigInteger, byte, short,int, long and the respective wrappers of the primitive types. Additionally supported by HV: any sub-type of CharSequence (the numeric value represented by the char sequence is evaluated), any sub-type of Number. 验证注解的元素值大于等于@Min指定的value值 @NotNull Any type 验证注解的元素值不是null @Null Any type 验证注解的元素值是null @Past java.util.Date, java.util.Calendar; Additionally supported by HV, if theJoda Time date/time API is on the class path: any implementations ofReadablePartial andReadableInstant. 验证注解的元素值（日期类型）比当前时间早 @Pattern(regex=正则表达式, flag=) String. Additionally supported by HV: any sub-type of CharSequence. 验证注解的元素值与指定的正则表达式匹配 @Size(min=最小值, max=最大值) String, Collection, Map and arrays. Additionally supported by HV: any sub-type of CharSequence. 验证注解的元素值的在min和max（包含）指定区间之内，如字符长度、集合大小 @Valid Any non-primitive type（引用类型） 验证关联的对象，如账户对象里有一个订单对象，指定验证订单对象 @NotEmpty CharSequence,Collection, Map and Arrays 验证注解的元素值不为null且不为空（字符串长度不为0、集合大小不为0） @Range(min=最小值, max=最大值) CharSequence, Collection, Map and Arrays,BigDecimal, BigInteger, CharSequence, byte, short, int, long and the respective wrappers of the primitive types 验证注解的元素值在最小值和最大值之间 @NotBlank CharSequence 验证注解的元素值不为空（不为null、去除首位空格后长度为0），不同于@NotEmpty，@NotBlank只应用于字符串且在比较时会去除字符串的空格 @Length(min=下限, max=上限) CharSequence 验证注解的元素值长度在min和max区间内 @Email CharSequence 验证注解的元素值是Email，也可以通过正则表达式和flag指定自定义的email格式 数据回显Spring MVC默认支持数据回显。 Spring MVC会在Controller返回之前,自动将形参中的JavaBean放入Request域中,默认名称为类名首字母小写。只要前端页面动态获取数据的key与JavaBean名称一致,即可完成数据回显。如果名称不一致,也可以用@ModelAttribute注解指定形参放入Request域中的key名。 文件上传Spring MVC使用Multipart解析器完成文件上传,而Multipart解析器需要依赖以下2个jar包: commons-fileupload commons-io 配置Multipart解析器 1234&lt;bean id="multipartResolver" class="org.springframework.web.multipart.commons.CommonsMultipartResolver"&gt; &lt;!-- 限制上传文件的最大长度 --&gt; &lt;property name="maxUploadSize" value="10*1024*1024"/&gt;&lt;/bean&gt; 需要在Controller形参中注入一个MultipartFile对象接收文件,该对象名字要与传入的文件名相同。文件上传示例 1234567891011121314151617181920212223// 判断是否上传了文件if(file!= null)&#123; //原始文件名称 String fileName = file.getOriginalFilename(); //判断文件名是否为空 if(fileName != null &amp;&amp; !"".equals(fileName)) &#123; // 文件存放的路径 String path = "D:\\temps\\"; // 判断文件存放路径是否存在 File dirFile = new File(path); // 文件夹不存在,自动创建一个文件夹 if(!dirFile.exists()) &#123; dirFile.mkdirs(); &#125; // 使用UUID拼接一个唯一文件名 String newFileName = UUID.randomUUID()+fileName.substring(fileName.lastIndexOf(".")); //新的文件 File newFile = new File(path+newFileName); //把上传的文件保存成一个新的文件 file.transferTo(newFile); &#125; &#125; InterceptorSpring MVC的拦截器是针对处理器进行拦截的,当HandlerMapping查找处理器时,会被拦截器拦截。 可以通过实现HandlerIntercepter接口,自定义一个拦截器。 123456789101112131415161718192021222324252627282930public class MyHandlerInterceptor implements HandlerInterceptor&#123; // Handler执行前调用,返回值为true则放行,false则不放行。 // 应用场景:登录验证,权限授权 @Override public boolean preHandle(HttpServletRequest request,HttpServletResponse response, Object handler) throws Exception &#123; return false; &#125; // Handler开始执行,并且在返回ModelAndView之前调用 // 应用场景:操作ModelAndView对象 @Override public void postHandle(HttpServletRequest request,HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; &#125; // Handler执行完后调用 // 应用场景:异常处理、日志 @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; &#125;&#125; 配置拦截器 12345678&lt;!-- 配置全局拦截器 --&gt;&lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;!-- /**表示所有URL和子URL路径 --&gt; &lt;mvc:mapping path="/**" /&gt; &lt;bean class="com.sun.interceptor.MyHandlerInterceptor" /&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; 拦截器的执行顺序 preHandle()为正向顺序执行,postHandle()和afterCompletion()为逆向顺序执行。 如果第一个拦截器返回值为false,那它之后所有的拦截器都不会执行。 如果一个拦截器返回值为false,那么它的postHandle()和afterCompletion()都不会执行。 只要有一个拦截器的返回值为false,则所有拦截器的postHandle()都不会执行。 RESTful 一种软件架构风格，设计风格而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制。 实现REST风格需要在DispatcherServlet中将url映射模式改为 / 1234&lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 但是当映射模式为/时会把静态资源一起拦截,需要配置才能访问静态资源。 123&lt;mvc:resources location="/js/" mapping="/js/**"/&gt;&lt;mvc:resources location="/css/" mapping="/css/**"/&gt;&lt;mvc:resources location="/jquery/" mapping="/jquery/**"/&gt; Springmvc会把mapping映射到ResourceHttpRequestHandler，这样静态资源在经过DispatcherServlet转发时就可以找到对应的Handler了。 在Controller中实现RESTful 12@RequestMapping("/update/&#123;id&#125;")public String update(@PathVariable Integer id,Model model) @PathVariable:将URL中的模板变量映射到形参上,如果形参与模板变量名称不一致,可以使用value=” “设置为模板变量的名称完成映射。 {id}:模板变量。例如URL为 xxxx/update/1,模板变量update/{id}就可以将1封装到{id}中。 JSONSpring MVC默认使用MappingJacksonHttpMessageConverter对json数据进行转换，所以需要导入Jackson的jar包。 Spring MVC可以使用2个注解完成JSON数据的交互操作。 @RequestBody:如果请求参数传入的是json数据,使用RequestBody可以将json转换为java对象。 @ResponseBody:将返回值的java对象转换为json输出。]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring MVC原理及流程]]></title>
    <url>%2F2016%2F06%2F15%2F2016-06-15-springmvc-process%2F</url>
    <content type="text"><![CDATA[Spring MVC介绍 Spring MVC属于SpringFrameWork的后续产品，已经融合在Spring Web Flow里面。 通过策略接口，Spring MVC是高度可配置的，而且包含多种视图技术，例如 JavaServer Pages（JSP）、Velocity、Tiles、iText和POI。 使用 Spring 可插入的MVC架构，从而在使用Spring进行WEB开发时，可以选择使用Spring的SpringMVC框架或集成其他MVC开发框架，如Struts1，Struts2等。 Spring MVC是一个完全基于MVC模式(Model、View、Controller)的框架。 Spring MVC工作原理 Spring MVC工作流程: 用户发起请求。 DispatcherServlet接收到请求,并去调用HandlerMapping查找处理器。 HandlerMapping根据请求的URL查找对应的处理器,并返回给前端控制器DispatcherServlet。 DispatcherServlet调用HandlerAdapter执行处理器。 HandlerAdapter先判断处理器的类型进行适配,然后执行处理器。 处理器进行数据和业务请求的处理,将ModelAndView对象返回给HandlerAdapter。 HandlerAdapter将ModelAndView对象返回到前端控制器DispatcherServlet。 DispatcherServlet调用ViewResolver解析逻辑视图ModelAndView。 ViewResolver通过逻辑视图的名称查找对应的视图对象,并返回给前端控制器。 DispatcherServlet调用View渲染视图到前端。 响应处理的结果。 Spring MVC主要由以下几个部分组成: 前端控制器(DispatcherServlet):接收请求并响应到前端,并且负责各组件职责的分派。 处理器(Handler):处理数据与业务请求,Handler需要符合适配器的规则。 处理器映射器(HandlerMapping):根据每个请求的URL查找对应的处理器Handler。 处理器适配器(HandlerAdapter):根据类型适配每个处理器Handler并执行。 视图解析器(ViewResolver):根据逻辑视图的名称将逻辑视图解析为视图对象。 视图(View):在Spring MVC中,View是一个接口,通过不同的实现类支持不同的类型。(jsp、freemarker等) 注： DispatcherServlet在创建时会默认从DispatcherServlet.properties中加载springmvc的各个组件配置。 如果在servletname-servlet.xml(Spring MVC配置文件)中配置了各个组件的配置则会优先使用servletname-servlet.xml中的配置。]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis学习笔记-02]]></title>
    <url>%2F2016%2F06%2F12%2F2016-06-12-mybatis-study%20notes02%2F</url>
    <content type="text"><![CDATA[关联映射1. 业务关系分析在进行多表关联查询之前,需要对表结构的关联关系与业务关系进行分析与理解。下图以用户,订单,明细,商品表为例分析: 用户表与订单表 一个用户可以创建多个订单 (一对多) 一个订单只能由一个用户创建(一对一) 订单表与明细表 一个订单可以含有多个订单明细(一对多) 一个订单明细只能对应一个订单(一对一) 明细表与商品表 一个订单明细只能对应一个商品(一对一) 一个商品可以被包含在多个订单明细中(一对多) 订单表与商品表 (间接关系) 一个订单可以拥有多个订单明细,而一个订单明细对应一个商品,即一个订单可以拥有多个商品。 order -&gt; orderdetail -&gt; items 一对多 一个商品可以被包含在多个订单明细中,而一个订单明细对应一个订单,即一个商品可以被包含在多个订单中。 item -&gt; orderdetail -&gt; order 一对多 订单与商品是多对多关系。 用户表与商品表 (间接关系) 一个用户可以创建多个订单,一个订单可以对应多个订单明细,一个订单明细对应一个商品,即一个用户可以购买多个商品。 user -&gt; order -&gt; orderdetail -&gt; item 一对多 一个商品可以被包含在多个订单明细中,一个订单明细对应一个订单,一个订单对应一个用户,即一个商品可以对应多个用户。 item -&gt; orderdetail -&gt; order -&gt; user 一对多 用户与商品是多对多关系。 2. 一对一关联查询 在多表查询时,单表的JavaBean不能满足结果集的映射,所以可以使用继承的方法来扩展JavaBean。 12345678910111213141516package com.sun.test.entity;// Orders的扩展类public class OrdersExtends extends Orders &#123; private User user; public User getUser() &#123; return user; &#125; public void setUser(User user) &#123; this.user = user; &#125;&#125; 上面的做法与在Orders类中添加一个User类型的属性作用相同。 映射文件配置 12345678910111213141516171819202122232425262728293031323334353637383940&lt;mapper namespace="com.sun.test.mapper.OrdersMapper"&gt; &lt;!-- selectOrdersAndUserRetMap --&gt; &lt;!-- resultMap可以使用extends字段继承一个type一致的resultMap --&gt; &lt;resultMap type="ordersExtends" id="selectOrdersAndUserRetMap"&gt; &lt;!-- 主键 --&gt; &lt;id column="id" property="id" /&gt; &lt;!-- 其他字段 --&gt; &lt;result column="user_id" property="userId"/&gt; &lt;result column="number" property="number"/&gt; &lt;result column="createtime" property="createtime"/&gt; &lt;result column="note" property="note"/&gt; &lt;!-- association:一对一映射 property:关联对象在JavaBean中封装的属性名 javaType:关联对象的Java类型 --&gt; &lt;association property="user" javaType="com.sun.test.entity.User"&gt; &lt;id column="user_id" property="id"/&gt; &lt;result column="username" property="username"/&gt; &lt;result column="address" property="address"/&gt; &lt;/association&gt; &lt;/resultMap&gt; &lt;!-- 查询订单与用户信息 --&gt; &lt;select id="selectOrdersAndUser" resultMap="selectOrdersAndUserRetMap"&gt; SELECT orders.id, orders.user_id, orders.number, orders.createtime, orders.note, `user`.username, `user`.address FROM orders, `user` WHERE orders.user_id = `user`.id &lt;/select&gt; &lt;/mapper&gt; 3. 一对多关联查询 一对多查询与一对一查询类似,我们要查询商品以及订单明细则需要在商品类中扩展一个List集合。 123456789101112131415public class Items &#123; private Integer id; private String name; private Float price; private String pic; private Date createtime; private String detail; // 一对多 一个商品对应多个订单明细 private List&lt;Orderdetail&gt; orderdetails; 映射文件配置 123456789101112131415161718192021222324252627282930313233&lt;mapper namespace="com.sun.test.mapper.ItemsMapper"&gt; &lt;!-- (一对多)查询商品以及对应的订单明细 --&gt; &lt;select id="selectItemsAndOrderdetail" resultMap="selectItemsAndOrderdetailRetMap"&gt; SELECT items.id, items.`name`, items.price, orderdetail.id orderdetail_id, orderdetail.items_num FROM items, orderdetail WHERE items.id = orderdetail.items_id; &lt;/select&gt; &lt;!-- selectItemsAndOrderdetailRetMap --&gt; &lt;resultMap type="items" id="selectItemsAndOrderdetailRetMap"&gt; &lt;id column="id" property="id"/&gt; &lt;result column="name" property="name"/&gt; &lt;result column="price" property="price"/&gt; &lt;!-- 一对多关联 collection:一对多映射 ofType:这个集合参数的类型 --&gt; &lt;collection property="orderdetails" ofType="orderdetail"&gt; &lt;id column="orderdetail_id" property="id"/&gt; &lt;result column="items_num" property="itemsNum"/&gt; &lt;/collection&gt; &lt;/resultMap&gt;&lt;/mapper&gt; 4. 多对多关联查询 查询商品以及对应的用户信息。 在商品类中添加一个订单明细集合(一个商品对应多个订单明细) 在订单明细类中添加一个订单属性(一个订单明细对应一个订单) 在订单类中添加一个用户属性(一个订单对应一个用户) 12345678// 一对多 一个商品对应多个订单明细private List&lt;Orderdetail&gt; orderdetails;// 一对一 一个订单明细对应一个订单private Orders orders;//一对一 一个订单对应一个用户private User user; 映射文件配置 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;!-- 查询商品以及对应的用户 --&gt;&lt;select id="selectItemsAndUser" resultMap="selectItemsAndUserRetMap"&gt; SELECT items.id, items.`name`, items.price, `user`.id user_id, `user`.username, `user`.address, orderdetail.id orderdetail_id, orders.id orders_id FROM items, orderdetail, orders, `user` WHERE items.id = orderdetail.items_id AND orderdetail.orders_id = orders.id AND orders.user_id = `user`.id&lt;/select&gt;&lt;!-- selectItemsAndUserRetMap --&gt;&lt;resultMap type="items" id="selectItemsAndUserRetMap"&gt; &lt;id column="id" property="id" /&gt; &lt;result column="name" property="name"/&gt; &lt;result column="price" property="price"/&gt; &lt;!-- 一个商品对应多个订单明细 --&gt; &lt;collection property="orderdetails" ofType="orderdetail"&gt; &lt;id column="orderdetail_id" property="id"/&gt; &lt;!-- 一个订单明细对应一个订单 --&gt; &lt;association property="orders" javaType="orders"&gt; &lt;id column="orders_id" property="id"/&gt; &lt;!-- 一个订单对应一个用户 --&gt; &lt;association property="user" javaType="user"&gt; &lt;id column="user_id" property="id"/&gt; &lt;result column="username" property="username"/&gt; &lt;result column="address" property="address"/&gt; &lt;/association&gt; &lt;/association&gt; &lt;/collection&gt;&lt;/resultMap&gt; 延迟加载resultMap中的association和collection具有延迟加载的功能。 延迟加载: 先加载主信息,在需要的时候加载关联信息,延迟加载即叫按需加载,也叫懒加载。 1. 设置延迟加载 MyBatis 默认是关闭延迟加载的,需要在配置文件中的标签中手动开启。 settings description 验证值组 默认值 lazyLoadingEnabled 全局性设置懒加载。 true false true aggressiveLazyLoading 积极性的懒加载,false则是按需加载 true false true 123456&lt;settings&gt; &lt;!-- 开启延迟加载 ,默认值true--&gt; &lt;setting name="lazyLoadingEnabled" value="true"/&gt; &lt;!-- 设置积极的懒加载,默认值true --&gt; &lt;setting name="aggressiveLazyLoading" value="false"/&gt;&lt;/settings&gt; 2. 在association中使用延迟加载1234567891011121314151617181920&lt;!-- 查询订单 并延迟加载用户信息 --&gt; &lt;select id="selectOrdersAndLazyLoadingUser" resultMap="selectOrdersAndLazyLoadingUserRetMap"&gt; SELECT * FROM orders &lt;/select&gt; &lt;!-- selectOrdersAndLazyLoadingUserRetMap --&gt; &lt;resultMap type="orders" id="selectOrdersAndLazyLoadingUserRetMap"&gt; &lt;id column="id" property="id"/&gt; &lt;result column="user_id" property="userId"/&gt; &lt;result column="number" property="number"/&gt; &lt;result column="createtime" property="createtime"/&gt; &lt;result column="note" property="note"/&gt; &lt;!-- 一对一关联并延迟加载用户信息 select:指定延迟加载执行的statementId (如果这个statement不在当前namespace中那么则需要使用namespace.statementid) column:需要关联查询的列，如果需要传入多个则需要使用 &#123;user_id=id,...&#125;的格式 --&gt; &lt;association property="user" column="user_id" select="com.sun.test.mapper.UserMapper.findById"&gt;&lt;/association&gt; &lt;/resultMap&gt; 查询缓存1. 一级缓存 一级缓存是SqlSession级别的缓存。在操作数据库时需要构造 sqlSession对象，在对象中有一个数据结构（HashMap）用于存储缓存数据。不同的sqlSession之间的缓存数据区域（HashMap）是互相不影响的。 当第一次查询对象1时,将先会去一级缓存中查找是否有对象1的数据信息,如果没有则从数据库中查询到数据信息并构造一个key存储到一级缓存(HashMap)中。当第二次查询对象1的时候，则可以凭借key在一级缓存中找到数据信息,而不用再去访问数据库。如果SqlSession进行了事务提交(commit),则会刷新(清空)缓存,目的是为了让缓存存储最新的数据,防止脏读。MyBatis默认开启一级缓存。 2. 二级缓存 二级缓存是namespace(mapper)级别的缓存，多个SqlSession去操作同一个namespace中的mapper的sql语句，多个SqlSession可以共用二级缓存，二级缓存是跨SqlSession的。二级缓存存储数据不一定是在内存中,所以需要给缓存的对象实现序列化接口。二级缓存需要手动设置开启。 开启二级缓存12345678910 在settings中设置&lt;!-- 开启二级缓存 --&gt;&lt;setting name="cacheEnabled" value="true"/&gt;在mapper映射文件中设置&lt;!-- 使用第三方(ehcache)二级缓存 flashInterval:设置定时刷新的间隔时间,单位是毫秒。 注:使用第三方缓存框架需要先导入jar包和整合包--&gt; &lt;cache type="org.mybatis.caches.ehcache.EhcacheCache" flashInterval="60000" /&gt; MyBatis与Spring整合 由Spring维护管理数据源、事务、SqlSessionFactory、mapper。MyBatis的配置文件只需要配置settings、别名等即可。 123456789101112131415Spring中的配置&lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;!-- mybatis的配置文件路径 --&gt; &lt;property name="configLocation" value="sqlMapConfig.xml"&gt;&lt;/property&gt; &lt;!-- 注入数据源 --&gt; &lt;property name="dataSource" ref="dataSource"&gt;&lt;/property&gt;&lt;/bean&gt;批量生成Mapper(生成的Mapper名字默认为首字母)&lt;bean class="org.mybatis.spring.mapper.MapperScannerConfigurer"&gt; &lt;!-- 指定需要扫描的mapper配置的包名 --&gt; &lt;property name="basePackage" value="com.sun.test.mapper"&gt;&lt;/property&gt; &lt;!-- 注入SqlSessionFactory --&gt; &lt;property name="sqlSessionFactoryBeanName" value="sqlSessionFactory"&gt;&lt;/property&gt;&lt;/bean&gt;]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>MyBatis</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis-XML配置与映射文件]]></title>
    <url>%2F2016%2F06%2F11%2F2016-06-11-MyBatis-XML%E9%85%8D%E7%BD%AE%E4%B8%8E%E6%98%A0%E5%B0%84%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[全局XML配置文件MyBatis配置文件的结构如下： configuration 配置 properties 属性 settings 参数设置 typeAliases 类型别名 typeHandlers 类型处理器 objectFactory 对象工厂 plugins 插件 environments 环境集合 environment 环境 transactionManager 事务管理器 dataSource 数据源 databaseIdProvider 数据库厂商标识 mappers 映射器 常用标签配置1. propertiesproperties标签可以读取properties配置文件,并引入到 MyBatis 的配置文件中。 可以通过子标签定义key/value。 例如： 1234&lt;properties resource="com/sun/test/dataSource.properties"&gt; &lt;property name="username" value="dev_user"/&gt; &lt;property name="password" value="F2Fa3!33TYyg"/&gt;&lt;/properties&gt; 其中的属性就可以在整个配置文件中使用key来替换需要动态配置的属性值。 例如： 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="$&#123;driver&#125;"/&gt; &lt;property name="url" value="$&#123;url&#125;"/&gt; &lt;property name="username" value="$&#123;username&#125;"/&gt; &lt;property name="password" value="$&#123;password&#125;"/&gt;&lt;/dataSource&gt;``` properties的加载顺序 - 最先加载properties中的子标签所定义的属性。 - 然后会加载resource标签所引入的properties配置文件中的属性,并覆盖已加载的同名属性。 - 最后加载作为方法参数(parameterType中的key/value)传递的属性，并覆盖已加载的同名属性。总结： - 通过方法参数传递的属性具有最高优先级。 - resource引用的配置文件中的属性次之。 - 最低优先级的是 properties 子标签中指定的属性。#### 2. **settings** settings会影响MyBatis的运行行为。 一个配置完整的 settings 的示例如下：``` stylus&lt;settings&gt; &lt;setting name="cacheEnabled" value="true"/&gt; &lt;setting name="lazyLoadingEnabled" value="true"/&gt; &lt;setting name="multipleResultSetsEnabled" value="true"/&gt; &lt;setting name="useColumnLabel" value="true"/&gt; &lt;setting name="useGeneratedKeys" value="false"/&gt; &lt;setting name="autoMappingBehavior" value="PARTIAL"/&gt; &lt;setting name="autoMappingUnknownColumnBehavior" value="WARNING"/&gt; &lt;setting name="defaultExecutorType" value="SIMPLE"/&gt; &lt;setting name="defaultStatementTimeout" value="25"/&gt; &lt;setting name="defaultFetchSize" value="100"/&gt; &lt;setting name="safeRowBoundsEnabled" value="false"/&gt; &lt;setting name="mapUnderscoreToCamelCase" value="false"/&gt; &lt;setting name="localCacheScope" value="SESSION"/&gt; &lt;setting name="jdbcTypeForNull" value="OTHER"/&gt; &lt;setting name="lazyLoadTriggerMethods" value="equals,clone,hashCode,toString"/&gt;&lt;/settings&gt; 3. typeAliases类型别名,可以对JavaBean设置一个别名,用来减少完全限定名的冗余。例如： 12345678910&lt;!-- 类型别名 --&gt; &lt;typeAliases&gt; &lt;!-- 设置单个别名 --&gt; &lt;typeAlias type="com.sun.test.entity.User" alias="user"/&gt; &lt;!-- package指定一个包名,会在包下搜索需要的JavaBean 如果没有注解,则默认设置为JavaBean的首字母小写的非限定类名作为它的别名。 如果有@Alias注解,则使用其注解值作为它的别名。 --&gt; &lt;package name="com.sun.test.entity"/&gt; &lt;/typeAliases&gt; MyBatis默认支持的类型别名如下,它们都是大小写不敏感的，需要注意的是由基本类型名称重复导致的特殊处理。 别名 映射的类型 _byte byte _long long _short short _int int _integer int _double double _float float _boolean boolean string String byte Byte long Long short Short int Integer integer Integer double Double float Float boolean Boolean date Date decimal BigDecimal bigdecimal BigDecimal object Object map Map hashmap HashMap list List arraylist ArrayList collection Collection iterator Iterator 4. Mappers映射器(Mappers)用于引入Mapper映射文件。 使用相对路径的方式引入 1234&lt;!-- 相对路径 --&gt;&lt;mappers&gt; &lt;mapper resource="com/sun/test/mapper/UserMapper.xml"/&gt;&lt;/mappers&gt; 使用绝对路径的方式引入 1234&lt;!-- 绝对路径s --&gt;&lt;mappers&gt; &lt;mapper url="file:///D:\sun\mappers\UserMapper.xml"/&gt;&lt;/mappers&gt; 使用Mapper接口的全限定类名(需要mapper接口与mapper映射文件名称相同,并且在同一个包下) 1234&lt;!-- Mapper接口的全限定类名 --&gt;&lt;mappers&gt; &lt;mapper class="com.sun.test.mapper.UserMapper"/&gt;&lt;/mappers&gt; 扫描指定包下的所有映射文件(需要mapper接口与mapper映射文件名称相同,并且在同一个包下) 1234&lt;!-- 扫描指定包下的所有映射文件 --&gt;&lt;mappers&gt; &lt;package name="com.sun.test.mapper"/&gt;&lt;/mappers&gt; Mapper映射文件 cache 缓存配置 cache-ref 其他namespace缓存配置的引用 resultType 结果集映射的类型 resultMap 结果集映射Map parameterType 输入映射类型 sql 可被引用的复用sql语句块 insert 映射插入语句 update 映射更新语句 select 映射查询语句 delete 映射删除语句 resultType注意事项: 如果使用resultType进行结果集映射,则需要查询出的列名与映射的JavaBean属性名称一致。 SQL语句中列名如果有别名,则列名为别名的名称。 如果查询的列名和JavaBean所映射的属性名全不一致,则映射的JavaBean对象为null。 如果查询的列名和JavaBean所映射的属性名少数不一致,则映射的JavaBean对象不为null,但只有名称一致的属性有值。 例如： 1234 &lt;!-- 根据id查询User --&gt;&lt;select id="findById" parameterType="int" resultType="com.sun.test.entity.User"&gt; select * from user where id = #&#123;id&#125;&lt;/select&gt; resultMap注意事项: 使用resultMap进行结果集映射,不需要列名与映射的JavaBean属性名称一致。 使用resultMap进行结果集映射,需要先定义一个resultMap。 例如: 12345678910&lt;!-- 定义一个resultMap --&gt;&lt;resultMap type="com.sun.test.entity.User" id="userResultMap"&gt; &lt;!-- 主键 --&gt; &lt;id column="id" property="id"/&gt; &lt;!-- 其他字段 --&gt; &lt;result column="username" property="username"/&gt;&lt;/resultMap&gt;&lt;select id="findByIdWithRetMap" parameterType="int" resultMap="userResultMap"&gt; select id,username from user where id = #&#123;id&#125;&lt;/select&gt; 动态SQL1. SQL片段sql片段可以存储动态的sql语句,提高复用性。 1234567891011&lt;!-- SQL片段,可以存储动态的SQL语句 --&gt; &lt;sql id="sql"&gt; &lt;!-- 判断用户名不为空 --&gt; &lt;if test="username != null and username != ''"&gt; and username = #&#123;usernmae&#125; &lt;/if&gt; &lt;!-- 判断性别不为空 --&gt; &lt;if test="sex != null and sex != ''"&gt; and sex = #&#123;sex&#125; &lt;/if&gt; &lt;/sql&gt; 2. if标签1234567&lt;select id="findByIdOrLikeName" resultType="com.sun.test.entity.User"&gt; select * from user where id = #&#123;id&#125; &lt;!-- 如果if返回为true 则会加上这条语句 --&gt; &lt;if test="username != null and username != ''"&gt; and username like '%$&#123;username&#125;%' &lt;/if&gt;&lt;/select&gt; 3. where标签1234567891011&lt;select id="findAllOrLikeName" resultType="com.sun.test.entity.User"&gt; select * from user &lt;!-- where标签只有在if为true的情况下才会添加where语句 并且会将第一条语句的and去掉。 --&gt; &lt;where&gt; &lt;if test="username != null and username != ''"&gt; and username like '%$&#123;username&#125;%' &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; 4. foreach标签foreach标签可以对一个集合进行遍历,通常是在构建一个in条件语句的时候使用。 1234567891011121314151617&lt;select id="selectUserIn" resultType="com.sun.test.entity.User" parameterType="list"&gt; select * from user &lt;where&gt; &lt;if test="list != null and list.size &gt; 0"&gt; &lt;!-- collection：集合参数的名称。 item：遍历集合时的值 open：遍历开始时需要拼接的sql语句 close：遍历结束时需要拼接的sql语句 separator：遍历过程中需要拼接的字符串 --&gt; &lt;foreach collection="list" item="id" open="and id in(" close=")" separator=","&gt; #&#123;id&#125; &lt;/foreach&gt; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; 5. bind标签bind标签可以从OGNL表达式中创建一个变量并将其绑定到上下文。 12345&lt;select id="selectUserLikeName" resultType="com.sun.test.entity.User"&gt; &lt;bind name="username" value="'%' + user.getUsername() + '%'" /&gt; select * from user where username like #&#123;username&#125;&lt;/select&gt;]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>MyBatis</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis学习笔记-01]]></title>
    <url>%2F2016%2F06%2F10%2F2016-06-10-mybatis-study%20notes01%2F</url>
    <content type="text"><![CDATA[1.什么是MyBatis? MyBatis 本是apache的一个开源项目iBatis, 2010年这个项目由apache software foundation 迁移到了google code，并且改名为MyBatis 。2013年11月迁移到Github。 iBATIS一词来源于“internet”和“abatis”的组合，是一个基于Java的持久层框架。iBATIS提供的持久层框架包括SQL Maps和Data Access Objects（DAO） MyBatis 是支持定制化 SQL、存储过程以及高级映射的优秀的持久层框架。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。 MyBatis 可以对配置和原生Map使用简单的 XML 或注解，将接口和 Java 的 POJOs(Plain Old Java Objects,普通的 Java对象)映射成数据库中的记录。 2.MyBatis框架原理MyBatis的功能架构分为三层 API接口层：提供给外部使用的接口API，开发人员通过这些本地API来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理。 数据处理层：负责具体的SQL查找、SQL解析、SQL执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作。 基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑。 MyBatis框架架构流程 MyBatis应用程序通过读取XML配置文件,构造出SqlSessionFactory(SQL会话工厂)。 SqlSessionFactory再根据配置,构造一个SqlSession(SQL会话)。MyBatis通过SqlSession完成数据库操作。 SqlSession本身不能直接操作数据库,它是通过底层Executor接口操作数据库的。Executor有2个实现类,默认使用缓存执行器。 Executor将处理的SQL信息封装到一个底层对象 MappedStatement 中。该对象包含了SQL语句、输入参数信息、输出结果集信息。 输入参数和输出结果集的映射类型为java的简单类型、HashMap集合类型、POJO对象类型。 3.MyBatis的优点与缺点优点 学习成本低、简单易学。 灵活性较高,可以直接对SQL进行性能优化。 SQL与业务逻辑代码低耦合,提高了维护性。 可以编写动态SQL语句。 缺点 SQL语句依赖数据库,即数据库发生变更时,需要重新写SQL语句, 移植性较差。 SQL语句繁多,工作量大。 二级缓存机制不佳。 4.MyBatis入门我们使用MyBatis完成一个根据id查询用户的需求。 创建一个JavaBean 1234567public class User &#123; private int id ; private String username ;// 姓名 private String sex ;// 性别 private Date birthday ;// 生日 private String address ;// 地址 为User类配置一个映射文件 1234567891011121314&lt;? xml version ="1.0" encoding= "UTF-8" ?&gt;&lt;! DOCTYPE mapperPUBLIC "-//mybatis.org//DTD Mapper 3.0//EN""http://mybatis.org/dtd/mybatis-3-mapper.dtd" &gt;&lt; mapper namespace ="com.sun.test.mapper.UserMapper" &gt; &lt;!-- 根据id查询User 在命名空间 com.sun.test.mapper.UserMapper中定义了一个id为findById的映射语句。 当我们调用时可以使用 namespace.id 来进行调用。 即：com.sun.test.mapper.UserMapper.findById --&gt; &lt;select id= "findById" parameterType ="int" resultType ="com.sun.test.entity.User" &gt; select * from user where id = #&#123;id&#125; &lt;/select &gt;&lt;/ mapper&gt; 命名空间(Namespace): 必须且非常重要,最好使用对应mapper接口的全限定名(比如:com.sun.test.mapper.UserMapper )。 parameterType: 输入参数的类型,可以使用类型的全限定名或者别名。 resultType: 返回参数的类型,如果是集合类型,则应使用集合内可以包含的类型,而不能是集合本身,可以使用类型的全限定名或者别名。 创建一个全局配置文件,并配置数据源、事务管理器、映射文件等基本配置。 123456789101112131415161718192021222324&lt;? xml version =&quot;1.0&quot; encoding= &quot;UTF-8&quot; ?&gt;&lt;! DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot; &gt;&lt; configuration&gt; &lt;environments default= &quot;development&quot;&gt; &lt; environment id= &quot;development&quot; &gt; &lt;!-- 配置事务管理器 --&gt; &lt; transactionManager type =&quot;JDBC&quot; /&gt; &lt;!-- 配置数据源 --&gt; &lt; dataSource type =&quot;POOLED&quot; &gt; &lt; property name =&quot;driver&quot; value =&quot;com.mysql.jdbc.Driver&quot; /&gt; &lt; property name =&quot;url&quot; value =&quot;jdbc:mysql:///mybatis&quot; /&gt; &lt; property name =&quot;username&quot; value =&quot;root&quot; /&gt; &lt; property name =&quot;password&quot; value =&quot;root&quot; /&gt; &lt;/ dataSource&gt; &lt;/ environment&gt; &lt;/environments &gt; &lt;!-- 配置映射文件 --&gt; &lt;mappers &gt; &lt;!-- 使用package可以查找到所有 mapper接口与 mapper映射文件但需要同名并且放在同一包下 --&gt; &lt; package name =&quot;com.sun.test.mapper&quot; /&gt; &lt;/mappers &gt;&lt;/ configuration&gt; 测试 12345678910111213141516171819@Test public void test01() throws IOException &#123; // 获得配置文件的资源流 InputStream inputStream = Resources.getResourceAsStream( "sqlMapConfig.xml"); // 加载配置文件,构造SqlSessionFactory SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream ); // 使用SqlSessionFactory构造一个SqlSession SqlSession sqlSession = sqlSessionFactory .openSession(); /** * 执行statement * 参数1：statement的id 建议使用 namespace.statementId * 参数2：输入参数的值,它的类型要和映射文件中的parameterType类型一致 */ User user = sqlSession .selectOne("com.sun.test.mapper.UserMapper.findById" , 1); // 输出结果 System. out .println(user ); // 关闭SqlSession sqlSession .close(); &#125; Resources：MyBatis中的一个工具类,可以从classpath或其他位置加载资源文件。 SqlSessionFactoryBuilder： 这个类可以被实例化、使用和丢弃,一旦创建了 SqlSessionFactory,就不再需要它了。因此 SqlSessionFactoryBuilder 实例的最佳范围是方法范围（也就是局部方法变量）。 SqlSessionFactory：一旦被创建就应该在应用的运行期间一直存在,所以它应该是单例的,且在应用运行期间不要多次创建。 SqlSession：用于操作数据库的对象,每个线程都应该有一个SqlSession实例,它是多例的,并且它是线程不安全的,不能放在全局变量上。 5.自增主键如果你的数据库支持自动生成主键的字段,那么你可以设置 useGeneratedKeys=”true”,再把keyProperty设置到目标属性上即可。 1234&lt;insert id=&quot;insertUser&quot; useGeneratedKeys=&quot;true&quot; parameterType =&quot;com.sun.test.entity.User&quot; keyProperty=&quot;id&quot;&gt; insert into user(username,birthday,sex,address) values(#&#123;username&#125;,#&#123;birthday&#125;,#&#123;sex&#125;,#&#123;address&#125;)&lt;/insert&gt; 如果不支持自动生成主键则可以使用另外一种方法生成主键 1234567891011121314&lt; insert id= &quot;insertUser&quot; parameterType =&quot;com.sun.test.entity.User&quot; &gt; &lt;!-- 返回自增主键 keyProperty:生成主键的属性 resultType:生成主键的类型 order:查询主键SQL的执行顺序,相对于insert,如果设置为 BEFORE，那么它会首先选择主键，设置 keyProperty 然后执行插入语句。 如果设置为 AFTER，那么会先执行插入语句。 LAST_INSERT_ID():MySQL的函数,获取最后插入的主键 --&gt; &lt; selectKey keyProperty =&quot;id&quot; resultType =&quot;int&quot; order= &quot;AFTER&quot;&gt; select LAST_INSERT_ID() &lt;/ selectKey&gt; insert into user(username,birthday,sex,address) values(#&#123;username&#125;,#&#123;birthday&#125;,#&#123;sex&#125;,#&#123;address&#125;) &lt;/insert &gt; #{} 与 ${}的区别 #{}可以表示为占位符?,#{id}里面的id表示输入参数的参数名称,如果是简单类型,则名称可以任意填写。 ${}表示字符串替换,${value},value表示输入参数的参数名称,如果是简单类型,则名称必须为value。 ${}是不安全的,会导致潜在的sql注入攻击,但如果要动态传入order by的列名则必须使用${} 即：order by ${columnName}。 6.使用Mapper构建Dao使用Mapper构建Dao则只需要开发接口即可。 规范 mapper接口的全限定名称要和mapper映射文件的namespace一致。 mapper接口的方法名称要和mapper映射文件的MappedStatement的id一致。 mapper接口的方法参数类型与返回值类型要和mapper映射文件的MappedStatement的parameterType与resultType一致。 例:]]></content>
      <categories>
        <category>后端</category>
        <category>Java</category>
        <category>MyBatis</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>MyBatis</tag>
      </tags>
  </entry>
</search>